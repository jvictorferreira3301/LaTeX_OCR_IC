\section{To Learn in This Chapter}
The skills we aim to develop in this chapter are:
\begin{itemize}
	\item Use finite-duration windows to model the extraction of a segment from a signal with a potentially infinite duration
	\item Apply windows to better control resolution and leakage in spectral analysis
	\item Interpret the power spectral density (PSD) %frequency domain
	%\item How to relate the digital frequency $\dw$ in radians and the analog frequency $\aw$ in radians/seconds.
	\item Distinguish PSD, ESD and the mean-square (power) spectrum (MS spectrum), knowing the cases in which one should be adopted
	\item Estimate PSD, ESD and the mean-square (power) spectrum (MS spectrum) using the FFT, and be familiar with concepts
	such as the periodogram and Welch's method
	\item Recognize that the periodogram variance does not decrease with the number of samples, and understand how Welch's method decreases the variance
	\item Learn how the noise floor decreases with an increased number $\nfft$ of FFT points
	\item Estimate the PSD from the autocorrelation
	\item Distinguish the unilateral and bilateral representations
	\item Perform spectrum analysis with a computer
	\item Be able to mathematically model the spectrum leakage when windowing a signal via the circular convolution of the original
	spectrum with the window's spectrum
	\item Observe that the FFT resolution $\Delta f = \fs / \nfft$ improves as $\nfft$ gets larger, but this cannot recover the \emph{leakage} that occurred due to windowing
	\item Know the Z transform property to easily normalize the samples of an impulse response $h[n]$ by their summation to have a gain of 0~dB at the DC $H(e^{j \dw})|_{\dw=0}$
	\item Understand the \emph{picket fence} effect when using FFTs for spectral analysis
	\item Learn the reasons that lead a stronger sinusoid that is not bin-centered to completely hide a weaker sinusoid in FFT-based spectral analysis
	\item Estimate not only the sinusoid frequency but also its correct amplitude and correct the window scalloping loss
	\item Distinguish a spectrum analyzer and a vector network analyzer (VNA)
	%\item Practice using windowing functions for spectral analysis
	\item Obtain the output PSD when the input to a LTI system is a WSS process with a given input PSD
	\item Model the filtering of white noise through LTI systems
	\item Perform parametric PSD estimation via autoregressive (AR) models
\end{itemize}

\section{Introduction}

This chapter presents a brief introduction to spectral analysis.
The main goal of spectral analysis is to estimate how the signal power is distributed over frequency. It is a very broad area with many applications. Beware that definitions and jargon vary throughout the literature! The discussion here will focus on the important task of estimating the \emph{power spectrum}\index{Power spectrum}\index{Power spectral density (PSD)} (also called power spectral density).
%from a power signal 
%based on the FFT. 

Spectral analysis differs from \emph{frequency estimation}\index{Frequency estimation}. The latter, also called \emph{spectral line analysis}, assumes there are few frequencies that need to be estimated while, in contrast, spectral analysis aims at estimating the whole spectrum. When comparing the two, it is intuitive that having only few frequencies of interest is beneficial to the estimation process.
%, in terms of computational cost and robustness. 

%Another interesting aspect is that while most frequency 

Frequency estimation could be eventually performed by obtaining the signal spectrum via an FFT and picking its magnitude peaks.
However, there are many specialized frequency estimation algorithms that can outperform this FFT-based strategy.
There are frequency estimation methods for a single tone, multi-tone and multi-harmonic signals, for example. In the latter case, the signal of interest is composed by a sum of harmonically related sinusoids. This text concentrates in \emph{spectral analysis} and the reader is encouraged to get information elsewhere about frequency estimation methods if he/she faces the task of finding a small set of frequencies that compose a given signal of interest.

The \emph{spectral analysis algorithms} can be broadly divided in \emph{parametric} and \emph{nonparametric}. The former category encompasses algorithms that assume a model for the signal under analysis and estimate the parameters of this model. Examples of such models are autoregressive (AR), moving-average (MA) and ARMA (combination of the two), which can be seen as stochastic processes with realizations obtained by passing white noise through digital filters of types all-poles IIR $1/A(z)$, FIR $B(z)$, and an IIR with finite zeros $B(z)/A(z)$,
% (see Section~\ref{sec:digital_filters}), 
respectively. 

For these parametric models, the ``parameters'' are the coefficients of the corresponding filter and the noise power. The nonparametric algorithms try to estimate the spectrum without imposing that the signal of interest was generated by a model. 
%\tabl{spectral_analysis_methods} lists 
This text discusses the following spectral analysis methods:
\begin{itemize}
\item \textbf{Nonparametric}: PSD estimation with the periodogram and Welch's algorithm
\item \textbf{Parametric}: estimation of an AR model via linear predictive coding (LPC)
\end{itemize}

We will start by studying nonparametric methods based on the FFT. Assuming we want to perform
spectral analysis on a discrete-time signal $x[n]$, we will go over two steps:
\begin{itemize}
\item Extract $N$ samples of $x[n]$ to enable using a FFT, and mathematically model this process as multiplying
$x[n]$ by a \emph{window} function $w[n]$ to create the windowed signal with DTFT $X_w(e^{j \dw})$.
\item Use the FFT to obtain $X_w[k]$, which consists of discrete values of the DTFT $X_w(e^{j \dw})$ over the discrete-time angular frequency $\dw$, as discussed in Section~\ref{sec:dtft_via_dft}.
\end{itemize}
These two processes bring artifacts, which will be discussed in the next sections: spectrum \emph{leakage} and \emph{picket-fence} effect, respectively. If one starts from an analog signal $x(t)$ instead of $x[n]$, he/she needs to also consider the C/D process that relates $x(t)$ and $x[n]$ and may be impaired by other artifacts, besides leakage and picket-fence.

%Because of their importance in the first stage above, 
To understand spectrum leakage, a brief review of windows for spectrum estimation is presented in the sequel.

%\begin{table}
%\centering
%\caption{Three spectral analysis methods discussed in this text.\label{tab:spectral_analysis_methods}}
%\begin{tabular}{|c|c|}
%\hline
%Nonparametric & Parametric \\ \hline
%Periodogram and Welch & AR estimation via LPC \\ \hline
%\end{tabular}
%\end{table}

%Before that, the ESD for energy signals is briefly presented.

\section{Windows for spectral analysis}
\label{sec:windows}

Windows were briefly discussed in the context of block processing and FIR design in Sections~\ref{sec:block_proc} and  \ref{subsec:fir_windowing}, respectively. Here they are applied to spectral analysis in order to better control trade offs such as \emph{resolution} and \emph{leakage}.
%, which is their second main application in signal processing. 

Windows are essential to model the extraction of a finite-duration segment from a signal with a potentially infinite duration. For example, a $\nfft$-points FFT is restricted to operate on $N$ signal samples, with $N \le \nfft$. And even if the user is not aware or explicitly modeling the windowing operation, obtaining a segment of $N$ samples from an eventually longer signal $x[n]$ is mathematically equivalent to multiplying $x[n]$ by a rectangular window $w[n]$ of duration $N$ samples (as implicitly done in Section~\ref{sec:block_proc}).
%Windowing can be used to better control trade offs in spectral analysis, such as resolution and \emph{leakage}.

This windowing operation in time domain can often be modeled by
\begin{equation}
x_w[n] = x[n] w[n],
\label{eq:time_domain_windowing}
\end{equation}
as a pre-processing step, to generate $x_w[n]$ with a finite duration of $N$ samples. Windowing enables calculating the FFT of the finite-duration $x_w[n]$, while the FFT of the original signal $x[n]$ would be unfeasible due to its long duration.

%To understand the leakage one can remember that the 
As informed by \equl{windowingConvolution}, this multiplication $x[n] w[n]$ in time-domain corresponds to a \emph{periodic convolution} in frequency-domain, as repeated here for convenience:
\begin{equation}
X_w(e^{j \dw}) = X(e^{j \dw}) \cconv W(e^{j \dw}),
\label{eq:freq_domain_windowing}
\end{equation}
between the DTFTs $X(e^{j \dw})$ and $W(e^{j \dw})$ of $x[n]$ and $w[n]$, respectively.
%\begin{equation}
%X_w(e^{j \dw}) = \frac{1}{2\pi} X(e^{j \dw}) \conv W(e^{j \dw}).
%\label{eq:windowingConvolution}
%\end{equation}

%Before detailing leakage and picket-fence, 
Five popular windows are discussed in the next subsection.

\subsection{Popular windows}

%Five of the most popular windows are described here.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/windowsTimeDomain}
\caption{Selected windows $w[n]$ of duration $N=32$ samples in time-domain.\label{fig:windowsTimeDomain}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/windowsFreqDomain}
\caption{The DTFTs $W(e^{j \dw})$ of the windows in \figl{windowsTimeDomain}.\label{fig:windowsFreqDomain}}
\end{figure}

\figl{windowsTimeDomain} shows some common windows for the case of $N=32$ samples in time-domain. Note that all of them were normalized to have the maximum amplitudes equal to one. \figl{windowsFreqDomain} shows these windows in frequency-domain, via their DTFTs.

\bExample \textbf{Designing windows in {\matlab}}.
\codl{snip_frequency_windows} lists the commands to design windows with $N=256$ samples
using {\matlab}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_windows}{snip_frequency_windows}
%\begin{lstlisting}
%N=256;%number of samples available of x1 and x2
%boxcar(N); %rectangular window
%hamming(N); %Hamming window
%hanning(N); %Hann window
%kaiser(N,7.85); %Kaiser with beta=7.85
%flattopwin(N); %Flat top window
%\end{lstlisting}

Assuming discrete-time windows, the only input parameter for their design is their duration \ci{N}. 
%For example, the command:
%\begin{lstlisting}
%w=hamming(32) %Hamming window with 32 samples
%\end{lstlisting}
%creates a Hamming window with 32 samples.
The Kaiser windows (also called Kaiser-Bessel window) is the only exception among the five, because it also has a parameter $\beta$. For example, increasing $\beta$ of a Kaiser window widens the main lobe and decreases the amplitude of the sidelobes.
\eExample

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/windowsFreqDomainNormalized}
\caption{The DTFT of windows in \figl{windowsTimeDomain} with their values normalized such that $|W(e^{j \dw})|=1$ for $\dw=0$~rad.\label{fig:windowsFreqDomainNormalized}}
\end{figure}

It is common to normalize a window such that its DTFT has a value equal to one at DC. This was done to generate 
\figl{windowsFreqDomainNormalized}.
To obtain this normalization, one can use the fact that the DC value $W(z)|_{z=1}$ coincides with the sum of the window coefficients. For example, a normalized Hamming window can be obtained with
\ci{w=hamming(32);w=w/sum(w)}.

\subsubsection{Equations describing popular windows}
The \emph{Hamming} window\index{Hamming window} adopted here is given by
\begin{equation}
w[n] = 0.54 - 0.46 \cos \left( \frac{2 \pi n}{N-1} \right), \textrm{~~~}n=0,\ldots,N-1,
\label{eq:hammingWindow}
\end{equation}
while the \emph{Hann} window\index{Hann window} is
\begin{equation}
w[n] = 0.5 - 0.5 \cos \left( \frac{2 \pi n}{N-1} \right), \textrm{~~~}n=0,\ldots,N-1.
\label{eq:hannWindow}
\end{equation}
Note that according to \equl{hannWindow}, the first and last samples ($n=0$ and $N-1$) are zero and this version of the Hann window is called ``periodic''. 
In Python, the commands \ci{import numpy} and \ci{w = numpy.hanning(5)} return \co{[0.,0.5,1.,0.5,0.]}. In \matlab, the command \ci{hanning(6,'periodic')} returns \co{[0;0.25;0.75;1.00;0.75;0.25]} while the commands \ci{hanning(5)} and \ci{hanning(5,'symmetric')} return \co{[0.25;0.75;1.00;0.75;0.25]}, without the zero-valued samples.

The \emph{flat-top} window\index{Flat-top window} is
\begin{align}
w[n] &= 0.21557895 -0.41663158 \cos \left( \frac{2 \pi n}{N-1} \right) + 0.277263158 \cos \left( \frac{4 \pi n}{N-1} \right)\\
 &- 0.083578947\cos \left( \frac{6 \pi n}{N-1} \right) +0.006947368 \cos \left( \frac{8 \pi n}{N-1} \right), \textrm{~~~}n=0,\ldots,N-1.\nonumber
\label{eq:flatTopWindow}
\end{align}

The \emph{Kaiser} windows\index{Kaiser window} for $N$ odd, such that $M=(N-1)/2$ is an integer, is
\begin{equation}
w[n] =  \frac{I_0 \left( \beta \sqrt{1 - \left(\frac{n-M}{M} \right)^2 } \right)}{I_0 \left(\beta \right)}, \textrm{~~~}n=0,\ldots,N-1,
\label{eq:kaiserWindow}
\end{equation}
where $I_0(x)$ is the $0^{\textrm{th}}$ order Bessel function of the first kind, which is calculated in {\matlab} with \ci{besseli(0,x)}, as indicated in the following commands:
\begin{lstlisting}
N=13; beta=7.85; %produce same window as w=kaiser(N,beta):
n=0:N-1; M=(N-1)/2; %auxiliary variables
w=abs(besseli(0,beta*sqrt(1-((n-M)/M).^2)))/abs(besseli(0,beta));
w=w/max(w); %normalize to have maximum value equal to 1
\end{lstlisting}
 that create a Kaiser window (for $N$ odd).

\subsection{Figures of merit applied to windows}

Four of the most important parameters of a window are:
\begin{itemize}
	\item width of its main lobe,
	\item amplitude of its sidelobes,
	\item rate of sidelobe decay as frequency increases,
	\item and scalloping loss.
\end{itemize}
Each of them are discussed in the sequel, starting by the scalloping loss. There are other figures of merit of windows, and the reader is invited to check the references about windows in 
Section~\ref{sec:spectral_further}.

\subsubsection{Estimating sinusoid amplitude and observing the scalloping loss}
\label{sec:sin_amplitude}

In some applications of FFT-based spectral analysis it is necessary not only detect the frequencies (their values in Hertz, for instance) but also the amplitudes (e.\,g., in volts) of the detected frequency components. In this case, the impact of windowing and then using an FFT needs to be taken in account because they impose an error (or loss) on the estimated sinusoid amplitude with respect
to the true value called ``scalloping''. 
For most windows, this so-called \emph{scalloping loss}\index{Scalloping loss} is worst when the frequency is half-way the two neighboring bins.

For instance, if one is calculating the harmonic distortion solely based on percentage values, such as: ``- the third harmonic has 12\% of the amplitude of the fundamental frequency component'', then mitigating the scalloping loss may not be needed. But if the analysis requires the absolute amplitude values of the harmonic components, the scalloping loss issue 
%procedure to be discussed in the next paragraphs 
becomes important.

%AK-IMPROVE - I am not sure if the analysis below is useful because it cites bin-centered. Is it valid for non-bin-centered?
%For convenience, we assume a complex exponential $x[n]= A e^{j2 \pi k'n/N}$, where $A$ is the amplitude, $N$ is the FFT-size and $k'$ is the FFT bin in which $x[n]$ resides. Note that $x[n]$ is bin-centered. The windowed signal $x_w[n]=A e^{j2 \pi k'n/N}w[n]$ is obtained by multiplying $x[n]$ by a window $w[n]$ with support of $N$ samples. Due to the orthogonality of the FFT basis functions, all FFT coefficients $X_w[k]$ will be zero but the one corresponding to bin $k'$. This coefficient is given by:
%\[
%X_w[k'] = \sum_{n=0}^{N-1} x_w[n] e^{-j2\pi k' n/N} = A \sum_{n=0}^{N-1} w[n] e^{j2\pi k' n/N} e^{-j2\pi k' n/N} = A \sum_{n=0}^{N-1} w[n].
%\]
%Therefore, to obtain an estimate $\hat{A}$ of the correct amplitude $A$ from an FFT value $X_w[k]$, one should use
%\begin{equation}
%\hat{A} = \frac{|X_w[k]|}{\sum_{n=0}^{N-1} w[n]}.
%\label{eq:scalloping_correction}
%\end{equation}
%Note that one divides the FFT result by $N$ when wants to interpret it in volts, but this is already incorporated in \equl{scalloping_correction}. If the division by $N$ is already incorporated in a given software routine, one needs to multiply the FFT $X[k]$ by $N / \sum_{n=0}^{N-1} w[n]$ in order to compensate the later division by $N$.

\codl{snip_frequency_scalloping} illustrates a procedure for estimating the amplitude $A=6$~V of a cosine using FFT and the
effect of the scalloping loss.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_scalloping}{snip_frequency_scalloping}

When \codl{snip_frequency_scalloping} is used with $\alpha=8$ to generate a bin-centered cosine,
the result using the rectangular window is \figl{scalloping_rect_centered}.
Changing the window to flattop leads to \figl{scalloping_flattop_centered}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/scalloping_rect_centered}
\caption{Comparison of DTFT and DFT/FFT of a bin-centered signal $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=8$ and $N=32$, using a rectangular window.\label{fig:scalloping_rect_centered}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/scalloping_flattop_centered}
\caption{Comparison of DTFT and DFT/FFT of a bin-centered signal $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=8$ and $N=32$, using a flattop window.\label{fig:scalloping_flattop_centered}}
\end{figure}

Both \figl{scalloping_rect_centered} and \figl{scalloping_flattop_centered} show that the maximum value of the FFT magnitude (3 in this case),
when normalized by the FFT size $N$ leads exactly to the area $\pi A$ (divided by $\pi$) of the impulse representing the cosine DTFT.
In this case of bin-centered sinusoids there is no visible scalloping loss.

However, if the cosine frequency $\dw_c$ is located half-way two neighboring bins (\ci{alpha=8.5}), the scalloping loss becomes
evident. When using the rectangular window with \ci{alpha=8.5}, one obtains \figl{scalloping_rect_noncentered} and 
the numerical results below:
\begin{verbatim}
Max(abs(scaled FFT)) = 1.9221
Scalloping loss = 1.0779
Correct amplitude (volts) = 6
Estimated amplitude (volts) = 3.8442
Amplitude error (volts) = 2.1558
Amplitude error (%) = 35.9301
\end{verbatim}
The amplitude error in this case is 35.9\%, which is due to the worst-case scalloping loss for the rectangular window in this situation of a sinusoid exactly half-way the bins.\footnote{Similar to the amplitudes, the absolute power values inferred from of an FFT-based analysis require correction~\cite{Brandt11}.}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/scalloping_rect_noncentered}
\caption{Comparison of DTFT and DFT/FFT of a non-bin-centered signal $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=8.5$ and $N=32$, using a rectangular window.\label{fig:scalloping_rect_noncentered}}
\end{figure}

In case one chooses the flat-top window in \codl{snip_frequency_scalloping} (\ci{window\_choice=3}), 
the corresponding figure is \figl{scalloping_flattop_noncentered} and the numerical results are:
\begin{verbatim}
Max(abs(scaled FFT)) = 2.9949
Scalloping loss = 0.0050728
Correct amplitude (volts) = 6
Estimated amplitude (volts) = 5.9899
Amplitude error (volts) = 0.010146
Amplitude error (%) = 0.16909
\end{verbatim}
The flat-top window does not have a good frequency resolution but reaches an amplitude estimation error of only 0.17\%. Because of that, it is widely adopt to calibrate equipment.\footnote{See, e.\,g., the presentation by Matt Ettus at \url{http://www.youtube.com/watch?v=wqKNUXDdIvU}.} 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/scalloping_flattop_noncentered}
\caption{Comparison of DTFT and DFT/FFT of a non-bin-centered signal $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=8.5$ and $N=32$, using a flattop window.\label{fig:scalloping_flattop_noncentered}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/scalloping_hann_noncentered}
\caption{Comparison of DTFT and DFT/FFT of a non-bin-centered signal $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=8.5$ and $N=32$, using a Hann window.\label{fig:scalloping_hann_noncentered}}
\end{figure}

As indicated in \figl{scalloping_hann_noncentered}, the Hann window is a trade-off between frequency resolution and amplitude error, reaching 16\% of amplitude error when one uses \ci{window\_choice=2} in \codl{snip_frequency_scalloping}, as listed:
\begin{verbatim}
Max(abs(scaled FFT)) = 2.5197
Scalloping loss = 0.48034
Correct amplitude (Volts) = 6
Estimated amplitude (Volts) = 5.0393
Amplitude error (Volts) = 0.96069
Amplitude error (%) = 16.0115
\end{verbatim}

This study just showed the concept of scalloping loss and the importance of choosing the proper window function (such as flattop). 
A better interpretation of scalloping loss is obtained by interpreting the FFT as a filter bank, 
as briefly discussed in the next subsection. %Application~\ref{app:fftFilterBank}.

\subsubsection{FFT Interpreted as a Filter Bank}
\label{sec:fftFilterBank}
Filter banks are often used for performing spectrum analysis and signal synthesis. A DFT can be seen as a \emph{uniform} filter bank due to the fact that all filters have the same bandwidth. 

The interpretation of FFT and other transforms as filter banks lead to improved insight. For example, the good result obtained by the rectangular window in \figl{windowsHarmonicAnalysis} is due to a particular property of its side-lobe structure: all side-lobes are zero in the center of the bins. When the spectrum being analyzed has non-zero components only in bin centers, the leakage due to the side-lobes will not appear in the final FFT result. The leakage could be seen in the DTFT though.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{./Figures/dftFilterBankRectangular}
\caption{DFT filter bank with rectangular window of $N=8$ samples. The circles mark the DFT bin centers. The filter for $k=3$ is emphasized.\label{fig:dftFilterBankRectangular}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{./Figures/dftFilterBankKaiser}
\caption{DFT filter bank with Kaiser window of $N=8$ samples. The filter for $k=6$ is emphasized.\label{fig:dftFilterBankKaiser}}
\end{figure}

\figl{dftFilterBankKaiser} and \figl{dftFilterBankRectangular} indicate the filter banks corresponding to an FFT with rectangular and Kaiser windows, respectively.
The rectangular has larger side-lobe levels, but all of them achieve a zero in the bin centers. The Kaiser window does not have side-lobes with amplitude equal to zero in the bin centers, but these side-lobe levels are very small.
%Modifying the scripts that obtained these figures allows to show the scalloping loss of each window, as discussed in \cite{Lyons11}.
And there are other strategies (see e.\,g.~\cite{Lyons11}) to mitigate the effects of the scalloping loss that complement choosing a good window function.

%\eApplication

%\codl{snip_frequency_fftCosineExample} allows to observe the scalloping loss when using the rectangular window for obtaining the FFT of a cosine.\footnote{Scalloping loss is better understood by interpreting FFT as a filter-bank, as in Application~\ref{sec:fftFilterBank}.}

\subsubsection{Evaluating windows using Matlab's wintool}

Matlab's \ci{wintool} (Window Design \& Analysis Tool) GUI is useful to explore trade offs in windowing. One can observe, for example, that increasing $\beta$ of a Kaiser window widens the main lobe and decreases the amplitude of the sidelobes. It also calculates the leakage factor, relative sidelobe attenuation and main lobe width.
%AKA \eExample

Windows differ in their main lobe width, which is typically inversely proportional to $N$, the length of the window. Thus, it is possible to achieve a better resolution in frequency by increasing $N$, but more signal samples are needed and sometimes they are not available.

Another important window parameter is the highest sidelobe level, which should have a small value.
When a window is used for FIR design, the smaller the sidelobe amplitude with respect to the amplitude of the main lobe, the higher the filter attenuation in the stopband.
From the spectra in \figl{windowsFreqDomainNormalized}, \tabl{windows_sidelobe} can be obtained, which indicates the difference in dB between the main lobe and the highest sidelobe amplitudes.
It should be noticed that, for the Hamming window, the highest sidelobe is the third one. The first sidelobe is localized in the frequency equal to 0.46 rad and has a level of $-50.53$~dB when compared to the main-lobe level. 

\begin{table}
\centering
\caption{Difference in dB between the window main lobe and highest sidelobe amplitudes.\label{tab:windows_sidelobe}}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Window &	Rectangular	& Hann	& Hamming	& Kaiser ($\beta=7.85$)& Flat top \\ \hline
Difference (dB) &	$-13$	& $-32$	& $-43$ &	$-57$&  $-68$ \\ \hline
\end{tabular}
\end{table}

When comparing spectral components that are close to each other, the adopted windows must have good resolution and a low highest sidelobe level. When the spectral components are far apart, having a large sidelobe decay can be more important.
	
This sidelobe fall-off indicates how the peak amplitude of the sidelobes decay with frequency and, for example, is a motivation for using the Hann window.
The side-lobe fall-off is dependent on the continuity of the window $w(t)$ in time-domain and its derivatives. For example, the Hann window is continuous, and besides, its first derivative is continuous too. Thus, its transform falls off at $1/\aw^3$ or, equivalently, at $-18$~dB per octave. In contrast, the rectangular and Hamming windows, which are not continuous at their endpoints, have a $-6$~dB per octave fall-off.


%AK-TODO - complete Scalloping
%For example, the rectangular window (see \figl{dftFilterBankRectangular}) creates a $\sinc(x)$ at the position of a sinusoid on the DTFT of the windowed sinusoid, , used together with a 8-points FFT (of resolution $\Delta \dw=2\pi/8$ rad), would sample the DTFT of a cosine of frequency $\dw=2\pi/16$~rad 

%described the spectral leakage that can occur (and in practice does) when using FFTs to perform spectral analysis due to the discretization of frequency and windowing. 

%\subsection{Most common windows}

%This of course impacts the frequency response of the FIR designed via windowing. 


%AK-IMPROVE redo hw4.m at \ url{C:\svns\laps\latex\dslbook\ThingsToInclude\Spectral\windows-espectro}

%Check \ url{C:\svns\laps\latex\dslbook\ThingsToInclude\GNURadio}
%Ettus presentation at \ url{http://www.youtube.com/watch?v=wqKNUXDdIvU} for flat top
%\ url{http://www.mathworks.com/help/signal/ref/flattopwin.html}

%\subsection{Windows for better managing the spectral analysis}

\subsubsection{Relation between roots location and spectrum}. 
A window $w[n]$ is used to multiply a signal in time-domain, but for the sake of observing its spectrum, it can be interpreted as the impulse response of a FIR filter, which does not have finite poles, only zeros. 
Under this assumption, there is an interesting relation between the zeros of the windows, and its spectrum $W(e^{j \dw})$. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{./FiguresNonScript/windows_root_zeros}
\caption{Relationship between roots location and spectrum for a rectangular window with $N=32$ samples.\label{fig:windows_root_zeros}}
\end{figure}

\figl{windows_root_zeros} shows the spectrum (left plot) of a rectangular window with $N=32$ and the roots (right plot) of this window. This plot outlines that there are $N-1$ zeros (the amount of roots of a polynomial of order $N$) and the width of the main lobe is obtained from the angle $\Theta = 2 \pi/N$ (in this case,  $\Theta = 2 \pi/32 \approx 0.1963$~rad) of the first root, given that there is no root at $\dw=0$ and the $N-1$ roots are uniformly spaced around the unit circle.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{./FiguresNonScript/windows_kaiser_root_zeros}
\caption{Relationship between roots location and spectrum for a Kaiser window with $N=32$ samples.\label{fig:windows_kaiser_root_zeros}}
\end{figure}

\figl{windows_kaiser_root_zeros} shows that a greater concentration of zeros in the region of the main lobe, allows the Kaiser window (with $\beta=7.85$) achieve a very small first side-lobe. However, the width of the main lobe increases considerably when compared to the rectangular window in \figl{windows_root_zeros}, due to the increase of $\Theta$.
In summary, the rectangular window has the smallest main lobe, but its first sidelobe level is just $-13$~dB below the main-lobe level. On the other hand, the Kaiser window has a wider main lobe, but this window has very small sidelobe levels.
%\eApplication

The next subsections discuss leakage and the picket-fence FFT effect.

\subsection{Leakage}

Leakage is a very important aspect on spectral analysis using FFT. To understand leakage one must remember that the multiplication $x[n] w[n]$ in time-domain as in \equl{time_domain_windowing}, corresponds to the convolution in frequency-domain as in \equl{freq_domain_windowing}.
The spectra of the windows in \figl{windowsFreqDomain} indicate that the values of the original spectrum
$X(e^{j \dw})$ will be surely altered if $X(e^{j \dw})$ is convolved with any finite-duration window.
% from \figl{windowsTimeDomain},
%This happens because the spectrum $W(e^{j \dw})$ of a finite-duration window is distinct from the ideal impulse $\delta(\dw)$ in frequency domain. 

%To make this point clear, consider that $X(e^{j \dw})$ is the DTFT of a cosine of amplitude $A$ and frequency $\dw_1$, such that the impulses of area $A \pi$ are located at $\pm \dw_1 + k 2 \pi, k \in \integers$. 
%Assuming the window DTFT $W(e^{j \dw})$ has value $B$ at $\dw=0$ (DC), according to \equl{windowingConvolution}, the 
%windowing operation will lead to
%a convolution result with shifted versions of $W(e^{j \dw})$ centered at $\pm \dw_1$ with a peak value of $0.5(A \times B)$. 

%To avoid this scaling by $B$, it is common to normalize the window such that its DTFT has a value equal to 1 at DC. This was done to generate \figl{windowsFreqDomainNormalized}.
%The normalization can use the fact that the DC value of a FIR filter coincides with the sum of its coefficients. For example, a normalized Hamming window can be obtained with
%\ci{x=hamming(32);x=x/sum(x)}.

The result of the convolution caused by windowing may not significantly impact the original spectrum $X(e^{j \dw})$ in case $W(e^{j \dw})$ is relatively narrow compared to variations in $X(e^{j \dw})$. But this condition implies that the window $w[n]$ is sufficiently long (i.\,e., $N$ must be sufficiently large).
% such that $W(e^{j \dw})$ is narrow compared to $X(e^{j \dw})$. 
As mentioned, the ideal situation, where windowing would not alter the original $X(e^{j \dw})$, is to have
$W(e^{j \dw})$ as an impulse in the range $-\pi < \dw \le \pi$ (i.\,e.
 $W(e^{j \dw})=\delta(\dw + k 2\pi), k \in \integers$), but this implies that $w[n]$ has infinite duration and all samples of the eventually eternal $x[n]$ are available for spectral analysis.

Hence, even if $W(e^{j \dw})$ is narrow compared to $X(e^{j \dw})$, the convolution $X_w(e^{j \dw}) = X(e^{j \dw}) \cconv W(e^{j \dw})$ may have non-zero (``leaked'') values in frequencies where the original signal $X(e^{j \dw})$ was zero.

Note that many times, leakage is erroneously interpreted as a consequence of using FFTs to perform spectral analysis. However, even when dealing with DTFTs, without the inherent discretization of frequency provoked by DFTs / FFTs, 
windowing creates ``frequency leaks'' that show up on the DTFT $X_w(e^{j \dw})$. 
The following paragraphs provide concrete examples of leakage.

\bExample \textbf{Examples of spectrum leakage due to windowing}.
\label{ex:leakage_example}
Spectrum leakage can be understood by observing the case of an eternal sinusoid $x[n]=A \cos(\dw_c n)$. The DTFT of $x[n]$
is $X(e^{j \dw}) = A \pi [\delta(\dw + \dw_c) + \delta(\dw - \dw_c)]$ within the range $-\pi \le \dw \le \pi$, then repeated in multiples of $2\pi$.

Assuming a rectangular window $w[n]$ with amplitudes equal to 1 from $n=0,1,\ldots,N-1$ and zero otherwise, the spectrum of $w[n]$ as given
by \equl{discrete_sinc_transform} is
	\begin{equation}
	W(e^{j \dw}) = \frac{\sin \left( \frac{N\dw}{2} \right) }{\sin \left( \frac{\dw}{2}\right) } e^{-j\frac{\dw(N-1)}{2}}.
	\label{eq:dtftRectangularWindow}
	\end{equation}

The DTFT $X_w(e^{j \dw})$ of the windowed cosine $x_w[n]=x[n] w[n]$ is the periodic convolution between $X(e^{j \dw})$ and the window DTFT $W(e^{j \dw})$. Recalling that the periodic convolution of \equl{freq_domain_windowing} has a factor $\frac{1}{2\pi}$, the resulting DTFT is
	\begin{equation}
	X_w(e^{j \dw}) = \frac{A}{2} \left[W(e^{j (\dw + \dw_c)}) + W(e^{j (\dw - \dw_c)})\right].
	\label{eq:dtftWindowedCosine}
	\end{equation}

\figl{leakage_example} depicts the example of a cosine with amplitude $A=3$~V and frequency $\dw_c=1$~rad. The data tip on
the top plot indicates the area of the impulses at $\dw=\pm \dw_c$ is $0.5 A \pi = 0.5 \times 3 \times \pi \approx 4.71239$.
The middle plot shows $|W(e^{j \dw})|$ and its data tip indicates that its value at DC ($\dw=0$) is $N$.
The bottom plot of \figl{leakage_example} shows $|X_w(e^{j \dw})|$. The value 22.6146 of the data tip 
at $\dw=-\dw_c$ is obtained from \equl{dtftWindowedCosine} as:
\begin{align}
	X_w(e^{j \dw})|_{\dw=-\dw_c} &= \frac{A}{2} \left[W(e^{j 0}) + W(e^{-j 2\dw_c})\right] \nonumber \\
	&= \frac{A}{2}  \left[N  + \frac{\sin \left( -N\dw_c \right) }{\sin \left( -\dw_c\right) } e^{j\dw_c(N-1)}\right] \nonumber \\
	&= \frac{3}{2} \left[14 + \frac{\sin(-14)}{\sin(-1)} e^{j 13}\right] \nonumber \\
	&= 22.6024 + 0.7420j. \nonumber
%\label{eq:flatTopWindow}
\end{align}
The magnitude $|22.6024 + 0.7420j|=22.6146$ is the data tip value shown in the bottom plot of \figl{leakage_example}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/leakage_example}
\caption{Spectrum leakage when a cosine $x[n]=3 \cos(\dw_c n)$, with $\dw_c=1$~rad, is windowed with a
rectangular window $w[n]$ of duration $N=14$ samples.\label{fig:leakage_example}}
\end{figure}

As discussed in Section~\ref{sec:sin_amplitude}, recovering exactly the amplitude $A$ from $X_w(e^{j \dw})$ is not an easy task because the scalloping loss is influenced by contributions from two replicas
of $W(e^{j \dw})$ in \equl{dtftWindowedCosine}.

\figl{leakage_example2} depicts the result when the window length is increased from $N=14$ to $N=50$ samples. In this 
case, $X_w(e^{j \dw})|_{\dw=-\dw_c}$ is given by
\begin{align}
	X_w(e^{j \dw})|_{\dw=-\dw_c} &= \frac{3}{2} \left[50 + \frac{\sin(-50)}{\sin(-1)} e^{j 49}\right] \nonumber \\
	&= \frac{3}{2} \left[50 + (-0.0937 + 0.2974j) \right]. \nonumber \\
	&= 74.8594 + 0.4461j \nonumber
%\label{eq:flatTopWindow}
\end{align}
The magnitude $|74.8594 + 0.4461j|=74.8607$ is slightly smaller than $\frac{3}{2}50=75$, which is the value that would be obtained by using only the first replica of $W(e^{j \dw})$. This uncertainty about the amplitude of the sinusoid provoked by the second replica of $W(e^{j \dw})$ can be mitigated using a flattop instead of the rectangular window, as discussed in Section~\ref{sec:sin_amplitude}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/leakage_example2}
\caption{Spectrum leakage when the cosine $x[n]=3 \cos(n)$ of \figl{leakage_example} is now windowed with a
rectangular window $w[n]$ of duration $N=50$ samples.\label{fig:leakage_example2}}
\end{figure}

The top plot in \figl{leakage_example} and \figl{leakage_example2} is showing impulses with the same area values because
the input signal does not change.
But the middle plots show that the DTFT peak gets larger when $N$ is increased.
\figl{leakage_example3} presents the results when the window length is increased to $N=1000$ samples.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/leakage_example3}
\caption{Spectrum leakage when the cosine $x[n]=3 \cos(n)$ of \figl{leakage_example} is now windowed with a
rectangular window $w[n]$ of duration $N=1000$ samples.\label{fig:leakage_example3}}
\end{figure}

Independent of the window length $N$, a key point made by Figs.~\ref{fig:leakage_example},
\ref{fig:leakage_example2} and \ref{fig:leakage_example3} is that
the spectrum $X_w(e^{j \dw})$ of a windowed sinusoid is not composed solely of impulses, as in $X(e^{j \dw})$ for the eternal sinusoid.
The DTFT $X_w(e^{j \dw})$ has sidelobes dictated by the window DTFT $W(e^{j \dw})$.
\eExample

As mentioned, the convolution in \equl{freq_domain_windowing} may not significantly alter $X_w(e^{j \dw})$ with respect to the original $X(e^{j \dw})$ in case $W(e^{j \dw})$ is relatively narrow compared to variations in $X(e^{j \dw})$. However, there
is a tradeoff between narrower bandwidths and longer duration over time. See, for example, the Fourier transform of a square pulse in \equl{sinc_transform} to remember the tradeoff between time and frequency.
%, this condition requires the window $w[n]$ to be sufficiently long (i.\,e., $N$ must be sufficiently large).
% such that $W(e^{j \dw})$ is narrow compared to $X(e^{j \dw})$. 
%In the extreme (and ideal) situation of windowing not altering the original $X(e^{j \dw})$, the window DTFT would be $W(e^{j \dw})=\delta(\dw + k 2\pi), k \in \integers$. But this is equivalent to having
Increasing the duration of $w[n]$ (using more samples from the original signal) mitigates leakage as indicated by the behavior of \figl{leakage_example3}, which shows the effect of increasing $N$ to obtain
a window that looks more similar to the ideal impulse.

Leakage is associated to windowing and should not be confused with the picket-fence effect caused by sampling the DTFT via a FFT, which is discussed in the sequel.

\subsection{Picket-fence effect}

As discussed, the result of windowing a signal and then taking the FFT of the windowed signal leads to two consequences: a) creation of spectrum $X_w(e^{j \dw})$ with leakage due to windowing, and then b) spectrum values $X_w[k]$ available only at a discrete frequency grid along the angular frequency $\dw$ imposed by the FFT.

The consequence of discretizing $\dw$ via an FFT is sometimes called \emph{picket-fence} effect\index{Picket-fence effect}, which corresponds to observing the DTFT of the signal under analysis just at the DFT/FFT bin centers, as dictated by \equl{fft_freq_tone_rad} (or \equl{fft_freq_resolution}, when using frequency in Hz).
To study the picket-fence effect, it is useful to place sinusoids at arbitrary positions within a given FFT bin. This is the topic of the next paragraphs.

\subsubsection{Positioning sinusoids with respect to the FFT bin}

Given that the bin width of an $N$-points DFT (or FFT) is $\Delta \dw = \frac{2 \pi }{N}$ radians, the angular frequency associated to the $k$-th FFT bin is $\dw_k = k \frac{2 \pi }{N}$ radians, as given by \equl{fft_freq_tone_rad}.
At this point, it may be useful to recall the definition of a bin center using \figl{fft_bin_definition}.

When performing the spectral analysis of a bin-centered sinusoid $x[n]=\cos(k \frac{2 \pi }{N}n)$ using an $N$-points FFT, the components of ``positive'' and ``negative'' frequencies of $x[n]$ will be mapped to the FFT bins $k$ and $(N-k)$, respectively. In order to investigate non-bin-centered sinusoids, it is useful to generalize the signal expression to 
\begin{equation}
x[n]=\cos \left( \alpha \frac{2 \pi }{N}n \right),
\label{eq:cosine_alternative}
\end{equation}
where $\alpha \in \Re$ is a real number in the range $[0, N/2]$ that allows to obtain any desired value for the positive angular frequency $\dw_c = \alpha \frac{2 \pi}{N}$, from 0 to $\pi$~rad.
One can write \equl{cosine_alternative} in terms of the FFT resolution $\Delta \dw = 2 \pi / N$ as:
\begin{equation}
x[n]=\cos \left( \alpha \Delta \dw \right).
\end{equation}

\subsubsection{Scalloping loss: the combined effects of leakage and picket-fence}
\label{sec:ExampleLeakagePicketFence}

It is informative to revisit the result of an FFT when the original input signal is a discrete-time sinusoid $x[n]=A \cos(\dw_c n)$ after windowing, as discussed in Section~\ref{sec:sin_amplitude} and \exal{leakage_example}. 

Recall that the DTFT of this eternal cosine is $X(e^{j \dw}) = A \pi [\delta(\dw + \dw_c) + \delta(\dw - \dw_c)]$ within the range $-\pi \le \dw \le \pi$. And the DTFT $W(e^{j \dw})$ of the rectangular window is given by \equl{dtftRectangularWindow}.

Assuming that $X_w(e^{j \dw})$ denotes the DTFT of the windowed cosine, the final FFT result depends on the two effects:
\begin{enumerate}
	\item \textbf{Leakage}: $X_w(e^{j \dw})$ is the periodic convolution between $X(e^{j \dw})$ and $W(e^{j \dw})$, and is
	given by \equl{dtftWindowedCosine}, which is repeated below for convenience:
	\[
	X_w(e^{j \dw}) = \frac{A}{2} \left[W(e^{j (\dw + \dw_c)}) + W(e^{j (\dw - \dw_c)})\right].
	\]
	\item \textbf{Picket-fence}: The FFT $X_w[k]$ has a resolution of $\Delta \dw = (2 \pi)/N$ and corresponds to samples $X_w[k] = X_w(e^{j \dw})|_{\dw = k \Delta \dw}$ of the DTFT $X_w(e^{j \dw})$ at frequencies $\dw = k \Delta \dw$.
\end{enumerate}

%For an arbitrary frequency $\dw_c$ rad, the first FFT bin $X_w[k]|_{k=0}$ corresponds to $\dw=0$ and is given by
%\begin{align}
%X_w[0] &= \frac{A}{2} \left[W(e^{j (\dw_c)}) + W(e^{j (-\dw_c)})\right]  \nonumber \\
%&=  \frac{A}{2} \frac{\sin \left( \frac{N\dw_c}{2} \right) }{\sin \left( \frac{\dw_c}{2}\right) } \left[ e^{-j\frac{\dw_c(N-1)}{2}} + e^{j\frac{\dw_c(N-1)}{2}} \right] \nonumber \\
%&= A \frac{\sin \left( \frac{N\dw_c}{2} \right) }{\sin \left( \frac{\dw_c}{2}\right) } \cos\left( \frac{\dw_c(N-1)}{2} \right)
%\label{eq:fftDCValueExample}
%\end{align}
%Using \equl{fftDCValueExample} for $\dw_c = 0$, one confirms that $X_w[k]|_{k=0} = A N$ using L'Hospital's rule.

%Based on \equl{fftDCValueExample}, \codl{snip_frequency_fftCosineExample} generates a graph to show how $X_w[0]$ varies with $\dw_c$. 

%From the result of \codl{snip_frequency_fftCosineExample}, it can be noted that $X_w[0]$ has the ``correct'' value $A \times N$ when $\dw_c=0$. But when $\dw_c$ is small but not zero, a relatively large value of $X_w[0]$ can appear when $N$ is small.

Two cases of interest are a bin-centered sinusoid ($\alpha \in \integers$), and $\dw_c$ half way two bin centers, exactly on their edges. The latter case represents a worst-case situation with respect to the scalloping loss.

Complementing the experiments in Section~\ref{sec:sin_amplitude}, here we study scalloping loss specifically adopting the rectangular window. For that, one can execute \codl{snip_frequency_fftCosineExample}  with the command \ci{snip\_frequency\_fftCosineExample(8, 2)}
and \ci{snip\_frequency\_fftCosineExample(8, 2.5)} to generate 
\figl{leak_picket_ex1} and \figl{leak_picket_ex2}, respectively. These figures are similar
to \figl{scalloping_rect_centered} and \figl{scalloping_rect_noncentered}.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_frequency\_fftCosineExample}{snip_frequency_fftCosineExample}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/leak_picket_ex1}
\caption{Comparison of DTFT and DFT/FFT for bin-centered $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=2$ and $N=8$ using a rectangular window.\label{fig:leak_picket_ex1}}
\end{figure}

\figl{leak_picket_ex1} indicates that when the sinusoid is bin-centered, the FFT outputs two peaks of value \ci{A*N/2}, which
in this example corresponds to \ci{6*8/2=24}. In this case, the scalloping loss is zero and there is no visible
leakage. This situation is interesting because as can be observed by the DTFT plot in \figl{leak_picket_ex1}, the windowing
operation already created leakage, but the picket-fence effect makes it not visible to the user that is observing
solely the FFT result $X_w[k]$, not the DTFT $X_w(e^{j \dw})$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/leak_picket_ex2}
\caption{Version of \figl{leak_picket_ex1} for a (worst-case) not bin-centered $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=2.5$ and $N=8$ using a rectangular window.\label{fig:leak_picket_ex2}}
\end{figure}

In contrast to \figl{leak_picket_ex1}, in \figl{leak_picket_ex2} the picket-fence effect on a non-bin-centered sinusoid clearly discloses an extreme leakage.
The adopted value $\alpha=2.5$, places the cosine right in the shared edge of bin centers $k=2$ and $k=3$. The FFT value for $k=2$ is slightly larger
($|X_w[2]|=16.8$) than the value for $k=3$ ($|X_w[2]|=14.4$) due to the influence of the first replica of $W(e^{j \dw})$ positioned at
$\dw_c = - \alpha 2 \pi / N \approx -1.9635$~rad.
In this example, the scalloping loss is calculated with
\begin{lstlisting}
disp(['Max(abs(FFT))=' num2str(max(abs(Xfft)))])
disp(['Scalloping loss in DTFT=' num2str(A*N/2-max(abs(Xfft)))])
\end{lstlisting}
and the result is 
\begin{lstlisting}
Max(abs(FFT))=16.7876
Scalloping loss in DTFT=7.2124
\end{lstlisting}
In this case, the expected (ideal) DTFT peak for the sinusoid with amplitude \ci{A=6} would be \ci{A*N/2=24}, but
the maximum $|X_w[k]|_{\textrm{max}}$ of the amplitude spectrum is 16.7876. Hence, the scalloping loss is $24 - 16.7876 = 7.2124$.

If one uses the fact that $|X_w[k]|_{\textrm{max}}$ should be equal to \ci{A*N/2}, the sinusoid amplitude can be estimated with $\hat A = 2 |X_w[k]|_{\textrm{max}} / N = 2 \times 16.7876 / 8 = 4.1969$~V. This corresponds to an error of 1.8031~V with respect to the correct sinusiod amplitude of 6~V.

\subsubsection{{\akadvanced} FFT of a sinusoid multiplied by a rectangular window}

Looking for a general expression for 
the FFT values $X_w[k]$ of a windowed cosine $x_w[n]= x[n] w[n]$, %A \cos(\dw_c n), n=0,\ldots,N-1$, 
%that is more general than \equl{fftDCValueExample}, consider 
with $\dw_c = \alpha \Delta \dw = \alpha (2 \pi)/N$ (recall that $\alpha \in \Re$ and $\alpha \in [0, N/2]$) and assuming
a rectangular window, one can substitute \equl{dtftRectangularWindow} into \equl{dtftWindowedCosine} to obtain:
\begin{equation}
X_w[k] = \frac{A}{2} \left[ \frac{\sin \left( (k+\alpha)\pi \right) }{\sin \left( \frac{(k+\alpha)\pi}{N}\right) } e^{-j\left(\frac{(N-1)\pi}{N}(k+\alpha)\right)} +
\frac{\sin \left( (k-\alpha)\pi \right) }{\sin \left( \frac{(k-\alpha)\pi}{N}\right) } e^{-j\left(\frac{(N-1)\pi}{N}(k-\alpha)\right)}
\right].
\label{eq:fftValuesExample}
\end{equation}
%which simplifies to \equl{fftDCValueExample} when $\alpha=0$. 

From \equl{dtftRectangularWindow}, the value of $W(e^{j \dw})$ for $\dw = 0$~rad can be found using L'Hospital's rule, which leads to $W(e^{j \dw})|_{\dw=0} = N$.

In the special case of $\dw_c=0$, the signal $x[n] = A$ has a constant amplitude, its DTFT is $X_w(e^{j \dw}) = A W(e^{j \dw})$ and the FFT DC bin $X_w[k]|_{k=0} = X_w[0]= A N$. This situation is depicted in \figl{leak_picket_ex3}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/leak_picket_ex3}
\caption{Comparison of DTFT and DFT/FFT for a constant signal $x[n]=6\cos((\alpha 2 \pi /N) n) = A$, with $\alpha=0$ (centered at DC) and $N=8$, using a rectangular window.\label{fig:leak_picket_ex3}}
\end{figure}

Let us assume the goal is to estimate the sinusoid amplitude $A$ in volts, via the DFT coefficients obtained from the signal FFT. 
In this case, \equl{dft_as_dtfs}, indicates that one needs to normalize the DFT by $N$. Hence, the normalized DC value for \figl{leak_picket_ex3} is $X_w[0]/N = A$. Similarly, when $\dw_c \ne 0$ as in \figl{leak_picket_ex1} and \figl{leak_picket_ex2}, the two DTFT peaks at $\dw = \pm \dw_c$ have amplitude $A N / 2$ that lead to an amplitude of $A/2$ for each after division by $N$. 
In summary, as already discussed in the context of \equl{dft_as_dtfs}, the estimate of amplitudes from a FFT result requires normalizing the absolute values $X_w[k]$ by $N$.

As indicated in \codl{snip_frequency_fftCosineExample}, the \emph{scalloping loss}\index{Scalloping loss} $S$ for a
windowed sinusoid with amplitude $A$ when a rectangular window and an $N$-points FFT are used, can be obtained with:
\begin{equation}
S = N \frac{A}{2} - \max \{ |X_w[k]| \},
\label{eq:scalloping_loss}
\end{equation}
where $\max \{ |X_w[k]|$ is the magnitude corresponding to the FFT bin center that best represents the windowed sinusoid $x_w[n]$ (its strongest FFT component). 

%\equl{scalloping_loss} assumes a rectangular window while Section~\ref{sec:sin_amplitude} discusses scalloping in the case of a generic window.

%To illustrate \figl{leak_picket_ex4} was obtained via
%\ci{snip\_frequency\_fftCosineExample(32, 8.3)} to execute 
%\codl{snip_frequency_fftCosineExample}.
%\figl{leak_picket_ex4} 
%compares the magnitudes of the DTFT and the $N=32$-points DFT for the signal $x[n] = \cos( \dw_c n)$, with $\dw_c = (2 \pi \alpha) / N$ and $\alpha=8.3$. This windowed cosine is not bin-centered and the leakage is visible through the ``picket-fence'' corresponding to the DFT bins. The cosine has an angular frequency $\dw_c=1.6297$~rad and the closest FFT bin is $k'=\textrm{round}(\alpha)=8$. In this case, the DFT magnitude $|X_w[k']|=84.77$ at $k'=8$ would lead to an estimated frequency $\hat{\dw}_c = 2\pi k'/N = 1.5708$~rad. The DTFT magnitude peak at $\dw_c=1.6297$~rad is $A N/2= 6 \times 32 / 2 = 96$, such that the scalloping loss is $96 - 84.77=11.23$.
%This is a result obtained with the rectangular window. 
%Section~\ref{sec:sin_amplitude} discusses scalloping in the case of a generic window.
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=\figwidthSmall]{./Figures/leak_picket_ex4}
%\caption{Version of \figl{leak_picket_ex1} for a not bin-centered $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=8.3$ and $N=32$ using a rectangular window.\label{fig:leak_picket_ex4}}
%\end{figure}
%
%If one varies $\alpha$ in \codl{snip_frequency_fftCosineExample} to $\alpha=8$ and $8.5$, the maximum DFT amplitudes at $k'=8$ are $|X_w[k']|=96$ and 61.51, respectively. Therefore, the corresponding scalloping losses are 0 (the minimum possible value) and $96-61.51=34.49$. Observe that when estimating the cosine amplitude, one would divide the FFT peak amplitude by $N$. For $\alpha=8.5$, this normalization would lead to an estimate of $2 \times 61.51/32 \approx 3.84$~volts instead of the correct value of $A=6$~V. This indicates how deleterious the combination of leakage and picket-fence can be with respect to errors of the estimated frequency
%of a sinusoid and also its amplitude.

\subsection{{\akadvanced} Only a bin-centered sinusoid leads to an FFT without visible leakage}

Leakage and picket-fence make harder to use FFT for analyzing eternal sinusoids.
In spite of that, the FFT is a flexible tool and its user needs to choose the proper
interpretation of its results.
For instance, one perspective from the FFT of a windowed signal $x_w[n]$ is that the basis
functions of the DFT have a finite duration of $N$ samples and can only represent a finite-duration $x_w[n]$.
In this case, the FFT user is not concerned with signal periodicity.
Another perspective is to consider that the FFT is estimating the DTFS coefficients and
$x_w[n]$ is a single period with $N$ samples of a periodic and eternal signal.
In any case, it is not always possible to conclude about the periodicity and
frequency components of the infinite duration $x[n]$ by observing only its windowed version
$x_w[n]$.

%Recall that a discrete-time sinusoid is periodic only if \equl{sinusoidPeriodicityCondition} is valid. Therefore, when studying the spectral analysis of sinusoid, it is useful to adopt a notation that can be explicitly mapped to the condition imposed by \equl{sinusoidPeriodicityCondition}.

%The digital frequency of a sinusoid will be denoted by $\dw_c = \frac{2 \pi}{N} \alpha$, where $N$ is the FFT size (here it is not the signal period) and $\alpha \in \Re$ allows to obtain any desired value for $\dw_c$. 
Recall that, to check the periodicity of $x[n]$, one can use \equl{sinusoidPeriodicityCondition} and write
\begin{equation}
\frac{m}{\Nperiod} = \frac{\dw_c}{2 \pi } = \frac{\frac{2 \pi}{N} \alpha}{2 \pi} = \frac{\alpha}{N},
\label{eq:sinusoid_alternative}
\end{equation}
which is a ratio $m/\Nperiod$ of two integers when $\alpha/N$ is a rational number ($\alpha/N \in \rationals$). 
In case $\alpha$ is an integer, then $\alpha/N$ is a rational number. But when $\alpha = 2.5$ and $N=10$, we still
have $\alpha/N = 1/4$ rational. Hence, $x[n]$ can be periodic even if $\alpha$ is not an integer. But $x[n]$
will be bin-centered, only if $\alpha$ is an integer. This is illustrated in the top plots of \figl{leak_picket_ex5}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/leak_picket_ex5}
\caption[Comparing FFT results for non-periodic and a periodic but not bin-centered signals.]{Top plots are obtained from the periodic but not bin-centered $x[n]=6\cos((\alpha 2 \pi /N) n)$, with $\alpha=2.5$ and its DTFT and FFT with a rectangular window and $N=10$. Bottom plots correspond to the non-periodic $x[n]$ with $\alpha=\pi$.\label{fig:leak_picket_ex5}}
\end{figure}

The bottom plots of \figl{leak_picket_ex5} were obtained from a non-periodic signal $x[n]=6\cos((\pi^2 / 5) n)$, but the DFT 
result presents a stronger peak than its top plot counterpart. By observing only the top DFT, one could be confused
whether $x[n]$ is composed of one or two sinusoids. On the other hand, observing the bottom DFT could lead to
concluding that $x[n]$ is periodic, while strictly it is not.
The idea of \figl{leak_picket_ex5} is that care must be exercised when interpreting FFT results.

Another couple of examples is provided. Assume now that the signal of interest is $x[n] = \cos( (100 \pi / 256) n)$, with $\alpha=50$ and $N=256$, where $N$ is the adopted length of the rectangular window and FFT size. In this special case, in which $\alpha \in \integers$, the fundamental period is $\Nperiod = 128$ samples, given that simplifying the fraction $\alpha / N$ leads to $m/\Nperiod = 25/128$, and this cosine is represented by the FFT bins $k=50$ and $k=256-50=206$. The DTFT and FFT magnitudes of $x[n]$ are depicted at the left plot in \figl{leak_picket_ex6}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/leak_picket_ex6}
\caption{Comparison of DTFT and DFT for $x[n]=6\cos((\alpha 2 \pi /N) n)$ with $\alpha=50$ (left) and $\alpha=30.5$ (right) $N=256$ using a rectangular window.\label{fig:leak_picket_ex6}}
\end{figure}

The right plot in \figl{leak_picket_ex6} used the signal $x[n] = \cos( (61 \pi / 256) n)$, still with a window and FFT size $N=256$. In this case, $\alpha = 30.5$ and $\alpha \notin \integers$, but $\alpha/N \in \rationals$ and can be written as a ratio $30.5/256 = 61/512$ of two integers $m=61$ and $\Nperiod=512$. The signal $x[n]$ has a period of $\Nperiod=512$ samples but only $N=256$ of its samples are being analyzed via the FFT. In this case, the FFT does not ``see'' a pure sinusoid. The component of $x[n]$ corresponding to the positive frequency will reside between the bins $k=30$ and $k=31$, while the negative-frequency component will be located between bins $k=224$ and 225. Leakage will occur for both components and will be more visible in their neighbor bins.

\subsection{Summarizing leakage and picket-fence effect}

In previous paragraphs, we observed that reduced resolution and leakage are the two primary effects on the spectrum as a result of applying a window to the signal. The resolution of the \emph{overall spectral analysis} is primarily influenced by the width of the main lobe of the window, which is inversely proportional to the window (and consequently signal) duration. The degree of leakage depends on aspects of the window DTFT such as the relative amplitude of the main lobe with respect to the amplitudes of the sidelobes.

An FFT-based spectral analysis resolution also depends on the \emph{FFT resolution} $\Delta f = \fs / \nfft$ as described by \equl{fft_freq_resolution}, with $\nfft$ being the number of FFT points. Given a windowed signal $x_w[n]$ with duration of $N$ samples, the FFT resolution can always be improved via zero-padding and adopting $\nfft > N$.

In summary, there are two artifacts when an FFT is used for spectral analysis:
\begin{itemize}
\item Leakage, which becomes stronger when the window duration is made shorter (or equivalently, $N$ made smaller)
\item FFT resolution $\Delta f$, which can be improved by increasing the number $\nfft$ of FFT points and using zero-padding.
\end{itemize}
These two effects appear in a combined way in FFT-based spectral analysis. And while $\Delta f = \fs / \nfft$ improves as $\nfft$ gets larger, this \emph{cannot recover the leakage that occurred due to windowing}.

These effects appear in a combined way in FFT-based spectral analysis, such as in
%In fact, due to the leakage, a given spectral component will contribute to the output at another frequency ?0, according to the gain of the window centered at the cosine frequency and measured at ?0.
Application~\ref{app:spectral_leakage}. 



\subsection{Example of using windows in spectral analysis} 

To illustrate the importance of windows other than the rectangular, two distinct signals, $x_1[n]$ and $x_2[n]$, each composed by a stronger and a weaker sinusoids, will have their spectra estimated using a FFT with $N=256$ points:
\begin{itemize}
\item Case 1: the stronger sinusoid is bin-centered - The first sequence is
\[
x_1[n] = 1 \cos( (64\pi/256) n + \pi/3) + 100 \cos( (76\pi/256) n + \pi/4).
\]
%$x_1[n]$ is a sum of a cosine of amplitude 1 Volt, frequency $\dw=64\pi/256$ rads and phase $\pi/3$ rads, and a cosine of amplitude 100 volts, frequency $76\pi/256$ rads and phase $\pi/4$ rads. 
The idea is that the weaker cosine (of amplitude 1 V) is located in the center of the FFT bin $k=32$, and the stronger is located in the center of bin 38. Having a bin-centered sinusoid corresponds to choosing the FFT size coinciding with a multiple of the sinusoid period.

\item Case 2: the stronger sinusoid is not bin-centered - The second sequence is
\[
x_2[n] = 1 \cos( (64\pi/256) n + \pi/3) + 100 \cos( (75\pi/256) n + \pi/4).
\]
%$x_2[n]$ is a sum of a cosine of amplitude 1 Volt, frequency $\dw=64\pi/256$ rads and phase equal to $\pi/3$ rads, and a cosine of amplitude 100 volts, frequency equal to $75\pi/256$ rads and phase equal to $\pi/4$ rads. 
In this case, the weaker cosine is located in the center of bin 32 but the stronger is located in the border of bins 37 and 38.
\end{itemize}
In this example the goal is to distinguish the sinusoids and estimate their frequencies. As in many applications of spectral analysis, the amplitudes are irrelevant and what is taken into account is the relative difference between the signal components in frequency domain. For both $x_1[n]$ and $x_2[n]$, the difference in power between the sinusoids is $20 \log_{10}(100/1) = 40$~dB. These sequences can be generated as in \codl{snip_frequency_sequence_generation}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_sequence\_generation}{snip_frequency_sequence_generation}
%\begin{lstlisting}
%N=256; %number of samples available of x1 and x2
%n=0:N-1; %abscissa
%kweak=32; %FFT bin where the weak cosine is located
%kstrong1=38; %FFT bin for strong cosine in x1
%weakSigal = 1*cos((2*pi*kweak/N)*n+pi/3); %common parcel
%x1=100*cos((2*pi*kstrong1/N)*n+pi/4) + weakSigal; %x1[n]
%kstrong2=37.5; %location for strong cosine in x2
%x2=100*cos((2*pi*kstrong2/N)*n+pi/4) + weakSigal; %x2[n]
%\end{lstlisting}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/windowsHarmonicAnalysis}
\caption{Comparison of spectra obtained with four windows in case both sinusoids are bin-centered (left plots) and not (right plots).\label{fig:windowsHarmonicAnalysis}}
\end{figure}

The commands that design the windows to pre-multiply \ci{x1} and \ci{x2} in \codl{snip_frequency_sequence_generation}, before calling the FFT, are given in \codl{snip_frequency_windows}.

The results with four windows from \codl{snip_frequency_windows} are shown in \figl{windowsHarmonicAnalysis}. The flat top window was not used due to its poor frequency resolution.
For example, the spectrum in \figl{windowsHarmonicAnalysis} for $x_2[n]$ (called \ci{x2} in the code) using Hamming was obtained with \codl{snip_frequency_analysis}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_analysis}{snip_frequency_analysis}
%\begin{lstlisting}
%dw=(2*pi)/N; %DFT spacing in rads
%w=-pi:dw:pi-dw; %abscissa for plots
%x=x2.*hamming(N)'; %perform windowing
%factor=max(abs(fft(x))); %normalization factor
%plot(w,fftshift(20*log10(abs(fft(x)/factor))));
%\end{lstlisting}
The graphs at the left column in \figl{windowsHarmonicAnalysis} correspond to estimated spectra of $x_1[n]$ (both sinusoids are bin-centered). For $x_1[n]$, it can be seen that the best result was obtained by a rectangular window while the other windows led to spectra with leakage (spurious power) near the cosines.
Based solely on the results obtained for case 1, one could erroneously concluded that the rectangular window is always the best. However, as shown in the top-right graph, when the strong cosine is not bin-centered, the rectangular window miserably fails to detect the weaker cosine.

In \figl{windowsHarmonicAnalysis}, the Hann window allows a marginal detection of the weaker cosine, while the Hamming window allows the detection of both cosines but contaminates the whole spectrum with spurious power along frequency just 50~dB below the power level of the strongest sinusoid. This is a consequence of the reduced side-lobe fall-off for the Hamming window, as indicated in \figl{windowsFreqDomainNormalized}. The Kaiser window achieves the best result when one takes in account both cases (bin-centered and not).

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/windowsKaiserNotCentered}
\caption{Individual spectra of the two sinusoids superimposed obtained using the Kaiser window.\label{fig:windowsKaiserNotCentered}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/windowsRectangularNotCentered}
\caption{Individual spectra of the two sinusoids superimposed, obtained using the rectangular window.\label{fig:windowsRectangularNotCentered}}
\end{figure}

To complement the analysis of estimating the spectrum of $x_2[n]$ (the stronger sinusoid is not bin centered),
\figl{windowsKaiserNotCentered} and \figl{windowsRectangularNotCentered} show the individual spectra of the two sinusoids superimposed, obtained using the Kaiser and rectangular windows, respectively. Note that,  \figl{windowsHarmonicAnalysis} shows exactly the combined effect of these individual spectra. \figl{windowsRectangularNotCentered} clearly shows that the significant leakage of the stronger cosine caused by the rectangular window, completely ``masks'' the weaker cosine. 
%In contrast, \figl{windowsKaiserNotCentered} shows that the convolution with the Kaiser window in frequency domain does not mask the weaker cosine.

Note that this was a specific application. For example, features such as side-lobe fall-off are important in other situations. A very general conclusion is to always use a window other than the rectangular when performing spectral analysis of a signal that is not guaranteed to have all components bin-centered. 


\subsubsection{When a DTFT has impulses, does this create a problem for an FFT-based analysis?}
\label{sec:fft_of_impulses}
A DTFT can have impulses such as, for example, the DTFT of a sinusoid.
One question that may arise then is: \emph{-If the FFT simply performs sampling of the DTFT in the frequency domain, what is the FFT value when exactly sampling an impulse? Should this value go to infinite?}
The answer requires noting that the impulse in the DTFT appears because $x[n]$ is an infinite-long signal (e.\,g., an eternal sinusoid). But an FFT has to be applied to a signal with a finite number $N$ of samples. Therefore, the FFT is always applied to a windowed signal $x_w[n]$, with a DTFT $X_w(e^{j \dw})$ that does not have impulses.

After this introduction to windowing, the next sections discuss three important functions in spectral analysis.

\section{The ESD, PSD and MS Spectrum functions}

While the PSD $\calP(f)$ targets \emph{power} signals, the ESD $\calG(f)$ is a convenient tool for analyzing \emph{energy} signals, informing how the total energy $E$ is distributed over frequency. Both are ``density'' functions, with the independent variable $f \in \Re$. The mean-square (MS) spectrum $S_{\textrm{ms}}[k]$ is a discrete-frequency function with the independent variable $k \in \integers$. We will discuss each of the three functions in this section.
 
\subsection{Energy spectral density (ESD)}
\label{sec:esd}

To understand the ESD definition, it is useful to recall the Parseval theorem.
The (Parseval) Theorem~\ref{th:parseval} %(at page \pageref{th:parseval}) 
for block transforms can be extended to continuous and discrete-time signals. For continuous-time, together with \equl{signal_energy}, one has
\begin{equation}
E = \int_{-\infty}^{\infty} |x(t)|^2 \textrm{d}t = \int_{-\infty}^{\infty} |X(f)|^2 \textrm{d}f,
\label{eq:parseval_ct}
\end{equation}
stating that the energy is the same in both time and frequency domains. This suggests defining
\begin{equation}
\calG(f) = |X(f)|^2
\label{eq:esd_definition}
\end{equation}
 as the ESD in continuous-time. Therefore, $\calG(f)$ describes how the signal energy $E$ is spread over frequency and its integral is the energy $E$. The adopted unit for the ESD is joules/Hz.

\bExample \textbf{ESD of a real-valued exponential}.
Consider the energy signal $x(t)= e^{-at}u(t)$ in volts, where $a=0.9$. Its Fourier transform is $X(f) = 1/(j2 \pi f + a)$ and the ESD in joules/Hz is
\[
\calG(f) = |X(f)|^2 = \frac{1}{(2 \pi f)^2 + a^2}.
\]
The signal energy is $E = \int_{-\infty}^{\infty} |x(t)|^2 \textrm{d}t = \int_{-\infty}^{\infty} \calG(f) \textrm{d}f \approx 0.55556$.
\codl{snip_frequency_exp_esd} illustrates how $E$ can be estimated in time-domain and also using $\calG(f)$.
In this case, the range $]-\infty,\infty[$ is simulated as $[0,30]$~s and $[-1000,1000]$~Hz in time and frequency-domain, respectively.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_frequency\_exp\_esd}{snip_frequency_exp_esd}

\codl{snip_frequency_exp_esd} also illustrates how $\calG(f)$ can be conveniently used to estimate the energy within a given frequency band. In this example, the energy within the range from 0.2 to 1~Hz, considering both negative and positive frequencies, is
$E_r = 2 \int_{0.2}^{1} \calG(f) \textrm{d}f \approx 0.16954$~J.
\eExample 

\bExample \textbf{ESD of a sinc in time-domain}.
\label{ex:sinc-time}
Now consider the energy signal is $x(t)= 400 \sinc(100 t)$ in volts. According to \equl{sinc_time_transform_Hz}, one
needs to consider that $2F=100$, such that $F=50$~Hz. With this assumption, \equl{sinc_time_transform_Hz} can be used together with
the linearity property of the Fourier transform. The linearity takes care of a factor of 4
that multiplies \equl{sinc_time_transform_Hz}, such that one has $X(f) = 4$ for $|f| \le 50$~Hz and 0 otherwise.
Hence, the ESD of $x(t)$ is $\calG(f)=16$ for $|f| \le 50$~Hz and 0 otherwise. In this case, using the ESD instead of $x(t)$, it is easy to conclude that the total signal energy is $E = 16 \times 100 = 1600$~joules.
\eExample 

\subsection{{\akadvanced} Units of ESD when angular frequencies are adopted}
\label{sec:digital_Hz}

Similar to the discussion in Section~\ref{sec:ctft_in_rads} regarding the Fourier transform $X(\aw)$ with $\aw$ in rad/s,
there is a subtle issue when using radians per second instead of Hertz for the ESD. In this case, the factor
$2 \pi$ needs to be taken in account such that $\calG(\aw)/(2\pi)=|X(\aw)|^2/(2\pi)$ has the unit of joules/(rad/s).
The energy over a band of frequencies specified in rad/s can then be calculated by integrating $|X(\aw)|^2/(2\pi)$ over this
band. For instance, considering a band $\aw \in [-\infty, \infty]$ the energy $E$ is:
\begin{equation}
E = \int_{-\infty}^{\infty} |x(t)|^2 \textrm{d}t = \frac{1}{2 \pi}\int_{-\infty}^{\infty} |X(\aw)|^2 \textrm{d}\aw = \frac{1}{2 \pi}\int_{-\infty}^{\infty} \calG(\aw) \textrm{d}\aw,
\label{eq:parseval_ct_rads}
\end{equation}
where $\calG(\aw) = \calG(2\pi f)$ and the unit of $\calG(\aw)/(2\pi)$ is joules/(rad/s). 

\bExample \textbf{The unit of $\calG(\aw)/(2 \pi)$ is joules/(rad/s)}. 
\label{ex:units_of_esd_psd}
For instance, take the sinc signal of 
\exal{sinc-time}, which has a flat ESD given by 
$\calG(f)=16$~J/Hz within the 
range $[-50,50]$~Hz and zero otherwise. The representation $\calG(\aw)$ of this ESD in 
rad/s has the same constant value $\calG(\aw)=16$ but now over the range $[-100\pi,100\pi]$~rad/s. 
The value $\calG(\aw)/(2 \pi)=16/(2 \pi)$ can be interpreted as a density in J/(rad/s). 
Within the range $[-100\pi,100\pi]$, one has $(16/(2 \pi)) \times 200 \pi = 1600$~J.
\eExample

%It is then more adequate to interpret that 
%$\calG(\aw)$ provides the same numerical values of $\calG(f)$ in J/Hz, but when $\calG(\aw)$  is shown in a plot or integrated, its abscissa range 
%is enlarged by a factor of $2 \pi$ to indicate angular frequency $\aw = 2 \pi f$.

%One could eventually adopt a definition $\tilde \calG(\aw)= \frac{1}{2 \pi}|X(\aw)|^2$  that incorporates the $1/(2\pi)$ factor such that $\tilde \calG(\aw)$ would be given in J/(rad/s). But this would break the connection between $\calG(f)$ and $\calG(\aw)$ and is not adopted. This reasoning is also applied when dealing with the power spectral density.


\emph{In continuous-time spectral analysis, the linear frequency in Hertz is more convenient and angular frequencies are seldom adopted}. But this is \emph{not} an option in discrete-time processing, given that $\dw$ is an angle (assumed in radians) and does not have a counterpart in Hertz. Hence, 
the ESD in discrete-time 
\begin{equation}
\calG(e^{j \dw})= |X(e^{j \dw})|^2
%\label{eq:}
\end{equation}
needs to be normalized by the factor $2 \pi$ to be interpreted as $\calG(e^{j \dw})/(2 \pi)$ in units of joules per radians.
Using this definition, one can properly interpret a version of the Parseval theorem in discrete-time:
\begin{equation}
E = \sum_{n=-\infty}^{\infty} |x[n]|^2 = \frac{1}{2 \pi}\int_{<2 \pi>} |X(e^{j \dw})|^2 \textrm{d}\dw.
\label{eq:parseval_dt}
\end{equation}

%\bExample \textbf{Creation of digital frequency Dhertz (DHz) such that the unit of $\calG(e^{j \dw})$ is joules/DHz}. 
%\label{ex:units_of_esd_discrete_time}
%Assume a \emph{normalized digital frequency} $\digif$ defined as
%\begin{equation}

%\digif \defeq \frac{\dw}{2 \pi},
%\label{eq:dhertz_definition}
%\end{equation}
%where $\dw$ is the discrete-time angular frequency in radians.
%The  digital linear frequency $\digif$ is also an angle but we will interpret its unit as Dhertz (Dhz), the \emph{digital Hertz}. 
%The continuous-time Hertz can be seen as the number of occurrences in one second, while Dhertz is interpreted as
%the number of occurrences between two neighboring discrete-time samples $n$ and $n+1$.
%Similar to $\aw_0 = 2\pi f_0 = 2 \pi /T_0$, the discrete-time versions are $\dw_0 = 2\pi \digif_0 = 2 \pi /\Nperiod$.
%
%An analogy follows. The continuous-time signal $x(t)$ with fundamental period $T_0 = 4$~s has frequency $f_0 = 1/T_0 = 0.25$~Hz. 
%The interpretation is that in one second, there are 0.25 occurrences of a period of $x(t)$.
%A discrete-time signal $x[n]$ with fundamental period $\Nperiod = 4$ samples has frequency $\digif = 1/\Nperiod = 0.25$~Dhz. 
%The interpretation is that between the occurrence of two consecutive samples of $x[n]$, there are 0.25 occurrences of its period.
%Instead of Dhz, {\matlab} uses the notation ``\emph{per sample}''.
%
%Using \equl{dhertz_definition}, one can have an ESD $\calG(e^{j \digif})$ with unit joules / Dhertz and the following property:
%\begin{equation}
%E = \int_{0}^{1}  \calG(e^{j \digif}) \textrm{d}\digif.
%\end{equation}
%Similar to what was discussed in \exal{units_of_esd_psd}, the units of both $\calG(e^{j \digif})$ and $\calG(e^{j \dw})$ is joules / Dhertz, with the abcissas being $\digif$ in Dhz and $\dw$ in radians, respectively.
%\eExample

\tabl{esd_functions} summarizes the discussed ESD functions.

\begin{table}
\centering
\caption{ESD functions. $E$ is the total energy and the column ``Variable'' indicates the units and symbols used for the independent variable of each function.
The units of $\calG(f)$, $\calG(\aw)/(2 \pi)$ and $\calG(e^{j\dw})/(2 \pi)$ are J/Hz, J/(rad/s) and J/rad, respectively.\label{tab:esd_functions}}
\begin{tabular}{|l|c|c|c|c|}
\hline
Time & Variable & ESD definition & ESD main property \\ \hline
Continuous-time & $f$ (Hz) & $\calG(f) = |X(f)|^2$ & $E = \int_{-\infty}^{\infty} \calG(f) \textrm{d}f$  \\ \hline
Continuous-time & $\aw$ (rad/s) & $\calG(\aw) = |X(\aw)|^2$ & $E=\frac{1}{2 \pi}\int_{-\infty}^{\infty} \calG(\aw) \textrm{d}\aw$  \\ \hline
Discrete-time & $\dw$ (rad) & $\calG(e^{j\dw}) = |X(e^{j\dw})|^2$ & $E=\frac{1}{2 \pi}\int_{<2 \pi>} \calG(e^{j\dw}) \textrm{d}\dw$  \\ \hline
\end{tabular}
\end{table}


\subsection{Power spectral density (PSD)}
\label{sec:psd}

In most cases, the signal under analysis has infinite energy, such as a deterministic power signal (e.\,g. a periodic signal) or realizations of a stationary random process. Therefore, the main interest in spectral analysis relies not on the ESD but on the PSD.

\subsubsection{Main property of a PSD}

Noticing from \equl{parseval_ct} and (\ref{eq:parseval_dt}) that squared magnitudes provide the energy distribution over frequency, to obtain the power distribution one can intuitively consider dividing the ESD by ``time'', via a normalization factor that converts energy into power.

The PSD 
%is a frequency-domain function with the 
has the important property that the average power $\calP$ can be obtained in continous-time by
\begin{equation}
\calP_c = \int_{- \infty}^\infty S(f) \textrm{d}f,
\label{eq:psd_integration_continuousTime}
\end{equation}
with $S(f)$ in watts/Hz.

Similar to the reasoning associated to \equl{parseval_ct_rads}, the version of \equl{psd_integration_continuousTime} when the continuous-time PSD $S(\aw)$ is a function of $\aw = 2 \pi f$ in radians/s is
\begin{equation}
\calP_c = \frac{1}{2\pi} \int_{- \infty}^\infty S(\aw) \textrm{d}\aw,
\label{eq:psd_integration_continuousTime_rads}
\end{equation}
and $S(\aw)/(2 \pi)$ is interpreted in units of watts per rad/s.

The PSD definition in discrete-time is
\begin{equation}
\calP_d = \frac{1}{2 \pi}\int_{<2 \pi>} S(e^{j \dw}) \textrm{d}\dw,
\label{eq:psd_integration}
\end{equation}
where $S(e^{j \dw})/(2 \pi)$ is interpreted in units of watts per radians.

%\subsubsection{{\akadvanced} Discrete-time PSD in digital Hz}
%If one insists in using a version of \equl{psd_integration_continuousTime} in discrete-time but not adopting 
%the angular frequency $\dw$ of \equl{psd_integration}, it is possible to use
%\begin{equation}
%\calP_d = \int_{<1>} S(e^{j \digif}) \textrm{d}\digif,
%\label{eq:psd_integral_dhertz}
%\end{equation}
%with $S(e^{j \digif})$ in watts/Dhz (or watts/``normalized frequency'') as described in Section~\ref{sec:digital_Hz}.

In all three equations above, the PSDs describe the distribution of power over frequency. The frequency can be linear $f$ in Hz, angular frequency $\aw$ 
in radians/s or discrete-time angular frequency $\dw$, which is an angle in radians.

%\subsection{Periodogram for discrete-time signals}

%\subsection{PSD for continuous-time signals}

\subsubsection{PSD definitions and the focus on random signals}
As discussed in Section~\ref{sec:signal_categorization}, there are many categories of signals.
\figl{signal_categories} illustrates some of them.
Different categories of signals require distinct definitions of PSD.
To simplify the discussion, some spectral analysis textbooks\footnote{For instance, see \cite{Hayes2009,Stoica05}.} choose to emphasize discrete-time random signals, 
and define the PSD for this kind of signals.
Moreover, the \emph{spectral estimation problem} is defined in these textbooks\footnote{See \cite{Stoica05}, page 13.} as:
\begin{itemize}
\item From a finite-length record $\{y[1], \ldots, y[N] \}$ of a second-order stationary random
process, find an estimate $\hat S(e^{j \dw})$ of its power spectral density $S(e^{j \dw})$ for $\dw \in [-\pi, \pi]$.
\end{itemize}
In other words, the context of this problem is restricted to discrete-time power signals with duration of $N$ samples.
Here we also discuss other cases, such as: a) continuous-time PSDs, b) when the signal is deterministic
and c) the theoretical PSD expressions for infinite-duration signals.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./FiguresNonScript/signal_categories}
\caption{Important categories of signals. Some of these features need to be taken in account when
defining the PSD function.\label{fig:signal_categories}}
\end{figure}



%\subsubsection{{\akadvanced} PSD of Random Signal}
It makes complete sense to focus the study of PSD estimation on random signals, because they are important in many applications, such as in digital communications. One useful model for these signals is the \emph{wide-sense stationary} (WSS) random process with autocorrelation $R(\tau)$ (see definitions in Appendix~\ref{app:stochasticprocesses}). In many spectral analysis problems,\footnote{See \cite{Hayes2009}, page 391, Chapter 8.} besides being WSS, the random process 
is assumed to be \emph{autocorrelation ergodic}.

The PSD $S(f)$ for a continuous-time power signal $x(t)$ corresponding to a realization of a WSS stochastic process $\calX(t)$ is defined as
\begin{equation}
S(f) \defeq \lim_{T \rightarrow \infty} \frac{\ev[|X_T(f)|^2]}{T},
\label{eq:psd_continuoustime}
\end{equation}
where $X_T(f)$ is the Fourier transform of a truncated (windowed) version of $x(t)$ with duration $T$. In other words, $X_T(f) = \calF \{ x_T(t) \}$ where $x_T(t)=x(t)$ for $-T/2 \le t \le T/2$ or zero otherwise.

An alternative to obtaining a PSD via \equl{psd_continuoustime} is using the Wiener-Khinchin theorem\index{Wiener-Khinchin theorem},\footnote{See, e.\,g. \akurl{http://en.wikipedia.org/wiki/Wiener-Khinchin_theorem}{3wie} and note that a realization $\rsx(t)$ of a WSS process $\calX(t)$ is not square integrable and does not have a Fourier transform.} %of \equl{Wiener-Khinchin} 
which states that the power spectral density (PSD) $S(f)$ of a WSS process is the Fourier transform of the corresponding autocorrelation function, i.\,e.
\begin{equation}
S(f) = \calF \{ R(\tau) \}.
\label{eq:auto_psd_continuous}
\end{equation}
Similar expressions exist in discrete-time and even for deterministic signals.

For example, in discrete-time processing, the PSD is the DTFT of the autocorrelation function:
\begin{equation}
S(e^{j \dw}) = \sum_{\ell=-\infty}^\infty R[\ell] e^{-j \dw \ell},
\label{eq:discrete_psd}
\end{equation}
where $\ell \in \integers$ is the lag, and the inverse DTFT gives
\begin{equation}
R[\ell] =  \frac{1}{2 \pi} \int_{-\pi}^{\pi}  S(e^{j \dw}) e^{j \dw \ell} \textrm{d}\dw.
\label{eq:inverse_discrete_psd}
\end{equation}

%\subsection{PSD for discrete-time signals}

Similar to \equl{psd_continuoustime}, the PSD $S(e^{j \dw})$ for a discrete-time power signal $x[n]$ corresponding to a realization of a WSS stochastic process $\calX[n]$ is defined as
\begin{equation}
S(e^{j \dw}) \defeq \lim_{N \rightarrow \infty} \frac 1 N \ev \left[ \left| \sum_{n=0}^{N-1} x[n] e^{-j \dw n} \right|^2  \right] = \lim_{N \rightarrow \infty} \frac 1 N \ev \left[ \left| X_N(e^{j \dw}) \right|^2  \right].
\label{eq:psd_discretetime}
\end{equation}
where $X_N(e^{j \dw})$ is the DTFT of $x_N[n]$, a truncated version of $x[n]$ obtained via a rectangular window of $N$ non-zero samples.
The unit of $S(e^{j \dw})/(2 \pi)$ is watts per radians.
%, due to the division by $2 \pi$ in \equl{psd_discretetime}.
%requires careful interpretation creates an issue regarding the unit of a discrete-time PSD $S(e^{j \dw})$.
%If there was no divisi
%If this unit is assumed to be W/rad, the integration of the PSD is equal to $2 \pi \calP$, where $\calP$ is the total signal power. In order to have the property that integration of a PSD leads to the total power, the unit of $S(e^{j \dw})$ should be interpreted as watts per normalized frequency $f' = \dw / (2\pi)$. The option here is to adopt W/rad as the unit of $S(e^{j \dw})$, but keep in mind that its integration needs to be normalized by $2 \pi$ to generate $\calP$.
%it is not consistent with Viberg_Complement on Digital Spectral Analysis.pdf, page 2
%He says it's W / f, where f = w/2pi.

In practice, there is a finite number of realizations of $\calX[n]$ and often only one realization $x[n]$ is available. Fortunately, \emph{ergodicity} of the autocorrelation can be assumed in many cases and the ensemble averages substituted by averages taken over time (see Appendix~\ref{app:probability}). Besides, the number of samples (the duration of $x[n]$) is often limited to a given value that can be determined, for example, by the time over which the process can be considered stationary. For example, in speech analysis applications, it is typically assumed the process of vowel production is quasi-stationary over segments with durations from 40 to 80~ms. With limited-duration signals, the challenge for spectral analysis is to obtain accurate estimates, as discussed in this chapter.
%~\ref{ch:frequency}.

For both \equl{psd_continuoustime} and \equl{psd_discretetime}, 
windows other than the rectangular can be used when the signal under analysis has a short duration. But a rectangular window
with infinite duration is the adequate window for the PSD definition.

\tabl{psd_functions} summarizes the discussed PSD functions.

\begin{table}
\centering
\caption{PSD functions. $\calP$ is the average power and the column ``Variable'' indicates the units and symbols used for the independent variable of each function.
$\calF_f$ and $\calF_{\aw}$ denote the Fourier transform in Hertz and rad/s, respectively.
The units of $S(f)$, $S(\aw)/(2 \pi)$ and $S(e^{j\dw})/(2 \pi)$ are W/Hz, W/(rad/s) and W/rad, respectively.\label{tab:psd_functions}}
\begin{tabular}{|l|c|c|c|c|}
\hline
Time & Variable & Definition & Main property \\ \hline
Continuous-time & $f$ (Hz) & $S(f) = \calF_f \{ R(\tau) \}$ & $\calP = \int_{- \infty}^\infty S(f) \textrm{d}f$  \\ \hline
Continuous-time & $\aw$ (rad/s) & $S(\aw) = \calF_{\aw} \{ R(\tau) \}$ & $\calP = \frac{1}{2\pi} \int_{- \infty}^\infty S(\aw) \textrm{d}\aw$  \\ \hline
Discrete-time & $\dw$ (rad) & $S(e^{j \dw}) = \calF \{ R[\ell] \}$ & $\calP = \frac{1}{2 \pi}\int_{<2 \pi>} S(e^{j \dw}) \textrm{d}\dw$  \\ \hline
\end{tabular}
\end{table}

\subsubsection{PSD of deterministic and periodic signals}
\label{sec:psdPeriodicSignals}

The PSD definition that is adopted for continuous-time deterministic signals (and does not require the expected value used in \equl{psd_continuoustime}) is
\[
S(f) = \lim_{T \rightarrow \infty} \frac{|X_T(f)|^2}{T},
\]
which is basically the ESD normalized by the time interval $T$.

A special case of deterministic signals are the periodic ones.
Assuming a continuous-time signal $x(t)$ with period $T_0$, its PSD is
\begin{equation}
S(f) = \sum_{k=-\infty}^{\infty} |c_k|^2 \delta(f - k F_0),
\label{eq:psdOfPeriodicSignals}
\end{equation}
where $c_k$ are the Fourier Series coefficients and $F_0=1/T_0$ is the fundamental frequency.
%and $S(f)$ is given in atts/Hz. 
Similarly, for $S(\aw)$, the expression is
\begin{equation}
S(\aw) = 2 \pi \sum_{k=-\infty}^{\infty} |c_k|^2 \delta(f - k \aw_0),
%\label{eq:}
\end{equation}
where $\aw_0 = (2 \pi) / T_0$~rad/s.

In summary, the PSD $S(f)$ of a deterministic (non-random) periodic signal is composed by impulses with areas determined by the squared magnitude of Fourier series coefficients.

\bExample \textbf{PSD of a continuous-time sinusoid}.
If $x(t)=A\cos(2 \pi f_c t)$, then 
$S(f) = \frac{A^2}{4}\left[\delta(f+f_c) + \delta(f-f_c) \right]$ and the average power is
$\int_{-\infty}^{\infty} S(f) \textrm{d}f = A^2/2$.
\eExample

When considering a discrete-time periodic signal $x[n]$, its PSD $S(e^{j\dw})$ can be obtained by first considering an expression $S'(e^{j\dw})$ for the frequency range $[0,2\pi[$:
\begin{equation}
S'(e^{j\dw}) = 2 \pi \sum_{k=0}^{\Nperiod-1} |\tilde X[k]|^2 \delta(\dw - k \dw_0),
%\label{eq:}
\end{equation}
where $\Nperiod$ is the period, $\dw_0 = (2 \pi) / \Nperiod$ is the fundamental frequency, and $\tilde X[k]$ the DTFS of $x[n]$. Finally, the PSD 
$S(e^{j\dw})$ is simply the periodic repetition of $S'(e^{j\dw})$:
\begin{equation}
S(e^{j\dw}) = \sum_{p=-\infty}^{\infty} S'(e^{j(\dw+p2\pi)}).
%\label{eq:}
\end{equation}

\bExample \textbf{PSD of a discrete-time sinusoid}.
If $x[n]=A\cos(\dw_1n)$ (assume $\dw_1$ obeys \equl{sinusoidPeriodicityCondition} to have $x[n]$ periodic), then %$\tilde X[k]=A/2$ for $k=\pm 1$ and 0 otherwise, such that 
$S'(e^{j\dw}) = \frac{\pi A^2}{2}\left[\delta(\dw+\dw_1) + \delta(\dw-\dw_1) \right]$ and
\[
S(e^{j\dw}) = \sum_{p=-\infty}^{\infty} \frac{\pi A^2}{2}\left[\delta(\dw+\dw_1+p2\pi) + \delta(\dw-\dw_1+p2\pi) \right]
\]
provides its PSD, which has period $2 \pi$, as expected.
\eExample

\subsection{{\akadvanced} Fourier modulation theorem applied to PSDs}
\label{sec:modulationTheoremPSD}

If $S_x(f)$ is the PSD of a WSS random process $\calX(t)$, the PSD $S_y(f)$ of a new process $\calY(t) = \calX(t) e^{j 2 \pi f_c t}$ is
\begin{equation}
S_y(f) = S_x(f-f_c).
\label{eq:psdComplexExpModulationTheorem}
\end{equation}
Similarly, if 
$\calY(t) = \calX(t) \cos(2 \pi f_c t)$, then
\begin{equation}
S_y(f) = \frac{1}{4} \left[S_x(f+f_c) + S_x(f-f_c) \right].
\label{eq:psdCosineModulationTheorem}
\end{equation}

To observe why \equl{psdComplexExpModulationTheorem} (and, consequently, \equl{psdCosineModulationTheorem}) is true, recall the Wiener-Khinchin theorem of \equl{auto_psd_continuous} and the autocorrelation definition of
\equl{wss_autocorrelation}. Generally speaking, when modifying a WSS process $\calX(t)$, the effect on its PSD can be obtained by checking how the modification affects its autocorrelation, and then relating this to frequency domain using \equl{auto_psd_continuous}. For example, multiplying $\calX(t)$ by a scalar $\alpha$ corresponds to scaling its autocorrelation $R_x(\tau)$ by $\alpha^2$ and leads to a PSD $\alpha^2 S_x(f)$ given the linearity of the Fourier transform.

According to this reasoning, a proof sketch of \equl{psdComplexExpModulationTheorem} follows:
\begin{align*}
S_y(f) &= \calF \{ R_y(\tau) \} \\
 &= \calF \{ \ev[\calY(t+\tau) \calY^*(t)] \} \\
 &= \calF \{ \ev[\calX(t+\tau)e^{j 2 \pi f_c (t+\tau)} \calX^*(t) e^{-j 2 \pi f_c t}] \} \\
 &= \calF \{ e^{j 2 \pi f_c \tau} \ev[\calX(t+\tau) \calX^*(t)] \} \\
 &= \calF \{ e^{j 2 \pi f_c \tau} R_x(\tau) \} \\
 &= S_x(f-f_c).
\end{align*}

\equl{psdCosineModulationTheorem} can be obtained by decomposing the cosine $\cos(2\pi f_c t )=1/2(e^{j2\pi f_c t}+e^{-j2\pi f_c t})$ into two complex exponentials and taking in account that the factor $\alpha=1/2$ leads to the $1/4$ in the PSD expression.
\equl{psdCosineModulationTheorem} allows to observe that the result of a signal multiplied by a cosine of unitary amplitude has half of the original signal power.


\subsection{Mean-square (MS) spectrum}

%\subsection{Normalizing the periodogram to obtain the MS spectrum}

%As explained, $\kappa=1/N$ when the MS spectrum is estimated from $\calG[k]$, which leads to
%\[
%\hat S_{\textrm{ms}}[k]  = \frac{\calG[k]}{N} = \frac{|\textrm{FFT}\{x[n]\}|^2}{N^2}.
%\]

The PSD is very useful especially when dealing with random signals and the power of the signal over a frequency range is obtained by integrating the PSD over that range. 
However, in some cases it is desired to use a function that allows to directly infer the average power of sinusoid components of a periodic signal, without the integration step. In these cases, the so-called MS or power spectrum is more convenient.\footnote{Matlab (but not Octave) has a \ci{msspectrum} spectral estimator and the private functions \ci{welch.m} and \ci{computeperiodogram.m} can be studied.} 
However, in applications characterized by a mixed signal in which there is a deterministic signal of interest that is contaminated by random noise (such as sinusoids contaminated by AWGN), the PSD representation is often more convenient than the MS spectrum.

While in continuous-time the PSD $S(f)$ unit is watts/Hz, the mean-square spectrum is given directly in watts. Assuming a signal with fundamental period $\Nperiod$ samples, the mean-square spectrum corresponds to the squared magnitude of the corresponding DTFS:
\begin{equation}
S_{\textrm{ms}}[k] = |\textrm{DTFS}\{x[n]\}|^2,
\label{eq:msspectrumDef}
\end{equation}
with the property
\begin{equation}
\sum_{k=0}^{\Nperiod-1} S_{\textrm{ms}}[k] = \calP,
\label{eq:periodic_ms_sumPower}
\end{equation}
where $\calP$ is the signal power. %and $N$ is the FFT-size. 

Recall from the discussion associated to \equl{dft_as_dtfs} that, if $x[n]$ is periodic with fundamental period $\Nperiod$, its DTFS $\tilde X[k]$ can be obtained with an $\Nperiod$-point FFT:
\begin{equation}
\tilde X[k] = \frac{\textrm{FFT}\{x[n]\}}{\Nperiod}.
\label{eq:msspectrumViaFFT}
\end{equation}

One often uses \equl{msspectrumViaFFT} even for a non-periodic $x[n]$, but then the result spectrum must be properly interpreted: as if the signal were a periodic version $\sum_{p=-\infty}^{\infty}x_N[n-pN]$ of the windowed version $x_N[n]$ of $x[n]$ using
$N$ samples. 

If the FFT size $N$ is chosen as $N=\Nperiod$, the $k$-th FFT value in \equl{msspectrumViaFFT} corresponds exactly to the $k$-th DTFS coefficient, that is, they both represent the same frequency $k (2 \pi / N) = k (2 \pi / \Nperiod)$. In case $N \ne \Nperiod$, \equl{ms_sumPower} is still a valid way to obtain the average power $\calP$, but the $k$-th bin frequency $\dw_k$ must be interpreted according to the FFT grid as $\dw_k = k (2 \pi / N)$.

In general, an \emph{estimate} $\hat S_{\textrm{ms}}[k]$ of the MS spectrum of a discrete-time signal $x[n]$ can be obtained from its $N$-length windowed version $x_N[n]$ with
\begin{equation}
\hat S_{\textrm{ms}}[k] = \left| \frac{\textrm{FFT}\{x_N[n]\}}{N}\right|^2
\label{eq:msEstimationViaFFT}
\end{equation}
and
\begin{equation}
\sum_{k=0}^{N-1} \hat S_{\textrm{ms}}[k] = \calP.
\label{eq:ms_sumPower}
\end{equation}
The ``hat'' in $\hat S_{\textrm{ms}}[k]$ indicates that in general, \equl{msEstimationViaFFT} is an estimate of the true MS spectrum.

%where it is implicitly assumed that $N = \Nperiod$ in case of a periodic signal.
%In words, the MS spectrum can be estimated by taking the square of the magnitude of a FFT of $N$-points. The DFT should use a factor $1/N$ in the direct transform, such that the DFT values $X[k]$ estimate the coefficients of a continuous-time Fourier series. The MS spectrum is then given by $|X[k]|^2$.

\section{Filtering Random Signals and the Impact on PSDs}

This section investigates the result of processing a random input signal through a system, and how the output
PSD relates to the input.

\subsection{Response of LTI systems to random inputs}
\label{sec:randomLTIInput}

An important result of stochastic processes theory is that when the input of a LTI system is a WSS process $\calX(t)$, the corresponding output is also a WSS process $\calY(t)$, as indicated below
\[
\calX(t) \arrowedbox{h(t)}  \calY(t),
\]
where $h(t)$ is the impulse response of the LTI system. 

Given that $H(f) = \calF \{ h(t) \}$ is the frequency response of the system, the relationship between the PSDs $S_y(f)$ and $S_x(f)$ of $\calY(t)$ and $\calX(t)$, respectively, is given by
\begin{equation}
S_y(f)=|H(f)|^2 S_x(f).
\label{eq:wss_continuous_lti_output_psd}
\end{equation}

Similarly, if a discrete-time WSS random process $\calX[n]$ is filtered by a LTI with impulse response $h[n]$ as in
\[
\calX[n] \arrowedbox{h[n]}  \calY[n],
\]
the PSD of the WSS output process $\calY[n]$ is given by
\begin{equation}
S_y(e^{j \dw}) = |H(e^{j \dw})|^2 S_x(e^{j \dw}),
\label{eq:discrete_filtered_wss}
\end{equation}
where $H(e^{j \dw})$ is the DTFT of $h[n]$.

\subsection{Filtering continuous-time signals that have a white PSD}
\label{sec:ctWhitePSD}

A signal model that is used in several applications is the white noise, previously discussed in Section~\ref{sec:examplesAutocorrelation}.
%Using the nomenclature of stochastic processes (see Section~\ref{sec:stochasticprocesses}), the WGN process is strict-sense stationary. It has 
A realization $\nu(t)$ of a continuous-time white noise process has autocorrelation
\begin{equation}
R(\tau)=\ev[\nu(t) \nu(t-\tau)]=\frac \no 2 \delta(\tau)
\label{eq:awgnAutocorrelation}
\end{equation}
and zero mean $\mu = \ev[\nu(t)]=0$.

Using \equl{auto_psd_continuous}, the \emph{bilateral} PSD of a white noise is 
\begin{equation}
S(f) = \calF \left\{ \frac \no 2 \delta(\tau) \right\} = \frac \no 2, \textrm{~~~} \forall f.
\label{eq:psdWhiteNoise}
\end{equation}
When a \emph{unilateral} representation is adopted, the white PSD is conveniently denoted as $S(f) = \no$. For instance, if $\no=6$~W/Hz is the constant level of a unilateral PSD, its bilateral representation has a constant level of $\no/2 = 3$~W/Hz.

One can write $\no/2$ or $\no$ for the bilateral and unilateral PSDs, respectively, and in both cases the average power over a given frequency band\footnote{Unless otherwise stated, the bandwidth of a signal is assumed to be over non-negative frequencies. But the bandwidth used for some signal processing can include the negative frequencies. This happens when one is calculating
the power of a signal based on a bilateral PSD representation.} $\BW$ is 
\begin{equation}
\calP = \no \BW
%\label{eq:}
\end{equation}
In other words, using this nomenclature, $\calP = \no \BW$ is valid for both unilateral and bilateral PSD representations, given that in the case of a bilateral PSD one needs to multiply the PSD level $\no/2$ by $2 \BW$ to find the average power $\calP$.

Note from \equl{awgnAutocorrelation} and recalling \equl{autocorrelationOriginIsPower} that the continuous-time white noise power has $R(0)=\infty$ because there is an impulse in $\tau=0$ (the area is $\no/2$ but the amplitude goes to infinite).
Another way of observing that the continuous time WGN has infinite power is looking at the frequency domain: its PSD is $S(f)= \frac \no 2$, such that from \equl{psd_integration_continuousTime}, the area under the curve goes to infinite.

With infinite power, a white noise cannot be measured (it would damage the measuring equipment!).  But white noise is a very good model for many practical applications, with an implicit assumption that limits the frequencies to some bandwidth of interest and makes measurements and simulations feasible. In other words, the noise can have a flat PSD only over a given finite bandwidth but, for the purpose of the experiment / simulation, be conveniently modeled as white noise.

Because power can be written as $\calP = \sigma^2 + \mu^2$, and $\mu = 0$ for a signal with a white spectrum, the continuous-time white noise variance $\sigma^2 = \calP$ is infinite. 

%or generated in a computer with a command such as \ci{x=sqrt(Inf)*randn(1,1000)}.

%\subsection{Response of LTI systems to white noise}

%\subsubsection{Filtering white Gaussian noise}

If the input $\calX(t)$ is white noise with autocorrelation $R(\tau)=(\no/2) \delta(\tau)$ and PSD $\no/2$, \equl{wss_continuous_lti_output_psd} informs that the PSD at the output of the LIT system is
\begin{equation}
S_y(f)=|H(f)|^2 \frac{\no}{2},
\label{eq:whiteNoiseLTIOutput}
\end{equation}
i.\,e., $S_y(f)$ is a scaled version of $|H(f)|^2$.

All the previous discussion assumed the input signal has a white PSD. But the signal amplitudes could have any probability distribution, such as Laplacian, uniform, etc. A very special case of white noise is when this probability distribution is Gaussian,
which corresponds to the so-called WGN, introduced earlier in Section~\ref{sec:examplesAutocorrelation}.

%For white noise, the autocorrelation $R(\tau)=(\no/2) \delta(\tau)$ at $\tau=0$ 
%is an impulse of area $\no/2$. This area should not get confused with the amplitude $R(0) = \infty$, which indicates a signal of infinite power.

\subsection{{\akadvanced} Filtering discrete-time signals that have a white PSD}
\label{sec:conversion_awgn}
%\bApplication
%\textbf{White noise conversion from continuous to discrete-time}.
%This subject will be further discussed in Section~\ref{sec:awgn}.
%\subsection{Relating the discrete and continuous-time AWGN channels}
%\label{sec:discreteAndContinuousAWGN}

It is important to distinguish the continuous-time white noise, which has a flat bilateral PSD with value $\no / 2$ watts/Hz and infinite power, from the discrete-time ``white noise'' that has finite power typically denoted by $\sigma^2$. The discrete-time ``white noise'' is ``white'' (has a constant PSD level) only over the frequency band $[-\fs/2, \fs/2[$ (but is still called ``white'').
To emphasize the similarities and differences between continuous and discrete-time white noise, the goals here are to discuss how to: 1) generate a discrete-time white noise, 2) interpret the conversion of a continuous-time white noise into a discrete-time version and 3) its conversion back to continuous-time.

\subsubsection{Discrete-time white noise}

As mentioned, a discrete-time white process $\calX[n]$ has its power $\ev[\calX^2[n]]=\sigma_x^2$ coinciding with the variance $\sigma_x^2$ because the mean $\mu_x$ has to be zero for white processes. Moreover, its \emph{bilateral} PSD level $S_x(e^{j \dw}) = \sigma_x^2$ coincides with its variance in watts. 
This PSD can be interpreted as $\sigma_x^2 / (2 \pi)$ 
watts per radians.
To observe that, 
note from \equl{psd_integration} that 
\[
\calP =  \frac{1}{2 \pi}\int_{<2 \pi>} S(e^{j \dw}) \textrm{d}\dw = \frac{\sigma^2}{2 \pi} \times 2 \pi = \sigma^2.
\]
In other words, the constant PSD level $\sigma^2$ has the same numerical value of the power $\sigma^2$ in watts.
%but should be interpreted as having unit of watts per normalized frequency, as discussed in \exal{units_of_esd_psd} and \exal{units_of_esd_discrete_time}. 
In case of a unilateral PSD, the constant value $2\sigma^2$ would be twice the power.

Therefore, a discrete-time bilateral white PSD (with a constant level) can be denoted as:
\begin{equation}
S(e^{j \dw}) = \frac{\no}{2} = \sigma_x^2 = \calP.
\label{eq:whiteNoisePSD}
\end{equation}
This result may be confusing because the values of power and spectral density coincide.

Denoting as $S_x(e^{j \dw}) = \sigma_x^2$ the PSD of a white noise at the input of a filter $H(e^{j \dw})$, 
\equl{discrete_filtered_wss} leads to an output PSD given by
\begin{equation}
S_y(e^{j \dw}) = \sigma_x^2 |H(e^{j \dw})|^2.
\label{eq:discrete_filtered_awgn}
\end{equation}
\equl{discrete_filtered_awgn} suggests that the output PSD $S_y(e^{j \dw})$ has its shape $|H(e^{j \dw})|^2$ imposed by the filter, given that the input PSD $\sigma_x^2$ is simply a scaling factor.

%\footnote{See sections 1.5.5.1 in \cite{Sklar01} about white noise and 3.1.3.4 about the variance of white noise. See also Eq. 3.27 in \cite{Sklar01}. In Cioffi, see section 1.3, especially 1.3.1 (Conversion from the continuous AWGN to a vector channel), page 23, where Lemma 1.3.1 (uncorrelated noise samples) is proved. See also Cioffi 1.3.1 Conversion from the Continuous AWGN to a Vector Channel.}

\bExample \textbf{Generation of uniform and Gaussian white noise using {\matlab}}.
A discrete-time white noise with i.\,i.\,d. samples is relatively easy to generate in {\matlab} using functions that implement random number generators. For example, \ci{x=rand(1,1000)-0.5} creates a signal with an autocorrelation that approximates an impulse at the origin and 1,000 samples that are uniformly distributed. The subtraction of 0.5 is necessary to make $\mu = 0$. Similarly, a discrete-time version of WGN (Gaussian, not a uniform distribution) can be generated with \ci{x=randn(1,1000)}. 
%For example, realizations of \ci{N=1000} samples of this noise with power 15~W can be obtained with the command \ci{N=1000; x=sqrt(15)*randn(1,N)}. 
\eExample

\subsubsection{Converting white noise from continuous to discrete-time}
%that at the lag $m=0$ (or $\tau=0$ for continuous-time), the autocorrelation provides the signal power
%\begin{equation}
%R_x[0] =  \ev[\calX^2[n]] = \frac{1}{2 \pi} \int_{-\pi}^{\pi}  S_x(e^{j \dw}) d\dw,
%\label{eq:power_via_psd}
%\end{equation}
%where $\calX[n]$ is the WSS process under analysis.

The signals discussed in the previous section were already created in discrete-time.
But in some cases the discrete-time white noise is obtained from a continuous-time version or should be interpreted as such. 

For example, assume $\nu(t)$ is a continuous-time WGN submitted to filtering by an ideal lowpass filter and A/D conversion, as illustrated by the following block diagram:
\begin{align*}
\nu(t) \rightarrow \fbox{$h(t)$} \arrowedbox{A/D}  \nu[n].
\end{align*}
From \equl{wss_continuous_lti_output_psd}, the PSD at the output of $h(t)$ is $S(f)=|H(f)|^2 \no/2$. If the filter is ideal and the A/D conversion does not incur in aliasing, the PSD $S(e^{j \dw})$ of $\nu[n]$ is a scaled and periodic version of $S(f)$. Therefore, it is white in discrete-time.

In more details: assume that $h(t)$ is an ideal lowpass filter of bandwidth $\BW=\fs/2$ (gain equals to 1 over the band $-\fs/2$ to $\fs/2$~Hz and zero otherwise), where $\fs$ is the A/D sampling frequency, 
then the PSD of the continuous-time signal at the output of $h(t)$ is $S(f)=\no/2$ over $\BW$ and its power is $\calP = (2\BW)(\no/2) = \BW \no$.
Under the same assumptions of \equl{energyConservationContinuousDiscreteTime}, the power values in discrete and continuous-time are the same, such that the power of $\nu[n]$ is denoted by its variance $\sigma^2 = \BW \no$.

\bExample \textbf{Discretizing white noise}.
\label{ex:discretizingWhiteNoise}
For example, given that white noise with $\no/2 = 3$ W/Hz is converted to a discrete-time signal $\nu[n]$ via the cascade of an ideal lowpass filter $h(t)$ with passband $\BW=\fs/2$ and an A/D process with $\fs = 200$~Hz, the power of the signal at the output of the ideal lowpass filter $h(t)$ is
\[
\BW \no = \frac {\fs}{2} \no = 100 \times 6 = 600~\textrm{W}
\]
and the power of $\nu[n]$ is $\sigma^2 = \BW \no = 600$~W.

\begin{figure}
\begin{center}
\includegraphics[width=\figwidthSmall]{./Figures/awgn_continuous_discrete}
\caption{PSD $S_x(f)$ of a continuous-time white noise with $\no/2=3$ W/Hz (top) and its discrete-time counterpart $S_x(e^{j \dw})$ obtained with $\fs=200$~Hz (bottom). \label{fig:awgn_continuous_discrete}}
\end{center}
\end{figure}

\figl{awgn_continuous_discrete} depicts the PSD for both continuous and discrete-time signals. Note that the abscissa of $S_x(e^{j \dw})$, which is periodic, is represented from 0 to $2 \pi$. As indicated by \equl{psd_integration}, the power $\sigma_x^2$ can be obtained by integrating the density value $S_x(e^{j \dw})/(2 \pi) = 600/(2 \pi)$~watts per radians over the range $[0, 2\pi[$.
\eExample

\bExample \textbf{Filtering white noise through systems with unitary-energy impulse responses}.
\label{ex:discretizingWhiteNoiseViaUnitaryEnergy}
Another situation of interest is when white noise is filtered by an LTI that
has impulse response $h(t)$ with energy $E_h=\int_{-\infty}^{\infty} |h(t)|^2 \textrm{d}t = 1$~joule, as depicted in:
\begin{align*}
\nu(t) \rightarrow \fbox{$h(t)$} \rightarrow x(t) \arrowedbox{A/D}  x[n].
\end{align*}

In this case, the Parseval relation of \equl{parsevalEnergy} indicates that 
$E_h=\int_{-\infty}^{\infty} |H(f)|^2 \textrm{d}f = 1$
and from \equl{whiteNoiseLTIOutput}, the power of $x(t)$ is
\begin{equation}
\calP_x = \int_{-\infty}^{\infty} |H(f)|^2 \frac{\no}{2} \textrm{d}f = \frac{\no}{2}.
\label{eq:discretizingWhiteNoiseViaUnitaryEnergy}
\end{equation}
Therefore, the power of $x[n]$ is $\sigma^2 = \no/2$ for any $h(t)$ (or, equivalently, any $H(f) = \calF \{ h(t) \}$)
if $h(t)$ has energy of 1 joule. However, $x[n]$ will not have a flat PSD unless
$h(t)$ is an ideal lowpass filter as in \exal{discretizingWhiteNoise}.
\eExample

\subsubsection{Converting white noise from discrete to continuous-time}

The conversion of a discrete-time white noise $\nu[n]$ into a continuous-time signal $x(t)$ is depicted as:
\begin{align*}
\nu[n] \arrowedbox{ \textrm{D/A} } \fbox{$h(t)$} \rightarrow x(t) .
\end{align*}

Similar to the previous discussion about white noise C/D conversion in this section,
if $h(t)$ is an ideal lowpass
filter with passband frequency $\fs/2$, $x(t)$ has a flat PSD $S_x(f)$ with value $\no/2 = \sigma^2 / \fs$, where $\sigma^2$ is the
power of $\nu[n]$ and $\fs$ the sampling frequency. Note that $S_x(f)=0$ 
for $f > |\fs/2|$. Therefore, after filtering $\nu[n]$ with an ideal lowpass filter, the resulting $x(t)$ is not a continuous-time WGN, but has a flat PSD within the
band $[-\fs/2,\fs/2]$.

\subsubsection{Gaussian signal filtered by LTI system remains Gaussian}

If the system is LTI and its input is a realization of a WGN process, then it is well-known from random processes textbooks that the filter output is another Gaussian process. In other words, a Gaussian signal filtered by a linear system remains Gaussian but, in the general case, with a non-white PSD that was shaped by the filter.

When the LTI is an ideal lowpass filter such as in \exal{discretizingWhiteNoise}, besides being Gaussian, the output is also flat within the filter passband (or ``white'' within this band). 
Hence, when the input is WGN, and $h(t)$ an ideal lowpass filter with passband frequency $\fs/2$ that has its output converted to discrete-time,  the resulting $\nu[n]$ is a \emph{white and Gaussian} process with PSD $S(e^{j \dw})$ (again, ``white'' within the band $-\pi$ to $\pi$ rad) and power $\sigma^2=\BW \no$ watts, as discussed in \exal{discretizingWhiteNoise}. 

\section{Nonparametric PSD Estimation via Periodogram}
\label{sec:nonparametric_psd_est}
The ESD, PSD and MS spectrum were defined without a discussion on how to estimate them. This section will exclusively concern the estimation of PSDs using the FFT, as indicated in \figl{func_psd_estimators}. The PSD estimation methods
can also be used to estimate the ESD and MS spectrum.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./FiguresNonScript/func_psd_estimators}
\caption{ESD, PSD and MS spectrum with respective units, and two methods for estimating PSDs that
can also be used to estimate the ESD and MS spectrum.\label{fig:func_psd_estimators}}
\end{figure}

The \emph{periodogram}\index{Periodogram} $\hat S[k]$ is a classical approach for PSD estimation. There are distinct definitions in the literature and here, especially for consistency with {\matlab}, the periodogram is an approximation of $S(f)$ and defined as
\begin{equation}
\hat S[k] \defeq \frac{1}{N \textrm{~} \BW} |\textrm{FFT} \{ x_N[n] \}|^2,
\label{eq:periodogram_discretetime2}
\end{equation}
where $x_N[n]$ is the $N$-samples windowed version of $x[n]$ and $\BW$ the assumed frequency bandwidth. If the window is other than the rectangular, $\hat S[k]$ is called the \emph{modified periodogram}\index{Modified periodogram}.

When compared to the actual PSD $S(f)$, notice that the periodogram $\hat S[k]$ may present aliasing in case $x_N[n]$ was obtained by sampling a continuous-time $x(t)$. It may also present leakage due to windowing. Besides, the
periodogram inherits the properties and pitfalls of an FFT, such as a frequency resolution $\Delta \dw = 2\pi / N$ that may not be enough to distinguish peaks separated by less than $\Delta \dw$ due to the picket-fence effect. 

Based on the definition of $\hat S[k]$, the signal power $\calP$ can be conveniently obtained by approximating the integral of \equl{psd_integration_continuousTime}. For instance, using the rectangle method one obtains
\begin{equation}
\calP \approx \Delta f \sum_{k=0}^{N-1} \hat S[k],
\label{eq:periodogram_discretetime3}
\end{equation}
where $\Delta f = \BW / N$ is the FFT frequency spacing.

Some important facts related to this definition:
\begin{itemize}
	\item The periodogram is associated by definition to the FFT operation and, consequently, operates on a finite-length discrete-time signal.
	\item The periodogram input is a discrete-time signal, but frequencies can be conveniently interpreted in Hertz (or rad/s) via $\aw = \fs \dw$ (\equl{freqdiscrete2continuous}) when the sampling frequency $\fs$ is specified.
	\item The analysis bandwidth $\BW$ is in Hertz, and obtained from the specified sampling frequency $\fs$. The $\BW$ is assumed to be from $-\fs/2$ to $\fs/2$ for a bilateral or $0$ to $\fs/2$ for a unilateral periodogram.
\end{itemize}

%When it was adopted $\BW=2 \pi$, 

%options for the normalization factor $\kappa$.
% that is adopted by a software routine to obey \equl{psd_integration_continuousTime}, \equl{psd_integration} or \equl{ms_sumPower}, for example.

%\subsection{Periodogram normalization factors}
%
%When using a software routine to estimate the periodogram, a crucial property of the result is that it should approximate \equl{psd_integration_continuousTime} or \equl{psd_integration} (or \equl{ms_sumPower}, in case the MS spectrum is of interest). 
%This is often accomplished by obtaining estimates $\hat S[k]$ of the PSD using \equl{periodogram_discretetime} and then multiplying the periodogram values by a normalization factor $\kappa$. 
%There are distinct PSD normalization factors and associated definitions. This can be confusing and the main sources of mistakes are highlighted in the following discussion.
%
%Before proceeding, it is recommended to observe \tabl{analogy_psd_pdf} and associated discussion, which summarizes an interesting analogy between the use of FFTs and histograms that helps understanding the options for the normalization factor $\kappa$.
%% that is adopted by a software routine to obey \equl{psd_integration_continuousTime}, \equl{psd_integration} or \equl{ms_sumPower}, for example.
%
%While the MS spectrum is a discrete function with the property specified by \equl{ms_sumPower}, the periodogram values $\hat S[k]$ ideally coincide with a continuous function of frequency: the PSD $S(f)$ or $S(e^{j \dw})$. Consequently, having an array of values $\hat S[k]$, the property that should be obeyed depends on the \emph{integral} \equl{psd_integration_continuousTime} or \equl{psd_integration}, respectively, not a \emph{summation} as in \equl{ms_sumPower}. 

%\subsubsection{Normalization factors for the adopted PSD definitions}
%
%To discuss $\kappa$, recall that the rectangle method of \equl{rectangle_method} can be used to estimate the integrals 
%in \equl{psd_integration_continuousTime}  and \equl{psd_integration}. The rectangles heights are the periodogram values and the rectangles bases are the FFT bin width, which is $\Delta f$ or $\Delta \dw$, for discrete or continuous-time signals, respectively. Both cases are investigated below.
%
%\begin{itemize}
%\item \textbf{For a discrete-time PSD}:
%Adopting the rectangle method to approximate the integral of \equl{psd_integration} leads to:
%\begin{equation}
%\calP = \frac{1}{2 \pi}\int_{<2 \pi>} S(e^{j \dw}) d\dw \approx \frac{1}{2 \pi} \Delta \dw \sum_{k=0}^{N-1} \hat S[k] = \frac{1}{N} \sum_{k=0}^{N-1} \hat S[k],
%\label{eq:powerForDiscreteTime}
%\end{equation}
%where $\Delta \dw = 2 \pi/N$ rad. In this case, $\kappa = 1/N$, assuming $\hat S[k]$ are the raw values estimated by the periodogram software routine.
%%which is an alternative explanation to the normalization by $1/N$ in \equl{fromPeriodogramToMS}.
%
%\item \textbf{For a continuous-time PSD}:
%%The previous discussion assumed the periodogram $\hat S(e^{j \dw})$ of a discrete-time signal. When using the periodogram (and consequently the FFT) to estimate PSDs that should be interpreted as describing continuous-time signals and obeying \equl{psd_integration_continuousTime}, 
%In this case, the approximation of \equl{psd_integration_continuousTime} using the rectangle method leads to
%\begin{equation}
%\calP = \int_{- \infty}^\infty S(f) \textrm{d}f \approx \Delta f \sum_{k=0}^{N-1} \hat S[k] = \frac{\fs}{N} \sum_{k=0}^{N-1} \hat S[k],
%\label{eq:powerForContinuousTime}
%\end{equation}
%where, from \equl{fft_freq_resolution}, $\Delta f = \fs/N$~Hz. In this case, $\kappa = \fs/N$
%\end{itemize} 
%

%Note that $\calP$ can be obtained from the periodogram of a discrete-time signal using two distinct interpretations: the periodogram values $\hat S[k]$ at the FFT grid represents $\hat S(e^{j \dw})$ and \equl{powerForDiscreteTime} is used or, alternatively, $\hat S[k]$ represents $\hat S(f)$ and \equl{powerForContinuousTime} is used with an implicitly assumed $\fs=1$~Hz.

\subsubsection{{\matlab} adopted conventions for periodograms}

Is should be noted that {\matlab} adopts the following:
\begin{itemize}
	\item When the sampling frequency $\fs$ is not specified, it is assumed $\fs = 2 \pi$ and \equl{psd_integration_continuousTime} is adopted.
	\item When $\fs$ is specified, the PSD frequency axis (abscissa) is assumed to be in Hertz. When $\fs$ is not specified
	the frequency axis unit is assumed to be ``rad/sample'' (which corresponds to $\dw$ in radians according to the nomenclature adopted in this text).
	\item When the signal is real, a unilateral PSD is adopted by default.
	\item By default, the \ci{periodogram.m} function uses a power-of-two FFT-length with a minimum of 256, adopting zero-padding in case the input signal has less than 256 samples.
	\item The periodogram values are converted to dB scale.
\end{itemize}
These conventions are discussed in the sequel.
% and contrasted with the ones adopted in this text.

%Having distinct expressions: \equl{psd_integration_continuousTime} and \equl{psd_integration} for continuous and discrete-time signal poses a difficulty to the intention of having a single software routine to calculate the periodogram.
%To avoid asking the user what equation should be used, {\matlab} always assumes \equl{psd_integration_continuousTime}. When the sampling frequency $\fs$ is not specified, instead of assuming a discrete-time signal representation and adopting \equl{psd_integration}, {\matlab} implicitly assumes that $\fs = 2 \pi$ and uses \equl{psd_integration_continuousTime}.

%This is a sensible strategy, because when $\fs$ is not specified, the periodogram graph has its abscissa specified by {\matlab} as normalized frequency $\dw / \pi$. In this ``discrete-time'' case, the normalization factor adopted by {\matlab} to obtain $\calP$ from the periodogram is given by \equl{powerForContinuousTime}: $\kappa = \fs / N = (2 \pi)/N$.

%As discussed just after \equl{powerPerTone}, the definitions adopted here are equivalent to implicitly using $\fs=1$ for discrete-time signals. Therefore, the choice of $\fs = 2 \pi$ in 
%{\matlab} creates a discrepancy with respect to the periodogram values and, consequently, normalization factors to obtain $\calP$. This is illustrated in Example~\ref{ex:matlabPeriodogram}.

%Hence, simply changing the value of $\fs$ that is informed to a software such as {\matlab}, will scale the values of the periodogram of a given signal $x[n]$. 

%The previous example did not show graphs. But 
%Therefore, when interpreting the PSD obtained with a software routine, it is very important to properly take in account the abscissa or, equivalently, the frequency range assumed for estimating the PSD. For example, {\matlab} will always scale the PSD such that 
%\equl{psd_integration_continuousTime} or \equl{psd_integration} are obeyed. 

As mentioned, a characteristic of {\matlab} is the adoption, by default, of a unilateral PSD when the signal is real and has a Hermitian-symmetric spectrum. In this case, only the nonnegative frequency range are used in the graphs and in the summation of the normalized periodogram to obtain $\calP$. For example, in continuous-time, instead of \equl{periodogram_discretetime3}, one would have
\begin{equation}
\calP = \int_{0}^\infty S(f) \textrm{d}f \approx \Delta f \sum_{k=0}^{(N/2)+1} \hat S[k],
\label{eq:unilateralPowerForContinuosTime}
\end{equation}
assuming $N$ is even. Hence, the values of $\hat S[k]$ in a unilateral periodogram are twice the values in the corresponding bilateral representation, with exception of the DC and Nyquist frequencies.

\subsection{Periodogram of periodic signals and energy signals}
\label{sec:periodogram_periodic_energy_signals}
A periodogram must be properly interpreted if applied to a periodic signal.
Note that, as indicated in Section~\ref{sec:psdPeriodicSignals}, while the PSD of a periodic signal is composed by impulses,  the periodogram $\hat S[k]$ obtained from a finite-duration window does not contain impulses due to the convolution between the
original spectrum and the window spectrum.
This issue was discussed in Section~\ref{sec:fft_of_impulses}.
Besides, a periodogram indicates the power distributed in bins with width $\Delta f$~Hz. 
Hence, if a sinusoid of power $\calP$ is located at a bin center, the periodogram estimates
the corresponding PSD level as $\calP / \Delta f$
This is a fact that needs always to be taken in account when using a periodogram to represent a periodic signal that
has impulses in its continuous-time PSD $S(f)$.

Care must also be exercised when a periodogram is applied to an energy signal.
As discussed in Section~\ref{sec:esd}, an energy signal should be associated to an ESD, not PSD. The periodogram of an energy signal such as $x[n]=u[n]-u[n-5]$, for example, is implicitly assuming that $x[n]$ is just a windowed version of a periodic signal.

\subsection{Examples of continuous-time PSD estimation using periodograms}

Before discussing more advanced concepts such as the Welch method for PSD estimation, few examples of periodograms are provided in this section to consolidate the basic theory.

\bExample \textbf{Using \ci{periodogram.m} to approximate the continuous-time PSD $S(f)$ of a sinusoid}.
%Similarly, the ordinates are related as $10 \log_{10} 8149 \approx 39.11$~dB, but one could ask what $\hat S[16]=8149$ means. 
When the sampling frequency $\fs$ is specified as input argument and without output arguments, the \ci{periodogram} of {\matlab} indicates the abscissa in Hertz. For example, \codl{snip_frequency_normalized_periodogram} was used to generate \figl{cosinePeriodogramFs8000}, which has the abscissa going from 0 to $\fs/2$~Hz because the signal is real and the default periodogram is unilateral in this case.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_normalized\_periodogram}{snip_frequency_normalized_periodogram}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/cosinePeriodogramFs8000}
\caption{Periodogram of $x[n]=10\cos((2\pi/64) n)$ in dBW/Hz estimated by \ci{periodogram.m} with a $1024$-point FFT and assuming $\fs=8$~kHz.\label{fig:cosinePeriodogramFs8000}}
\end{figure}


The signal power is $\calP = 50$~W.
\figl{cosinePeriodogramFs8000} indicates a peak at $f=125$~Hz, which corresponds to tone $k=16$. This can be confirmed by observing that $\Delta f = \fs / N = 8000/1024 \approx 7.8125$~Hz, such that $f_k = k \Delta f = 16 \times 8000/1024 = 125$~Hz. 
Because there was no visible leakage in this case, the sinusoid power $\calP=50$~W 
is completely residing in bin $k=16$.
When using periodograms to represent sinusoids, as explained in Section~\ref{sec:periodogram_periodic_energy_signals}, the sinusoid power is divided by the frequency range of the bin (the bin width).
Hence, from \equl{powerPerTone}, the expected PSD value at this bin is $\hat S[k]|_{k=16} = \calP / \Delta f = 50 / 7.8125 = 6.4$~W/Hz. Converting it to dBW leads to $10 \log_{10} 6.4 \approx 8.062$~dBW/Hz, as indicated in \figl{cosinePeriodogramFs8000}.
In summary, \emph{the PSD value in W/Hz at a given FFT bin is the power in watts concentrated in this bin divided by the bin width in Hz}.

%, divided by its bin width $\Delta f$ in Hz. The PSD is denoted as $S(f)$ in spite of $f$ assuming only discrete values $f_k$.
One aspect of PSD graphs in dB scale is that dynamic ranges larger than hundreds of dB should be associated to numerical errors, given that \ci{10*log10(eps)=-156.5}.
\eExample

\bExample \textbf{The power ratio between sinusoids can be obtained via PSDs}.

\codl{snip_frequency_cosines_psd} provides an example of a signal composed of the sum of two bin-centered sinusoids.
The periodogram is calculated via {\matlab} and also according to its definition (\ci{Sdef}). Both ways lead to the same results, because the \ci{periodogram} used the default rectangular window.
Adopting a different window would lead to discrepant results. The power calculated in time (\ci{Power\_time}) and frequency (\ci{Power\_freq}) domains also coincide, having the value of 50.5~W.
\figl{twocosinesPeriodogram} shows the graph generated by \codl{snip_frequency_cosines_psd}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_cosines\_psd}{snip_frequency_cosines_psd}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/twocosinesPeriodogram}
\caption{Periodogram of two bin-centered sinusoids at 250 and 500~Hz, calculated via Matlab and its definition.\label{fig:twocosinesPeriodogram}}
\end{figure}

The amplitudes of the sinusoids are 10 and 1~V, corresponding to power values of $\calP_1=50$ and $\calP_2=0.5$~W, respectively. These values in dB scale correspond to 16.9897 and $-3.0103$~dBW, which corresponds to a power ratio $\calP_1/\calP_2=50/0.5=100$ in dB of $16.9897-(-3.0103) = 10 \log_{10}(\calP_1/\calP_2)=20$~dB.

Given that the bin width is
$\fs/N = 2000/128=15.625$~Hz, the PSD values are $50/15.623 \approx 3.2$ and $0.5/15.623 \approx 0.032$ at their corresponding frequencies of 500 and 250~Hz, respectively. 
The PSD values in dBW/Hz are $10 \log_{10} 3.2 \approx 5.0515$ and $10 \log_{10} 3.2 \approx -14.9485$~dBW/Hz. Note that
subtracting the PSD values leads to $5.0515 - (-14.9485)=20$~dB, as obtained when manipulating the power values.
This is an interesting aspect: the relative difference in power between the two sinusoids can be directly obtained from
the periodogram (or equivalently, from the PSD). The estimated PSD graph does not allow directly reading the absolute power values but explicitly informs the relation between powers of signal components that reside in single bins.

We now modify the call to the \ci{periodogram} function to use the Hamming window with \ci{[S,f] = periodogram(x,hamming(N),N,Fs)} in \codl{snip_frequency_cosines_psd}. The corresponding result is depicted in \figl{hamming_periodogram}, which
compares the periodograms with rectangular and Hamming windows. As mentioned in Section~\ref{sec:nonparametric_psd_est}, the latter is called a ``modified'' periodogram.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/hamming_periodogram}
\caption{Periodograms of two bin-centered sinusoids at 250 and 500~Hz, calculated with the rectangular and Hamming windows.\label{fig:hamming_periodogram}}
\end{figure}

\figl{hamming_periodogram} shows a clear advantage of the rectangular window in this case in which the
two sinusoids are bin-centered. We modify again the script in \codl{snip_frequency_cosines_psd} to
position the cosine frequencies halfway of bin centers with
\ci{k1=N/4+0.5; k2=N/8+0.5}. This leads to the frequencies \ci{f1=507.8125} and \ci{f2=257.8125}~Hz.
\figl{hamming_nonbincenter} shows the corresponding results.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/hamming_nonbincenter}
\caption{Periodograms of two non-bin-centered sinusoids at \ci{f1=507.8125} and \ci{f2=257.8125}~Hz calculated with the rectangular and Hamming windows.\label{fig:hamming_nonbincenter}}
\end{figure}

\figl{hamming_nonbincenter} shows datatips for the rectangular-window periodogram at the FFT bin centers corresponding to 250 and 500~Hz, which are the
bins in the FFT frequency grid immediately before the cosine frequencies \ci{f1=507.8125} and \ci{f2=257.8125}~Hz. 
In this case, both rectangular and Hamming periodograms do not achieve the correct (``true'') PSD values neither
in frequency nor in amplitude. For instance, the periodogram with the rectangular window indicates a PSD
value of 1.10219~dBW/Hz at 500~Hz for the stronger cosine, while the true value is 5.0515~dBW/Hz at 507.8125~Hz,
as indicated in \figl{twocosinesPeriodogram}.
\eExample

\bExample \textbf{PSD of a sinusoid contaminated by AWGN}. Special care must be exercised when interpreting a PSD that represents both broadband and narrowband signals. An example is when using the PSD to estimate the SNRs of sinusoids immersed in white noise.
\figl{cosineOnAWGN} shows the periodogram of a cosine contaminated by AWGN generated with \codl{snip_frequency_noisy_cosine}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_noisy\_cosine}{snip_frequency_noisy_cosine}
%\begin{lstlisting}
%N=1024; A=4; %# of samples and cosine amplitude of A volts
%Fs=8000; Ts=1/Fs; %sampling frequency (Hz) and period (s)
%f0=915; %cosine frequency in Hz
%noisePower=16; %noise power in watts
%noise=sqrt(noisePower)*randn(1,N);
%t=0:Ts:(N-1)*Ts; %N time instants separated by Ts
%x=A*cos(2*pi*f0*t) + noise;%generate cosine with AWGN
%\end{lstlisting}
\codl{snip_frequency_noisy_cosine} was used to generate the top plot with \ci{subplot(211)}. Then, the bottom plot was obtained by increasing \ci{N} from 1024 to 16384. All the absolute values of the two periodograms differ due to their dependence on \ci{N}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{./Figures/cosineOnAWGN}
\caption{Periodograms of a cosine contaminated by AWGN at an SNR of $-3$~dB with an FFT of $N=1024$ points (top plot) and 16384 (bottom). In this case the SNR cannot be inferred directly from the noise level.\label{fig:cosineOnAWGN}}
\end{figure}

It can be noticed from \codl{snip_frequency_noisy_cosine} that the cosine power is $\calP_c = A^2/2 = 8$~W while the noise power is 16~W, leading to a $\snr=10 \log_{10}(8/16)\approx -3.01$~dB. However, one may be tempted by \figl{cosineOnAWGN} to erroneously infer that SNR is positive given that the noise level (sometimes called \emph{noise floor}\index{Noise floor}) is more than 20~dB below the cosine peak for the top graph. Directly observing the ratio of powers in PSD graphs was valid in \figl{sumOfCosinesPeriodogramMSS}, but in that case,  the cosines were assumed to have all power confined in their respective bins. This is not the case with signals that have power spread over several bins and especially with white noise.

Because the white noise power is spread over all bins of a periodogram, the larger the number $N$ of FFT bins, the less noise power each bin carries and the larger the difference between the sinusoid power (assumed to be confined in a single bin) and the noise floor. In \figl{sumOfCosinesPeriodogramMSS}, the bottom plot shows that the cosine power is approximately 30~dB above the noise level when \ci{N} is increased to 16384. However, in both plots, the SNR is $-3$~dB as attested by \codl{snip_frequency_noisy_cosine}. 

In summary, the proper way of measuring the signal power is to account for the power in all bins that represent the signal.% (equivalent to integrating over frequency in the continuous-time case).
\eExample

%AK-IMPROVE - \textbf{PSD of sinusoid digitized with ADC: how noise floor varies with $\fs$}.
%\ignore{
%\bExample
%\textbf{PSD of sinusoid digitized with ADC: how noise floor varies with $\fs$}.
%There are situations where the white noise is the assumed model for a signal that has an approximately constant PSD level $\no/2$ over the frequency range of interest. In this case, the larger the bandwidth $\BW$, the larger the power $\calP = 2 \BW \no/2 = \no \BW$. For example, if the frequency band is $\BW=1.1$~MHz (from DC to 1.1~MHz) and a background noise has a white PSD of $\no/2=-140$~dBm/Hz (equivalent to $\no/2=10^{-140/10} \times 10^{-3} = 10^{-17}$~W/Hz), the power is $\calP = 2 \times 10^{-17} \times 1.1 10^{6} = 2.2 10^{-11}$~W or $-106.6$~dBm. Doubling the bandwidth would double $\calP$.
%In other situations, a given signal is generated with power $\calP$ and the larger $\BW$, the smaller $\no/2$. This is what happens when an ADC uses oversampling as discussed in the next example.
%%
%Assuming the quantization noise generated by the ADC has power $\calP_n$ given by \equl{quantizationNoisePower}, the PSD for a sampling frequency $\fs$ is $\no/2 = \calP_n / \fs$. If the band of interest is $\BW$ and oversampling is used, such that $\fs \gg 2\BW$, one can then later filter the oversampled signal and get rid of the parcel of the quantization noise that was located ``out-band'' (from $\BW$ to $\fs/2$).
%\eExample
%}
%%TO-DO
%%Note that PSD is not good for deterministic signals:
%%\ url{http://www.mathworks.com/help/signal/examples/measuring-the-power-of-deterministic-periodic-signals.html}
%
%
%%Show how the signal spectrum is modified when multiplied by a window. How to deal with these plots in continuous time in Matlab? Can use ezplot: syms x, f=sinc(x), ezplot(f,[-100 100])
%%Number of points is number of pixels - bad solution
%
%%\texorpdfstring{TEX text}{Bookmark Text}
%%\subsection{
%%\ifpdf
%%	\texorpdfstring{Relating discrete $x[n]$ and analog $x(t)$ signals.}{Relating discrete x[n] and analog x(t) signals.}
%%\else
%%	Relating discrete $x[n]$ and analog $x(t)$ signals.
%%\fi
%%}
%
%\bExample \textbf{FALTA TITULO}
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=8cm]{Figures/autocorrelationWhiteNoise}
%\caption{Autocorrelation of white noise with variance 8.\label{fig:autocorrelationWhiteNoise}}
%\end{figure}
%
%\figl{autocorrelationWhiteNoise} was generated with \codl{snip_frequency_noise_autocorrelation}.
%\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_noise\_autocorrelation}{snip_frequency_noise_autocorrelation}
%%\begin{lstlisting}
%%power=8; %watts
%%N=10000; %number of samples
%%x=sqrt(power)*randn(1,N); %white noise signal
%%Fs=40; maxLag=100; %sampling frequency and maximum lab
%%[R,l]=xcorr(x,x,maxLag,'biased'); %biased autocorrelation
%%stem(l,R);xlabel('lag l');ylabel('autocorrelation R[l]');
%%\end{lstlisting}
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=8cm]{Figures/psdsWhiteNoise}
%\caption{PSDs of white noise with variance 8. Top: $\fs=40$ Hz, level $\calN_0/2=10 \log_{10}(8/20) \approx -4$ dB/Hz. Bottom: no normalization, level $\calN_0/2=10 \log_{10}(8/\pi) \approx 4$.\label{fig:psdsWhiteNoise}}
%\end{figure}
%
%\figl{psdsWhiteNoise} depicts the PSD obtained with the \ci{pwelch} command, corresponding to the autocorrelation in \figl{autocorrelationWhiteNoise}. 
%More specifically, the top plot was obtained with 
%\ci{pwelch(x,[],[],[],Fs)}
%and the bottom one with \ci{pwelch(x)}.
%In this case (Matlab behavior), the PSD level $\calN_0/2$ is given by the power (8 W in this case) divided by the frequency bandwidth $\fs/2$ (range $[0, \fs/2]$). For example, when $\fs=40$~Hz, $\calN_0/2=8/20 \approx 0.4$~W/Hz, which corresponds to $-3.98$~dBW/Hz. 
%
%
%It is very common to express a PSD as dB/Hz instead of dBW/Hz, and this (questionable) convention is adopted in \figl{psdsWhiteNoise}. When $\fs$ is not specified and the signal is real, the \ci{pwelch} command assumes the range is $[0, \pi]$ and
%%\begin{equation}
%%[0, \pi]
%%\label{eq:pwelchDefaultRange}
%%\end{equation}
%%Assume one wants to find the value of $\fs$ to be specified to \ci{pwelch} in order to get the same result that \ci{pwelch} would provide when $\fs$ is not specified. In this case, for a real signal, specifying 
%$\fs = 2 \pi$. Note that, in this case, the unilateral PSD is still specified in the range $[0, \fs/2]$. 
%%which would coincide with the range $[0, \pi]$ that is assumed when $\fs$ is not specified.
%\eExample

\bExample \textbf{Bilateral versus unilateral periodograms}.
The bilateral and unilateral periodograms are contrasted in \codl{snip_frequency_comparePeriodograms} and \codl{snip_frequency_correctPeriodograms}.
%which avoids zero-padding via the third input argument of \ci{periodogram.m}.
%Another example is when a discrete-time signal has its spectrum represented from $-\pi$ to $\pi$, the integral is over $\dw$ and the factor $1/(2\pi)$ in \equl{psd_integration} must be used. If, alternatively, the spectrum of the discrete-time signal is represented with a normalized frequency XXX

%The following example details how to interpret the result of the \ci{periodogram} function in {\matlab}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_comparePeriodograms}{snip_frequency_comparePeriodograms}

As indicated in \codl{snip_frequency_comparePeriodograms}, to obtain the power $\calP$ from the periodogram, the proper normalization factor is the bin width $\Delta f$=\ci{BW/N} for both cases, uni and bilateral.
In this case $\Delta f =2 \pi/1024$ because $\fs$ was not specified when calling the \ci{periodogram.m} routine and 
the default $\fs=2\pi$ is assumed.  
%But it is interesting to note that in {\matlab}, it is possible to obtain an approximate value for $\calP$ using \ci{Power2 = sum(H)*w(end)/length(H)}, for both unilateral and bilateral periodograms. The following discussion assumes $\fs$ is not indicated and $N$, the FFT-length used by \ci{periodogram.m}, is even. %In this case, instead of the correct $\Delta_{\dw}=2 \pi/1024$. 

A minor detail is that when $N$ is even and the periodogram is unilateral, \ci{f\_uni(end)} is $\fs/2$~Hz and \ci{length(f\_uni)} is $(N/2)+1$.
\codl{snip_frequency_correctPeriodograms} illustrates this point by indicating in the last line of the script that
\ci{length(f\_uni)} $= 513$~samples and \ci{f\_uni(end)} $= 250$~Hz, given that $N=1024$ in this case.

If a bilateral periodogram is calculated, \ci{f\_bil(end)} is $(\fs/2)-\Delta f$ and \ci{length(f\_bil)} is $N$. 
%Therefore, to obtain exactly the correct $\calP$, the results obtained with \ci{Power2 = sum(H)*w(end)/length(H)} must be multiplied by the factors $1+2/N$ and $N/(N-1)$, respectively. Both factors are close to 1 for large enough $N$ and may be discarded.
%For the example in \codl{snip_frequency_comparePeriodograms}, these situations correspond to factors of $\pi/513$ and $$, respectively.
% instead of the correct $\Delta_{\dw}=2 \pi/1024$. 
%If a bilateral periodogram is calculated, \ci{w(end)} is $2\pi-\Delta_{\dw}$ and \ci{length(H)} is $N$, which leads to a factor $\Delta_{\dw}-\Delta_{\dw}/N$. The same approximated values occur when $\fs$ is specified.
The script in \codl{snip_frequency_correctPeriodograms} indicates that
\ci{length(f\_bil)} $= 1024$~samples and \ci{f\_uni(end)} $= 499.5117 = 500 - \Delta f$~Hz.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_correctPeriodograms}{snip_frequency_correctPeriodograms}

\ignore{
The reasoning in \codl{snip_frequency_correctPeriodograms} also applies when $\fs$ is specified, as in:
\begin{lstlisting}
[H,w] = periodogram(x,[],[],500); %Unilateral periodogram, Fs=500 Hz
[H2,w2] = periodogram(x,[],[],500,'twosided'); %Bilateral, Fs=500 Hz
\end{lstlisting}
%
In summary, when using {\matlab}, the periodograms (unilateral and bilateral, with $\fs$ specified or not, with zero-padding or not) are calculated in such a way that one can use the second command below to estimate the average power:
\begin{lstlisting}
[H,w] = periodogram(x); %Calculate the periodogram 
Power = sum(H)*w(end)/length(H) %Average power estimate
\end{lstlisting}
and eventually use the discussed correction factors to improve this estimate.
}

All the previous calculations in \codl{snip_frequency_comparePeriodograms} and \codl{snip_frequency_correctPeriodograms} (e.\,g., \ci{sum(S)}) used the periodogram in linear scale. But another convention adopted by {\matlab} is to convert, by default, the periodogram values to dBW/Hz when the function \ci{periodogram.m} is invoked without output parameters. As we warned in Appendix~\ref{app:decibel}, the periodogram unit is informed to be dB/Hz instead of dBW/Hz. We consider
the latter option more appropriate to emphasize that is an absolute value of power density instead of a relative (dB) value.
\eExample 

%The following examples illustrates this issue.

\subsection{Relation between MS spectrum and periodogram}

The FFT-based estimations of PSD and MS spectrum are closely related. From \equl{msEstimationViaFFT} and \equl{periodogram_discretetime2}, the MS spectrum $\hat S_{\textrm{ms}}[k]$ can be obtained by multiplying the periodogram $\hat S[k]$ (PSD estimation) by
the FFT frequency resolution $\Delta f$:
\begin{equation}
\hat S_{\textrm{ms}}[k] = \frac{\BW}{N} \hat S[k] = \Delta f \hat S[k].
\label{eq:fromPeriodogramToMS}
\end{equation}
In other words, the periodogram $\hat S[k]$ (PSD estimation) can be obtained by dividing $\hat S_{\textrm{ms}}[k]$ by $\Delta f$.

%Comparing \equl{powerForDiscreteTime} and 
%Observing \equl{periodogram_discretetime3}, it can be concluded that for PSD estimation the scaling factor that multiplies the periodogram corresponds to the FFT frequency resolution  $\Delta f$ in Hertz.
%, respectively, and there is an extra factor $1/(2\pi)$ in case of a discrete-time PSD. 
%In summary,

Because $\hat S[k]$ is a PSD estimate, the user of a PSD estimation routine that returns $\hat S[k]$ just needs to know the bin width $\Delta f$ to be able to obtain the average power $\calP$ from $\hat S[k]$, and/or individual values of $\hat S_{\textrm{ms}}[k]$.
This reasoning is valid when $\calP$ should be calculated over the whole frequency range, or shorter frequency intervals, because the amount of power $\calP_k$ at a single FFT bin (the $k$-th bin) in a periodogram $\hat S[k]$ is
\begin{equation}
\hat S_{\textrm{ms}}[k] = \calP_k = \Delta f \hat S[k].
\label{eq:powerPerTone}
\end{equation}

For discrete-time signals assuming $\BW = 2 \pi$ or $\BW=1$, $\calP_k = (\BW/N) \hat S[k]$. In both continuous and discrete-time cases, $\calP = \sum_{k=0}^{N-1} \calP_k$.

\codl{snip_frequency_mssFromPeriodogram} illustrates how \equl{fromPeriodogramToMS} can be used to obtain $\hat S_{\textrm{ms}}[k]$.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_mssFromPeriodogram}{snip_frequency_mssFromPeriodogram}

%It should be noted that $\sum_k \hat S[k] \ne \calP$, i.\,e., the summation of the periodogram values is not equal to the signal power $\calP$. In fact, from \equl{fromPeriodogramToMS}, one can conclude that $(\BW/N) \sum_k \hat S[k] = \calP$. 

%The factor $1/N$ will be further discussed in the next subsection. Before that, the following example is useful to get more familiar with the periodogram as an estimate of the PSD and the concept of MS spectrum.

While the MS spectrum is a discrete function with the property specified by \equl{ms_sumPower}, the periodogram values $\hat S[k]$ ideally coincide with a continuous function of frequency: the PSD $S(f)$ or $S(e^{j \dw})$. Consequently, having an array of values $\hat S[k]$, the property that should be obeyed depends on the \emph{integral} \equl{psd_integration_continuousTime} or \equl{psd_integration}, respectively, not a \emph{summation} as in \equl{ms_sumPower}. 

It is recommended to observe \tabl{analogy_psd_pdf} and associated discussion, which summarizes an interesting analogy between the use of FFTs and histograms that helps understanding the difference betweeen $\hat S[k]$ and $\hat S_{\textrm{ms}}[k]$.

\subsection{Estimation of discrete-time PSDs using the periodogram}

The periodogram is an approximation of the continuous-time PSD $S(f)$, but one can ``trick'' the software routine and obtain an estimate of $S(e^{j \dw})$ by imposing $\fs = \BW = 1$~Hz. In this case, the numerical values of the periodogram $\hat S[k]$ provide an estimate $\hat S(e^{j \dw})$ of the discrete-time PSD $S(e^{j \dw})$ as follows:
\begin{equation}
\hat S(e^{j \dw})|_{\dw = k (2 \pi/N)} = \hat S[k].
\label{eq:periodogram_discretetime4}
\end{equation}

Having the periodogram calculated with $\fs=1$, allows to interpret $\hat S[k] / (2 \pi)$ in watts/rad, as 
indicated in \equl{parseval_dt}. 
%discussed in \exal{units_of_esd_discrete_time}.

However, when invoking a periodogram software routine for discrete-time signals without specifying $\fs$, the assumed default value is $\fs = 2 \pi$, not $\fs=1$. The values of $\hat S[k]$ will differ in these two cases by $2 \pi$. But, independent on $\fs$, the signal average power can always be obtained with \equl{periodogram_discretetime3}.

%When dealing with $x[n]$ that was not obtained from a continuous-time signal or the sampling frequency $\fs$ is unknown, with an abuse of notation, it is assumed by default in software routines that $\BW=2 \pi$ and \equl{periodogram_discretetime3} can be used with $\Delta f = (2 \pi) / N$ (which numerically coincides with the FFT resolution $\Delta \dw$). 

%but then one must take care of the factor $1/(2\pi)$ and interpret the results accordingly.	


%It is an approximation of 
%Appendix shows alternative definitions.
%
%The \emph{periodogram}\index{Periodogram} $\hat S(e^{j \dw})$ is the classical approach for PSD estimation and can be defined as
%\begin{equation}
%\hat S(e^{j \dw}) \defeq \frac{1}{N} |X_N(e^{j \dw})|^2,
%\label{eq:periodogram_discretetime2}
%\end{equation}
%where
%\[
%X_N(e^{j \dw}) = \sum_{n=0}^{N-1} x[n] e^{-j \dw n}.
%\]
%The subscript $N$ emphasizes that $X_N(e^{j \dw})$
%is the DTFT of a windowed version of $x[n]$ with $N$ samples.  


%because the periodogram adopts an FFT, 
%is estimated at specific frequencies $\dw = k (2\pi/N)$ via an $N$-point FFT. This makes the 

%This section assumes that a FFT is used to estimate the PSD. Hence, discrete-time signals are assumed in the following discussion, but frequencies in radians can be conveniently converted to Hertz via $\aw = \fs \dw$ (\equl{freqdiscrete2continuous}) when the sampling frequency $\fs$ is specified.

%Using an FFT routine, the periodogram can be calculated at the frequencies imposed by the FFT picket-fence effect as
%\begin{equation}
%\hat S[k]  = \hat S(e^{j \dw})|_{\dw = k (2\pi/N)} = \frac{|\textrm{FFT}\{x[n]\}|^2}{N}.
%\label{eq:periodogram_discretetime}
%\end{equation}
%For simplicity, $\hat S[k]$ is \emph{also called the periodogram} but, strictly, it is only a discretized version of $\hat S(e^{j \dw})$.


\subsection{Examples of discrete-time PSD estimation}

\bExample \textbf{Discrete-time PSD of a sinusoid using the \ci{periodogram} function}.
\label{ex:matlabPeriodogram}
\codl{snip_frequency_periodogram} uses the {\matlab} \ci{periodogram} function to estimate the discrete-time PSD of a cosine and compares it with a periodogram directly calculated based on the definition adopted in this text. \figl{cosinePeriodograms} depicts the result.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_periodogram}{snip_frequency_periodogram}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/cosinePeriodograms}
\caption{Discrete-time PSD of $x[n]=10\cos((2\pi/8) n)$ in linear scale estimated with periodograms.\label{fig:cosinePeriodograms}}
\end{figure}

\codl{snip_frequency_periodogram} returns the value $\calP=50$~W for both \ci{Power} and \ci{Power2}. 
The cosine angular frequencies are $\pm 0.7854$~rad as indicated in the top plot of \figl{cosinePeriodograms} for the positive frequency. The periodogram values at $k=-2$ and $k=2$ 
are $\hat S[k]|{k=\pm 2}= (\calP/2) / \Delta f = 400$, given that $\Delta f = \BW / N$ and $\BW=1$.

There was no leakage in the top plot of \figl{cosinePeriodograms}. But in case one allows the \ci{periodogram.m} routine to use zero-padding as in:
\begin{lstlisting}
[Smatlab,f]=periodogram(x,[],[],BW,'twosided'); %zero-padding
\end{lstlisting}
the result is the bottom plot of \figl{cosinePeriodograms}. In this case, \ci{periodogram.m} adopted zero-padding to reach $N=256$ samples. This can be confirmed with
\begin{lstlisting}
Power3= (BW/256)*sum(Smatlab) %Power from periodogram function
\end{lstlisting}
which returns \ci{Power3=50}. Another discrepancy between the two plots in \figl{cosinePeriodograms} is that the bottom plot uses the range $\dw \in [0, 2\pi[$ while the top plot adopted $[-\pi, \pi[$ via \ci{fftshift.m}.
\eExample

\bExample \textbf{MS spectrum and PSD of a sum of discrete-time sinusoids}.
A 1024-points DFT is adopted to inspect a signal composed by two discrete-time cosines that are not bin-centered and have amplitudes 10 and 1~V, as indicated in \codl{snip_frequency_not_bin_cent_cos}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_not\_bin\_cent\_cos}{snip_frequency_not_bin_cent_cos}

\figl{sumOfCosinesPeriodogramMSS} depicts the obtained result. To obtain cosines that are not bin-centered, non-integer numbers \ci{k1=115.3} and \ci{k2=500.8} were specified to create the angular frequencies. Hence, each cosine has most of its power located at bins $k=115$ and 501 as indicated by the data tips in \figl{sumOfCosinesPeriodogramMSS}.

The top graph shows the bilateral MS spectrum with the positive frequencies (the ones that have associated data tips), from $k=0$ to 512 preceding the ``negative'' frequencies from $k=513$ to 1023. The function \ci{fftshift} could be used to make the ``negative'' precede the positive frequencies.
Note that the implicitly adopted rectangular window leads to considerable leakage.
%, otherwise the \ci{log10} function would generate .
The bottom graph is a unilateral representation of the periodogram.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/sumOfCosinesPeriodogramMSS}
\caption{Periodogram and MS spectrum for a sum of sinusoids. Both are in dB scale.\label{fig:sumOfCosinesPeriodogramMSS}}
\end{figure}

The \ci{Power} calculated by the code was 50.5397~W while the theoretical result for eternal sinusoids is $10^2/2 + 1^2/2=50.5$. The small discrepancy is due to the fact that the sinusoids are not bin-centered and, consequently, the signal duration does not correspond to an integer number of periods of the sinusoids. Besides, one should notice that the time-domain signal \ci{x} itself has power $\calP=50.5397$~W, which coincides with \ci{Power}.%\ci{N} is not a multiple of both periods.
Bin-centered cosines with power 50 and 0.5~W would lead to values $10 \log_{10}(25) \approx 13.98$~dB and $10 \log_{10}(0.25) \approx -6.02$~dB at their corresponding frequencies in a bilateral MS spectrum.  Given that $\BW=2 \pi$ and the FFT bin width is $\BW/N=2\pi/1024 \approx 0.0061$, the theoretical values for the unilateral periodogram are $10 \log_{10}(50/0.0061) \approx 39.11$ and $10 \log_{10}(0.5/0.0061) \approx 19.11$. \figl{sumOfCosinesPeriodogramMSS} presents slightly smaller values for both MS spectrum and periodogram due to leakage. Because $\BW=2 \pi$ instead of $\BW=1$, the depicted values are not the ones of a discrete-time PSD $S(e^{j\dw})$.

An interesting aspect is that the relative difference in power between the two sinusoids can be obtained in both periodogram and MS spectrum, leading to $37.79-18.63\approx 19.2$~dB and $12.66-(-6.497)\approx 19.2$~dB, respectively. In other words, the PSD does not allow directly reading the absolute power values but can inform the relation between powers of signal components that reside in specific bins.

% given that the periodogram differs from the MS spectrum by the constant $N$.
\eExample

\bExample \textbf{Periodogram variance does not decrease with number of samples}.
\figl{periodogram} illustrates the adoption of periodograms $\hat S[k]$ to estimate the discrete-time PSD $S(e^{j\dw})$ of a white noise with power of 600~W. 
% discussed in Application~\ref{app:conversion_awgn}. 
The estimation used $N=300$ (top) and $N=3000$ (bottom) samples of the noise signal. The code for signal generation and periodogram estimation for \ci{N=300} (the same code was used for \ci{N=3000} samples) is:
\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_noise\_PSD}{snip_frequency_noise_PSD}
Note that due to the limited number \ci{N} of samples, the \ci{actualPower} values (e.\,g., 512.3 and 601.0~W), may differ from the desired value \ci{power\_x=600}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{./Figures/periodogram}
\caption{Periodograms of a white noise with power equal to 600~W estimated with $N=300$ (top) and $N=3000$ (bottom) samples.\label{fig:periodogram}}
\end{figure}

Recall from \equl{periodogram_discretetime4}, that in the case of $\fs=1$, the periodogram 
%needs to be scaled by $2 \pi$ in order to obtain the 
coincides with the discrete-time PSD estimate $\hat S(e^{j \dw})|_{\dw = k (2 \pi/N)}$.

This example illustrates that the periodogram values with $\fs=1$ have an average \ci{mean(Sk)} that coincides with \ci{actualPower} (because $\Delta f = 1/N$ in this case). However, the variance of the estimate does not decrease with $N$.  For $N=300$ and 3000, the standard deviations are 593.1 and 599.4, respectively. This issue is discussed in Section~\ref{sec:welch}, in which the Welch's PSD estimation method is presented.
\eExample



\subsection{Estimating the PSD from Autocorrelation}

%As suggested by \equl{discrete_psd}, a
An alternative way of estimating the PSD 
%obtaining the periodogram 
of a finite duration discrete-time signal is via the autocorrelation.
The autocorrelation for such finite-duration signals can be obtained via \equl{xcorrFiniteDuration},
%\[
%\hat R_x[k] = \frac{1}{N} \sum_{n=k}^{N-1} x[n] x[n-k],
%\]
and then the PSD estimated with
\begin{equation}
\hat S(e^{j \dw}) = \sum_{k=-N+1}^{N-1} \hat R[k] e^{-j \dw n}.
\label{eq:psd_via_xcorr}
\end{equation}
As previously done, an FFT can be used to obtain the values of $\hat S(e^{j \dw})$ at the FFT frequency grid.

\bExample \textbf{PSD of filtered white-noise via its autocorrelation}.
\codl{snip_frequency_PSD_using_xcorr} illustrates how \equl{psd_via_xcorr} can be used to generate \figl{psd_of_pulse_via_xcorr}.
The theoretical expression for the PSD in \figl{psd_of_pulse_via_xcorr} was obtained from \equl{whiteNoiseLTIOutput}.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_frequency\_PSD\_using\_xcorr}{snip_frequency_PSD_using_xcorr}
%\begin{lstlisting}
%N = 3000; %total number of signal samples
%L = 4; %number of non-zero samples of h[n]
%power_x = 600; %noise power in watts
%x=sqrt(power_x) * randn(1,N); %Gaussian white noise
%h=ones(1,L); %shaping pulse with square waveform
%y=conv(h,x); %filter signal x with filter
%H=fft(h,4*N); %DTFT (sampled) of the impulse response
%Sy_th=power_x*abs(H).^2; %PSD theoretical expression
%M=256; %maximum number of lags
%[Ry,lags]=xcorr(y,M,'biased'); %estimating autocorrelation
%Sy_corr=abs(fft(Ry)); %take magnitude of DTFT (sampled)
%\end{lstlisting}


\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/psd_of_pulse_via_xcorr}
\caption{PSD of a filtered white noise $x[n]$ estimated via the autocorrelation.\label{fig:psd_of_pulse_via_xcorr}}
\end{figure}

%In practice, even when $N$ signal samples are available, the autocorrelation is calculated for a maximum lag $M < N$. There is a rule of thumb that suggests adopting $M = N/10$.

Note that for an input signal with $N$ samples, the function \ci{xcorr} would, by default, output an autocorrelation with lags from $-N$ to $N$. 
However, to avoid noisy estimations at the autocorrelation tails, it is a good strategy to use a maximum lag $M \ll N$, if possible. There is a rule of thumb that suggests adopting $M = N/10$. This was used in \codl{snip_frequency_PSD_using_xcorr}, where \ci{Ry} was calculated for a maximum lag of \ci{M=256} while the signal had \ci{N=3000} samples.
%In case \ci{[Ry,lags]=xcorr(y,N,'biased')} is adopted, the PSD estimate is noisier.
\eExample 
% and the required normalization factor $1/N$ can be obtained from the definitions or, alternatively, as follows.

\bExample \textbf{PSD of a discrete-time impulse}.
\label{ex:psdOfImpulse}
%Another example is provided in the sequel. 
Let $x[n]= A \delta[n-n_0]$ be a discrete-time impulse with amplitude $A$ that is
multiplied by a rectangular window of $N$ samples for FFT-based spectral analysis. 
For example, with $N=5$ and $A=6$ and \ci{x=[6, 0, 0, 0, 0]} is the vector representing $x_w[n]$. 
Because the PSD disregards the phase information, the value of $n_0$ and the position of the peak with amplitude $A$ within the windowed signal $x_w[n]$ is not relevant. For the given example, using \ci{x=[0, 0, 0, 6, 0]} would lead to the same results.
This signal $x_w[n]$ also has an autocorrelation that is an impulse. And its PSD is white with a constant  value of $S(e^{j \dw})=\calP=36/5=7.2$, as indicated in \equl{whiteNoisePSD}. One can interpret the
density $7.2/(2 \pi)$ in units of W/rad.
\codl{snip_frequency_impulse_PSD} calculates the periodogram and MS spectrum.
% using the periodogram of \equl{periodogram_discretetime}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_impulse\_PSD}{snip_frequency_impulse_PSD}

The last commands obtain the average power equal to 7.2~W using three distinct alternatives. All elements of \ci{Sms} have a value of $7.2/5=1.44$~W, given that the power was uniformly distributed over the five discrete frequency values. The bilateral periodogram was calculated assuming a bandwidth $\BW=1$ such that the returned value has all values equal to 7.2, coinciding with $S(e^{j \dw})=7.2$ (see \equl{periodogram_discretetime4}).
\eExample


\section{Nonparametric PSD Estimation via Welch's method}
\label{sec:welch}

%\section{More Discussion about PSD}

This section discusses Welch's method, one of the most popular nonparametric PSD estimation method.
%Having familiarity with these basic aspects of PSD estimation allows discussion of the following important aspect.
%: the periodogram variance does not decrease with the number of samples.

\subsection{The periodogram variance does not decrease with \emph{N}}

It may seem counterintuitive but, as \figl{periodogram} indicates, 
increasing the number $N$ of samples does not decrease the variance of the periodogram estimates $\hat S[k]$. 

%One can observe in \figl{periodogram} that the increase in $N$ did not decrease the variance of the estimate. 
In fact, it is well-known in spectral analysis theory\footnote{See, e.\,g.,~\cite{Hayes96,Stoica05} and \akurl{http://www.math.chalmers.se/Stat/Grundutb/CTH/mve135/1011/Complement/Complement.pdf}{4spe}.} that the periodogram $\hat S[k]$ is an unbiased estimator of the true PSD $S(f)$, i.\,e.,
\[
\lim_{N \rightarrow \infty} \ev[\hat S[k]] = S(f).
\]
In other words, the average periodogram converges to the right values as $N$ increases.

However, the variance of the periodogram estimator is approximately $S^2(f)$ even for large $N$. This is a consequence of a result that is valid under mild conditions:
\begin{equation}
\ev[ (\hat S[k] - S(f))^2 ] = S^2(f) + R_N,
\label{eq:variancePSD}
\end{equation}
where $R_N$ tends to zero when $N \rightarrow \infty$.
In other words, the standard deviation of the periodogram is approximately as large as the spectrum it should estimate.
%Multiplying the signal $x[n]$ by a window $w[n]$ can alleviate the artifacts imposed by taking a finite number $N$ of samples. In fact, extracting $N$ samples can be modeled as the result of multiplying the original signal by a rectangular window of $N$ samples equal to 1 and 0 outside the corresponding observation interval. 
Using windows such as Hamming and Hann can improve the spectral analysis, but does not help with respect to the variance of the periodogram.

One may expect that increasing the number $N$ of samples would lead to a better estimate of the PSD. In fact, the estimation does improve, but with respect to its frequency resolution $\Delta f=\fs/N$, not the variance.

\subsection{Welch's method for PSD estimation}

The main strategy to decrease the variance of a PSD estimation is to split the available data into $K$ segments of $M$ samples each, calculate the periodogram for each segment and then take their average. Methods such as Bartlett's and Welch's are based on this principle.
The disadvantage of using less ($M < N$) samples is that the frequency resolution $\Delta f$ decreases (from $\Delta f=\fs/N$ to $\fs/M$).

The main distinction between Bartlett's and Welch's methods is that the latter tries to combat the decrease in $\Delta f$ by overlapping segments, such that for a given $K$, the value of $M$ can be larger than $M=N/K$, where $N$ is the total number of available samples. For example, an overlap of 50\% of segments of length $M=4$ means that the first segment corresponds to samples with indices $n=0,1,2,3$, the second segment has samples with $n=2,3,4,5$, the third with $n=4,5,6,7$ and so on.

In {\matlab}, the command \ci{Sx=pwelch(x)} estimates a PSD via Welch's method using default values ($K=8$, Hamming window and an overlap of 50\%). An example of a complete command is
\begin{lstlisting}
[Sx,w] = pwelch(x,window,Num_overlap,N_fft,Fs,'twosided')
\end{lstlisting}
where \ci{window} is the window, \ci{Num\_overlap} is the number of samples that are shared between two neighboring segments, \ci{N\_fft} is the number $N$ of FFT points used for calculating the periodograms, \ci{Fs} is the sampling frequency and \ci{'twosided'} can be used to force \ci{pwelch} to return values from 0 to $2\pi$, even if the signal is real. The number $M$ of samples per segment corresponds to the window length.
The companion function \ci{ak\_psd.m} illustrates how to invoke \ci{pwelch.m} and is convenient when the sampling frequency is given in Hz.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{./Figures/welch_awgn_3000}
\caption{PSD of a white noise $x[n]$ with power equal to 600~W estimated by Welch's method with $M=32$ (top) and $M=256$ (bottom) samples per segment.\label{fig:welch_awgn_3000}}
\end{figure}

\bExample \textbf{Variance reduction with Welch's method}.
\codl{snip_frequency_pwelch} provides an example of using \ci{pwelch} for PSD estimation of the same signal as \figl{periodogram} and leads to \figl{welch_awgn_3000}. The top plot was obtained with \ci{Nfft=32} and then, the
code was modified to use \ci{Nfft=256} and generate the bottom plot. Note that the total number of samples is \ci{N=3000} in both cases. The standard deviations among the periodograms were 46.5 and 132.0, respectively. The smaller \ci{Nfft}, a larger number of periodograms is calculated and their averaging reduces the variance.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_pwelch}{snip_frequency_pwelch}

When comparing the plots in \figl{welch_awgn_3000} with the bottom plot in \figl{periodogram}, which also used $N=3000$ samples, 
%with $N=3000$ samples (bottom) and \figl{welch_awgn_3000}, 
it is evident that averaging periodograms (for example, via Welch's method) decreases the variance. \figl{welch_awgn_3000} also illustrates the tradeoff between variance reduction and frequency resolution as the top graph with $\Delta \dw = 2 \pi/32$ is much smoother than the bottom one with $\Delta \dw = 2 \pi/256$.

A detail is that \codl{snip_frequency_pwelch} uses the default of the window overlap (the third argument of \ci{pwelch.m} as \ci{[]}) because Octave assumes it is a percentage while Matlab assumes it is the number of samples.
\eExample 

\bExample \textbf{Estimating the PSD of filtered white noise via Welch's method}.
\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/periodogram_of_pulse}
\caption{PSD of a filtered white noise $x[n]$ estimated by Welch's method.\label{fig:periodogram_of_pulse}}
\end{figure}

\codl{snip_frequency_filtered_noise_PSD} illustrates an application of $S_y(e^{j \dw}) = \sigma_x^2 |H(e^{j \dw})|^2$ (\equl{discrete_filtered_awgn}), where the system impulse response is \ci{h=[1,1,1,1]} and $H(e^{j \dw})$ is a sinc. 

\includecodelong{MatlabOctaveCodeSnippets}{snip\_frequency\_filtered\_noise\_PSD}{snip_frequency_filtered_noise_PSD}

\codl{snip_frequency_filtered_noise_PSD} was used to generate \figl{periodogram_of_pulse}, where one can notice that, even with the averaging used in \ci{pwelch}, the variance continues proportional to the PSD magnitude, as indicated by \equl{variancePSD}.
\eExample 

\bExample \textbf{Welch's method using the Goertzel algorithm}.
Note that in Matlab, when the call to \ci{pwelch} specifies the frequency points, \ci{pwelch} can use the Goertzel algorithm instead of an FFT. An example is the following code:
\begin{lstlisting}
x=randn(1,10000); Fs=1e4; %some random vector and sampling freq.
f=linspace(-Fs/2,Fs/2,1024); %create a frequency axis
Px=pwelch(x,hamming(1024),512,f,Fs); %Octave has distinct syntax!
plot(f,10*log10(Px)); %show PSD from -Fs/2 to Fs/2
\end{lstlisting}
The Goertzel algorithm\index{Goertzel algorithm} can slow down the computations when compared to the FFT-based PSD estimation because it targets situations in which the frequencies are not uniformly spaced such as in DTMF detection algorithms (see \figl{dtmf_spectrogram_exercise}).
\eExample 

\section{Parametric PSD Estimation via Autoregressive (AR) Modeling}
\label{sec:arModelingPSD}

In Section~\ref{sec:arma}, the terms AR and MA (autoregressive and moving-average, respectively) 
were associated to filters. Here they will also denote WSS random processes that have white noise as the
input to a LTI filter $H(z)$ that is AR or MA, respectively.

%Teach LPC that will be useful for adaptive filtering
%Do not I need ``systems'' ? Z-transform, before?
The periodogram and Welch's methods are categorized as ``classical'' or ``nonparametric'' spectral analysis.
This section discusses a method from ``modern'' spectral analysis, which consists in indirectly estimating a PSD $S(e^{j \dw})=|H(e^{j \dw})|^2$ by first estimating the parameters of the autoregressive model (or filter):
\begin{equation}
H(z) = \frac{g z^P}{\prod_{k=1}^P (z-p_k)} = \frac{g}{A(z)} = \frac{g}{1+a_1 z^{-1} +a_2 z^{-2} + \ldots  +a_P z^{-P}},
\label{eq:ar_modelP}
\end{equation}
which corresponds to \equl{ar_model}, repeated here for the convenience of calling $P$ (instead of $N$) the system order.

Such ``parametric'' spectral estimator has the advantage that there are potentially fewer parameters to be estimated when compared to the values of a PSD. For example, when using an $M$-point FFT for PSD estimation using Welch's algorithm, $M$ values need to be estimated. In contrast, the AR model requires estimating only $P \ll M$ parameters and the gain $g$. On the other hand, if the assumed model is incorrect, the parametric estimator may lead to highly inaccurate results.

The goal of AR-based PSD estimation is to find $H(z)=g/A(z)$ that, when excited by white noise $x[n]$ generates an output that has the same statistics of $y[n]$, i.\,e.,
\[
x[n] \rightarrow\boxed{\frac{g}{A(z)}}\rightarrow y[n].
\]
%Summarizing what has been discussed, the PSD of a discrete-time random signal $y[n]$ is estimated here by 
Hence, it is assumed that $y[n]$ is a realization of an ergodic autoregressive random process of order $P$, denoted as AR($P$).


The problem is posed mathematically as finding the FIR filter $A(z)=1+\sum_{i=1}^P a_i z^{-i}$ that minimizes the energy $\sum_n |x[n]|^2$ of the output signal $x[n]$ in
\[
y[n] \arrowedbox{A(z)} x[n].
\]
The filter $A(z)=1-\tilde A(z)$ is called the \emph{prediction-error filter} because $\tilde A(z)=-\sum_{i=1}^P a_i z^{-i}$ predicts an estimate $\tilde y[n]$ of $y[n]$ as
\begin{equation}
\tilde y[n] = -\sum_{i=1}^P a_i y[n-i],
\label{eq:lpcPrediction}
\end{equation}
based on the $P$ past samples of $y[n]$. This is depicted in \figl{prediction_error_filter}. The prediction error is $x[n] = y[n] - \tilde y[n]$ and, when $A(z)$ is an optimum solution, $x[n]$ has characteristics of white noise with power $\sigma_x^2$. The signal $x[n]$ is also called \emph{prediction error}\index{Prediction error} and, therefore, $g^2$ is called the prediction error power. 
%Funny is to estimate a system, and from there, the spectrum of a signal.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./FiguresNonScript/prediction_error_filter}
\caption{The prediction-error filter is $A(z)=1-\tilde A(z)$, where $\tilde A(z)$ provides a prediction $\tilde y[n]$ of the current $n$-th signal sample $y[n]$, based on previous samples $y[n-1], \ldots, y[n-P]$.\label{fig:prediction_error_filter}}
\end{figure}

%But using predictionError = filter(A,1,y) does not lead to to a signal with power given by LPC

AR-based modeling using \equl{lpcPrediction} is widely used in speech coding and, because $\tilde y[n]$ in \equl{lpcPrediction} is a linear combination of past inputs, 
it is called \emph{linear predictive coding} (LPC)\index{Linear predictive coding}.

The filter $A(z)$ is often used to perform \emph{decorrelation}\index{Decorrelation} of a signal ($y[n]$ according to the adopted notation), while $1/A(z)$ is called the \emph{synthesis filter} because it can generate a signal ($y[n]$) with a given PSD from white noise ($x[n]$).

\bExample \textbf{Experimenting with Matlab's aryule function.}
Two examples of autoregressive modeling using Matlab are provided here. Similar commands can be used with Octave, as later suggested in \codl{snip_frequency_PSD_estimation}.

The function \ci{aryule.m} returns the filter \co{A} and the power \co{Perror} of the white noise signal $x[n]$, corresponding to $A(z)$ and $g^2$, respectively. When a white signal with power \ci{Perror} is used as input of the synthesis filter $1/A(z)$, the resulting signal has the power of $y[n]$.

As an example, the result of the following code composes \tabl{lpcExample}.
\begin{lstlisting}
y=(1:100)+randn(1,100); %signal composed by ramp plus noise
for P=1:4 %vary the LPC order
	[A,Perror]=lpc(y,P) %estimate filter of order P
end
\end{lstlisting}

\begin{table}
\centering
\caption{LPC result for different orders $P$ for a ramp signal with added noise.\label{tab:lpcExample}}
\begin{tabular}{|c|c|c|}
\hline
Order $P$ & Filter $A(z)$ & \ci{Perror} ($g^2$) \\ \hline
1 & [1.0,   -0.9849] & 102.1144 \\ \hline
2 & [1.0,   -0.9883,    0.0035] & 102.1131 \\ \hline
3 & [1.0,   -0.9884,    0.0082,   -0.0047] & 102.1109  \\ \hline
4 & [1.0,   -0.9884,    0.0082,   -0.0138,    0.0092] & 102.1022 \\ \hline
\end{tabular}
\end{table}

\tabl{lpcExample} illustrates that the LPC filter $A(z)=1-0.9849 z^{-1}$ of order $P=1$ can extract most of the correlation among the samples of the input signal \ci{y}. For $P>1$, the coefficients $a_i$ for $i>1$ have relatively small values. Besides, \ci{Perror} does not decrease significantly with $P$. A second example illustrates a situation where $P=2$ outperforms $P=1$.
\codl{snip_frequency_aryule} is similar to the previous one, but simulates an AR(2) process and allows to create \tabl{lpcExample2}.
%\begin{lstlisting}
%N=100; x=randn(1,N); %WGN with zero mean and unit variance
%y=filter(4,[1 0.5 0.98],x); %realization of an AR(2) process
%for P=1:4 %vary the LPC order
%    [A,Perror]=lpc(y,P) %estimate filter of order P
%end
%\end{lstlisting}
\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_aryule}{snip_frequency_aryule}

%As an example, the result of executing \codl{snip_frequency_aryule} composes \tabl{lpcExample}.

\begin{table}
\centering
\caption{LPC result for different orders $P$ for an AR(2) realization.\label{tab:lpcExample2}}
\begin{tabular}{|c|c|c|}
\hline
Order $P$ & Filter $A(z)$ & \ci{Perror} ($g^2$) \\ \hline
1 & [1.0, 0.2566] & 513.6893\\ \hline
2 & [1.0, 0.5089,    0.9832] & 17.1227\\ \hline
3 & [1.0, 0.4601,    0.9579,   -0.0496] & 17.0805\\ \hline
4 & [1.0, 0.4595,    0.9694,   -0.0441,    0.0120] & 17.0781\\ \hline
\end{tabular}
\end{table}

As expected from the analysis of an AR(2) realization, \tabl{lpcExample2} shows a drastic improvement when transitioning from $P=1$ to 2, but without significant decrease in \ci{Perror} afterward. Note that the correct filter was $H(z)=4/(1+0.5z^{-1}+0.98z^{-2})$, while the estimation for $P=2$ was $\hat H(z)=\sqrt{17.1227}/(1+0.5089z^{-1}+0.9832z^{-2})$. Increasing the number of samples to \ci{N=10000} leads to the accurate estimation $\hat H(z)=\sqrt{16.3780}/(1+0.5010z^{-1}+0.9822z^{-2})$.
\eExample

%AR and MA random processes
\subsection{{\akadvanced} Spectral factorization}

Spectral factorization\index{Spectral factorization}\footnote{See, e.\,g., \cite{Barry04} (pages 25 and 32).} is an important tool for generating a signal with a given discrete-time PSD $S(e^{j\dw})$ via a rational system function $H(z)$. Instead of using Fourier transforms, it is easier to work with the more general z transform $S(z)$. Assuming a valid PSD is real and obeys $S(e^{j\dw}) \ge 0, \forall \dw$, if its corresponding $S(z)$ is \emph{rational}, it can be uniquely factored as
\begin{equation}
S(z) = \gamma^2 H(z) H^*(1/z^*),
\label{eq:spectral_factorization}
\end{equation}
where $\gamma^2$ is the geometrical mean of $S(e^{j\dw})$, $H(z)$ is \emph{monic}\index{Monic signal} and \emph{loosely minimum-phase}. 

To obtain $H^*(1/z^*)$, first $H(1/z^*)$ can be obtained by substituting $z$ by $1/z^*$ in $H(z)$, and then its complex-conjugate is obtained according to the strategy discussed in Section~\ref{sec:complex_conjugate}. For instance, suppose $H(z) = (z-2e^{j4}) (z-3+j5) / (z+6e^{-j4})$, then		
	$H(1/z^*) = ((1/z^*)-2e^{j4}) ((1/z^*)-3+j5) / ((1/z^*)+6e^{-j4})$ and 
	\begin{equation}
	H^*(1/z^*) = \frac{((1/z)-2e^{-j4}) ((1/z)-3-j5)}{((1/z)+6e^{j4})} = \frac{(z^{-1}-2e^{-j4}) (z^{-1} -3-j5)}{(z^{-1} +6e^{j4})}.
	\label{eq:example_reflected_function}
	\end{equation}	
%	In this case, the overall operation corresponded to substituting $z$ by $z^{-1}$ in $H(z)$ and calculating the conjugate only for the numbers.

To understand \equl{spectral_factorization}, it is also useful to note:
\begin{itemize}
	\item Definition of \emph{monic}: signal that has unity-valued amplitude $h[n]|_{n=0}=1$ at $n=0$. In this case, the corresponding $H(z)$ or $H(z^{-1})$ is also monic.
	\item Definition of the geometrical mean
\[
\gamma^2 = e^{\left[\frac{1}{2\pi} \int_{<2 \pi>} \ln S(e^{j\dw}) \textrm{d}\dw \right]}.
\] 
	\item A minimum-phase $H(z)$ has all its poles and zeros inside the unit circle, while a loosely minimum-phase $H(z)$ has at least one zero or pole on the unit circle and, if it is monic, can be written as
	\begin{equation}
	H(z) = \frac{ \prod_{k=1}^M (1-c_k z^{-1})}{\prod_{k=1}^N (1-d_k  z^{-1})}, |c_k| \le 1, |d_k| \le 1,
	\label{eq:monic_rational_function}
	\end{equation}	
	where $c_k$ and $d_k$ are zeros and poles, respectively, which are not outside the unit circle.
	$H(z)$ is monic because its independent term is 1, i.\,e., $H(z)|_{z \rightarrow \infty} = 1$.
\end{itemize}

Now the intuition behind \equl{spectral_factorization} can be developed as follows. Suppose one wants to generate a discrete-time signal with a given PSD $S_y(e^{j\dw})$, by filtering white noise with PSD $S_x(e^{j\dw})$ and 
using $S_y(e^{j\dw}) = |H(e^{j\dw})|^2 S_x(e^{j\dw})$ from \equl{discrete_filtered_wss}. Using the fact that multiplying a complex-number $c e^{j \theta}$ by its conjugate $c e^{-j \theta}$ leads to its squared magnitude $c e^{j \theta} c e^{-j \theta} = c^2$, one can write $|H(e^{j\dw})|^2 = H(e^{j\dw}) H^*(e^{j\dw})$. Also,
\[
H^*(1/z^*)|_{z=e^{j\dw}} = H^*(1/e^{-j\dw}) = H^*(e^{j\dw}),
\]
such that 
\[
|H(e^{j\dw})|^2 = \left. H(z) H^*(1/z^*) \right|_{z=e^{j\dw}}.
\]
This shows that \equl{discrete_filtered_wss} can be obtained by the more general expression
\begin{equation}
S_y(z) = H(z) H^*(1/z^{*}) S_x(z),
\label{eq:discrete_filtered_wssInZDomain}
\end{equation}
where $H^*(1/z^{*})$ is the \emph{reflected} transfer\index{Reflected transfer function} function of $H(z)$. 

The term reflected is adopted for $H^*(1/z^{*})$ because its poles and zeros are at the \emph{conjugate-reciprocal} locations of respective poles and zeros of $H(z)$, i.\,e., they were reflected through the unit circle. Consider $H(z)$ given by \equl{monic_rational_function}, then 
\begin{equation}
H^*(1/z^{*}) = \frac{ \prod_{k=1}^M (1-c_k^* z)}{\prod_{k=1}^N (1-d_k^*  z)}.
%\label{eq:}
\end{equation}
A parcel $(1-c_k z^{-1})$ of $H(z)$ becomes $(1-c_k^* z)$ in $H^*(1/z^{*})$. Hence, a zero $c_k$ of $H(z)$ turns into a zero $(1/c)^*$ of $H^*(1/z^{*})$. The magnitude $|c_k|$ turns into $1/|c_k|$, while the phase is preserved. This  happens for both zeros and poles.
For example, the zeros of $H^*(1/z^{*})$ in \equl{example_reflected_function} are $0.5 e^{j4}$ and $(3-j5)/\sqrt{34}$, and the pole is $-(1/6)e^{-j4}$. These are the reflected values of zeros and pole of $H(z)$: $2 e^{j4}$, $3-j5$ and $-6e^{-j4}$, respectively. Therefore, if $H(z)$ is loosely minimum-phase, then $H^*(1/z^{*})$ is loosely maximum-phase.
This allows to conveniently factor a rational $|H(e^{j\dw})|^2$ into minimum and maximum phase systems. And because the minimum-phase $H(z)$ is causal and stable, one can generate a process with a given PSD as described in the next paragraphs.

\bExample \textbf{Generating the PSD of a first-order moving-average process}.
\label{ex:ma1Example}
A first-order moving-average process, 
denoted as MA(1), can be described by the difference equation $y[n]=b_0 x[n] + b_1 x[n-1]$,
where $x[n]$ is white noise with PSD $S_x(z)=\sigma^2$. The system function is $H(z)=b_0 + b_1 z^{-1}$. Using \equl{discrete_filtered_wssInZDomain}, its output PSD can be obtained by
\begin{equation}
S_y(z) = (b_0 + b_1 z^{-1}) (b_0^* + b_1^* z) \sigma^2 = \left[b_0 b_1^* z + |b_0|^2 + |b_1|^2 + b_0^* b_1 z^{-1}\right] \sigma^2.
%\label{eq:}
\end{equation}
The frequency response is $S_y(e^{j\dw}) = \left[b_0 b_1^* e^{j\dw} + |b_0|^2 + |b_1|^2 + b_0^* b_1 e^{-j\dw}\right] \sigma^2.$
\codl{snip_frequency_ma1} provides an example with $b_0=1+j3$, $b_1=-0.8-j2$ and $\sigma^2=1$.

\lstinputlisting[caption={MatlabOctaveCodeSnippets/snip\_frequency\_ma1.m},label=code:snip_frequency_ma1]{./Code/MatlabOctaveCodeSnippets/snip_frequency_ma1.m}

The output process has a PSD shaped by the highpass filter. Uncommenting the second line in \codl{snip_frequency_ma1} to adopt \ci{B=[1 0.8]} leads to a lowpass output PSD.

The autocorrelation
\begin{equation}
R_y[\ell] = \left( b_0 b_1^* \delta[\ell+1] + (|b_0|^2 + |b_1|^2)\delta[\ell] + b_0^* b_1 \delta[\ell-1] \right) \sigma^2.
\label{eq:ma1ExampleXcorr}
\end{equation}
is obtained via an inverse Fourier transform of $S_y(z)$ and provides the average power of $y[n]$ as $R_y[0]=(|b_0|^2 + |b_1|^2) \sigma^2$.
\eExample

%This power value is dependent of the original continuous-time WGN.
%proportional to $\no/2$.
%Assume a continuous-time random process $\calX(t)$ with a white PSD $\no/2$ W/Hz (recall that $\calX(t)$ has infinite power). A realization of this process is filtered and then digitized with sampling frequency $\fs$~Hz. 
%If $h(t)$ has gain equal to 1 over the band $-\fs/2$ to $\fs/2$~Hz and zero otherwise, the power of the obtained discrete-time signal $\nu[n]$ is 
%\[
%\sigma^2 = \BW \no = \fs \frac {\no}{2}
%\]
%where $\BW=\fs/2$ in this case.
%Increasing $\fs$ would, accordingly, increase $\sigma^2$. 

%Therefore, because the discrete-time signal has a maximum frequency $\fs/2$, a discrete-

%To understand why $S_x(e^{j \dw}) = \sigma_x^2$, 

%Application~\ref{app:conversion_awgn} provides an example of modeling white processes.

%\eApplication
%\eExample

%\bExample
%Observe that the bandwidth of a discrete-time signal is $\pi$ rad. The nomenclature for the value of a white PSD is $\no$ when the PSD is unilateral and $\no/2$ for bilateral PSDs. Hence, for unilateral and discrete-time white PSDs:
%\begin{equation}
%\no = \frac{\calP}{\pi},
%\label{eq:unilateralDiscrete}
%\end{equation}
%where $\calP$ is the power of the discrete-time signal.
%The division requires $\calP$ to be in linear scale, not dB. The conversion to dB can be done after the division.
%For bilateral and discrete-time white PSDs:
%\begin{equation}
%\no/2 = \frac{\calP}{2 \pi}
%\label{eq:unilateralDiscrete2}
%\end{equation}
%such that for both unilateral and bilateral PSDs, one can recover $\calP$ with $\calP = \pi \no$.


%\bExample
%\subsection{PSD and periodogram of ``white noise''}
%
%In fact, to better interpret \figl{periodogram}, it is useful to invoke \equl{psd_integration} in the case where the (bilateral) PSD is a constant value denoted as $\no/2$, such that 
%\[
%\calP = \frac{1}{2 \pi}\int_{<2 \pi>} S(e^{j \dw}) d\dw = \frac{1}{2 \pi}\int_{<2 \pi>} \frac{\no}{2} d\dw
%\]
%and
%\begin{equation}
%S(e^{j \dw}) = \frac{\no}{2} = \calP.
%\label{eq:whiteNoisePSD}
%\end{equation}
%This is the result that the periodograms in \figl{periodogram} approximate.
%
%Because it has a fixed PSD $S(f) = \no/2$ for an infinite bandwidth, if a continuous-time white noise signal could exist, it would have infinite power. When such signal model is represented in a computer as a discrete-time signal, the bandwidth is restricted to be $\fs$. In this case, from \equl{psd_integration_continuousTime}
%\[
%\calP = \int_{- \infty}^\infty S(f) \textrm{d}f = \int_{- \fs/2}^{\fs/2} \frac{\no}{2} \textrm{d}f = \fs \frac{\no}{2}
%\]
%that can be written as
%\begin{equation}
%S(f) = \frac{\no}{2} = \frac{\calP}{\fs}.
%\label{eq:whiteNoisePSDContinuousTime}
%\end{equation}
%for the interpretation of a digitized white noise as a continuous-time signal.
%\eExample


\subsection{AR modeling of a discrete-time PSD}

\equl{discrete_filtered_wss} informs that, if the input $x[n]$ to a LTI system $H(z)$ is white noise with a PSD consisting of a constant value $S_x(e^{j \dw})=\no/2$, the output PSD is $S_y(e^{j \dw})=\no/2 |H(e^{j \dw})|^2$. Recall that, for discrete-time PSDs, the white noise power $\sigma_x^2$ coincides with the PSD level $\no/2$ such that 
one can write $S_y(e^{j \dw})=\sigma_x^2 |H(e^{j \dw})|^2$.

%\rightarrow 
%Hence, the autoregressive estimation of a spectrum is based on 
As discussed, in AR modeling, the PSD $\hat S(e^{j \dw})$ is obtained by first estimating $A(e^{j \dw}) = A(z)|_{z=e^{j \dw}}$ and $g$, and then using
\begin{equation}
\hat S(e^{j \dw}) = |H(e^{j \dw})|^2 = \frac {g^2} {|A(e^{j \dw})|^2},
\label{eq:ar_psd}
\end{equation}
where it is assumed that $x[n]$ has unit variance $\sigma_x^2=1$. Alternatively, $H(z)$ could be restricted to have a numerator equal to one, i.\,e. $H(z)=1/A(z)$ and the squared gain $g^2$ interpreted as the white noise power $\sigma_x^2$ that allows to generate $y[n]$ with power $\calP_y$. This power is given by
\[
\calP_y = \frac{1}{2\pi}\int_{<2\pi>} \hat S(e^{j \dw}) d \dw = \frac{1}{2\pi} \int_{<2\pi>} \frac{g^2}{|A(e^{j \dw})|^2} d \dw
\]
and can be solely controlled by $H(z)$, specially by $g$.

Recall from \equl{powerOutputLTI}, that when the input signal to an LTI system is white, the output power is $\calP_y = E_h \sigma_x^2$, where $E_h$ is the energy of the impulse response $h[n]=\calZ^{-1} \{H(z)\}$.
\codl{snip_frequency_lpcExample} shows how $\calP_y = E_h \sigma_x^2$ can be used to relate $\calP_y$ and $\sigma_x^2$ (or $g^2$).

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_lpcExample}{snip_frequency_lpcExample}

\codl{snip_frequency_AR_PSD} compares a PSD estimated with Welch's method and the one obtained via autoregressive modeling for a realization of an AR(2) process.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_AR\_PSD}{snip_frequency_AR_PSD}

When the power value $\sigma^2$ is used in \codl{snip_frequency_AR_PSD}, it is in fact representing the PSD level $\no/2 = \sigma^2$, which coincide in a discrete-time white PSD as indicated in \equl{whiteNoisePSD}. 
This is not the case for continuous-time PSDs, where the total power of a white noise will be explicitly distributed over frequency to obtain its PSD, as discussed in the sequel.

\subsection{AR modeling of a continuous-time PSD}

If the goal is to use a discrete-time signal $y[n]$ to estimate a continuous-time PSD $\hat S(f)$, then the average power $\sigma_x^2$ should be normalized by $\fs$ to obtain $S_x(f)=\no/2 = \sigma_x^2 / \fs$ and the frequency $\dw$ mapped via $\aw=\dw \fs$ or, equivalently,
$\dw=2 \pi f \ts$, such that $A(f) = A(e^{j \dw})|_{\dw=2 \pi f \ts}$ and
\begin{equation}
\hat S(f) = \frac {\no/2} {|A(e^{j 2 \pi f \ts})|^2}.
\label{eq:ar_psd_continuousHz}
\end{equation}
For example, if $A(z)=1+0.5z^{-1}+0.98z^{-2}$ and $\fs=10$~Hz, then 
\[
A(e^{j 2 \pi f \ts}) = 1+0.5e^{-j 0.2 \pi f}+0.98e^{-j 0.4 \pi f}.
\]
As illustrated in the last line of \codl{snip_frequency_AR_continuousPSD}, the normalization $A(f) = A(e^{j \dw})|_{\dw=2 \pi f \ts}$ can be obtained by simply changing the abscissa.

\codl{snip_frequency_AR_continuousPSD} is similar to \codl{snip_frequency_AR_PSD} but aims at estimating a continuous-time PSD.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_AR\_continuousPSD}{snip_frequency_AR_continuousPSD}

The command \ci{N0div2=Perror/Fs} in
\codl{snip_frequency_AR_continuousPSD}
 converts the power \ci{Perror} into a PSD level $\no/2$, 
while \ci{Shat=N0div2*(abs(H).\string^2)} 
 uses the frequency response \ci{H} obtained from $H(z)=1/A(z)$ to implement \equl{whiteNoiseLTIOutput}.

Care must be exercised with respect to normalization factors when comparing spectra obtained with distinct methods. For example, the bandwidth for normalizing the PSDs depends on the adoption of a unilateral or bilateral spectrum. 
Another detail is that, when converting from bilateral to unilateral representations and vice-versa, one needs to take in account that the values at DC and Nyquist frequencies should not be doubled, as done in \codl{snip_frequency_AR_continuousPSD}.

\subsection{{\akadvanced} Yule-Walker equations and LPC}

There are many techniques to estimate the AR model $1/A(z)$. Here the \emph{Yule-Walker equations}\index{Yule-Walker equations} are adopted given the existence of fast and robust algorithms for solving them, such as  \emph{Levinson-Durbin}\index{Levinson-Durbin algorithm}. 

To keep the discussion relatively short, the goal here is to practice how to use AR estimation to obtain the spectrum of a signal. The algorithms and their properties have been widely discussed in the parametric estimation literature and the reader is encouraged to use them and pursue further knowledge (see Section~\ref{sec:spectral_further}).

Both Matlab and Octave have several interrelated functions for AR modeling such as \ci{lpc}, \ci{pyulear}, \ci{aryule}, \ci{levinson}, etc. The first two will be discussed here.
%, with \ci{lpc} returning \co{A} and \co{g} corresponding to $A(z)$ and $g$, respectively. 

If, for example, the Matlab command
\begin{lstlisting}
[A,Perror]=lpc(x,2) %estimate a second order filter
\end{lstlisting}
returns \ci{A=[1.0,   -1.7196,    0.81]} and \ci{Perror=10}, then the filter $H(z)=\sqrt{10}/(1-1.7196 z^{-1}+   0.81z^{-2})$ has poles at $z=0.9^{\pm j 0.3}$ and, consequently, the corresponding PSD estimate $\hat S(e^{j \dw})$ has a peak at frequency $\dw=0.3$~rad with value $10/|A(e^{j 0.3})|^2 \approx 3082.5$.

In contrast, \ci{pyulear} returns an estimation of $\hat S(f)$ and is useful when the intermediate step of dealing with $g/A(z)$ is not of interest. 

The following two commands can be incorporated to the end of \codl{snip_frequency_AR_continuousPSD}:
\begin{lstlisting}
P=2; %AR filter order
[Syule,f]=pyulear(y,P,Nfft,Fs); %Directly get the PSD, as via LPC
plot(f,10*log10(Syule),'r') %compare in dB
\end{lstlisting}
It can be seen that \ci{pyulear} gives the same PSD as the one obtained by first using \ci{lpc} and then \equl{ar_psd_continuousHz}.

\subsection{Examples of autoregressive PSD estimation}

Two examples will be used to address the issues of AR PSD estimation. In the first one, the signal $y[n] = x[n] \conv h[n]$ is generated as a realization of an autoregressive model, where $x[n]$ is white Gaussian noise and $h[n]$ is the impulse response of an all-poles IIR filter $H(z)=1/A(z)$. 
In this case, the assumed model matches the actual signal. A mismatched condition is simulated in the second example, where $H(z)=B(z)$ is a FIR filter.

%AK-IMPROVE explain the residue and prediction from there \url{http://www.mathworks.com/products/demos/shipping/signal/lpcardemo.html?product=SG}

\bExample \textbf{Evaluating the PSD of an autoregressive random process}.
\codl{snip_frequency_PSD_estimation} creates an all-poles $H(z)$ of order $P=5$ and creates an AR(5) signal $y[n]$ with power $\calP_y=3$~W. 
It then estimates the PSD by solving Yule-Walker's equations via the Levinson-Durbin algorithm and compares it to a PSD estimate via Welch's method and a theoretical expression, as shown in \figl{arSpectrumMatched}.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_frequency\_PSD\_estimation}{snip_frequency_PSD_estimation}
The code above uses \equl{powerOutputLTI} to generate $x[n]$ (implemented as vector \ci{x}) with the proper power \ci{Py\_desired/Eh}. 

%Moreover, 
%
%When comparing the examples of generating realizations of a random process with a FIR or IIR filter, an interesting aspect is that in the case of FIR, 
%because the impulse response \ci{h} of a FIR coincides with the filter coefficients \ci{h=Bsystem}, one can obtain a filter that outputs a signal with power $\calP_y$ equals to the power $\calP_x$ of the white noise input by normalizing the coefficients with
%\begin{lstlisting}
%Eh=sum(Bsystem.^2); %energy of FIR's impulse response
%Bsystem=Bsystem/sqrt(Eh); %new FIR with Eh=1
%\end{lstlisting}
%\codl{snip_frequency_PSD_estimation} used a similar reasoning to obtain \ci{Px}.

%The signal \ci{y} in \codl{snip_frequency_PSD_estimation} is a realization of an autoregressive process of order 5, denoted as AR(5). 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/arSpectrumMatched}
\caption{PSDs estimated from a realization $y[n]$ of a autoregressive process. The model adopted for the AR-based estimation matches the one used to generate $y[n]$.\label{fig:arSpectrumMatched}}
\end{figure}

The signal \ci{y} in \codl{snip_frequency_PSD_estimation} is a realization of an autoregressive process AR(5) of order 5. Hence, the model $H(z)$ perfectly matches the process. Smaller values of \ci{P} would potentially increase the estimation error while larger values lead to curves that are not as smooth as the theoretical one due to the extra (unnecessary) poles in the estimated $H(z)$.

If zoomed, the theoretical and AR estimate curves present some discrepancy at the peak around $2546.5$~Hz, which corresponds to the pair of poles \ci{p4} and \ci{p5} at frequencies $\dw_0 = \pm 2$~rad (recall $\aw=\dw \fs$ and in this case $f_0=\dw_0/\fs/(2\pi)\approx 2546.5$~Hz).
\eExample

The next example presents a situation where the assumed AR model does not match the FIR filter used to generate $y[n]$.

\bExample \textbf{Evaluating the PSD of a moving average random process}.
\figl{arSpectrumUnmatched} was generated with the code \ci{figs\_spectral\_whitenoise.m}, which is not repeated here, but was created according to the editions listed in \codl{snip_frequency_MA_process_PSD}. As indicated, the code uses a FIR to create the signal $y[n]$ corresponding to a MA(10) process.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_MA\_process\_PSD}{snip_frequency_MA_process_PSD}
%\begin{lstlisting}
%f=[0 0.25 0.3 0.55 0.6 0.85 0.9 1]; %frequencies
%Amp=[1 1 0 0 1 1 0 0]; %amplitudes
%M=10; %filter order
%Bsystem=firls(M,f,Amp); %design FIR with LS algorithm
%Asystem = 1; %the FIR filter has denominator equal to 1
%h=Bsystem; %impulse response of a FIR coincides with B(z)
%... %here goes the code of previous example, up to P=5
%P=20;%we do not know correct order of A(z). Use high value
%... %code of previous example continues from here
%\end{lstlisting}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/arSpectrumUnmatched}
\caption{PSDs estimated from a realization $y[n]$ of a moving average process that does not match the model adopted for the AR-based estimation.\label{fig:arSpectrumUnmatched}}
\end{figure}

The code snippet indicates that the FIR has order \ci{M=10} but even using an AR model of order \ci{P=20}, there is significant discrepancy of the AR-based estimated spectrum in \figl{arSpectrumUnmatched}, especially at the valley regions. In this example, due to the model mismatch, the PSD estimated via the (non-parametric) periodogram achieves better accuracy than the one estimated with the AR model.
\eExample

A key component of autoregressive spectral estimation is the choice of the model order and there are many methods and criteria for that, such as the \emph{minimum description length}. A simple alternative is to plot the energy of the prediction residue as the model order increases and choose the best. It helps choosing the model order if one knows aspects of the signal. For example, in speech analysis, each pair of complex-conjugate poles corresponds to a vocal tract resonance known as \emph{formant} frequency. If one is looking for approximately five formants, a model of order 10 or 12  (considering that real poles can occur) is a reasonable choice. In general, if the order is too low, resolution suffers. If one increases the order too much, spurious peaks may appear.

%AK-LATER: PSD of A/D: how noise floor varies with Fs
\ignore{
\section{
\ifpdf
	\texorpdfstring{PSD of A/D: how noise floor varies with $\fs$}{PSD of A/D: how noise floor varies with Fs}
\else
	PSD of A/D: how noise floor varies with $\fs$
\fi
}
%what I want to do is to show that oversampling reduces the noise at ADC but 
%the code below is all wrong. I do not manage to show that. Missing some concept.
close all, clear all
wantDebug = 0
N=1638400; A=4; %# of samples and cosine amplitude of A volts
Fs=48000; %sampling frequency (Hz) for main processing block
oversamplingFactor=1000;
Fs_analog=oversamplingFactor*Fs; %sampling frequency to mimic analog signal
Ts_analog=1/Fs_analog; Ts=1/Fs;
f0=100; %cosine frequency in Hz
t_analog=0:Ts_analog:(N-1)*Ts_analog; %N time instants separated by Ts_analog
M=100; %filter order
Fc=Fs/4;
B=fir2(M,[0 Fc Fc+2*f0 Fs/2]/(Fs/2),[1 1 0 0]);
%
if wantDebug == 1
    t=0:Ts:t_analog(end); %time instants separated by Ts_analog
    plot(t_analog,x_analog,'xy',t,x,'--');
    pause
end
if 0
    x_analog=A*cos(2*pi*f0*t_analog);
else
    awgnSignal=randn(1,N);
    [H,w]=freqz(B,1);
    pause
    x_analog=filter(B,1,awgnSignal);
end
x=x_analog(1:oversamplingFactor:end);
%
Vmax=max(abs(x));
%x=A*cos(2*pi*f0*t);%generate cosine
%
%model quantization
b=12;
delta=(2*Vmax)/(2^b-1);
[xq,ind] = ak_quantizer(x,delta,b);
%
eq = x-xq;
%psd(eq), pause
%
Pn = mean(eq.^2)
Pn_theoretical = delta^2/12
%
discardSegment = M;
%
%yq=filter(B,1,xq);
upsampled_xq=zeros(1,length(xq)*oversamplingFactor);
upsampled_xq(1:oversamplingFactor:end)=xq;
yq=filter(B,1,xq);
%
%align signals
yq(1:M/2)=[];
xq=xq(1:length(yq));
%x=x(1:length(yq));
x=x_analog(1:length(yq));
%
eqy = x(discardSegment:end)-yq(discardSegment:end);
%
Pny = mean(eqy.^2)
%
plot(x_analog), hold on, plot(yq,'r')
}
%
\section{Time-frequency Analysis using the Spectrogram}
%
%\subsection{Spectrogram}
%

\subsection{Definitions of STFT and spectrogram}
\label{sec:spectrogram}

The PSD is a powerful tool to analyze signals in the frequency-domain. However, a single PSD fails when the signal ``changes'' over time. More strictly, if the signal cannot be assumed (WSS) stationary, alternative tools are potentially needed to describe how information varies in frequency and time domains. One relatively simple technique is the \emph{short-time Fourier transform} (STFT)\index{Short-time Fourier transform}.

The concept behind STFT is to extract segments of the signal under analysis using windowing and calculate several Fourier transforms, one for each segment. Mathematically, the STFT of a continuous-time signal $x(t)$ is
\begin{equation}
X(\tau,f) = \int_{-\infty}^{\infty} x(t) w(t-\tau) e^{-j 2 \pi f t} \textrm{d}t,
\label{eq:stft}
\end{equation}
where $\tau$ is used to shift the window $w(t)$ originally centered at $t=0$.
\equl{stft} can be interpreted by fixing $\tau = \tau_0$ and observing that $X(\tau_0,f)$ is the Fourier transform of the windowed signal $x(t) w(t-\tau_0)$. The STFT is invertible and allows for recovering $x(t)$. However, in the sequel it is assumed that the phase can be discarded given that the main interest is to observe the distribution of power along frequency and time.

The \emph{spectrogram}\index{Spectrogram} (for continuous-time) 
\begin{equation}
S(\tau,f) = |X(\tau,f)|^2
\label{eq:spectrogram}
\end{equation}
is defined as the squared magnitude of the STFT and is widely used to analyze nonstationary signals. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{./Figures/cosinesSpectrogram}
\caption{PSD (top) and spectrogram (bottom) of a cosine that has its frequency increased from $\dw=2\pi/30$ to $2\pi/7$ and its power decreased by 20~dB at half of its duration.\label{fig:cosinesSpectrogram}}
\end{figure}

The \ci{specgram} function in {\matlab} can be used to estimate spectrograms $S(\tau,\dw)$ for discrete-time signals. Because $S(\tau,\dw)$ is restricted to real numbers, a color scale can be used instead of a 3-d graph.
For example, \codl{snip_frequency_cosine_spectogram} was used to generate \figl{cosinesSpectrogram}.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_frequency\_cosine\_spectogram}{snip_frequency_cosine_spectogram}
%\begin{lstlisting}
%N=3000; %total number of samples
%n=0:N-1; %abscissa
%x1=100*cos(2*pi/30*n); %first cosine
%x2=1*cos(2*pi/7*n); %second cosine
%x=[x1 x2]; %concatenation of 2 cosines
%subplot(211), pwelch(x) %PSD
%subplot(212), specgram(x), colorbar %spectrogram
%\end{lstlisting}
The code and \figl{cosinesSpectrogram} illustrate that the PSD describes only the existence of two cosines but is not capable of informing their location in time. The spectrogram also indicates, by color, that the first half of the signal is composed of a cosine \ci{x1} with power (20~dB) greater than \ci{x2}. The burst of power spread over the whole bandwidth at approximately $n=1500$ occurs because the windowed signal at this specific FFT is composed by incomplete cycles of both cosines.

Matlab (but not Octave) has the \ci{spectrogram} function. The companion software has \ci{ak\_specgram}, which represent two alternative functions to \ci{specgram}\footnote{Specgram was discontinued in Matlab.} with distinct input parameters.


\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/dtmf_spectrogram}
\caption{All twelve DTMF symbols: 1-9,*,0,\#, each one composed by a sum of a low [697,770,852,941] and a high [1209,1336,1477] (Hz) frequencies.\label{fig:dtmf_spectrogram}}
\end{figure}

As another spectrogram example, \figl{dtmf_spectrogram} shows a sequence of all twelve dual-tone multi-frequency (DTMF)\index{Dual-Tone Multi-Frequency (DTMF)} tones generated by the script \ci{figs\_spectral\_dtmf.m}. In this case, each symbol has a 100~ms duration. It is possible to visually decode the signal. For example, the first symbol (left-most) is composed by a sum of sines of frequencies 697 and $1,209$~Hz (representing ``1'') while the second is composed by frequencies 697 and $1,336$~Hz (symbol ``2'') and so on. Note again the bursts of power at the transitions between symbols.

After creating \ci{dtmfSignal} with $\fs=8$~kHz, the spectrogram of \figl{dtmf_spectrogram} was generated with the commands below, and for a better visualization, the dynamic range was restricted to 40~dB via the parameter \ci{thresholdIndB}:
\begin{lstlisting}
filterBWInHz=40; %equivalent FFT bandwidth in Hz
samplingFrequency=8000; %sampling frequenci in Hz
windowShiftInms=1; %window shift in miliseconds
thresholdIndB=40; %discards low power values below it
ak_specgram(dtmfSignal,filterBWInHz,samplingFrequency,...
   windowShiftInms,thresholdIndB) %calculate spectrogram
\end{lstlisting}

\subsection{{\akadvanced} Wide and narrowband spectrograms}

A fundamental restriction of the STFT and, consequently, spectrograms, is the tradeoff between time and frequency resolution. When the window is made longer (its duration is increased), the frequency resolution improves but the time resolution gets worse. A spectrogram is called narrowband when the window is long and the FFT invoked by the spectrogram routine is equivalent to a bank of filters (see Section~\ref{sec:fftFilterBank}) with relatively narrow bandwidth. In contrast, a wideband spectrogram uses a short window and, consequently, the FFT corresponds to filters with relatively large bandwidths.
The two spectrograms are contrasted here via an example using a speech signal. Speech is highly non stationary given that the information regarding the phonemes is encoded in segments composed of distinct frequencies. The sentence ``We were away'' was recorded with $\fs=8000$~Hz using the Audacity free software and stored as a (RIFF) wav file.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/narrowbandSpectrogram}
\caption{Example of narrowband spectrogam of a speech signal.\label{fig:narrowbandSpectrogram}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/widebandSpectrogram}
\caption{Example of wideband spectrogam of a speech signal.\label{fig:widebandSpectrogram}}
\end{figure}

\figl{narrowbandSpectrogram} and \figl{widebandSpectrogram} were generated with \codl{snip_frequency_narrow_wide_spec}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_narrow\_wide\_spec}{snip_frequency_narrow_wide_spec}
	
\figl{widebandSpectrogram} shows a broadband spectrogram (good time resolution but poor frequency resolution) calculated with frames of 64 samples obtained by a Hann window. The frames had an overlap of 3/4 of the frame size, and the spectrum of each windowed signal is calculated through a 1024-points FFT. Zero-padding was used (1024 instead of 64) in order to sample more densely the DTFT of the windowed signal. 
The user is invited to try the command \ci{specgram(s,M,Fs,hann(M),0)}, which corresponds to not using zero-padding and overlapping to notice the improvements these two strategies bring.

\figl{narrowbandSpectrogram} simply increases the window length from 64 to 256 to create a narrowband spectrogram (poor time resolution and good frequency resolution). The narrowband version allows to see the harmonic structure due to the pitch (see Application~\ref{app:pitch}) as horizontal strips in the graph. This harmonic structure appears in \figl{widebandSpectrogram} as vertical strips.

\section{Applications}
\bApplication \label{ex:fftLeakagePicketFence}\textbf{FFT leakage and picket-fence effects}.
This application explores a script that illustrates the results discussed in Section~\ref{sec:ExampleLeakagePicketFence} and can be found at folder \ci{Applications/FFTLeakagePicketFenceEffects}. The version \ci{ak\_window4\_noGUI.m} runs on both Matlab and Octave while \ci{ak\_window4gui.m} incorporates a Matlab GUI.\footnote{The files \ci{ak\_window4gui.m} and \ci{ak\_window4gui.fig} should be at the same folder.}

Basically the software varies the frequency $\dw_c$ of a cosine and compares the magnitudes of its DTFT and FFT. The goal is to show how the cosine is represented by two frequency components $\pm \dw_c$, and the interaction of the corresponding ``positive'' and ``negative'' sinc functions for composing the DTFT by their sum. Also, the script indicates how the FFT discretizes the frequency axis, which creates the picket-fence effect, and the leakage that an FFT user observes when $\dw_c$ does not coincide with a frequency bin.

\figl{fftEffectsOnlyDTFT} is a screenshot when the cosine frequency is $\dw_c=1.7279$~rad and the corresponding DTFT magnitude. The cosine is represented by two spectral lines (tones), at normalized frequencies $\pm \dw_c/\pi= \pm 0.55$. The (dashed) reference line indicates the value $N(A/2)=24$ given the FFT-length $N=8$ and cosine amplitude $A=6$. Note that the DTFT, which is the sum of two sinc functions, surpasses this reference value for normalized frequencies around $\pm 0.55$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthLarge]{./FiguresNonScript/fftEffectsOnlyDTFT}
\caption{DTFT magnitude of a cosine of frequency $\dw_c=1.7279$~rad.\label{fig:fftEffectsOnlyDTFT}}
\end{figure}

\figl{fftEffectsFFTandDTFT} corresponds to a situation where $\dw_c=2.1206$~rad and the FFT magnitude values are superimposed to the DTFT. Besides, the two sinc functions centered at normalized frequencies $\pm 0.675$ are also displayed such that one can see the DTFT being composed by their summation.  

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthLarge]{./FiguresNonScript/fftEffectsFFTandDTFT}
\caption{DTFT and FFT magnitudes of a cosine of frequency $\dw_c=2.1206$~rad.\label{fig:fftEffectsFFTandDTFT}}
\end{figure}

In \figl{fftEffectsFFTandDTFT}, $\dw_c$ does not coincide with an FFT bin and the leakage is observed by the FFT values, which are samples of the DTFT. The picket-fence is clearly seen because in this case the resolution is low, and only $N=8$ DTFT values in the range $[-\pi, \pi[$ are obtained by the FFT. Increasing $N$ alleviates this effect. The reader is invited to run the code with different settings.
\eApplication

%\bApplication 
%\textbf{Zero-padding increases the FFT resolution but does not alleviate the leakage due to windowing}.
%Recall from \equl{fft_total_duration2} that the resolution of the spectral analysis depends on the duration of the signal $x[n]$ under analysis. In other words, having a too short segment of the signal to be analyzed can impair the spectral analysis. It may seem that using zero-padding can circumvent this limitation but, as discussed in the next paragraphs, it is not equivalent to having a longer segment of $x[n]$.
%
%Assume that a window $w[n]$ of $M$ samples is used to obtain a segment $y[n]=x[n] w[n]$ of the original signal $x[n]$. To analyze the spectrum of $y[n]$ (that should resemble the spectrum of $x[n]$), using an FFT of $M$ points would lead to a frequency resolution of $\dw = 2\pi/M$. It is possible to use zero-padding to improve the resolution to $\dw = 2\pi/N$, $N>M$. Conceptually, zero-padding consists in creating a new signal $z[n]$ by extending $y[n]$ with $N-M$ zero-valued samples. The important point here is that zero-padding improves the FFT resolution but cannot recover the eventual leakage that occurred when $y[n]=x[n] w[n]$ was obtained. In fact, the DTFTs of $z[n]$ and $y[n]$ are the same and zero-padding is simply a strategy to sample the DTFTs using an FFT with a finer frequency grid. 
%
%For example, assuming $x[n]$ is an eternal sinusoid of frequency $\dw_0$ rad, its spectrum has impulses at $\pm \dw_0$ and their replicas separated by multiples of $2 \pi$. Assuming $w[n]$ is a rectangular window with spectrum $W(e^{j \dw})$, the spectrum of $y[n]$ has replicas of $W(e^{j \dw})$ at the positions of the impulses at $\pm \dw_0 + k 2 \pi$.
%Using zero-padding and an FFT to obtain the spectrum of $z[n]$ allows to observe the replicas of $W(e^{j \dw})$ and, consequently, the effect of windowing. To make the example more concrete, assume a signal $x[n]$ composed by the sum of two eternal sinusoids as in the not bin-centered case in \figl{windowsHarmonicAnalysis}. A na\"ive strategy to better distinguish the two sinusoids would be to use zero-padding. However, as shown in \figl{zeroPaddingAndLeakage}, zero-padding simply allows a better visualization of the DTFT of $y[n]$.
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=\figwidthSmall]{./Figures/zeroPaddingAndLeakage}
%\caption{Improved frequency sampling of the DTFT via zero-padding. The top plot shows the result with a Kaiser window of $M=256$ samples and the bottom one used a FFT of $N=2048$ points.\label{fig:zeroPaddingAndLeakage}}
%\end{figure}
%
%\figl{zeroPaddingAndLeakage} was obtained using $y[n]=x_2[n] w[n]$, where $x_2[n]$ is the signal from \figl{windowsHarmonicAnalysis} where the strongest sinusoid is not bin-centered and $w[n]$ is a Kaiser window with $\beta = 7.85$. It can be seen in \figl{zeroPaddingAndLeakage} that both DTFTs, of $y[n]$ and $z[n]$ are the same. 
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=\figwidthSmall]{./Figures/moreSamplesForLessLeakage}
%\caption{Achieving better frequency resolution by increased number of windows samples (from $M=256$ to 2048) with respect to \figl{zeroPaddingAndLeakage}.\label{fig:moreSamplesForLessLeakage}}
%\end{figure}
%
%Now, instead of zero-padding, \figl{moreSamplesForLessLeakage} was obtained by increasing the number of samples from $M=256$ to 2048. This way the window spectrum $W(e^{j \dw})$ becomes narrower in frequency and the leakage is less pronounced. 
%
%In summary, as indicated by \equl{fft_total_duration2}, having a longer segment of $x[n]$ is required for a better analysis of its spectrum. Zero-padding does allows a better analysis of the windowed signal $y[n]$ DTFT, but $Y(e^{j \dw})$ has already the effects of leakage provoked by a relatively short window. If using zero-padding on $y[n]$ could allow a better estimation of $X(e^{j \dw})$, zero-padding would be equivalent to a ``free-lunch''. However, there is a price (as always) to be paid when aiming at a better estimate of $X(e^{j \dw})$: more data from $x[n]$.
%\eApplication 

\bApplication \textbf{Using Welch's method to estimate the mean square (MS) spectrum}.
As explained, to decrease the variance of the estimated spectrum, it is useful to adopt Welch's method. In this case, one should consider the effect of the window $w[n]$ of $L$ samples on the estimation. 

In {\matlab} it is convenient to estimate a MS spectrum using \ci{pwelch}, which takes care of segmenting the input signal. Matlab has support for MS spectrum estimation using \ci{pwelch} while Octave does not.

When used for PSD estimation, \ci{pwelch.m} scales the periodogram dividing it by the energy of the window:
\[
E = \sum_{n=0}^{L-1} w^2[n].
\]
In contrast, when estimating a MS spectrum, the periodogram should be divided by the square of the window DC value
\[
W_{\textrm{DC}}^2 = \left( \sum_{n=0}^{L-1} w[n] \right)^2.
\]
The reason is that the window is convolved with each power spectrum peak and $W_{\textrm{DC}}$ should be 1 to avoid modifying the peak height. 

Matlab has the undocumented option of invoking \ci{pwelch.m} with the argument 'ms' for MS spectrum, such as in:
\begin{lstlisting}
H = pwelch(x,window,[],[],Fs,'twosided','ms');
\end{lstlisting}
which uses the adequate scaling factor to estimate a MS spectrum. 

Because 'ms' is not supported in Octave, \codl{snip_frequency_msspectrum} illustrates that a workaround is to multiply the estimated spectrum by $E/W_{\textrm{DC}}^2$.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_msspectrum}{snip_frequency_msspectrum}

Using either Matlab or Octave, this code provides a pair of peaks of approximately 25~W for \ci{H}, estimated via Welch's method. 
%The spectrum \ci{Xk} is less accurate. 
Note that the cosine is not bin-centered and there is leakage. Matlab users\footnote{Because the flat top window has negative values, it trigs a bug~\akurl{https://mailman.cae.wisc.edu/pipermail/bug-octave/2008-February/011471.html}{obug} in Octave's \ci{pwelch} (\ci{signal} package, version 1.2.2).} can try a flat top window\index{Flat top window} in place of  Hamming's with the command \ci{myWindow=flattopwin(N)}. When estimating the MS spectrum, a flat top window helps because it widens any peak in the original spectrum, such that the wider range of values has more chances of coinciding with a FFT bin. In other words, the flat top is not accurate to locate the sinusoid in frequency (bad frequency resolution) but helps when the goal is to find the sinusoid amplitude.

%Does not work as well as flat top in this case:
%Fs = 1; window = hamming(N); %specify Fs and window

%A pertinent question is why bother using \ci{pwelch.m} when a FFT achieved the same result. 
Another example with a sinusoid that is not bin-centered is provided below. It allows to observe the better performance of averaging segments of the signal using \ci{pwelch.m}, as indicated in \codl{snip_frequency_not_bin_cent_pwelch}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_not\_bin\_cent\_pwelch}{snip_frequency_not_bin_cent_pwelch}
%\begin{lstlisting}
%N=1000; x = 10*cos(2*pi/64*(0:N-1)); %generate a cosine
%Xk = (abs(fft(x,N))/N).^2; %MS spectrum |Xk|^2
%Fs = 1; window = hamming(128); %specify Fs and window
%H=pwelch(x,window,[],128,Fs,'twosided'); %Welch's estimate
%H = H * sum(window.^2)/sum(window)^2; %scale for MS
%disp(['Peak from pwelch = ' num2str(max(H)) ' watts'])
%disp(['Peak when using one FFT = ' num2str(max(Xk)) ' W'])
%\end{lstlisting}
This code informs that \ci{Xk} has peaks of 15.66~W and \ci{H} (estimated via \ci{pwelch.m} with segments of $M=128$ samples) has peaks of 25.02~W (recall that the correct value is 25~W).
In this and many practical cases, averaging windowed segments with \ci{pwelch.m} leads to better results than using a single FFT.
\eApplication 

	
\ignore{
	An analysis uniform filter bank consisting of N filter Hk(?), k=0, 1,?, N-1, obtained from the DFT, is derived from a prototype filter H0(?), where
 			k=1, 2, ?, N-1
%
	The frequency response of the filter Hk(?) is simply obtained by uniformly shifting the frequency response of the prototype filter by multiples of 2? / N. The DFT-based analysis and synthesis filter banks are shown in Figure 3.5. The impulse response h0(n) of the prototype filter H0(?) corresponds to the window sequence.
% 
%Figure 3.5- DFT as a uniform filter bank [Proakis, 96].
%
	To Think in the DFT as a filter bank, is the motivation to find the frequency response of the analysis filter bank (Figure 3.5a). Figure 3.6 shows the frequency response for N=4, when h0(n) is the Rectangular window. For N=4, the prototype filter H0(?) is uniformly shifted to be centered in the frequencies 2?k/4, where k=1, 2, 3. Figure 3.6 shows that all side-lobes achieve a zero in a bin center. Thus, if this filter bank is used to analyze a signal with components in the frequencies 2?k/4, where k=0, 1, 2, 3; there will be no leakage, because the side-lobes structure will not modify the signal spectrum.
	It is instructive to see the frequency response of the DFT-based filter bank for the Triangular window. It is shown in Figure 3.7. From the homework 3, we know that the Triangular window has twice the angular spacing among roots when compared to the Rectangular window, i.e., the double zeros of the Triangular window are located in the frequencies 2?k/N, where k is 2, 4,?, N (considering N even). For the Triangular window, the filter Hk(?), located in the center of bin k, has a non-zero magnitude at bins k+1, k+3,?., as can be seen in Figure 3.7. This is the reason for the very poor performance of the Triangular window showed in Figure 5.7a.
In general, a window has not a zero magnitude in the bin centers (the Rectangular is an exception). In order to circumvent this problem, good windows have small side-lobe levels. The problems with the Triangular window are: (a) it does not have the property of the Rectangular window (side-lobes are zero in the bin centers), (b) its side-lobe levels are large, as can be seen in Figure 5.2c.
%
Figure 3.6- Frequency response of the DFT (N=4) filter bank using a Rectangular window.
%
Figure 3.7- Frequency response of the DFT (N=4) filter bank using a Triangular window.	
}

\bApplication \textbf{Smoothing an FFT result by segmenting the signal and averaging the individual FFTs.}
\label{app:smoothingFFT}
When the task is to calculate the FFT of a noisy signal with $N$ samples, it helps to segment it in $M$ blocks and average the result of $M$ FFTs.
This is a basic strategy in spectral estimation, which is adopted in Welch's method, for example.
Here, the noisy signal is assumed to be the one recorded in the file \ci{impulseResponses.wav}, as suggested in Application~\ref{app:latency}.
%Hence, it is necessary to follow Application~\ref{app:latency} until generating the WAVE file with the system response to the reconstructed discrete-time impulses.

For isolating the response to an impulse using the signal described in Application~\ref{app:latency}, 
%the first task is to isolate the system response to one of the four ``impulses'' (recall they are just gross approximations to $\delta(t)$). A
after zooming \figl{estimatedImpulseResponse} it was decided to consider the range of samples from $n=12650$ to 22050 (given the third impulse was generated at $n=22051$). This way the selected signal does not have many samples with small amplitudes before it actually begins.

In case you do not have available the companion file \ci{impulseResponses.wav} and cannot generate yours due to the lack of a loopback cable, then use the signal suggested in the script below by changing the \ci{if} instruction in line 3:
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_smothing\_FFT}{snip_systems_smothing_FFT}
%Chapter~\ref{ch:frequency} will discuss 
Instead of the FFT, another option is to use
the command \ci{pwelch(h,N,N/2,N,Fs)} to observe the PSD.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/loopbackFreqResponse}
\caption{Sound system magnitude frequency response $|H(f)|$ estimated from an impulse response.\label{fig:loopbackFreqResponse}}
\end{figure}

\figl{loopbackFreqResponse} is the result obtained by running the previous script. It can
be seen that the filters along the processing chain that includes the DAC and ADC 
present strong attenuation after 20~kHz and extra gain from DC to approximately 800~Hz.
Choosing $\fs$ different than 44.1~kHz would move the cutoff frequencies $f_c$ of the anti-aliasing and reconstruction filters.
These filters are analog, but are programmable in the sense that $f_c$ can be modified by software (after the user chooses a new $\fs$ in Audacity, for example). For example, \emph{switched-capacitor filters}\index{Switched-capacitor filters} use a technology that allows this programmability feature. To check this feature in your sound board, repeat the experiment with $\fs=8,000$ and 22,050~Hz.

Another alternative for obtaining an estimate of $|H(f)|$ is to use a white noise as input to the system and use
\equl{wss_continuous_lti_output_psd}.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/loopbackFreqResponseViaNoise}
\caption{Sound system magnitude frequency response $|H(f)|$ estimated from a white noise input.\label{fig:loopbackFreqResponseViaNoise}}
\end{figure}

Audacity can conveniently generate white noise via the menu ``Generate - Noise''. Another option is to create the noise in {\matlab}, save as a WAVE file and read it with Audacity. \figl{loopbackFreqResponseViaNoise} was obtained using  the former procedure to generate noise, play it back and record the system response using a loopback cable as for \figl{loopbackFreqResponse}. Then, the transient in the beginning of the recorded signal was discarded and approximately 32 thousand samples were saved as a WAVE file for processing in {\matlab} via the script:
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_recorded\_noise}{snip_systems_recorded_noise}
\figl{loopbackFreqResponseViaNoise} and \figl{loopbackFreqResponse} are consistent but differ with respect to ordinate values and variance of the estimate. \equl{whiteNoiseLTIOutput} states that the input noise level scales the output PSD. Take this in account and repeat the experiment with an input noise of controlled power, such that you can use \equl{whiteNoiseLTIOutput} to properly scale the estimate and make the values closer to the ones in \figl{loopbackFreqResponse}. Investigate other factors that improve the relation between the two figures.
\eApplication

\bApplication \textbf{Speech formant frequencies via LPC analysis}.

An interesting application of LPC analysis is to estimate the \emph{formant} speech frequencies. The formants are  related to the peaks of the spectrum of a vowel sound and are relatively well-defined in a sentence such as ``We were away'', which does not have consonant sounds. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/formantsAndSpectrogram}
\caption{Spectrogram and tracks of the first four formant frequencies estimated via LPC for a speech sentence ``We were away''.\label{fig:formantsAndSpectrogram}}
\end{figure}

\codl{snip_frequency_formant_frequencies} was used to obtain \figl{formantsAndSpectrogram}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_formant\_frequencies}{snip_frequency_formant_frequencies}
The code illustrates how a long signal can be segmented via windowing. It is interesting to notice that the Hamming window is typically adopted in LPC analysis, and the code uses another (Blackman) window for the spectrogram.
An LPC of order 10 was used to estimate four formants. The code eliminates frequencies below 5~Hz because LPC sometimes returns real poles that correspond to a zero frequency.

\figl{formantsAndSpectrogram} indicates that at the sentence endpoints and at the pauses between words, when the vowel sounds are not well-defined, the formant estimation is noisy. In the middle of the sentence, around 0.5~s, the four formants are clearly identified. In the region around 0.2~s and also the one around 0.8~s, the estimation is  problematic because more than four formants are necessary to describe the signal.
\eApplication

\bApplication \textbf{Spectral distortion}.
\label{sec:spectral_distortion}
When the task is to compare two PSDs, the spectral distortion (SD) can be used. It is given by
\begin{equation}
\label{eq:sd1}
\textrm{SD} = \sqrt{\frac{1}{2\pi} \int_{-\pi}^{\pi}\left[ 10 \log_{10} \left( \frac{S_x(e^{j \dw})} {S_y(e^{j \dw})} \right) \right]^2 \, \textrm{d}\dw}
\end{equation}
where $x[n]$ and $y[n]$ are discrete-time signals and $S_x(e^{j \dw})$ and $S_y(e^{j \dw})$ their respective PSDs. SD is given in dB and corresponds to the root mean square
 value of the error between $10 \log_{10} S_x(e^{j \dw})$ and $10 \log_{10}  S_y(e^{j \dw})$ over frequency $\dw$. It should be noted that the SD can also be used to compare two MS spectra.

Because both the PSD and MS spectrum are normalized versions of the squared magnitude of a DTFT and the SD divides two spectra, any normalization factor is canceled out in \equl{sd1}. Therefore, \equl{sd1} can be written in terms of DTFTs as
\begin{align}
\textrm{SD} &= \sqrt{\frac{1}{2\pi} \int_{-\pi}^{\pi}\left[10\log_{10} \left( \frac{|X(e^{j \dw})|^2}{|Y(e^{j \dw})|^2} \right) \right]^2 \, \textrm{d}\dw} \\
&= \sqrt{\frac{1}{2\pi} \int_{-\pi}^{\pi}\left(20\log_{10} |X(e^{j \dw})| - 20\log_{10}|Y(e^{j \dw})|\right)^2 \, \textrm{d}\dw}
\label{eq:sd2}
\end{align}
where $X(e^{j \dw})$ and $Y(e^{j \dw})$ are the respective DTFTs.
%\lstinputlisting[caption={MatlabThirdPartyFunctions/ak\_universalChannel2.m},label=code:universalChannel2,linerange={20-26},firstnumber=20]{./Code/MatlabThirdPartyFunctions/ak_universalChannel2.m}

\equl{sd2} can be approximated by
\begin{equation}
\label{eq:sd3}
\textrm{SD} \approx \sqrt{ \frac{1}{N}\sum_{k=0}^{N-1} \left(20\log_{10} |X[k]| - 20\log_{10}|Y[k]|\right)^2}
\end{equation}
where $X[k]$ and $Y[k]$ are the respective $N$-points FFTs.
\codl{ak_spectralDistortion} illustrates a software routine to calculate SD.

\lstinputlisting[caption={MatlabOctaveFunctions/ak\_spectralDistortion.m},label=code:ak_spectralDistortion,linerange={1-4,46-54}]{./Code/MatlabOctaveFunctions/ak_spectralDistortion.m}

One issue when using logarithms is to deal with argument values equal to zero. For example, in {\matlab}, \ci{log(0)} gives \ci{-Inf}  and can lead to \ci{NaN} after operations such as \ci{0*log(0)}. To avoid numerical problems,
\codl{ak_spectralDistortion} uses a floor value based on a threshold to limit the minimum value of an argument for logarithm functions. Some lines of \codl{ak_spectralDistortion} that are not being shown also deal with special situations and are an example of how to prevent problems via proper exception treatment when using software. For example, some lines 
of \codl{ak_spectralDistortion} deal with the situation where both signals $x[n]$ and $y[n]$ have only zero values.
\eApplication

\bApplication \textbf{Spectral distortion of speech autoregressive models}.
In speech coding applications, the signal is segmented into frames. Given two speech signals, it is discussed here how to compare their spectra using AR models in a frame-by-frame basis. While Application~\ref{sec:spectral_distortion} used one value of SD for the whole duration of the signals, here the SD is calculated for each frame.

The pair of signals can be found at folder \ci{Applications/SpeechAnalysis}. One of the files correspond to the digit ``eight'' spoken by a male speaker. The other file was generated by a computer (more specifically, using the Klatt speech synthesizer) and aims at sounding  indistinguishable from the first, or target speech.

\codl{spectralDistortion} shows a code snippet with the part that segments the signals into frames and calculates the SD as described in \codl{ak_spectralDistortion} and also two other SD versions. These SD versions are comparisons between autoregressive models and are calculated with the function \ci{ak\_ARSpectralDistortion.m}.
\codl{spectralDistortion} uses the array \ci{isNotSilence}, which was obtained from function \ci{Applications/SpeechAnalysis/endpointsDetector.m}, to avoid computing the SD values for frames that have a low power and can be considered as silence, not speech.

\lstinputlisting[caption={Applications/SpeechAnalysis/spectralDistortion.m},label=code:spectralDistortion,linerange={26-40},firstnumber=26]{./Code/Applications/SpeechAnalysis/spectralDistortion.m}

Autoregressive models are widely used in speech coding applications and their quantization is often evaluated according to the AR version of the SD. In this context, \emph{transparent quantization} is obtained when the average SD is not larger than 1~dB, having no outliers with SD larger than 4~dB and at most 2\% of frames with SD between 2 and 4~dB.
\eApplication


%Muito interessante: tem uns exemplos de sinais a serem analisados e suas respostas.
%\ url{http://www.baudline.com/mystery_signal/14_answer.html#answer}

%Show how Matlab calculates. The influence of the sampling frequency.
%In \ url{C:\ak2008\Classes\TransmissaoDigital\Exams} tem \ url{2007_quiz2_matlab_solutions.m}


%\subsection{Spectrum Analyzers and Equalizers}
%\subsubsection{Using the PC Sound Board}
%
%Check \ url{http://sirk.sytes.net/software/libs/kjdsp/index.htm} for a Java spectrum analyzer using FFT.
%
%\subsubsection{Commercial Spectrum Analyzers and Equalizers}
%\subsection{Observing the Frequency Components of Music}
%\subsection{Changing the Frequency Components of Music with a Equalizer}

\section{Comments and Further Reading}
\label{sec:spectral_further}

Spectral analysis is the topic of several good books, such as \cite{Stoica05}.
%The required normalization for obtaining a PSD from the ESD is the key and general ideal. But 
As discussed in these books dedicated to spectral analysis, there are involved issues on whether or not random signals can be analyzed with Fourier transforms in the same way done for deterministic power signals. Because it turns out the answer is yes, and the result is elegant and even intuitive, the proof and corresponding discussion was omitted here.
Another strategy adopted here to avoid going deeper in the theory of spectral analysis was to focus primarily on discrete-time signals.

Two classic works about windows for FFT-based analysis are \cite{Harris78,Nuttall81}.
Scalloping loss and its compensation are well described in \cite{Lyons11}.
The windows described here are the so-called ``symmetric'' windows in Matlab's documentation, which also discusses ``periodic'' windows. Another variation is to center the window at the origin $n=0$ as discussed in \cite{Harris78}, while here a window with $N$ samples starts at $n=0$ and ends at $n=N-1$.
The issue of correcting the amplitude and power of a FFT-based spectral analysis is discussed in \cite{Brandt11}.

There are distinct definitions in the literature for the periodogram. For example, it can be defined as e.\,g. in \akurl{http://www.math.chalmers.se/Stat/Grundutb/CTH/mve135/1011/Complement/Complement.pdf}{4spe}: 
\begin{equation}
\hat S(e^{j \dw}) \defeq \frac{1}{N} |\textrm{DTFT} \{ x_N[n] \}|^2
\label{eq:periodogram_notusedDefinition}
\end{equation}
to be an approximation of the discrete-time PSD $S(e^{j \dw})$. However, this definition is not directly compatible with the \ci{periodogram} function in {\matlab}.

More information about linear prediction can be obtained in specialized textbooks such as 
\emph{The Theory of Linear Prediction}, by P. P. Vaidyanathan, 2008, available on the Web~\akurl{http://authors.library.caltech.edu/25063/1/S00086ED1V01Y200712SPR003.pdf}{4ppv}.

In~\cite{Huang90b}, it is discussed the minimum description length (MDL) criteria applied to autoregressive models. In spite of the relative simplicity of MDL, its application to autoregressive modeling is involved and \cite{Huang90b} addresses several important issues.

The PSD and, equivalently, the autocorrelation, cannot convey all the information of a signal. 
Similarly, spectrograms are just one of the many techniques for time-frequency analysis. For example, wavelets and Wigner-Ville distributions~\cite{VetterliK95,Mallat08} have been applied in several problems.

In many applications, higher-order spectra analysis~\cite{Nikias93} can bring additional insight when compared to the techniques discussed in this chapter. Another very powerful tool is cyclostationary analysis~\cite{BookGardner94,Giannakis99}, which is important in applications such as blind signal identification~\cite{Gini98} and spectrum sensing for cognitive radios~\cite{Deepa2010}.

\ignore{
\subsection{Higher-Order Spectral Analysis}
%
There is much more information in a stochastic non-Gaussian or deterministic signal than is conveyed by its autocorrelation and power spectrum. Higher-order spectra which are defined in terms of the higher-order moments or cumulants of a signal, contain this additional information.
\begin{verbatim}
http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=3013&objectType=file
\end{verbatim}
%
\subsubsection{Cyclostationary analysis}
%
\ url{http://en.wikipedia.org/wiki/Cyclostationary_process}
%
Vide pp. 74 do livro do Barry.
%
\subsubsection{Wigner-Ville distribution}
}

\ignore{
If WINDOW is a window
other than a rectangular, the resulting estimate is a modified
periodogram.  
%
Como apresentar:
%
Primeiro spectral analysis, depois TF. Trazer a parte de Haar para TF.
Os materiais estao em (VIDE FONTE) %\ url{C:\svns\laps\latex\dslbook_ThingsKeptOutOfSVN\Chapter3_Frequency}
%
Tem o codigo do Stoica em \ url{sm-matlab-2ed.zip}.
%
Em spectral analysis:
%
1) apresentar ESD usando material de \ url{esd_Chapter8.pdf}
%
2) Depois PSDs, constrastar energy and power signals. Notar que ESD nao tem grandes misterios pois o sinal tem finita duracao e eh mais problema estimar PSD a partir de sinais de potencia.
%
Mostrar que periodogram eh algo bem simples.
%
E que o interesse da definicao de PSD eh o limite em \ url{https://ccrma.stanford.edu/~jos/sasp/Periodogram.html}, eq. 7.24
%
E o material do MIT mostra relacao entre continuous e discrete time.
%
3) O formalismo de PSD pode sair de \ url{mit_rowell_lecture_23.pdf}
%
4) Parte de cicloestacionariedade \ url{unm_bsanthan_rp1.pdf} e material do Claudomir.
%
}
\section{Exercises}

\begin{exercises}  % problem set

\item Using the DTFT $W(e^{j \dw})$ of the $N$-samples rectangular window given in \equl{dtftRectangularWindow}, prove that the DTFTs of the Hann and Hamming windows, given in \equl{hannWindow} and \equl{hammingWindow}, are $0.5 W(e^{j \dw}) - 0.25 W(e^{j (\dw+2\pi/(N-1))}) - 0.25 W(e^{j (\dw-2\pi/(N-1))})$ and $0.54 W(e^{j \dw}) - 0.23 W(e^{j (\dw+2\pi/(N-1))}) - 0.23 W(e^{j (\dw-2\pi/(N-1))})$, respectively.

\item A cosine $x[n]=A \cos(\dw_1 n)$ is multiplied by a rectangular window of $N=8$ samples, from $n=0,\ldots,7$ and the windowed signal $x_w[n]$ has its spectrum estimated by an FFT with $N$ points. The FFT resolution is $\Delta \dw = (2\pi)/N$ rad. Using the DTFT of $x_w[n]$, what are the FFT values for $X[k]|_{k=3}$ assuming: a) $\dw_1=3 \Delta \dw$  is centered in the fourth bin and b) $\dw_1=2.5 \Delta \dw$ is half-way the third and fourth bins?

\item Given the signal $x(t)= 12 \sinc(6 t)$ in volts, what are its: a) total signal energy and b) energy within the frequency band $[-1, 12]$~Hz?

\item What is the MS spectrum $\hat S_{\textrm{ms}}[k]$ of $x[n] = 3 \cos( (\pi / 4) n)$ volts? Using the values of $\hat S_{\textrm{ms}}[k]$, how can one obtain the average power of $x[n]$ in watts?

\item Assume $X[k]$ is the $N$-points FFT of a periodic signal $x[n]$ of period $\Nperiod=8$ samples. When $N=8$,
%and a sampling frequency $\fs=10$~Hz, 
$X[k]$ is $[0,3+4j,0,6,0,6,0,3-4j]$. Inform: a) the estimated MS spectrum $\hat S_{\textrm{ms}}[k]$ of $x[n]$, b) the signal power $\cal P$ and c) the new $\hat S_{\textrm{ms}}[k]$ in case $N=16$ samples of $x[n]$ were used, together with a 16-points FFT.

\item Assume $X=[8,8,8,8]$ is the $4$-points FFT of a discrete-time signal $x[n]$. What are the values of its: a) DTFS, b) MS spectrum $\hat S_{\textrm{ms}}[k]$, c) the periodogram $\hat S[k]$ (using {\matlab} convention of $BW=2\pi$), d) the estimated PSD $\hat S(e^{j \dw})$ and e) the signal power $\cal P$.

\item a) What is the PSD $S(e^{j \dw})$ of a complex exponential $x[n]=e^{j \dw_1 n}$, with $\dw_1=\pi/4$~rad after multiplication by a rectangular window of $N=10$ samples? b) When the periodogram $\hat S[k]$ of this signal is estimated with a FFT of $N=10$ points, what are the values of $\hat S[0]$ and $\hat S[1]$?

\item The periodogram of a sinusoid immersed in AWGN was calculated with an $8$-points FFT as
$[2,2,2,6,2,6,2,2]$ in watts/Hz, assuming $\fs=500$~Hz. The noise and the sinusoid are uncorrelated, such that, at the sinusoid bins, the power is the sum of the sinusoid power and the noise power at that bin. Inform: a) the sinusoid average power, b) the noise average power, c) the SNR in dB.

\item The bilateral PSD of a continuous-time white noise signal $\nu(t)$ is $\no/2=8$~W/Hz. This signal was digitized using an ideal lowpass filter and $\fs=20$~kHz, creating a discrete-time signal $\nu[n]$. Inform: a) the average power of $\nu(t)$, b) the average power of $\nu[n]$ and c) the power corresponding to a single periodogram bin of a windowed $\nu[n]$ estimated with an FFT of length $N=256$ when $\fs=20$~kHz and $\fs=2 \pi$ are informed.

\item Assuming $\fs=100$~Hz, the result of Welch's method with 8-length FFTs for a real signal composed by a sum of two sinusoids was \ci{S = [0,100,0,20,0]} in dBW/Hz. Inform: a) the sinusoid frequencies, b) the sinusoid powers in watts and c) their power ratio in dB.
Hint: one can use {\matlab} to investigate this setup with the commands:
\begin{lstlisting}
N=8; n=0:N-1; x1=4*cos(pi/4*n); x2=10*cos(3*pi/4*n); x=x1+x2;
Fs=100; [S,f]=pwelch(x,rectwin(N),0,N,Fs), S*f(2), SdB=10*log10(S)
\end{lstlisting}

\item A signal $x[n]$ has its PSD estimated via AR modeling with the result:
\ci{A=[1.0,   -1.8151,    0.9025]} and \ci{Perror=4} (for example, with the Matlab command \ci{[A,Perror]=lpc(x,2)}). What is the frequency of the peak of this PSD and its amplitude?

\item An autoregressive model $H(z)$ of order one was obtained with the command \ci{[A,Perror]=lpc(x,1)}), where \ci{A=[1, -0.75]} and \ci{Perror=0.04} watts. a) What is the expression for $H(z)=g/A(z)$ assuming that $g$ incorporates the information from \ci{Perror}? b) What is the expression for the PSD $\hat S(f)$ corresponding to this model assuming $\fs=100$~Hz (the expression must depend only on $f$)? c) What is the value of $\hat S(f)$ at frequency $f=2$~Hz?

\item The unilateral PSD of a continuous-time white noise signal $\nu(t)$ is $\no=4$~W/Hz. The signal $\nu(t)$ is the input to a lowpass filter with real-valued coefficients, gain $g=3$ and zero phase within the passband from 0 to 200~Hz (this filter is not realizable). The filter output $x(t)$ is converted to a discrete-time signal $x[n]$ using a C/D process with sampling frequency $\fs=1$~kHz. The average power values of $x(t)$ and $x[n]$ are the same and denoted as $\calP$. Inform: a) the value $\calP$ in watts, b) the detailed graphs of the PSDs $S_{\nu}(f)$, $S_{x}(f)$, and $S_x(e^{j\dw})$ corresponding to the signals $\nu(t)$, $x(t)$ and $x[n]$, respectively. Note that $x[n]$ is not a discrete-time white noise, because its PSD does not have a constant value over the range $[-\fs/2, \fs/2[$.

\item The periodogram of a signal $x[n]$ with $N=100$ samples is estimated via Welch's method using a rectangular window $w[n]$ with $M=25$ samples. The sampling frequency is $\fs=1/\ts=100$~Hz. The window shift is $M$ samples, such that there is no overlapping among windows, and the samples of $x[n]$ are organized into four blocks of $M$ samples each. After zero-padding, each block of samples is converted to frequency domain using an FFT of $N_f=128$ points. a) What is the frequency spacing $\Delta_f$ in Hertz between neighboring periodogram bins? b) What is the frequency resolution $\Delta_m$ in Hertz imposed by the DTFT $W(e^{j\dw})$ of the windows $w[n]$? Assume that $\Delta_m$ is the range between the two zeros of $W(e^{j\ts 2 \pi f})$ that define its main lobe, where $W(e^{j\ts 2 \pi f})$ is the DTFT converted to continuous-time using $\dw=\ts \aw= \ts 2 \pi f$.  c) Is the overall frequency resolution limited by $\Delta_f$ or $\Delta_m$? d) Why is it that someone cannot consistently improve the overall resolution in spectral analysis by simply using zero-padding and improving $\Delta_f$ by using a larger number $N_f$ of FFT points?

\end{exercises}

\section{Extra Exercises}
\begin{exercises}

\item Taking \figl{dtmf_spectrogram} as a reference, decode the DTMF symbols of 
\figl{dtmf_spectrogram_exercise}, which correspond to a phone number with eight digits.
\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/dtmf_spectrogram_exercise}
\caption{Eight DTMF symbols, each one with a 100~ms duration.\label{fig:dtmf_spectrogram_exercise}}
\end{figure}

\item Can a discrete-time random process $\calX[n]$ have infinite power? If yes, under what conditions? Can these conditions be achieved in practice?

\item Modify \codl{snip_frequency_testHannDTFT} to investigate how the sidelobes are decreased in the DTFT of the Hann window by combining three DTFTs of a rectangular window. 
Then, perform the same analysis for the Hamming window.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_frequency\_testHannDTFT}{snip_frequency_testHannDTFT}

\item \textbf{Explore the computation and relationships among convolution, correlation and
FFT.}
Assume a vector \ci{x=[0 1 4 8 0]} with $M=5$ elements. a) Explain
why in Matlab, \ci{conv(x,x)}, the convolution of \ci{x} with
\ci{x} itself results into a vector with $2M-1$ elements. b)
Compare the equation of a convolution \ci{x[n]$\ast$h[n]}
$$y[n]=\sum_{k=-\infty}^\infty h[k] x[k-n]$$
with the equation of an autocorrelation
$$R[\ell]=\sum_{k=-\infty}^\infty x[k] x[k+\ell].$$
Then, prove that \ci{x[n]$\ast$x[$-$n]=R[n]} and confirm it in
Matlab observing that \ci{conv(x,fliplr(x))} and \ci{xcorr(x)}
lead to (approximately) the same result in Matlab. Plot
\ci{conv(x,x)} and \ci{xcorr(x)}. c) The results differ in b)
mainly because \ci{xcorr} uses a FFT to compute the correlation.
Try to explain how the following code is used to compute the
autocorrelation
\begin{verbatim}
    % Autocorrelation
    % Compute correlation via FFT
    X = fft(x,2^nextpow2(2*M-1));
    c = ifft(abs(X).^2);
\end{verbatim}
You may want to use \ci{type xcorr} and take a look at the source
code to see how the vector \ci{c} above is modified to give the
expected result (i.e., there is a postprocessing step after the
two lines above).

\item \textbf{Short-time energy.} Use the same procedure as before
to record the sentence ``We were away'' or similar.
Calculate the speech energy $e$ for each frame of $t$ milliseconds
(ms), without overlap among frames. Use $t \in \{10, 30, 100,
200\}$ ms to get four energy trajectories. Estimate the
first-derivative of $e$ using $\Delta e_n = e_n-e_{n-1}$. Estimate
the second-derivative using $\Delta \Delta e_n = \Delta e_n-
\Delta e_{n-1}$. Use \ci{subplot} to show simultaneously the
waveform (\ci{subplot(411)}) , $e$ (\ci{subplot(412)}), $\Delta e$
(\ci{subplot(413)}) and $\Delta\Delta e$ (\ci{subplot(414)}). Do
that for each $t$ and choose the value of $t$ that leads to the
best plots in terms of distinguishing when it is silence or
speech.

\item \textbf{Spectrograms.} Use a function such as \ci{specgram} to plot both the wide and
narrowband spectrograms of the sentence recorded in the previous
exercise. In both cases, a) what were the window lengths and
equivalent bandwidths you used? b) What were the complete
commands?

\item \textbf{Linear regression}. Assume there are $N$ points $(x,y)$, where $x \in \Re$ and
$y \in \Re$. Find the parameters $a$ and $b$ of a first order
polynomial $\tilde{y}=ax+b$ that best fits the data in terms of
the minimum total squared error $E=\sum_{n=1}^N
(y_n-\tilde{y}_n)^2$. 
%You may want to download the file tcc-raquel97.doc from the class web site and take a look at how the equations (2.13) and (2.14) were derived (note there are some mistakes). 
However, try to write the two final equations using
correlations (for example, the autocorrelation
$R_x(0)=\sum_{n=1}^N x_n^2$ at lag $m=0$ is one possible term).
Use your equations to fit a line to the following training data:
\ci{x=rand(1,100); y=3*x+4+0.5*randn(1,100); plot(x,y,'x')} and
plot the line superimposed to the training data.

\item \textbf{LPC analysis using the autocorrelation method}. Assume
we want to calculate a LPC filter of order $P=3$ for the signal
$x[n]=(10, 8, 3, -2, -4, -2, 1, 2)$ using the autocorrelation
method. The 3-rd order predictor is given by the difference
equation
$$\tilde{y}[n]=a_1 x[n-1] + a_2 x[n-2] + a_3 x[n-3].$$
By convention, the LPC filter is given (in the Z-domain) as
$$A(z)=1 - a_1 z^{-1} + a_2 z^{-2} + a_3 z^{-3}$$
and you can get it using \ci{lpc(x,3)} in Matlab. a) Try to prove
that the coefficients $a_i$ can be found by solving
$$
\begin{bmatrix}
R(0) & R(1) & R(2) \\
R(1) & R(0) & R(1) \\
R(2) & R(1) & R(0) \\
\end{bmatrix}
\begin{bmatrix}
a_1 \\
a_2 \\
a_3\\
\end{bmatrix}
=
\begin{bmatrix}
R(1)\\
R(2)\\
R(3)\\
\end{bmatrix}
$$
where $R(m)$ is the correlation at lag $m$. 
%You may want to download the file tcc-raquel97.doc from the class web site and take a look at how the equations (2.33) and (2.37) were derived (note there are some mistakes). 
b) One can note that the matrix in
the equation above is symmetric and, besides, has equal elements
along the diagonals. This is called a Toeplitz matrix. The matrix
can be inverted with the Levinson-Durbin algorithm below, that is
$\calO(P^2)$ instead of a generic inversion method that is
$\calO(P^3)$.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_frequency\_Levinson\_Durbin}{snip_frequency_Levinson_Durbin}
%\begin{lstlisting}
%function [a,k,E]=my_durbin(R,p)
%% function [a,k,E]=my_durbin(R,p)
%% Inputs:
%% p - order of LPC analysis
%% R - p+1 sample correlation function values, R(0)...R(p)
%% Outputs:
%% a - p LPC coefficients, from a(1) to a(p)
%% k - p reflection coefficients, from k(1) to k(p)
%% E - E energies of error, from E(1) to E(p)
%%Initialization
%k(1)=R(1+1)/R(0+1); a(1)=k(1); E(1)=R(0+1)*(1-k(1)^2);
%%Recursion
%for i=2:p
%    k(i)=(R(i+1)-sum(a(1:i-1).*R(i:-1:2)))/E(i-1);
%    a(i)=k(i);
%    a(1:i-1)=a(1:i-1)-k(i)*a(i-1:-1:1);
%    E(i)=E(i-1)*(1-k(i)^2);
%end
%\end{lstlisting}
The code above has errors, but it should give the same result as
the function \ci{levinson} in Matlab. For example,
\ci{levinson([202 114 -2 -68],3)} gives the right result, but
\ci{my\_durbin([202 114 -2 -68],3)} does not. Unfortunately,
\ci{levinson} is a Mex (compiled) file and one cannot see the
code. Consult a reference about LPC and fix the code above.

\end{exercises}  % problem set