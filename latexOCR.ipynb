{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jvictorferreira3301/LaTeX_OCR_IC/blob/main/latexOCR.ipynb\" target=\"_parent\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"height: 27px; margin-right: 10px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LATex OCR üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTeX-OCR is an advanced application that combines OCR and Transformers technologies to automate the conversion of mathematical expressions into LaTeX code, bringing efficiency to academic and scientific activities. Although it still faces challenges with complex data and manuscripts, it represents an important evolution in the use of AI to facilitate the digitization and editing of complex mathematical content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <figure style=\"margin-right: 10px; text-align: center;\">\n",
    "        <img src=\"./assets/wf.png\" width=\"auto\" height=\"auto\" style=\"display: flex; margin:0\">\n",
    "        <figcaption>Fig. 1 - Flowchart of application. </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"text-align: center;\">\n",
    "        <img src=\"./assets/gif.gif\" width=\"auto\" height=\"auto\" tyle=\"display: flex; margin:10px\">\n",
    "        <figcaption> Fig. 2 - Example of use with GUI.</figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "        <img src=\"./assets/timeline.svg\" width=\"auto\" height=\"auto\">\n",
    "        <figcaption>Fig. 3 - Timeline of models.</figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Optical Character Recognition (OCR):** is the technology used to identify and extract text from images. In applications like LaTeX-OCR, OCR needs to go beyond simple text reading and recognize the hierarchical structure of mathematical expressions.\n",
    "\n",
    "- **Image Tokenization:** Tokenization is the process of breaking data into smaller units, called tokens, that can be processed individually. In traditional text OCR, each character or word may be a token; in images of equations, each part of the expression (symbol, operator, or substructure) can be tokenized.\n",
    "\n",
    "- **Attention and Self-Attention:** Attention is a mechanism that allows the model to ‚Äúfocus‚Äù on specific parts of the data when making decisions. Self-attention enables each part of the sequence (or image) to consider all other parts, capturing complex dependencies.\n",
    "\n",
    "- **Positional Encoding**: In neural networks, especially Transformers, positional encoding is used to preserve the order of tokens, as the model itself does not maintain an explicit sequence.\n",
    "\n",
    "- **Transformers and Encoder-Decoder Networks:** Transformers are neural networks that use attention mechanisms to process data sequences. Encoder-Decoder models are a common Transformer architecture for translating input sequences to output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the foundation, we will train the model using our dataset extracted from the LaTeX files of the following books by Prof. Aldebaro Klautau.\n",
    "\n",
    "- _Digital Signal Processing with Python, Matlab or Octave_\n",
    "- _Digital Communications with Python, Matlab or Octave_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Extract equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all equations from .tex files to a single .tex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% ChatGPT :D\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from wand.image import Image\n",
    "from wand.color import Color\n",
    "import os\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def load_macros(macros_file):\n",
    "    \"\"\"Reads macros from a LaTeX macros file and returns a dictionary of replacements.\"\"\"\n",
    "    macros = {}\n",
    "    macro_pattern = r'\\\\(?:def|newcommand)\\s*\\\\(\\w+)\\s*(?:\\[(.*?)\\])?\\s*(\\{(.*)\\})'\n",
    "    \n",
    "    with open(macros_file, 'r') as file:\n",
    "        for line in file:\n",
    "            match = re.match(macro_pattern, line.strip())\n",
    "            if match:\n",
    "                command, _, definition, inside_def = match.groups()\n",
    "\n",
    "                macros[rf'\\\\{command}'] = inside_def  # Store macro with regex-escaped backslash\n",
    "\n",
    "                # print(f'{command} {inside_def}')\n",
    "            \n",
    "    return macros\n",
    "\n",
    "def replacement_function(match, definition, param_mapping):\n",
    "    if len(param_mapping) > 0:\n",
    "        print(f'{match} {definition} {param_mapping}')\n",
    "\n",
    "def replace_macros(equations, macros):\n",
    "    \"\"\"Replaces macros in equations using the given macros dictionary, ensuring standalone replacements.\"\"\"\n",
    "    for i, equation in enumerate(equations):\n",
    "        for macro, definition in macros.items():\n",
    "            # Replace each macro with its definition\n",
    "            # This regex captures the argument inside the curly braces\n",
    "            equation = re.sub(\n",
    "                rf'(?<!\\\\)({macro})\\{{(.*?)\\}}',  # Match macro and its argument\n",
    "                lambda match: definition.replace('#1', match.group(2).replace('$', '')),  # Replace #1 with the captured argument\n",
    "                equation\n",
    "            )\n",
    "          \n",
    "\n",
    "        equations[i] = equation\n",
    "    return equations\n",
    "\n",
    "def replace_macros2(equations, macros):\n",
    "    \"\"\"Replaces macros in equations using the given macros dictionary, ensuring standalone replacements.\"\"\"\n",
    "    for i, equation in enumerate(equations):\n",
    "        for macro, definition in macros.items():\n",
    "            # Check if the definition contains placeholders\n",
    "            param_pattern = re.findall(r'#(\\d+)', definition)\n",
    "\n",
    "            # Create a mapping for parameters if there are placeholders\n",
    "            param_mapping = {}\n",
    "            for j in range(len(param_pattern)):\n",
    "                param_index = int(param_pattern[j]) - 1  # Convert to 0-based index\n",
    "                # Capture parameters from the equation based on their positions\n",
    "                matches = re.findall(r'(\\{.*?\\}|\\S+)', equation)  # Capture everything in braces and standalone words\n",
    "                if param_index < len(matches):\n",
    "                    param_mapping[f'#{j + 1}'] = matches[param_index]\n",
    "\n",
    "            # Replace each macro using a lambda to avoid escape sequence issues\n",
    "            equation = re.sub(\n",
    "                rf'(?<!\\\\)({macro})(?![a-zA-Z0-9])',\n",
    "                lambda match: re.sub(\n",
    "                    r'(#\\d+)',\n",
    "                    lambda m: param_mapping.get(m.group(0), m.group(0)),  # Substitute placeholders with mapped values\n",
    "                    definition\n",
    "                ),\n",
    "                equation\n",
    "            )\n",
    "        equations[i] = equation\n",
    "    return equations\n",
    "\n",
    "def extract_equations(input_tex_file, macros_file):\n",
    "    # Load macros from macros_file\n",
    "    macros = load_macros(macros_file)\n",
    "    \n",
    "    # Regular expressions to match equations\n",
    "    equation_patterns = [\n",
    "        # r'\\$\\$(.*?)\\$\\$',            # Inline equations with $$ ... $$\n",
    "        # r'\\$(.*?)\\$',                # Inline equations with $ ... $\n",
    "        r'\\\\\\[(.*?)\\\\\\]',            # Displayed equations with \\[ ... \\]\n",
    "        r'\\\\begin\\{equation\\}(.*?)\\\\end\\{equation\\}',  # equation environment\n",
    "        # r'\\\\begin\\{align\\}(.*?)\\\\end\\{align\\}'         # align environment\n",
    "    ]\n",
    "    \n",
    "    # Read the input LaTeX file\n",
    "    with open(input_tex_file, 'r') as file:\n",
    "        tex_content = file.read()\n",
    "    \n",
    "    # Find all matches for each pattern\n",
    "    equations = []\n",
    "    for pattern in equation_patterns:\n",
    "        matches = re.findall(pattern, tex_content, re.DOTALL)\n",
    "        equations.extend(matches)\n",
    "    \n",
    "    # Replace macros in equations\n",
    "    equations = replace_macros(equations, macros)\n",
    "    equations = replace_macros2(equations, macros)\n",
    "\n",
    "    return equations\n",
    "    \n",
    "tex_files_folder = 'bootor_tex'\n",
    "\n",
    "output_tex_file = f'outputs/extracted_equations.tex'\n",
    "\n",
    "macros_file = 'macros/macros.tex'\n",
    "\n",
    "total_equations = 0\n",
    "\n",
    "if not os.path.exists('outputs'):\n",
    "        os.mkdir('outputs')\n",
    "\n",
    "# Write equations to a new .tex file\n",
    "with open(output_tex_file, 'w') as output_file:\n",
    "    output_file.write('\\\\documentclass{article}\\n')\n",
    "    output_file.write('\\\\usepackage{amsmath}\\n')\n",
    "    output_file.write('\\\\usepackage{amssymb}\\n')\n",
    "    output_file.write('\\\\begin{document}\\n')\n",
    "\n",
    "    for subfolder in Path(tex_files_folder).glob('*'):\n",
    "\n",
    "        for file in subfolder.glob('*'):\n",
    "            equations = extract_equations(file, macros_file)\n",
    "\n",
    "            # Write each extracted equation\n",
    "            for eq in equations:\n",
    "                output_file.write('\\n\\\\begin{equation}\\n')\n",
    "                output_file.write(eq.strip())\n",
    "                output_file.write('\\n\\\\end{equation}\\n')\n",
    "\n",
    "            print(f\"Extracted {len(equations)} equations from {file}.\")\n",
    "            total_equations = total_equations + len(equations)\n",
    "\n",
    "    # Write end of the document\n",
    "    output_file.write('\\\\end{document}\\n')\n",
    "\n",
    "print(f'Found {total_equations} equations in the folder {tex_files_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Equations to .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse extracted equations to a .txt where each line will be a different equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_equations(input_file, output_file):\n",
    "    # Regular expression for LaTeX equations in \\begin{equation} ... \\end{equation}, ignoring \\label{}\n",
    "    equation_pattern = r'\\\\begin\\{equation\\}(.+?)\\\\end\\{equation\\}'\n",
    "    label_pattern = r'\\\\label\\{[^}]*\\}'  # Pattern to match and remove labels\n",
    "\n",
    "    # Read input file and extract equations\n",
    "    with open(input_file, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "        # Find all matches for the equation pattern\n",
    "        equations = re.findall(equation_pattern, content, re.DOTALL)\n",
    "        \n",
    "        # Process each equation to remove labels and convert to a single line\n",
    "        cleaned_equations = []\n",
    "        for eq in equations:\n",
    "            cleaned_eq = re.sub(label_pattern, '', eq)  # Remove label\n",
    "\n",
    "            eq_lines = cleaned_eq.split('\\n')\n",
    "\n",
    "            poped = 0\n",
    "            for i in range(len(eq_lines)):                \n",
    "                if eq_lines[i - poped].replace(' ', '').startswith('%'):\n",
    "                    eq_lines.pop(i - poped)\n",
    "                    poped += 1\n",
    "\n",
    "                if (len(eq_lines[i - poped].split('%')) > 1):\n",
    "                    eq_lines[i - poped] = eq_lines[i - poped].split('%')[0]\n",
    "\n",
    "            cleaned_eq = ''.join(eq_lines)\n",
    "\n",
    "            cleaned_eq = re.sub(r'\\s+', ' ', cleaned_eq)  # Replace multiple spaces/newlines with a single space\n",
    "            cleaned_eq = cleaned_eq.strip()  # Trim leading and trailing spaces\n",
    "            cleaned_equations.append(cleaned_eq)\n",
    "\n",
    "    # Write each cleaned equation to a new line in the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        total_equations = 0\n",
    "        for equation in cleaned_equations:\n",
    "            if len(equation) > 0:\n",
    "                file.write(equation + '\\n')\n",
    "                total_equations += 1\n",
    "\n",
    "        print(f'Extracted {total_equations} usable equations!')\n",
    "\n",
    "# Usage\n",
    "input_tex_file = 'outputs/extracted_equations.tex'\n",
    "output_txt_file = 'outputs/extracted_equations.txt'\n",
    "extract_equations(input_tex_file, output_txt_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Generate PDFs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Compile extracted equations to PDF (each equation generates a different file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_equations(input_txt_file):\n",
    "    \"\"\"Extract equations from the input .txt file, each line being an equation.\"\"\"\n",
    "    equations = []\n",
    "    with open(input_txt_file, 'r') as file:\n",
    "        # Read each line and strip any extra whitespace\n",
    "        for line in file:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line:  # Only add non-empty lines\n",
    "                equations.append(stripped_line)\n",
    "    return equations\n",
    "\n",
    "def create_tex_file(equation, index):\n",
    "    \"\"\"Create a .tex file for the given equation.\"\"\"\n",
    "    tex_content = r\"\"\"\\documentclass{article}\n",
    "\\usepackage{amsmath}\n",
    "\\usepackage{amssymb}\n",
    "\\usepackage{xcolor}\n",
    "\\begin{document}\n",
    "\\pagestyle{empty}\n",
    "\\begin{equation*}\n",
    "\"\"\" + equation + r\"\"\"\n",
    "\\end{equation*}\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "    file_name = f\"{str(index).zfill(5)}.tex\"\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(tex_content)\n",
    "    return file_name\n",
    "\n",
    "def compile_tex_file(tex_file):\n",
    "    \"\"\"Compile the .tex file using XeLaTeX.\"\"\"\n",
    "    subprocess.run(['xelatex', tex_file], check=True)\n",
    "\n",
    "def compile_tex_files(tex_files):\n",
    "    \"\"\"Compile multiple .tex files concurrently.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(compile_tex_file, tex_files)\n",
    "\n",
    "def main(input_tex_file):\n",
    "    if not os.path.exists('outputs/pdfs'):\n",
    "        os.makedirs('outputs/pdfs')\n",
    "\n",
    "    equations = extract_equations(input_tex_file)\n",
    "\n",
    "    tex_files = []\n",
    "    for index, equation in enumerate(equations):\n",
    "        tex_file = create_tex_file(equation.strip(), index)\n",
    "        tex_files.append(tex_file)\n",
    "\n",
    "    # Compile all .tex files concurrently\n",
    "    compile_tex_files(tex_files)\n",
    "\n",
    "    # Move PDFs to the output directory and clean up auxiliary files\n",
    "    for tex_file in tex_files:\n",
    "        pdf_file = Path(tex_file).with_suffix('.pdf')\n",
    "        if pdf_file.exists():\n",
    "            pdf_file.rename(Path('outputs/pdfs') / pdf_file.name)\n",
    "            # Clean up auxiliary files\n",
    "            os.remove(tex_file)\n",
    "            os.remove(tex_file.replace('.tex', '.aux'))\n",
    "            os.remove(tex_file.replace('.tex', '.log'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_tex_file = 'outputs/extracted_equations.txt'\n",
    "    main(input_tex_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. PDF to PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert PDF files to PNG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = 'outputs/pdfs'\n",
    "TRG_PATH = 'outputs/images'\n",
    "RESOLUTION = 200  # lower resolution for faster processing\n",
    "\n",
    "def convert_pdf_to_png(pdf_path):\n",
    "    output_path = Path(TRG_PATH) / f\"{pdf_path.stem}.png\"\n",
    "    with Image(filename=str(pdf_path), resolution=RESOLUTION) as img:\n",
    "        img.format = 'png'\n",
    "        img.trim(color=Color('rgba(0,0,0,0)'), fuzz=0)  # Trim transparent areas\n",
    "        img.background_color = Color('white')  # Set white background\n",
    "        img.alpha_channel = 'remove'           # Remove transparency\n",
    "        img.save(filename=str(output_path))\n",
    "    print(f\"Converted {pdf_path} to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists('outputs/images'):\n",
    "        os.mkdir('outputs/images')\n",
    "\n",
    "    pdf_dir = Path(SRC_PATH)\n",
    "    pdf_files = list(pdf_dir.glob('*.pdf'))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(convert_pdf_to_png, pdf_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "        <img src=\"./assets/00103.png\" width=\"auto\" height=\"auto\">\n",
    "        <img src=\"./assets/00227.png\" width=\"auto\" height=\"auto\">\n",
    "        <img src=\"./assets/00283.png\" width=\"auto\" height=\"auto\">\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using code from the original repo\n",
    "\n",
    "!python LaTeX-OCR/pix2tex/dataset/preprocessing/preprocess_formulas.py -i extracted_equations.txt -o norm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted LaTeX:\n",
    "``\\hat {\\cal P} = 10 \\log_{10} {\\cal P}\\textrm{~~~~~~~(${\\cal P}$ in W and $\\hat {\\cal P}$ in dBW)}.``\n",
    "\n",
    "Normalized LaTeX:\n",
    "``\\hat { \\cal P } = 1 0 \\log _ { 1 0 } { \\cal P } \\textrm { ~ ~ ~ ~ ~ ~ ~ ( { \\cal P } i n W a n d \\hat { \\cal P } i n d B W ) } .``\n",
    "\n",
    "\n",
    "Extracted LaTeX:\n",
    "``{ \\cal X } [ n ] \\rightarrow \\boxed { M _ { x } ^ { - 1 } ( z ) } \\rightarrow { \\cal I } [ n ] \\rightarrow \\boxed { M _ { x } ( z ) } \\rightarrow { \\cal X } [ n ]``\n",
    "\n",
    "Normalized LaTeX:\n",
    "``{\\cal X}[n] \\rightarrow\\boxed{M_x^{-1}(z)}\\rightarrow {\\cal I}[n] \\rightarrow\\boxed{M_x(z)}\\rightarrow {\\cal X}[n]``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply padding to images, width and size needs to be divisible by the patch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Code from original repo\n",
    "def pad(img: Image, divable=32): \n",
    "    \"\"\"Pad an Image to the next full divisible value of `divable`. Also normalizes the image and inverts if needed. \n",
    "    Args: \n",
    "        img (PIL.Image): input image \n",
    "        divable (int, optional): . Defaults to 32. \n",
    "    Returns: \n",
    "        PIL.Image \n",
    "    \"\"\" \n",
    "    data = np.array(img.convert('LA')) \n",
    "    data = (data - data.min()) / (data.max() - data.min()) * 255 \n",
    "    if data[..., 0].mean() > 128: \n",
    "        gray = 255 * (data[..., 0] < 128).astype(np.uint8)  # To invert the text to white \n",
    "    else: \n",
    "        gray = 255 * (data[..., 0] > 128).astype(np.uint8) \n",
    "        data[..., 0] = 255 - data[..., 0] \n",
    "\n",
    "    coords = cv2.findNonZero(gray)  # Find all non-zero points (text) \n",
    "    a, b, w, h = cv2.boundingRect(coords)  # Find minimum spanning bounding box \n",
    "    rect = data[b:b+h, a:a+w] \n",
    "    if rect[..., -1].var() == 0: \n",
    "        im = Image.fromarray((rect[..., 0]).astype(np.uint8)).convert('L') \n",
    "    else: \n",
    "        im = Image.fromarray((255 - rect[..., -1]).astype(np.uint8)).convert('L') \n",
    "    dims = [] \n",
    "    for x in [w, h]: \n",
    "        div, mod = divmod(x, divable) \n",
    "        dims.append(divable * (div + (1 if mod > 0 else 0))) \n",
    "    padded = Image.new('L', dims, 255) \n",
    "    padded.paste(im, im.getbbox()) \n",
    "    return padded \n",
    "\n",
    "def convert_images(input_dir, output_dir):\n",
    "    \"\"\"Convert all images in the input directory to grayscale and save in the output directory.\"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process each image file in the input directory\n",
    "    for img_file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "        \n",
    "        # Check if the file is a PNG image\n",
    "        if img_path.endswith('.png'):\n",
    "            # Read the image with PIL\n",
    "            img = Image.open(img_path)\n",
    "            pad_img = pad(img)\n",
    "\n",
    "            # Convert the padded image back to a NumPy array for saving with OpenCV\n",
    "            pad_img_np = np.array(pad_img)\n",
    "            \n",
    "            # Create output file path\n",
    "            output_path = os.path.join(output_dir, img_file)\n",
    "            \n",
    "            # Save the grayscale image using OpenCV\n",
    "            cv2.imwrite(output_path, pad_img_np)\n",
    "            print(f\"Converted and saved: {output_path}\")\n",
    "\n",
    "input_dir = 'outputs/images' \n",
    "output_dir = 'outputs/images_pad'\n",
    "\n",
    "convert_images(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of input (41):\n",
    "\n",
    "c _ { k } = \\frac { 1 } { T _ { 0 } } \\int _ { \\langle T _ { 0 } \\rangle } x ( t ) e ^ { - j 2 \\pi k f _ { 0 } t } d t \\quad , k = - \\infty , \\ldots , - 1 , 0 , 1 , \\ldots , \\infty \\,\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "        <img src=\"./assets/00041.png\" width=\"auto\" height=\"auto\">\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The model consist of a ViT [1] encoder with a ResNet backbone and a Transformer [2] decoder.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "{\n",
    "  backbone_layers: [2, 3, 7],\n",
    "  betas: [0.9, 0.999],\n",
    "  batchsize: 10,\n",
    "  bos_token: 1,\n",
    "  channels: 1,\n",
    "  debug: true,\n",
    "  wandb: true,\n",
    "  decoder_args:\n",
    "    {\n",
    "      \"attn_on_attn\": true,\n",
    "      \"cross_attend\": true,\n",
    "      \"ff_glu\": true,\n",
    "      \"rel_pos_bias\": false,\n",
    "      \"use_scalenorm\": false,\n",
    "    },\n",
    "  dim: 256,\n",
    "  encoder_depth: 4,\n",
    "  eos_token: 2,\n",
    "  epochs: 50,\n",
    "  gamma: 0.9995,\n",
    "  heads: 8,\n",
    "  id: null,\n",
    "  load_chkpt: null,\n",
    "  lr: 0.001,\n",
    "  lr_step: 30,\n",
    "  max_height: 192,\n",
    "  max_seq_len: 512,\n",
    "  max_width: 672,\n",
    "  min_height: 32,\n",
    "  min_width: 32,\n",
    "  model_path: checkpoints_bootor,\n",
    "  name: bootor_40epoch,\n",
    "  num_layers: 4,\n",
    "  num_tokens: 8000,\n",
    "  optimizer: Adam,\n",
    "  output_path: outputs,\n",
    "  pad: false,\n",
    "  pad_token: 0,\n",
    "  patch_size: 16,\n",
    "  sample_freq: 2000,\n",
    "  save_freq: 1,\n",
    "  scheduler: StepLR,\n",
    "  seed: 42,\n",
    "  temperature: 0.2,\n",
    "  test_samples: 5,\n",
    "  testbatchsize: 20,\n",
    "  valbatches: 100,\n",
    "  tokenizer: data_bootor/tokenizer.json,\n",
    "  valdata: data_bootor/val.pkl,\n",
    "  data: data_bootor/train.pkl,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"vocab\": {\n",
    "            \"[PAD]\": 0,\n",
    "            \"[BOS]\": 1,\n",
    "            \"[EOS]\": 2,\n",
    "            \"!\": 3,\n",
    "            \"&\": 4,\n",
    "            \"(\": 5,\n",
    "            \")\": 6,\n",
    "            \"*\": 7,\n",
    "            \"+\": 8,\n",
    "            \",\": 9,\n",
    "            \"-\": 10,\n",
    "            \".\": 11,\n",
    "            ....,\n",
    "            \"alpha\": 198,\n",
    "            \"box\": 199,\n",
    "            \"eta\": 200,\n",
    "            \"ray\": 201,\n",
    "            \"array\": 202,\n",
    "            \"ed\": 203,\n",
    "            \"boxed\": 204,\n",
    "            \"ƒ†8\": 205,\n",
    "            \"lo\": 206,\n",
    "            \"ƒ†o\": 207,\n",
    "            \"ƒ†B\": 208,\n",
    "            \"ƒ†W\": 209,\n",
    "            \"bb\": 210,\n",
    "            \"mathbb\": 211,\n",
    "            \"ƒ†6\": 212,\n",
    "            \"cos\": 213,\n",
    "            \"langle\": 214,\n",
    "            \"rangle\": 215,\n",
    "            \"ƒ†\\\\\\\\\": 216,\n",
    "            \"hat\": 217,\n",
    "            \"ƒ†*\": 218,\n",
    "            \"ƒ†9\": 219,\n",
    "            \"sin\": 220,\n",
    "            \"ƒ†u\": 221,\n",
    "            \"maths\": 222,\n",
    "            \"log\": 223,\n",
    "            \"mathsf\": 224,\n",
    "            \"delta\": 225,\n",
    "            \"ert\": 226,\n",
    "            \"ƒ†7\": 227,\n",
    "            \"nd\": 228,\n",
    "            \"ime\": 229,\n",
    "            \"qrt\": 230,\n",
    "            \"sqrt\": 231,\n",
    "            \"hi\": 232,\n",
    "            \"Delta\": 233,\n",
    "            \"Vert\": 234,\n",
    "            \"ap\": 235,\n",
    "            \"er\": 236,\n",
    "            \"ld\": 237,\n",
    "            \"app\": 238,\n",
    "            \"ƒ†Q\": 239,\n",
    "            \"ƒ†Y\": 240,\n",
    "            \"be\": 241,\n",
    "            \"end\": 242,\n",
    "            \"gin\": 243,\n",
    "            \"begin\": 244,\n",
    "            \"sig\": 245,\n",
    "            \"ƒ†g\": 246,\n",
    "            \"sigma\": 247,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% Train\n",
    "\n",
    "20% Test\n",
    "\n",
    "10% Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating PKLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(f'data_{dataset}', exist_ok=True)\n",
    "\n",
    "dataset = 'bootor'\n",
    "\n",
    "# Extract train data\n",
    "!python -m pix2tex.dataset.dataset --equations ../datasets/{dataset}/labels.txt --images ../datasets/{dataset}/images/train --out data_{dataset}/train.pkl\n",
    "\n",
    "# Extract validation data\n",
    "!python -m pix2tex.dataset.dataset --equations ../datasets/{dataset}/labels.txt --images ../datasets/{dataset}/images/val --out data_{dataset}/val.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train using our data (1029 equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from original repo\n",
    "!python -m pix2tex.train --config config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "        <img src=\"./assets/tl_0.png\" width=\"auto\" height=\"auto\">\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function: 0.3729 at the last batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scores (validation)\n",
    "\n",
    "- val/edit_distance\t2.190018917194931 # Measures how many edits are required to transform the predicted text into the reference text.\n",
    "- val/token_acc\t0.04835147319716357 # Percentage of tokens in the prediction that match the tokens in the ground truth.\n",
    "- val/bleu\t0.0589972678833401 # BLEU (Bilingual Evaluation Understudy): Similarity between generated and reference text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Truth: 2b a^{n}\\cos(\\theta n+\\alpha)u[n].\n",
    "- Prediction: {\\textbf A}=H{\\textbf A}{\\textbf A}{\\textbf A}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Truth: ||{\\textbf X}||=||{\\textbf A}{\\textbf x}||=||{\\textbf x}||.\n",
    "- Prediction: \\overline{{b}}=b/N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "        <img src=\"./assets/tl_1.png\" width=\"auto\" height=\"auto\">\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function: 0.020166 at the last batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scores (validation)\n",
    "\n",
    "- edit_distance 0.18750186692017756\n",
    "- token_acc\t0.5445008777037618\n",
    "- val/bleu\t0.7937541433238978"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Truth: 2b a^{n}\\cos(\\theta n+\\alpha)u[n].\n",
    "- Prediction: 2b a^{n}\\cos(\\theta n+\\alpha)u[n]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Truth: ||{\\textbf X}||=||{\\textbf A}{\\textbf x}||=||{\\textbf x}||.\n",
    "- Prediction: \\lVert{\\textbf x}\\rVert=\\rVert{\\textbf A}{\\textbf x}\\rVert=\\rVert{\\textbf x}\\rVert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Perfomance with test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
