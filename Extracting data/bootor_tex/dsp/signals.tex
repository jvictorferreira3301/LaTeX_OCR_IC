\section{To Learn in This Chapter}

The skills we aim to develop in this chapter are:
\begin{itemize}
  \item Recognize what a signal is
	\item How to digitize signals using software, controlling sample frequency and number of bits per sample
	\item Artificially generate signals (random and deterministic) for simulations
	\item Analyze signals using {\matlab} and Python
	\item Use actual hardware to digitize signals 
	\item Read/write digitized signals from/to binary files
	\item Distinguish digital, analog, continuous-time, discrete-time and sampled signals
	\item Represent arbitrary signals using impulses, step, sinc and rect functions
	\item Analytically manipulate the signal dependent and independent variables
	\item Implement block (or window) processing using software
	\item Relate signals via C/D, D/C, S/D and D/S conversions, where C, D and S stands for continuous-time,
	discrete-time and sampled signals, respectively, and understand how these conversions are used to model
	states of the A/D and D/A processes, where A and D stand for analog and digital signals, respectively
	\item Understand mathematical models for sampling, quantization and reconstruction
	\item While the unit for the continuous-time angular frequency
	$\aw$ is radian/s, observe that the angular frequency $\dw$ in digital domain is an angle and its unit is radian
	\item Relate the frequencies between continuous and discrete-time versions of a signal
	\item Design a quantizer that achieves a target signal-to-quantization-noise ratio (SQNR) given the signal statistics
	\item Review binary number systems used in digital signal processing
	\item Identify important signal categories (even/odd, periodic/aperiodic, energy/power) and use their properties
        %\item Understand the analog to digital (A/D) conversion
	\item Calculate power, energy, fundamental period and autocorrelation of a signal
	\item Use autocorrelation for detecting periodicity and crosscorrelation for aligning signals
	%\item And practice:
	%\begin{itemize}
	%\end{itemize}
\end{itemize}
Specific topics are organized as ``Examples'' along the text or ``Applications'' in the end of the chapter. These can be eventually skipped, but the reader is invited to stop just reading and explore these topics using a computer.

\section{Analog, Digital and Discrete-Time Signals}
\label{sec:ana_dig_dis}

In general, a \emph{signal}\index{Signal} is anything that can be transmitted or stored, and represents useful information. 

Few examples can illustrate what information means in this context.
For example, monochromatic and color \emph{images} are two-dimensional (2D) and 3D signals, respectively, in which the information are the pixel values. For instance, a color image $\bX$ with resolution of $480 \times 640$ pixels can be represented in the RGB color space\footnote{RGB stands for the colors red, green and blue.} by a 3D matrix with dimension $480 \times 640 \times 3$.

A \emph{video} is a sequence $\bX[n]$ of images (each one called a \emph{frame}) that are indexed by the integer $n$. While an image $\bX$ does not depend on time, the value of $n$ in the video $\bX[n]$ is interpreted as the time instant. For example, a video $\bX[n]$ with 500 frames can be represented by a 4D multidimensional array (also called \emph{tensor}) of dimension $500 \times 480 \times 640 \times 3$, in which $\bX = \bX[3]$ is the image (frame) with dimension $480 \times 640 \times 3$ corresponding to time $n=3$.

Another example of a signal is the \emph{electrocardiogram} (ECG)\index{Electrocardiogram (ECG)}. Typically, the ECG is recorded with several channels and constitutes a \emph{multivariate}\index{Multivariate signal} or multidimensional signal $\bx(t)$, where $t$ is the time dimension. The information in the ECG corresponds to the amplitudes of each channel, which for a given $t$ can be organized as a vector $\bx$. For instance, assuming six channels and a given time $t_0$, the vector $\bx=\bx(t_0)$ contains the six amplitude values.

The provided examples illustrate that there are signals with very different characteristics. To simplify the initial discussion, unless otherwise stated, it is assumed hereafter a \emph{one-dimensional real-valued signal} that describes how a single \emph{amplitude} varies over time.

It is useful to classify signals according to the behavior of these two variables: the \emph{independent} variable representing progress in time and the amplitude, which is the \emph{dependent} variable. If time evolution is represented by a real-valued variable $t \in \reals$, the function $x(t)$ is called a \emph{continuous-time signal}. If the progress over time is represented by an integer index $n \in \integers$, the sequence $x[n]$ is called a \emph{discrete-time signal}.

%The same dichotomy of real or integer numbers to represent time is useful to characterize signals according to their amplitude. An amplitude value can also be a real or an integer number. or quantized. More rigorously,
Similarly, the amplitude can freely assume any real value or be restricted to only pre-specified values from a finite set $\setM$. In the latter case, the amplitude is said to be \emph{quantized}. The quantized amplitudes can be eventually non-integer numbers. The number $M$ of elements in $\setM$, i.\,e., the cardinality $M=|\setM|$, indicates whether the system is binary ($M=2$), quaternary ($M=4$) or $M$-ary. A subscript $q$ will be used to denote a quantized signal, such as in $x_q(t)$.

Based on the previous definitions, it is possible to define \emph{analog} and \emph{digital} signals, which are the most common signals in practice.
A digital signal \index{Digital signal} $x_q[n]$ is a discrete-time signal with quantized amplitudes. An analog signal \index{Analog signal} $x(t)$ is a continuous-time signal in which the amplitudes are not quantized (are not restricted to a finite number of $M$ distinct values). Table~\ref{tab:signals_classification} summarizes a useful taxonomy of signals.
\figl{analogsignal} and \figl{digitalsignal} provide examples of analog and digital signals, respectively. 

%TC:ignore
\begin{table}
 \centering
 \caption{Notation used for continuous and discrete-time signals. \label{tab:signals_classification}}
 \begin{tabularx}{\textwidth}{lXX}
 \toprule
 %Amplitude & Concerning time & \\ \hline
 							& Continuous-time, $t \in \reals$ & Discrete-time, $n \in \integers$ \\\midrule
Quantized amplitude		& $x_q(t)$ & $x_q[n]$ (\textbf{digital}) \\ 
Not quantized & $x(t)$ (\textbf{analog}) & $x[n]$ \\ \bottomrule
\end{tabularx}
\end{table}
%TC:endignore

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/analogsignal}		
	\caption{Example of analog signal. Note the abscissa is continuous and the amplitude is not quantized, assuming an infinite number of possible values.\label{fig:analogsignal}}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/digitalsignal}
		\caption[Example of digital signal obtained by digitalizing the analog signal in \figl{analogsignal}]{Example of digital signal obtained by digitalizing the analog signal in \figl{analogsignal}. Note both the abscissa (dimensionless indices) and amplitude of the digital signal are discrete. In this case, the quantized amplitudes assume $M=10$ possible values from $\setM=\{-3,-2,\ldots,5,6\}$.\label{fig:digitalsignal}}
\end{figure}

%Signals that are transmitted in real-world channels are inherently analog.
Signals that exist in the real-world are inherently analog.
Even a DC power supply regulated to output 0 or 5 volts will present a small random amplitude fluctuation due to circuit imperfections and noise. It could then be (strictly) classified as an analog signal $x(t)$. 
But the circuits of a traditional digital system (e.\,g., a computer) can tolerate amplitude variations within a given range and, consequently, one may want to model such signal as continuous in time and with quantized amplitudes, denoting it as $x_q(t)$. It is also possible to find authors calling such continuous-time signal with quantized amplitudes $x_q(t)$ a ``digital'' signal, but this nomenclature will be avoided in this text. 

The interfaces between digital systems and the analog world require analog to digital (A/D) and digital to analog (D/A) conversions, which will be discussed in Section~\ref{sec:addaprocesses}.

%$$x(t) \arrowedbox{SAMPLING} \arrowedbox{QUANTIZATION } x_q[n].$$

\subsection{{\akadvanced} Ambiguous notation: whole signal or single sample}
\label{sec:signal_notation}

It should be noted that the notation $x[n]$ (the same happens for $x(t)$) is ambiguous in the sense
that it is widely used to represent both: a) the complete sequence and b) a sample at time $n$.
In most scenarios both interpretations are valid because if someone provides an equation such as
\begin{equation}
x[n] = 3 n,
\label{eq:exampleAmbiguousNotation}
\end{equation}
which is valid for all $n \in \integers$, this equation can be repeatedly used to reconstruct the whole sequence by varying $n$,
or be interpreted as a single sample for a specific value $n=n_0$.
%Hence, the notation $x[n]$ may be interpreted as the whole sequence or specifically the sample $x[n]$ at ``time'' $n$.
In some cases, to disambiguate the two interpretations, a notation such as $x[n_0]$ or $x(t_0)$ is adopted, where $n_0$ and $t_0$ denote specific time instants instead of a generic variable that can be iteratively used to represent the complete sequence.\footnote{This notation will be adopted, for instance, in \equl{impulses}.}

\subsection{Digitizing Signals}

In many digital signal processing applications, it is required to convert a real-world analog signal into a digital format, and then process it with a computer, microcontroller, FPGA (field programmable gate array)\index{Field programmable gate array (FPGA)} or \emph{digital signal processor}\index{Digital signal processor (DSP)} (DSP) chip, for example. Therefore, a brief review of the A/D process is discussed in the sequel.

%The next section (\ref{sec:representing_signals}) will provide rigorous definitions of analog and digital signals, while here the goal is to get familiar with basic concepts of the analog to digital (A/D) conversion.

The A/D converter (or ADC chip)\index{ADC chip} transforms the input analog signal $x(t)$ into a digital signal $x_q[n]$, consisting of a sequence of quantized samples. The ADC executes two tasks:
\begin{itemize}
	\item sampling: periodically extract samples to accurately describe the signal; 
	\item quantization: represent each of these samples with a reasonable accuracy. 
\end{itemize}

\emph{Sampling} depends on the adopted sampling frequency $\fs$, which is the number of samples extracted from the signal per time unit (more specifically, one second).\footnote{Some authors prefer to reserve the unit Hertz (Hz) to only represent the oscillation rate of signals, electrical fields, etc., sticking with its historical use. With this motivation, they avoid, for example, to denote the sampling frequency $\fs$ of an ADC in Hz and use samples per second (SPS)\index{Samples per second (SPS)} instead. In this text, Hz is ``overloaded'' to be a unit that can account for everything repeating a given number of times per second and $\fs$ is reported in Hz.} For example, $\fs = 8000$~Hz corresponds to obtaining 8000 samples to represent each signal segment with a duration of 1 second. The higher $\fs$, the more accurate the representation tends to be.

\emph{Quantization} depends on the number $b$ of bits used to represent each sample. For example, if $b=2$, each sample can be represented by only $N=4$ distinct values:
%, which will be internally represented by the processor as 
00, 01, 10 and 11. The mapping between these binary values and amplitudes is somehow arbitrary. For example, 00 can represent $-10$ V while 01 can represent $-5$ V. In general, an ADC of $b$ bits can output $N=2^b$ distinct quantized values.

ADCs with large values for $\fs$ and $b$ are more expensive. Sometimes the tradeoff is to use a relatively large $\fs$ with small $b$ (e.g., $\fs=2.2$~GHz with $b=10$~bits) or vice-versa (e.g., $\fs=2.5$~MHz with $b=24$~bits).

The chip that performs the digital to analog conversion is called DAC. It also operates according to the values of $\fs$ and $b$.

Note that the operation performed by an ADC chip is in general \emph{lossy}\index{Lossy operation} and, consequently, non-invertible.\footnote{A \emph{lossless} operation\index{Lossless operation} from $x$ to $y$, allows recovering $\hat x$ from $y$, such that $\hat x=x$. If the operation is lossy, then $\hat x = x + e$, where $e$ is a non-zero error.} Therefore, cascading an ADC and a DAC chips recovers only an approximation of the original signal $x(t)$. In this text we will learn how to properly choose $\fs$ and $b$ to control the A/D and D/A processes, in order to recover $x(t)$ with the accuracy that the given application demands. These two parameters are the most relevant in the A/D and D/A processes, but when choosing commercial chips, there are many others (see exercises in Section~\ref{sec:chap1Exercises}). One important figure of merit to estimate this accuracy is the \emph{signal-to-noise ratio} (SNR)\index{Signal-to-noise ratio}, which is the ratio between the power of the input $x(t)$ and the power of the ``noise'' signal, which in this case is the total error caused by the sampling and quantization processing stages. When this error is solely caused by quantization, the SNR is called \emph{signal-to-quantization-noise ratio} (SQNR)\index{Signal-to-quantization-noise ratio}.

%In Section~\ref{sec:applications_signal},
%Applications~\ref{app:recording_sound} to \ref{app:sound_board_quantizer} 
%some experiments are suggested for practicing the concepts of A/D conversion using a computer sound board. Before proceeding, it is recommended to try them.% and realize how these concepts are used via simple experiments. 

\subsection{Discrete-time signals}

In practice, the A/D conversion is typically performed by a monolithic ADC chip. But for theoretical studies, it is convenient to mathematically model this A/D conversion by splitting it in the two mentioned stages: \emph{sampling} and \emph{quantization}.
%in spite of the actual electronics in the ADC not necessarily using them. 
One important reason for adopting these two stages when modeling the A/D conversion is that, while sampling is a \emph{linear} operation, the input-output relation of quantizers is \emph{nonlinear} (you may want to take a peek at \figl{quantizerdelta1} and note that the relation corresponds to a non-linear stairs function).  There are several tools, such as the Laplace and Fourier transforms discussed in Chapter~\ref{ch:transforms} for dealing with linear operations. On the other hand, working with nonlinear systems is more evolved. Because of that, most ``digital'' signal processing theory does not assume the amplitude is quantized to enable the usage of tools restricted to linear operations. Hence, more strictly, most of the DSP theory could be called ``discrete-time'' signal processing. However, the name ``digital signal processing'' is more popular. 

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/discrete_timesignal}		
	\caption[Example of discrete-time signal]{Example of discrete-time signal $x[n]$. Note the abscissa is discrete, but the amplitude (ordinate) is not quantized.\label{fig:discrete_timesignal}}
\end{figure}

\figl{discrete_timesignal} illustrates a discrete-time signal. Note the abscissa is given in discrete-time $n \in \NN$, similar to the notation adopted for digital signals. But for a discrete-time signal $x[n]$, the amplitude is not quantized and can assume any value.
\figl{discrete_timesignal} should be compared with \figl{analogsignal} and \figl{digitalsignal}, observing each axis.

\bExample \textbf{Example of creating a discrete-time signal from a continuous-time sinusoid}.
Consider extracting samples from the analog signal $x(t)=6\cos(2\pi 400 t)$ with amplitude 6~volts and a frequency $f_c=400$~Hz (the subscript $c$ is just to indicate this is a specific and fixed frequency value).
The goal is to use $\fs = 8000$ samples to represent each segment of one second, where $\fs$ is the \emph{sampling frequency}. The time interval between consecutive samples is $\ts=1/\fs$, which in this case is $\ts = 1/8000 = 125$~\mus. \codl{snip_signals_signal_generation} illustrates one possible implementation.

%\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_signal\_generation}{snip_signals_signal_generation}
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_signal\_generation}{snip_signals_signal_generation}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/discrete_cosine}		
	\caption{Example of a discrete-time cosine generated with \codl{snip_signals_signal_generation}.\label{fig:discrete_cosine}}
\end{figure}

\figl{discrete_cosine} indicates the result of executing \codl{snip_signals_signal_generation}. This cosine has a period of $T=1/f_c = 1/400=2.5$~ms. Therefore, each of its period is being represented by $T / \ts = (2.5 \times 10^{-3}) / (125 \times 10^{-6}) = 20$ samples. Note in \figl{discrete_cosine} that, in this specific case, at each sample $n = 20, 40, 60, \ldots$, the sample $x[0]=6$ at $n=0$ is repeated, and a new cosine cicle starts.
\eExample

%\section{Manipulating and representing signals}	
\section{Basic Signal Manipulation and Representation}

%There is nothing more practical than a good theory. 
%Hence, i
This section discusses basic tools for mathematically representing and manipulating signals. The motivation is that it is important not only being knowledgeable on manipulating signals using software, but also algebraically, developing expressions and theoretical analysis. 

\subsection{Manipulating the independent variable}

Many signal processing tasks require manipulating $t$ (for continuous-time signals) or $n$ (for discrete-time).
A convenient way to introduce this manipulation is by using examples. 
Define the signal $x[n]=n^2$ for $n=3,4,5$ and 6, and zero otherwise. This signal has only four non-zero amplitude values: $9, 16, 25$ and 36,' at $n=3,4,5$ and 6, respectively. The task here is to find new signals based on $x[n]$, namely: $y_1[n]=x[n-2]$, $y_2[n]=x[-n+3]$, $y_3[n]=x[2n]$ and $y_4[n]=x[n^2]$. 

An interpretation of $y_1[n]=x[n-2]$ is that it is a sequence of events that is related to the original (``mother'') sequence $x[n]$ but with a time difference. For example, if $n=6$ is interpreted as 6 o'clock and $x[6]=36$ is the heart rate of an animal at that moment, the same measurement $y_1[8]=36$ is found at $y_1[n]$ but two time units later (8 o'clock). Hence, the notation $y_1[n]=x[n-2]$ indicates that $x[n]$ and $y_1[n]$ present the same ordinate (in this case, amplitude) values, but these values occur two samples later in $y_1[n]$ when compared to $x[n]$.

One can create a mapping such as Table~\ref{tab:manipulate_n}, which fills up the first column with values of $n$ in the region of interest. Then, it is simple to get columns for $n-2$, $-n+3$, $2n$ and $n^2$. 
%Avoid a common mistake: thinking of ``change of variables'', such as $n'=n-2$ or $n'=2n$. You should \emph{not} consider that there are two ``time basis'' $n$ and $n'$. There is only one ``time basis'' $n$ for all signals. 
%For example, $y_1[n]=x[n-2]$ indicates that the amplitude assumed by $y_1[n]$ at $n$ is the same amplitude assumed by $x[n]$ at $n-2$ such that $y_1[6]=x[4]=16$.
Columns $y_1[n], y_2[n], y_3[n]$ and $y_4[n]$ are filled up based on the auxiliary columns $n-2$, $-n+3$, $2n$ and $n^2$, but they correspond to amplitudes at the value of $n$ given in column 1. For example, the second row indicates that $y_1[-2]=0, y_2[-2]=25, y_3[-2]=0$ and $y_4[-2]=16$.

%\ifskipduetexcount %disable the table if texcount is running
%\else

%TC:ignore
\begin{table}
\centering
\caption{New signals $y_1[n]=x[n-2]$, $y_2[n]=x[-n+3]$, $y_3[n]=x[2n]$ and $y_4[n]=x[n^2]$, obtained by manipulating $x[n]=n^2(u[n-3] - u[n-7])$. \label{tab:manipulate_n}}
\begin{tabularx}{\textwidth}{*{10}{>{\centering$}X<{$}}}\toprule
	\multicolumn{1}{C}{$n$} & \multicolumn{1}{C}{$x[n]$} & \multicolumn{1}{C}{$n-2$} & \multicolumn{1}{C}{$y_1[n]$} & \multicolumn{1}{c}{$-n+3$} & \multicolumn{1}{C}{$y_2[n]$} & \multicolumn{1}{C}{$2n$} & \multicolumn{1}{C}{$y_3[n]$} & \multicolumn{1}{C}{$n^2$} & \multicolumn{1}{C}{$y_4[n]$}\\\midrule
	-3& 0 & -5 & 0 & 6 & 36 & -6 & 0 & 9 & 0\\ 
	-2& 0 & -4 & 0 & 5 & 25 & -4 & 0 & 4 & 16\\ 
	-1& 0 & -3 & 0 & 4 & 16 & -2 & 0 & 1 & 0\\ 
	0 & 0 & -2 & 0 & 3 & 9 & 0 & 0 & 0 & 0\\ 
	1 & 0 & -1 & 0 & 2 & 0 & 2 & 0 & 1 & 0\\ 
	2 & 0 & 0  & 0 & 1 & 0 & 4 & 16 & 4 & 16\\ 
	3 & 9 & 1  & 0 & 0 & 0 & 6 & 36 & 9 & 0\\ 
	4 & 16 & 2 & 0 & -1& 0 & 8 & 0 & 16 & 0\\ 
	5 & 25 & 3 & 9 & -2& 0 & 10 & 0 & 25& 0\\ 
	6 & 36 & 4 & 16& -3& 0 & 12 & 0 & 36& 0\\ 
	7 & 0 & 5  & 25& -4& 0 & 14 & 0& 49& 0\\ 
	8 & 0 & 6  & 36& -5& 0 & 16 & 0 & 64& 0\\ 
	9 & 0 & 7  & 0 & -6& 0 & 18 & 0 &81& 0 \\
	\bottomrule
\end{tabularx}
\end{table}
%TC:endignore

\bExample \textbf{Time-reversal of signals.}
%CODE OF SPECIAL EFFECTS WITH AUDIO. JAVA AND C CODE. TO PLAY WITH MATLAB. EXEMPLO DO TIME REVERSAL.
The operation $y[n]=x[-n]$ corresponds to flipping the signal over the ordinate axis. In {\matlab} it can be implemented with \ci{fliplr} (left-right) or \ci{flipud} (up-down) for row and column vectors, respectively. For example, assume that $x[n]$ has amplitudes $3, 4$ and 5, at time instants $n=0, 2$ and 4, respectively (after discussing the impulse $\delta[n]$ in Section~\ref{sec:impulses_to_represent_signals}, we will describe this signal as $x[n]=3 \delta[n] + 4 \delta[n-2] + 5 \delta[n-4]$). One can represent $x[n]$ as the row vector \ci{x=[3 0 4 0 5]} and flip it using \ci{y=fliplr(x)} in {\matlab}.
It is important to note that, when representing a signal, one vector can store the amplitude values but not their respective time instants. An additional vector is required to store the time information. \codl{snip_signals_timereversal} is a snippet (part of the script) used to generate \figl{timereversal}. It illustrates the care that must be exercised to properly represent the signals $y[n]=x[-n]$ using a computer program. Note that the user must relate the amplitude vector and the ``time'' vector.
\enlargethispage{2\baselineskip}
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_timereversal}{snip_signals_timereversal}
%\begin{lstlisting}
%x=[3 0 4 0 5]; %some signal samples
%y=fliplr(x); %time-reversal
%n1=0:4; %the 'time' axis
%n2=-4:0; %the 'time-reversed' axis
%subplot(211); stem(n1,x); title('x[n]'); 
%subplot(212); stem(n2,y); title('y[n]'); 
%\end{lstlisting}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/timereversal}		
	\caption{Representation of signals related by $y[n]=x[-n]$.\label{fig:timereversal}}
\end{figure}

\figl{timereversal} was produced using three dots to indicate that the signals have infinite duration in spite of being represented by finite-length vectors. Another convention is that, when an amplitude value is not explicitly shown, it is assumed to be zero.
\eExample
%Extend the exercise by recording sound and representing it as a signal $x[n]$. You may use palindromes,\footnote{On the Web, you can find palindromes such as ``A Man, A Plan, A Canal: Panama'' or ``God saw I was dog''.} which are words or phrases which read the same in both directions. For example, record $x[n]$ for the word ``deed'' and listen to $x[-n]$. Compare the waveforms of $x[n]$ and $x[-n]$. You may find useful the \ci{specgram} function in {\matlab} to obtain an alternate view of the signals.
%\eExample


\subsection{When the independent variable is not an integer}
%Before discussing a general way to manipulate $t$ and $n$, i

Almost all manipulations of $t$ are equally applied to manipulating $n$ of discrete-time signals. 
But a discrete-time signal $x[n]$ is undefined if $n \not\in \integers$ (e.\,g. $x[1.5]$ in undefined). 
%Hence, when the equation that determines the independent variable of a discrete-time signal $x[n]$ leads to a non-integer number, which is invalid because $n \in \integers$.

For instance, $x[2n]$ behaves slightly different than $x(2t)$ given that the discrete-time signal is not defined for non-integer $n=\pm 0.5, \pm 1.5, \pm 2.5, \ldots$. Therefore, only the amplitude values of $x[n]$ for $n$ even are present in $x[2n]$, while the values for $n$ odd are discarded.

Using a similar reasoning, when dealing with a discrete-time signal $y[n]=x[n/2]$, it is important to adopt a definition such as:
\begin{equation}
y[n] = \begin{cases} x[n/2], & \text{when~} n/2 \text{ is an integer} \\ 0, & \text{otherwise} \\ \end{cases}
\end{equation}
that emphasizes the assumption $n \in \integers$.

\subsection{Frequently used manipulations of the independent variable}

In signal processing, one is often manipulating the dependent variable (for example, multiplying the signal amplitude by two) and also the independent variable, which is the time abscissa in most cases in this text. These two ways of manipulating a variable are rather different. This section discusses manipulating the independent variable.

One can always use a procedure such as the one illustrated in \tabl{manipulate_n} to obtain the signal after a manipulation of the independent variable. However, it is worth to memorize the resulting signals in two common situations. The first one is the \emph{time shift}\index{Time shift} and the second situation is when simultaneously \emph{scaling and shifting}\index{Scaling and shifting} the original signal. They are discussed in the next paragraphs.

\bExample \textbf{Time advance and delay rules of thumb}.

It may be convenient to memorize the \emph{time advance and delay} rules\index{Time advance and delay rules}. Assuming $t_0 > 0$, these rules are ($t$ and $t_0$ are assumed, but the same applies to discrete-time $n$ and $n_0$):
\begin{itemize}
	\item $x(t-t_0)$ is a delayed version (right-shifted) of $x(t)$ and $x(t+t_0)$ is an anticipated version (left-shifted) of $x(t)$;
	\item $x(-t+t_0)$ is obtained by first considering $x(t+t_0)$, which is obtained by advancing the signal by $t_0$, and then 
	flipping the result $x(t+t_0)$ with respect to the $y$-axis to obtain $x(-t+t_0)$
	\item $x(-t-t_0)$ is also obtained by delaying $x(t)$ to create $x(t-t_0)$ and then flipping this intermediate result over the $y$-axis.	
	%version (right-shifted) of $x(t)$ and $x(t+t_0)$ is an anticipated version (left-shifted) of $x(t)$.
	\end{itemize}	
%However, other manipulations are trickier. 
For example, $x(-t+3)$ corresponds to finding the temporary result $x(t+3)$ by advancing 3 units of time and then
flipping $x(t+3)$ over the $y$-axis. Alternatively, one can think of obtaining $x(-t+3)$ by first flipping $x(t)$ with respect to the $y$-axis and then delaying $x(-t)$ by 3 (instead of anticipating it). However, a common \emph{mistake} is to think that $x(-t+3) = x(-(t-3))$, and start by delaying $x(t)$ by 3 and then flipping over the vertical axis. In case of any confusion, the safest option is to map some values of the abscissa such as \tabl{manipulate_n}.
%In summary, this rule is valid only for the two specific cases it was stated.
\eExample

%In spite of such exceptions, it is convenient to simultaneously study the manipulation of $t$ and $n$.

\figl{time_scaling} provides the example $y(t)=x(t-1)$ of time-shift, corresponding to a delay of 1. This figure also contrasts manipulating the independent and dependent variables. In the latter case, it is the ordinate ($y$-axis) that is modified, with $y(t)=2x(t)$ changing the $y$-axis peak value from 8 to 16.  Besides, \figl{time_scaling} illustrates other two manipulations: \emph{contraction}\index{Contraction} and \emph{dilation}\index{Dilation}. Note that all three examples of manipulating the independent variable did not modify the ordinate peak value (it is equal to 8). Contraction and dilation are the topic of the next example.

\begin{figure}
	\centering
		\includegraphics[width=0.85\textwidth,keepaspectratio]{FiguresNonScript/time_scaling}
	\caption{Examples of manipulating the independent variable: time-shift, contraction and dilation.\label{fig:time_scaling}}
\end{figure}

\bExample \textbf{Time scaling rules of thumb: contraction and dilation}.
\begin{itemize}
	\item Assuming that $\alpha > 1$, $x(\alpha t)$ corresponds to contracting the time axis by a factor
	of $\alpha$ while $x(t / \alpha)$ corresponds to a dilation by $\alpha$.
\end{itemize}
For instance, a pulse $x(t)$ with \emph{support}\index{Support (signal)}\footnote{The support of a function is the subset $\{x : f(x) \ne 0\}$ of the function domain that are not mapped to zero. A similar definition can be applied to signals. For instance, the support of a signal $x(t)$ is the set of ranges of $t$ for which $x(t)$ is non-zero.} from $-T_1/2$ to $T_1/2$~s leads to a total support duration of $T_1$ seconds, while $x(2t)$ and $x(t/5)$ have support durations of $T_1/2$~s and $5 T_1$~s, respectively.
In \figl{time_scaling}, the original signal $x(t)$ has a support from $t=3$ to 9~s, with a total support duration of 6~s, while $x(3t)$ and $x(t/2)$ have supports with total duration of 2 and 12~s, respectively.
\eExample


Another important manipulation of the independent variable is the simultaneous combination of time-shift and scaling, 
%such as $x(\alpha t + \beta)$, 
discussed next.% (\exal{simulScaleShift}).

\bExample \textbf{Simultaneous scale and shift rules of thumb}.
\label{ex:simulScaleShift}
The signal $x(\alpha t + \beta)$, with $\alpha, \beta \in \Re$, is commonly found in
signal processing operations. It can also be found as $x( (t + \gamma) / \xi)$, where $\gamma, \xi \in \Re$. These two representations are related by $\alpha = 1/\xi$ and $\beta=\gamma/ \xi$. %Few examples are discussed.
The rules of thumb are:
\begin{itemize}
\item for $y(t) = x \left(\frac{t + \gamma}{\xi}\right)$, first expand or contract $x(t)$ by a factor of $\xi$ and then shift the intermediate result by $\gamma$;
\item for $y(t)=x(\alpha t + \beta)$, first expand or contract the original signal $x(t)$ according
to the value of $\alpha$, then shift this intermediate result by $\beta/\alpha$.
\end{itemize}
\figl{scaleshift} provides two examples of simultaneously scaling and shifting a signal $x(t)$. 
\eExample 

The first example in \figl{scaleshift} aims at interpreting
$y_1(t)=x\left(\frac{t-3}{2}\right)$. 
One can always carefully map the signals as done in \tabl{manipulate_n}. But using the rule of thumb is faster.
In this case, $x(t)$ can be expanded by a factor of $\xi=2$ and the result shifted to the right (delayed) by $\gamma = -3$.
Note that the support of $x(t)$ is 6, in the range $t \in [4,10]$. The support of $y_1(t)$ is $t \in [11, 23]$, 
twice the support of $x(t)$. As expected, the amplitude does not change: for both signals, it has a maximum value of 5.

\begin{figure}
	\centering
		\includegraphics[width=0.85\textwidth,keepaspectratio]{FiguresNonScript/scaleshift}
	\caption{Three examples of simultaneously scaling and shifting a signal $x(t)$.\label{fig:scaleshift}}
\end{figure}

The second task suggested in \figl{scaleshift} is to obtain $y_2(t)=x(2t+6)$. One possibility is to convert the
representation $x(\alpha t + \beta)$ 
into $x( (t + \gamma) / \xi)$ and proceed as done for $y_1(t)$. In this case, $y_2(t)=x(2t+6)=x\left(\frac{t+3}{0.5}\right)$.
To obtain $y_2(t)$, $x(t)$ can be scaled (in this case a contraction) by a factor of $\xi=0.5$ and the result shifted to the left by $\gamma = 3$. The support of $y_2(t)$ is $t \in [-1, 2]$.

Instead of using $x( (t + \gamma) / \xi)$, one can directly operate on $x(\alpha t + \beta)$, but noting that the time-shift is $\beta/\alpha$ instead of $\beta$. 
In other words, it is an \emph{error} to try to obtain $x(\alpha t + \beta)$ by first finding $x(\alpha t)$ and then shifting by $\beta$.
For instance, $y_2(t)=x(2t+6)$ in \figl{scaleshift} can be quickly obtained by noting that $\alpha=2$ made the support of $x(2t)$ to be $t \in [2, 5]$ (half of the support of $x(t)$), and the time advance $\beta/\alpha = 6/2 = 3$ shifted $x(2t)$ to the left.

As a final example that incorporates three manipulations, consider the case of $y_3(t)=x(-4 t + 6)$ in \figl{scaleshift}. The signal $y_3(t)$ can be obtained
by first finding $x(4t)$ (a new signal with support from $t=1$ to $2.5$), then anticipating $x(4t)$ by $\beta/\alpha = 6/4 = 1.5$  (that creates an intermediate signal with support from $t=-0.5$ to $1$) and then flipping $x(4t+6)$ with respect to the y-axis (as in $x(-t)$) to obtain the final result with support from $t=-1$ to $0.5$.

\figl{sinc_scaling} provides two examples of manipulating the independent variable of the \emph{sinc} function, which is defined in Appendix~\ref{sec:sinc}. The $\sinc$ function is zero when the abscissa $x$ is an integer number $x \in \integers$, with the exception of $x=0$. Hence, for $x > 0$, the first zero of $y_1(x)=\sinc(x/4)$ is at $x=4$.
The first zero of the signal $y_2(t)=\sinc(3x-6)$ for $x>0$ can be found using $3x -6 =1$, which leads to $x=7/3 \approx 2.33$, as indicated in \figl{sinc_scaling}. The contraction and time-shift of $\sinc(t)$ is widely used as part of the D/A conversion, as will be discussed in \equl{signalReconstructionViaSincs}.

\begin{figure}
	\centering
		\includegraphics[width=0.85\textwidth,keepaspectratio]{FiguresNonScript/sinc_scaling}
	\caption{Examples of manipulating the independent variable (in this case $x$) of a $\sinc$ function.\label{fig:sinc_scaling}}
\end{figure}

\bExample \textbf{Generating graphs of the sinc function}.
The function \ci{sinc} in {\matlab} can be used to plot a generic sinc with amplitude $A$, support $\xi$ and centered at $\gamma$ seconds. The command \ci{x = A*sinc((t-gamma)/xi)}, where \ci{t} is an array of time instants, provides the amplitude values for the signal
\[
x(t) = A \textrm{~} \sinc \left( \frac{t-\gamma}{\xi} \right).
\]

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_sinc}{snip_signals_sinc}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/sinc_plot}		
	\caption{Graph of a continuous-time sinc function centered at $t=0.5$~s, obtained with \codl{snip_signals_sinc}.\label{fig:sinc_plot}}
\end{figure}

\codl{snip_signals_sinc} provides an example of using \ci{sinc} to create a graph of a continuous-time signal sinc, which is depicted in \figl{sinc_plot}. The continuous-time sinc in \codl{snip_signals_sinc} corresponds to:
\[
x(t) = 3 \textrm{~} \sinc \left( \frac{t-0.5}{0.2} \right),
\]
and the chosen sampling interval was $\ts=0.001$~s. We chose a relatively small value for $\ts$ to obtain a smooth curve representing a continuous-time signal.
\eExample

\subsection{Using impulses to represent signals}
\label{sec:impulses_to_represent_signals}

If you are not familiar with \emph{impulses}\index{Impulses}, please first check Appendix~\ref{app:impulse}.

Recall that the notation $\delta(t-t_0)$ indicates that a continuous-time impulse occurs at time $t=t_0$. 
% and we will discuss here only the former case. 
For example, $\delta(t-3)$ is a delayed impulse occurring at $t=3$ and $\delta(t+2)$ is an anticipated impulse occurring at $t=-2$.
A similar reasoning applies to the discrete-time $\delta[n-n_0]$.

Any discrete-time signal can be conveniently represented by a sum of scaled and shifted impulses. To get the basic idea, consider the following example. An array $[3.0, -1.3, 2.1, 0, 4.2]$ stores the only non-zero samples of a signal $x[n]$, with the first element corresponding to time $n=0$, the second one to $n=1$ and so on. How would you write an expression for $x[n]$ without using impulses? A wrong guess would be $x[n]=3.0 -1.3 + 2.1 + 0 + 4.2$ because that corresponds to having a constant DC value of $x[n]=8, \forall n$. \emph{The impulse helps locating the desired amplitude in time}. In this example, given that the first sample occurs at $n=0$, an expression for $x[n]$ using impulses is $x[n]=3.0 \delta[n] -1.3\delta[n-1] + 2.1\delta[n-2]  + 4.2\delta[n-4]$. Generalizing, any discrete-time signal can be written as
\begin{equation}
x[n] = \sum_{n_0=-\infty}^\infty x[n_0]\delta[n-n_0].	
	\label{eq:impulses}
\end{equation}
In our example, $x[n_0]$ assumes the values $3.0, -1.3, 2.1, 0, 4.2$ for $n_0=0,\ldots,4$, respectively. As explained, writing $x[n]=x[0] + x[1] + x[2] + x[3] + x[4]$ does not lead to the proper result because shifted impulses $\delta[n-n_0]$ are required to specify the corresponding locations of the amplitudes as they vary over time.

\bExample \textbf{Revisiting the notation for signals}.
To complement the discussion in Section~\ref{sec:signal_notation}, consider 
interpreting \equl{impulses} as a single sample at a specific ``time'' $n=n_1$. To make that clear, one can write:
\begin{equation}
x[n_1] = \sum_{n_0=-\infty}^\infty x[n_0]\delta[n_1-n_0]
\label{eq:impulses2}
\end{equation}
and interpret that $\delta[n_1-n_0]=1$ if $n_1=n_0$ and zero otherwise. Hence, the summation eliminates all amplitudes of $x[n]$ other than $x[n_1]$, which is the amplitude of $x[n]$ at $n_1$. 
In this interpretation, \equl{impulses} and \equl{impulses2} represent a single sample.
Alternatively, one can interpret \equl{impulses} as the whole sequence.
Understanding both interpretations is also useful when dealing with continuous-time impulses as discussed in the next paragraph.
\eExample 

%An alternative view is to ``visualize'' that the summation in \equl{impulses} is multiplying all samples of a sequence $x[n_0]$ by the sequence $\delta[n-n_0]$, which is non-zero only at $n_0=n$. creates a train of impulses 

%. Again, varying $n_1$ one can reconstruct the whole sequence $x[n]$ and \equl{impulses} is more adequate when the idea is to describe the sequence.

%represents a sequence while $x[n_0]$ and $x[n_1]$ are samples, i.e., the specific values at $n=n_0$ and $n_1$, respectively. Writing $x[n_0]$ is shorter than (maybe the more adequate notation) $x[n]|_{n=n_0}$ but can be dubious. For example, \equl{impulses} is typically interpreted as the whole sequence but could be seen as a specific sample. 

%The impulses can be used to denote any discrete-time. What is the property name? Oppenheim?

The continuous-time version of \equl{impulses} is 
\begin{equation}
x(t) = \int_{-\infty}^\infty {x(\tau) \delta(\tau-t)} d\tau,
\label{eq:impulses_ct}
\end{equation}
which uses the \emph{sifting} property of impulses (see Appendix~\ref{app:impulse}) to
show that any continuous-time signal $x(t)$ can be represented as a composition of impulses. 
Besides, similar to
to its discrete-time version (\equl{impulses}), \equl{impulses_ct} can be interpreted as representing the whole signal or 
a single sample value at time $t$.

%Observe that $\int_{-\infty}^\infty {x(\tau) \delta(\tau-t)} d\tau$ leads to the scalar $x(t)$, which is the amplitude of the signal at $t$. Similar to the discrete-time case, varying the position $\delta(\tau-t), \forall t$, describes the whole signal $x(t)$.

%It is important to interpret signals or their samples, according to the context.
%Besides, the reader must be familiar with sums of scaled and shifted impulses.

\subsection{Using step functions to help representing signals}
\label{sec:step_function}
The continuous-time step function $u(t)$ is defined as 1 for $t>0$ and 0 for $t<0$. There is a discontinuity at $t=0$ and the amplitude is conveniently assumed to be $u(t)|_{t=0}=0.5$. The discrete-time version $u[n]$ does not have discontinuities and is defined as 1 for $n \ge 0$ and 0 for $n<0$. The step function is very useful to indicate signals that are zero for negative values of the independent variable $t$ or $n$. Another use is to describe finite-duration signals. 
%More strictly, the \emph{support}\index{Support (signal)} of a function $f(x)$ can be defined as the set of values of $x$ for which $f(x)=0$.
When a signal ($x[n]$ or $x(t)$) has finite support, it is called a finite-duration signal.

It is easy to represent finite-duration signals using a programming language (Python, Java, C, etc.) or an environment such as Matlab or Octave. The step function can be used with the same goal when dealing with expressions.
Say for example that the signal $x[n]$ has only four non-zero samples $[9, 16, 25, 36]$ at $n=3,4,5$ and 6. This signal could be written as $x[n]=n^2(u[n-3] - u[n-7])$.

A side note is that $u(t)$ can simplify integrals by limiting the integration interval.
For example, $\int_{- \infty}^\infty {x(t)\textrm{d}t}$ when $x(t)=e^{-2t}u(t)$ can be obtained by
\begin{equation}
\int_{- \infty}^\infty{e^{-2t}u(t)\textrm{d}t} = \int_{0}^\infty{e^{-2t}\textrm{d}t},
\label{eq:stepExample}
\end{equation}
where the role played by $u(t)$ is absorbed by changing the inferior limit to 0. After this step, $u(t)$ can be eliminated from the integral. In other words, when $u(t)$ appears in integrals it can be often taken care of by simply adjusting the integral limits.\footnote{There is no need for using, e.\,g., the integration by parts of \equl{integration_parts} to solve \equl{stepExample}.}

\subsection{The rect function}
\label{sec:rect_function}

The rectangular or \emph{rect}\index{Rect function} function corresponds to a normalized pulse 
 with unitary support from $t=-0.5$ to 0.5 and amplitude equals to 1. It can be written as $\rect(t) = u(t+0.5) - u(t-0.5)$ and in some textbooks it is denoted as $\Pi(t)$. The $\rect(t)$ is defined here as a continuous-time function, but there are
other definitions, for rectangular discrete-time signals.

%As discussed in \exal{simulScaleShift}, 
As an example of a signal derived from $\rect(t)$, consider a pulse $x(t)$ with amplitude $A=4$~volts and support from 5 to 8 seconds can be denoted as $x(t) = 4\rect( (t-6.5)/3)$. In this case, as explained in \exal{simulScaleShift}, the value $6.5$ is obtained by observing that the intermediate result $\rect(t/3)$ has support (or width) from $t=-1.5$ to 1.5,
 and this range must be delayed by 6.5 to obtain the new support as $t=5$ to 8~s.

\bExample \textbf{Generating graphs of continuous and discrete-time signals with the rect function}.
The function \ci{rectpuls} in {\matlab} can be used to plot a generic pulse with amplitude \ci{A}, support of \ci{T} and centered at \ci{tc} seconds. The command \ci{x = A*rectpuls(t-tc,T)}, where \ci{t} is an array of time instants, provides the amplitude values for the signal
\[
x(t) = A \textrm{~} \rect \left( \frac{t-t_c}{T} \right).
\]

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_rect}{snip_signals_rect}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/rect_plots}		
	\caption{Graphs of continuous and discrete-time signals obtained with the rect function in \codl{snip_signals_rect}.\label{fig:rect_plots}}
\end{figure}

\codl{snip_signals_rect} provides examples of using \ci{rectpuls} to create graphs of discrete and continuous-time signals, which are depicted in \figl{rect_plots}. The continuous-time rect in \codl{snip_signals_rect} corresponds to:
\[
x(t) = 2.5 \textrm{~} \rect \left( \frac{t-0.4}{0.2} \right).
\]
One important trick to get a graph that resembles a pulse is to use a small sampling interval, as done in \codl{snip_signals_rect}.

In spite of $\rect(t)$ being defined in continuous-time, it is possible to use it to obtain a discrete-time signal as 
done, e.\,g., in \codl{snip_signals_rect}. Defining the sampling interval as $\ts = 1$~second, allows to create a discretized
time axis $n$ composed only by integers, and then generate
\[
x(t) = x[n] = 7 \textrm{~} \rect \left( \frac{n+3}{5} \right),
\]
with the key assumption of $\ts=1$.
\eExample

\bExample \textbf{Generating a signal composed by several rect functions}.
The goal in this example is to generate the signal
\begin{equation}
x(t) = \rect \left( \frac{t}{0.2} \right) - 3 \textrm{~} \rect \left( \frac{t-0.2}{0.2} \right) + 3 \textrm{~} \rect \left( \frac{t-0.4}{0.2} \right)
\label{eq:three_rects}
\end{equation}
using \ci{rectpuls} in \matlab. 
\codl{snip_signals_three_rects} provides a solution, and generates \figl{three_rects}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_three\_rects}{snip_signals_three_rects}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/three_rects}		
	\caption{Graph of the signal described in \equl{three_rects}.\label{fig:three_rects}}
\end{figure}

\codl{snip_signals_three_rects} illustrates a general procedure: one can define a common time axis $t$ and compose
a sophisticated signal $x(t)$ by summing the appropriate parcels.
\eExample

\section{Block or Window Processing}
\label{sec:block_proc}
Many algorithms segment the signal into blocks of samples and process each block individually. For example, when writing  {\matlab} code for DSP, it is tempting to organize the information in a single vector, which is then processed and converted into another vector. This works well when the number of elements in each vector is relatively small. However, many situations require processing billions of samples, and the corresponding long vectors would eventually not fit in memory. In these cases, the best strategy is to write DSP code using block processing\index{Block-based processing}, also called window processing and frame-based processing.

When processing a discrete-time ``infinite'' duration signal $x[n]$  in blocks (also called \emph{window} or \emph{frames}\index{Frame-based processing}), $x[n]$  is iteratively segmented and represented by finite dimensional vectors $\bx_m$ with $N$ elements each. 
%Block transforms operate on group of samples organized as a column vector $\bx$ of dimension $N$. 
%Typically these blocks (also called frames) are extracted from a discrete-time signal $x[n]$. 
Assuming that $x[n]$ is a right-sided signal ($n=0$ corresponds to the first non-zero sample, $n=1$ to the second and so on), the first block $\bx_0$ is formed by the samples
\[
\bx_0 = [x[0], x[1], \ldots, x[N-2], x[N-1]]^T.
\]
In general, the $m$-th block is represented by the column vector
%As in {\matlab}, the first index of an array is assumed to be 1, not 0.
\begin{equation}
\bx_m = [x[mN], x[mN+1], \ldots, x[mN+N-2], x[mN+N-1]]^T.
\label{eq:block_processing}
\end{equation}
To avoid too many indexes, the dependence on $m$ will be omitted hereafter.

\bExample \textbf{Segmenting a signal into blocks}.
Suppose the task is to segment a very long signal into blocks and then calculate the power of
each block.
\codl{snip_transforms_segmentation} illustrates how to perform this task in {\matlab}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_transforms\_segmentation}{snip_transforms_segmentation} 

Note that \equl{block_processing} assumes the first index is 0, which is adopted
in Python, C, Java and other languages. However, in {\matlab} the first index is 1 and this required using \ci{beginIndex = m*N+1} instead of \ci{beginIndex = m*N} as in \equl{block_processing}. 

The reader is invited to use a speech signal instead of the random signal and observe
how the signal power varies over time, comparing it for vowels and consonants.
\eExample

\subsection{{\akadvanced} Block processing with overlapped windows}

The next paragraphs discuss an alternative to deal with observation windows containing $L$ samples (or observations).
It is assumed that for each window, it is desired to obtain a single estimate. For instance, one can use windows with $L=100$ samples to estimate the average signal power within that window. The goal is to extract more information from the available collection of samples and alleviate the noise and blocking effects. Two strategies to improve block processing are overlapping windows and
multiplying the block of $L$ samples by a function $w[n]$ with support of $L$ samples and that goes to zero at its endpoints. The function $w[n]$ is called \emph{window} and is discussed in Section~\ref{sec:windows}. Here, we will solely rely on overlapping windows, as illustrated in~\figl{overlapped_windows}.

To specify the extraction of overlapping windows, besides the length $L$, one needs to define the \emph{shift}\index{Window shift} $S$ in samples. When the window shift (also called \emph{stride}\index{Stride} in \emph{deep learning}) is $S < L$, neighboring windows share $L-S$ samples. \figl{overlapped_windows} provides an example in which $S=1$ and $L=4$. As also indicated in this figure, non-overlapping windows are obtained with
\begin{equation} 
S = L.
\label{eq:non-overlapping-windows}
\end{equation}
With $S=1$, for each new input sample, there is a new observation window and consecutive windows share $L-S=3$ samples. For example, in \figl{overlapped_windows}, when sample $x[4]$ comes, the observation window slides to the right and encompasses $x[1]$ to $x[4]$. When $x[5]$ comes, the window covers $x[2]$ to $x[5]$, and so on. In this case, because there is a new estimate for each new sample, the estimator's output rate is the same as its input rate despite the value of $L$. Another application of overlapping window is in estimating the \emph{spectogram}, as explained in Section~\ref{sec:spectrogram}.

\begin{figure}
	\centering
		\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/overlapped_windows}
	\caption{The top representation shows non-overlapping windows of $L=4$ samples, with both non-windowed indexing $x[n]$ and windowed indexing $x[k,m]$. The	bottom representation shows overlapping windows with $L=4$ and shift $S=1$ sample using non-windowed indexing.\label{fig:overlapped_windows}}
\end{figure}

When dealing with block processing, it is sometimes useful to adopt a windowed indexing in which $x[k,m]$ represents the $m$-th sample of the $k$-th window, as depicted in \figl{overlapped_windows}.

When there are $N$ available samples, the number $M$ of windows with $L$ samples and shifted by $S$ samples that can be extracted is given by
\begin{equation}
M = \left\lfloor \frac{N-L}{S} \right\rfloor + 1,
\label{eq:num_of_windows}
\end{equation}
where all values $N$, $L$ and $S$ are greater than zero, and $N \ge L$ (otherwise there is not a single window). For example, if we have $N=13$ samples, with windows of $L=5$ samples and shifted by $S=3$, the result is $M = \lfloor (13-5)/3 \rfloor + 1 = 3$ windows.

To prove \equl{num_of_windows}, it must be taken into account that of the $N$ input samples, we need $L$ to make one valid window
and hence the summation of 1. As depicted in \figl{number_of_windows}, for the remaining samples, $N-L$, each extra valid window requires $S$ samples. The \emph{floor} function $\lfloor \cdot \rfloor$ eliminates incomplete windows.
This is illustrated in \figl{number_of_windows}, where the red line separates, at the left, $L=5$ samples for the first window, and the remaining $N-L=8$ samples are enough for two extra valid windows (each one requires $S=3$ samples). In this example, the last two of the $N=13$ samples are not used in any window.

\begin{figure}
	\centering
		\includegraphics[width=0.4\textwidth,keepaspectratio]{FiguresNonScript/number_of_windows}
	\caption{Interpreting \equl{num_of_windows} to obtain $M=3$ windows of $L=5$ samples from a total of $N=13$ samples using a shift of $S=3$ samples.\label{fig:number_of_windows}}
\end{figure}


%\section{Representing Signals More Formally: Sampled and Discrete-time Signals}

\section{{\akadvanced} Complex-Valued and Sampled Signals}
\label{sec:representing_signals}
%\section{\texorpdfstring{\textcolor{red}{This is a red section title}}{This is a red section title}}
%\section{ {{\color{red} Advanced:}} Complex-Valued and Sampled Signals} %Discrete-time already defined
%\section{Sampled Signals and Relation to Discrete-time Signals}


%It is important to have precise definitions of signals other than analog and digital. This is the goal of the next paragraphs.

Some signals are useful for developing theoretical models, even if they do not exist in practice. 
This section describes two important mathematical representations for signals. 

% have stages that demand intermediate representations that are neither analog or digital signals: , which are represented in \figl{sampledanalog} and \figl{discrete_timesignal}. 

\subsection{Complex-valued signals}

%, which may not exist in practice, 
The definition of a signal $x(t)$ (or $x[n]$) can be expanded to include complex-valued amplitudes $x(t) \in \complex$. In several situations, it is  mathematically convenient to use a single complex-valued signal\index{Complex-valued signals} to represent two (typically related) real-valued (or simply real) signals\index{Real-valued signals}. 
%as will be discussed in Chapter~\ref{ch:digi_comm}. 
For instance, the complex-valued exponential $z(t)=e^{j w_c t} = \cos(w_c t) + j \sin(w_c t)$ (see Euler's \equl{euler}) plays important role in Fourier analysis and simplifies the
notation that, otherwise, would rely on two signals: $\cos(w_c t)$ and $\sin(w_c t)$. 

A complex-valued signal $x(t) = x_{\textrm{re}}(t) + j x_{\textrm{im}}(t)$ can be interpreted as representing two distinct signals, corresponding to the real $x_{\textrm{re}}(t)$ and imaginary $x_{\textrm{im}}(t)$ components. This representation is widely used in digital communications to compose 
\emph{quadrature}\index{Quadrature (signal)} signals. 
%Chapter~\ref{ch:digi_comm} 
%represent passband\index{Passband (signal)} and 
%For instance, multiplying a signal $z(t)$ by $e^{j w_c t}$ results in shifting the spectrum of $z(t)$ 
%There are tricks to create a real-valued signal that incorporates both real $x_r(t)$ and imaginary $x_i(t)$ components, and
%extract back these components from the real-valued signal. 
%One of these tricks is processing \emph{quadrature}\index{Quadrature (signal)} signals. 
In quadrature processing, the component $x_{\textrm{re}}(t)$ is called in-phase and denoted as \emph{$x_i(t)$} while 
$x_{\textrm{im}}(t)$ is called the quadrature component and denoted as $x_q(t)$.
The quadrature $x_{\textrm{quadrature}}(t)$ signal is real-valued and can be obtained via
\begin{equation}
x_{\textrm{quadrature}}(t) = x_i(t) \cos(\aw_c t) + x_q(t) \sin(\aw_c t),
\label{eq:qam_real_version}
\end{equation}
where $\aw_c$ is a carrier frequency in radians per second.
The name quadrature comes from the fact that the sine is a cosine delayed by 90 degrees.
% or a real-valued version of it.

An alternative way of implementing \equl{qam_real_version} is by defining the complex-valued
signal $x_{\textrm{ce}}(t) = x_i(t) - j x_q(t)$ and using the compact notation
\begin{equation}
x_{\textrm{quadrature}}(t) = \real{ x_{\textrm{ce}}(t) e^{j \aw_c t} },
\end{equation}
where $\real{\cdot}$ denotes the real part of a complex-valued signal.

Later (for instance, after transmitting the quadrature signal through a communication channel), the components $x_i(t)$ and $x_q(t)$ can be recovered from $x_{\textrm{quadrature}}(t)$. The systems that recover these components typically use \emph{quadrature sampling}\index{Quadrature sampling}, in which two ADCs deal with the two components $x_i(t)$ and $x_q(t)$ that may be interpreted as composing a complex-valued signal $x_{\textrm{ce}}(t)$.

Note that the notation $x_q(t)$ is also used for quantized continuous-time signals in Section~\ref{sec:ana_dig_dis},
but the context will disambiguate these two categories of signals.

%It should be observed that some existing physical channels are not able to transmit a complex analog signals $x(t)$, but many softwares adopt complex-valued digital signals, such as the DSP code of smartphones.
%In practice, the real and imaginary parts of a complex $x(t)$ are distinct real signals. 
%One reason is that it is  mathematically convenient to use a single complex-valued signal\index{Complex-valued signals} to represent two (typically related) real-valued (or simply real) signals\index{Real-valued signals}. 
%Similarly, discrete-time signals are convenient mathematical abstractions, which will be discussed in the sequel.

\subsection{Sampled signals}

In a \emph{sampled signal \index{Sampled signal}} $x_s(t)$, the information is represented by uniformly spaced impulses with distinct areas. Because the impulses have undefined (infinite) amplitude values, $x_s(t)$ will not be considered here a continuous-time signal, but an intermediate representation between the continuous and discrete-time worlds as indicated in \figl{signals}.
Signals other than impulses (e.\,g., pulses) can be used for sampling, but an impulse train is the only option discussed here due to its mathematical convenience.
 Assuming impulses, the sampled signal is not physically realizable but it is very useful for mathematically modeling the sampling step of the A/D conversion, as will be discussed in the next paragraphs.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/signals}		
	\caption{Signals classification including the sampled signals.\label{fig:signals}}
\end{figure}

%\emph{discrete} 

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/sampledanalog}		
	\caption[Example of sampled signal]{Example of a sampled signal obtained from \figl{analogsignal}. Note the abscissa is continuous but the amplitudes of the impulses go to infinite. Conventionally, the impulse height is used to indicate the impulse area and the contour created by the arrow heads resembles the original analog signal.\label{fig:sampledanalog}}
\end{figure}

In this text, a sampled signal $x_s(t)$ is assumed to be the result of periodic sampling a continuous-time signal $x(t)$. Hence, the sampled signal is composed by periodically spaced impulses whose areas correspond to the value of the original analog signal $x(t)$ at the impulse location. There is a convention for showing plots of sampled signals with the impulse heights corresponding to their \emph{areas} (see Appendix~\ref{app:impulse}), but the infinite \emph{amplitude} of $x_s(t)$ is considered $\pm \infty$ at the impulses positions. The amplitude is zero if $t$ is not a multiple of $T_s$, i.\,e., $x(t)=0, \forall t \ne k T_s$, where $k \in \integers$ is any integer.\footnote{A signal $z(t)$ that contain impulses $\delta(t)$ but is not obtained via periodic sampling, will not be denoted with the subscript $s$ nor considered a sampled signal.}

\figl{sampledanalog} presents an example of a sampled signal. It can be compared with \figl{analogsignal} and \figl{digitalsignal}. \figl{sampledanalog} was obtained assuming the time interval between consecutive impulses is $\ts=125$~$\mu$s (or, equivalently, that $\fs = 1/\ts = 8$ kHz). Note also that the sampled signal $x_s(t)$ in \figl{sampledanalog} exists for all $t$, while a discrete-time signal $x[n]$ is considered to exist only for $n \in \integers$. 
Another distinction is that $t$ in $x_s(t)$ (as well as in $x(t)$) has dimension of time (assumed to be in seconds) while $n$ in $x[n]$ is dimensionless.
%and is treated as an index. 

%\section{Signal Classification: Randomness, Periodicity and Power}
\section{Signal Categorization}
\label{sec:signal_categorization}

This section discusses important categories of signals and their distinctive properties.

\subsection{Even and odd signals}\index{Even and odd parts of a signal}
\label{sec:even_odd}

A signal $x[n]$ is called \emph{even}\index{Even signal} if $x[n]=x[-n]$, and \emph{odd}\index{Odd signal} if $x[n]=-x[-n]$. The definitions are also valid for a continuous-time signal and, in general, for any function $f(x)$. An even function $f(x)$ has the property that $f(x) = f(-x)$, while an odd function has the property that $f(x)=-f(-x)$. For instance, the cosine is an even function while the sine is an odd function.

Interestingly, any function can be decomposed into  even $f_e(x)$ and odd $f_o(x)$ parts, such that $f(x) = f_e(x) + f_o(x)$.  
The two component parts can be obtained as
\[
f_e(x) = \frac  {f(x) + f(-x)} 2
\textrm{~~~~and~~~~}
f_o(x) = \frac  {f(x) - f(-x)} 2.
\]

Similarly, any signal $x[n]$ (or $x(t)$) can be obtained as the sum of an even $x_e[n]$ and odd $x_o[n]$ parcels, which can be found as follows:
\begin{equation}
x_e [n] = 0.5 (x[n]+x[-n])
\label{eq:evenParcel}
\end{equation}
and
\begin{equation}
x_o [n] = 0.5 (x[n]-x[-n]),
\label{eq:oddParcel}
\end{equation}
respectively, such that $x[n] = x_e[n] + x_o[n]$.

For example, assume the unit step function $x(t)=u(t)$. Its even and odd parts are $x_e(t)=0.5, \forall t$ and $x_o(t)=0.5u(t)-0.5u(-t)$, respectively.

The function \ci{ak\_getEvenOddParts.m} can be used to obtain the even and odd components of arbitrary finite-duration sequences. Three examples are provided in \figl{evenodd_step}, \figl{evenodd_parabola} and \figl{evenodd_triangle}.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/evenodd_step}		
	\caption{Even and odd components of a signal $x[n]$ representing a finite-duration segment of the step function $u[n]$. Note the symmetry properties: $x_e[n]=x_e[-n]$ and $x_o[n]=-x_o[-n]$.\label{fig:evenodd_step}}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/evenodd_parabola}		
	\caption{Even and odd components of a signal $x[n]=n^2 u[n]$ representing a parabolic function.\label{fig:evenodd_parabola}}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/evenodd_triangle}		
	\caption{Even and odd components of a signal $x[n]$ representing a triangle that starts at $n=21$ and has its peak with an amplitude $x[60]=40$ at $n=60$. Note that the peak amplitude of the two components is 20.\label{fig:evenodd_triangle}}
\end{figure}


\subsection{Random signals and their generation}
\label{sec:randomSignals}

Random signals are important to represent noise or other non-deterministic signals.
Discrete-time random signals of finite duration can be represented by vectors in which the elements are outcomes of random variables (see Appendix~\ref{app:probability}).
For example, assuming all elements of \co{[2, 0, 2, 3, 3, 2, 3]} are outcomes of the same random variable
$\rvx$, one can calculate the average $\ev [\rvx] \approx 2.14$, the standard deviation $\sigma \approx 1.07$ and other statistical moments of $\rvx$. %This vector could be modeled as a finite-duration random signal. %$\rsx[n]$.
%It is convenient to model $\rsx[n]$ as 

Alternatively, a vector with random samples may correspond to a \emph{realization} of a discrete-time random process
(see Appendix~\ref{app:stochasticprocesses}). 

%In some cases, a random signal is denoted as $x[n]$, the same notation for non-random signals. 
%Similarly, continuous-time random signals are modeled as realizations of continuous-time random processes.

%SHOW EXAMPLES IN MATLAB TO GENERATE, PLOT OVER TIME AND FDP. SHOW CODE OF A HISTOGRAM.

\bExample \textbf{Random Gaussian signals: generation, waveform and histogram}.
It is easy to generate random signals in {\matlab}. For example, the command
\ci{x=randn(1,100)} generates 100 samples distributed according to a \emph{standard Gaussian}\index{Standard Gaussian} (zero-mean and unity-variance) distribution $\calN(0,1)$, where the notation $\calN(\mu,\sigma^2)$ assumes the second argument is the variance $\sigma^2$, not the standard deviation $\sigma$. These signals can be visualized as a time-series,\footnote{As usually done for signals with many samples, the discrete-time $x[n]$ was depicted as a continuous-time signal with the \ci{plot} function instead of \ci{stem}.} such as in 
\figl{zeromean_randomsignal}.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/zeromean_randomsignal}		
	\caption{Waveform representation of a random signal with 100 samples draw from a Gaussian distribution $\calN(0,1)$.\label{fig:zeromean_randomsignal}}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/simple_histogram}		
	\caption{Histogram of the signal in \figl{zeromean_randomsignal} with 10 bins.\label{fig:simple_histogram}}
\end{figure}

The time-domain visualization can be complemented by plotting the probability distribution of the random signal.
\figl{simple_histogram} illustrates the histogram of the signal depicted in \figl{zeromean_randomsignal}. The histogram is calculated by dividing the dynamic range in \emph{bins} and counting the number of samples that belong to each bin. \figl{simple_histogram} was obtained by using $B=10$ bins (the default).

\figl{histogram_example} can help in case the reader is not familiar with the notion of a histogram \emph{bin}\index{Bin (histogram)}\index{Histogram}.
An example of {\matlab} commands that would lead to these $B=3$ bins are: \ci{x=[1, 3.1, 2.6, 4]; [counts,centers] = hist(x,3)}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./FiguresNonScript/histogram_example}
\caption{Example of histogram with $B=3$ bins. The centers are 1.5, 2.5 and 3.5, all marked with '$\times$'. The
bin edges are $1, 2, 3$ and 4.\label{fig:histogram_example}}
\end{figure}

Histograms can also be easily generated with Python. For instance, using \ci{numpy} one can create a histogram 
with the $B=3$ bins in \figl{histogram_example} with the commands:
\begin{lstlisting}[language=Python]
import numpy as np
x=np.array([1, 3.1, 2.6, 4])
hist, bin_edges = np.histogram(x,3)
\end{lstlisting}
While {\matlab} returns the bin centers, Python's numpy returns the bin edges. 
In both languages, the first and last edge values (1 and 4, in the example of \figl{histogram_example}) correspond to the minimum $x_{\textrm{min}}$ and maximum $x_{\textrm{max}}$ values of the input signal. The bin width is given by
\begin{equation}
\textrm{bin\_width}= \frac{x_{\textrm{max}}-x_{\textrm{min}}}{B},
\label{eq:hist_bin_width}
\end{equation}
which leads to $\textrm{bin\_width}=1$ in \figl{histogram_example}.
\eExample 

The histogram indicates the number of occurrences of the input data within the ranges (or bins) indicated in its abscissa. The histogram is also very useful to estimate a probability mass function (PMF)\index{probability mass function} and probability density function (PDF)\index{probability density function} from available data, 
which are used for discrete and continuous random variables, respectively. 

\bExample \textbf{Using a normalized histogram as an estimate of the probability mass function}.
Even when the elements of the input vector \ci{x} are real values, the histogram calculation ``quantizes'' these input values
into only $B$ values, which represent the ranges of the histogram bins. In other words, the bin centers can
be seen as the result of a quantization process. Besides peforming this ``quantization'', the histogram
also indicates the number of occurrences of these
$B$ values in \ci{x}.

The $B$ center bins of a histogram can be interpreted as the possible distinct values
of a discrete random variable. Therefore, normalizing the histogram by the total number $N$ of samples
provides an estimate of the PMF of this discrete random variable.

%After calculating the histogram using a vector with $N$ elements, dividing the number of occurrences by $N$ provides an estimate of the PMF. 

For instance, suppose a vector \ci{x} with \ci{N=100} elements. 
Calculating the histogram and dividing the number of occurrences by \ci{N} provides an estimate of the PMF of \ci{x}.
In \matlab, after obtaining the histogram with \ci{[occurrences, bin\_centers]=hist(x)}, the PMF can be estimated 
using \ci{stem(bin\_centers, occurrences/N)}.
\eExample 

Normalizing the histogram by the total number $N$ of samples provides an estimate of the PMF of a discrete random variable, with values represented by centers of the histogram bins. When this result is further normalized by the bin width, an estimate of the PDF is obtained, as discussed in the next example.

\bExample \textbf{Using a normalized histogram as an estimate of the probability density function}.
Sometimes we know the input data \ci{x} is composed of realizations of a continuous random variable with a known PDF. 
When the task is to superimpose a PDF estimated from data to the actual PDF, one needs to properly normalize
the histogram (see Appendix~\ref{sec:normalization_factors_psd_pdf}).

The function \ci{ak\_normalize\_histogram.m} can be used to estimate a PDF from a histogram and was used to obtain
\figl{normalized_simple_histogram} according to the commands in \codl{snip_signals_estimate_pdf}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_estimate\_pdf}{snip_signals_estimate_pdf}
%\begin{lstlisting}
%B=10; %number of bins
%x=randn(1,100); %random numbers ~ G(0,1)
%[n2,x2]=ak_normalize_histogram(x,B);%PDF via normalized histogram
%a=-3:0.1:3; %use range of [-3std, 3std] around the mean
%plot(x2,n2,'o-',a,normpdf(a),'x-') %compare estimate vs. theoretical PDF
%\end{lstlisting}

\begin{figure}
	\centering		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/normalized_simple_histogram}		
	\caption[PDF estimate from the histogram in \figl{simple_histogram}]{PDF estimate from the histogram in \figl{simple_histogram}. The histogram values were divided by the product of the total number of samples and bin width. A standard Gaussian PDF is superimposed for the sake of comparison.\label{fig:normalized_simple_histogram}}
\end{figure}

\figl{normalized_simple_histogram} indicates that 100 samples and 10 bins provide only a crude estimation.
\eExample 

\bExample \textbf{Changing the mean and variance of a random variable}.
Assume a random variable $\rvx$ has a mean $\eta = \ev[\rvx]$ and $N$ of its realizations compose a vector \ci{x}. 
If one wants to create a new random variable $\rvy$ with $\ev[\rvy] = \eta + 3$, this can be done with
$\rvy = \rvx + 3$, or using the realizations: \ci{y=x+3}. 
This is valid for any constant value $\kappa$.

To prove it, observe that the expected value is a \emph{linear operator} (see, Appendix~\ref{sec:expected_value}), such that when $\rvy = \rvx + \kappa$, one has
\begin{equation}
\ev[\rvy] = \ev[\rvx + \kappa] = \ev[\rvx] + \ev[\kappa] = \eta + \kappa.
\label{eq:ev_mean_linear}
\end{equation}

Similarly, if the variance of $\rvx$ is $\sigma_x^2$, a random variable $\rvy = \sqrt{\kappa} \rvx$ has variance $\sigma_y^2 = \kappa \sigma_x^2$.
The proof is based on \equl{variance_alternative} and linearity:
\begin{align}
\sigma_y^2 &= \ev[(\rvy- \ev[\rvy])^2] = \ev[\rvy^2] - \ev[\rvy]^2 = \ev[(\sqrt{\kappa} \rvx)^2] - (\ev[\sqrt{\kappa} \rvx])^2 = 
 \kappa (\ev[ \rvx^2] - (\ev[\rvx])^2) \nonumber  \\
  &= \kappa \sigma_x^2.
\label{eq:ev_variance_linear}
\end{align}

Take now the particular example of the function \ci{randn} in \matlab, which generates realizations of a random variable $\rvx$ that is distributed according to the normalized
Gaussian $\calN(0,1)$. Based on \equl{ev_mean_linear} and \equl{ev_variance_linear}, if one creates a new random variable $\rvy = \sigma \rvx + \eta$, the mean and variance of $\rvy$ are $\eta$ and $\sigma^2$, respectively.

Considering \matlab, the command \ci{x=sqrt(newVariance)*randn(1,N)+newMean} provides Gaussians with arbitrary mean and variance. 

\codl{snip_signals_gaussian_rand_gen} was used to generate the samples of $\calN(4,0.09)$ and illustrates how to draw samples from a Gaussian with any given mean and variance from calls to a random number generator that outputs samples from a standard Gaussian $\calN(0,1)$.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_gaussian\_rand\_gen}{snip_signals_gaussian_rand_gen}
%\begin{lstlisting}
%newMean=4; %new mean
%newVariance=0.09; %new variance
%N=10000; %number of random samples
%x=sqrt(newVariance)*randn(1,N)+newMean;
%\end{lstlisting}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/normalized_histogram}		
	\caption[Comparison of normalized histogram and the correct Gaussian $\calN(4,0.09)$ when using 10,000 samples and 100 bins]{Comparison of normalized histogram and the correct Gaussian $\calN(4,0.09)$ when using 10,000 samples and 100 bins. Note the likelihood can be larger than one because it is not a probability.\label{fig:normalized_histogram}}
\end{figure}

\figl{normalized_histogram} was obtained by using 10 thousand samples from a Gaussian $\calN(4,0.09)$ and using 100 bins for the histogram. Now the Gaussian estimation is relatively good when compared to the one depicted in \figl{normalized_simple_histogram}. 

It should be noted that the normalized histogram of a continuous PDF indicates \emph{likelihood}\index{Likelihood function}, not probability. The likelihood function is the PDF viewed as a
function of the parameters. Therefore, it is possible to have values \emph{larger than one} in the ordinate, such as in \figl{normalized_histogram}.
\eExample

While the previous example assumed a Gaussian PDF, the next one deals with a uniform PDF.

\bExample \textbf{Calculating and changing the mean and variance of a uniform probability density function}.
A random variable $\rvx$ distributed according to a uniform PDF with 
\emph{support}\index{Support of a PDF} $[a, b]$ (range from $a$ to $b$ of values that have non-zero likelihood) has
a mean given by
\begin{equation}
\mu = \ev[\rvx] = (a+b)/2
\label{eq:uniform_mean}
\end{equation}
and variance:
\begin{equation}
\sigma^2 = \ev[(\rvx-\mu)^2] = (b-a)^2/12
\label{eq:uniform_variance}
\end{equation}

\equl{uniform_variance} can be proved observing that the uniform PDF $f_{\rvx}(x)$ is a constant $1/(b-a)$ over the range $[a,b]$ and zero otherwise. Hence, using \equl{function_continuous_rv} with the function $g(\rvx)=g((\rvx-\mu)^2)$ leads to
\begin{equation}
\sigma^2 = \ev[(\rvx-\mu)^2] = \int_{-\infty}^{\infty} f_{\rvx}(x) (x-\mu)^2 dx = \int_{a}^{b} \frac{1}{b-a}\left( x - \frac{(a+b)}{2} \right)^2 dx =\frac{S^2}{12},
\label{eq:variance_uniformrv}
\end{equation}
where $S=b-a$ is the PDF support.

When using the random number generator \ci{rand} for uniformly distributed samples, one should notice that the dynamic range is $[0, 1]$, i.\,e., $a=0$ and $b=1$. Hence, \equl{uniform_mean} indicates the mean is 0.5 and the variance given by \equl{uniform_variance} is
$S^2/12=1/12 \approx 0.0833$.

Drawing from \ci{rand}, it is possible to generate \ci{N} samples uniformly distributed in an arbitrary range $[a,b]$ with the command
\ci{x = a + (b-a) * rand(N,1)}.
\eExample

%AKGUIDELINE
%I am not going to define causal, stable, etc. This should come before "power/energy" because power talks about periodic signals.
%\section{Classification of signals and systems}


\subsection{Periodic and aperiodic signals}

\subsubsection{Periodicity in continuous-time}
A signal $x(t)$ (same discussion applies to $x[n]$) is periodic if a given segment of $x(t)$ is eternally repeated, such that $x(t) = x(t+T)$ for some $T>0$, where $T$ is called the period. For example, if $T=10$ seconds and $x(t)=x(t+10)$, for all values of $t$.

In the example of $T=10$, it is easy to check that $x(t)=x(t+20)$, $x(t)=x(t+30)$ and so on. In other words, a signal with period $T$ is also periodic in $2T, 3T,\ldots$. The \emph{fundamental period}\index{Fundamental period} $T_0$ is the smallest value of $T$ for which $x(t) = x(t+T_0)$. Note that the definition imposes $T_0>0$ and a constant signal $x(t)=\kappa$ is not considered periodic.
% and the same is required for the period $N_0 > 0$ of a discrete-time signal.
%, , which excludes the DC signal (constant amplitude) to be considered a periodic signal.

\bExample \textbf{Using the LCM and GCD for a periodic signal composed by commensurate frequencies}.
\label{ex:commensurate}
Two frequencies $f_1$ and $f_2$ are called \emph{commensurate} if their ratio $f_1/f_2$ can
be written as a rational number $m/n$, where $m,n$ are non-zero integers. Instead of frequencies, one can use their associated time periods. 

Assume a signal $x(t)=\sum_{i=1}^N x_i(t)$  is composed by the sum of $N$ periodic components 
$x_i(t)$, each one with period $T_i$ and frequency $f_i=1/T_i$. The set of frequencies $\{f_i\}$
is commensurate if all pairs are commensurate. In this case, the fundamental 
period $T_0$ of $x(t)$ can be found
using the least common multiple (LCM) of the periods $\{T_i\}$ while the fundamental frequency $F_0=1/T_0$ can 
be found using the greatest common divisor (GCD) of the frequencies $\{f_i\}$. Assuming both LCM
and GCD are defined only for integer numbers, it may be needed to extract a common factor and later
reintroduce it. A numerical example helps: let $x(t)=\cos(2\pi f_1 t)+\cos(2\pi f_2 t + \pi/2)+\sin(2\pi f_3 t)$ be composed by sinusoids with frequencies $f_1=5/2$, $f_2=1/6$ and $f_3=1/8$~Hz, which
corresponds to periods $T_1=0.4$, $T_2=6$ and $T_3=8$~seconds, respectively.
To find the LCM, one may need to multiply all periods by 10 and then calculate that 
$\textrm{LCM}(4,60,80)=240$. Dividing this result by the factor 10 leads to $T_0=24$~s.
This LCM could be obtained in {\matlab} with \ci{lcm(lcm(4,60),80)} given that this function
is limited to accepting only two input arguments.
\eExample

\subsubsection{Periodicity of a generic discrete-time signal}

A discrete-time signal is periodic if $x[n] = x[n+\Nperiod]$ for some integer $\Nperiod>0$. 
Similar to the continuous-time case, the value $\Nperiod$ is called the \emph{fundamental period} if it corresponds to the minimum number of samples in which the amplitudes repeat.

\subsubsection{Periodicity of discrete-time sinusoids}

One important thing is that the discrete-time counterpart of some periodic analog signals may be non-periodic. The next paragraphs discuss the periodicity of discrete-time sinusoids such as $\cos((\pi/4)n)$, $\sin(3n)$ and $e^{\pi n}$.
For example the signal $x(t)=\cos(3 t)$ is periodic with period $T=2 \pi / 3$~s. However, the discrete-time signal $x[n]=\cos(3 n)$ is non-periodic.

A discrete-time sinusoid such as $x[n]=A \cos(\dw n + \phi)$ is periodic only if $\dw/(2 \pi)$ is a ratio $m/\Nperiod$ of two integers $m$ and $\Nperiod$ as proved below.\footnote{When $m/\Nperiod$ is not a rational number, the discrete-time sinusoid is called \emph{almost-periodic}~\cite{Corduneanu68,Giannakis99}.\index{Almost-periodic signals}} One can write:
\[
x[n+\Nperiod] = A \cos(\dw (n+\Nperiod) + \phi) = A \cos(\dw n + \dw \Nperiod + \phi).
\]
If the parcel $\dw \Nperiod$ in previous expression is a multiple of $2 \pi$, then $x[n+\Nperiod]=x[n], \forall n$. Hence, periodicity requires $\dw \Nperiod = 2 \pi m$, which leads to the condition
\begin{equation}
\frac{m}{\Nperiod} = \frac{\dw}{2 \pi}
\label{eq:sinusoidPeriodicityCondition}
\end{equation}
for a discrete-time sinusoid to be periodic. 

\bExample \textbf{Checking the periodicity of discrete-time sinusoids.}
For example, $x[n]=\cos(3 n)$ is non-periodic because $\dw=3$ and $3/(2\pi)$ cannot be written as a ratio of two integers. In contrast, $x[n]=\cos((2 \pi / 8) n + 0.2)$ is periodic with period $\Nperiod=8$ ($m=1$ in this case). The signal $x[n]=\cos(7 \pi n)$ is periodic because $\dw = 7 \pi$ and $\dw/(2\pi) = 7/2$, with $m=7$ and $\Nperiod=2$.
\eExample

If $m / \Nperiod$ in \equl{sinusoidPeriodicityCondition} is an \emph{irreducible fraction}, then $\Nperiod$ is the \emph{fundamental} period. Otherwise, $\Nperiod$ may be a multiple of the fundamental period.

\bExample \textbf{Finding the fundamental period requires reducing the fraction $m/\Nperiod$.}
For instance, the signal $x[n]=\cos( (12 \pi/28) n)$ is periodic because $\dw = (12 \pi/28)$ and $\dw/(2\pi) = 6/28$, with $m=6$ and $\Nperiod=28$. However, if one is interested on the fundamental period, it is necessary to reduce the fraction $6/28$ to $3/14$, and obtain the  fundamental period as $\Nperiod = 14$ samples.
\eExample

In summary, when contrasting continuous and discrete-time sinusoids, to find the period $T$ of a continuous-time cosine $\cos(\aw t + \phi)$, one can obtain the term $\aw$ that multiplies $t$ and calculate
\[T= \frac{2 \pi}{\aw},\]
which is given in seconds if $\aw$ is in rad/s. Hence, a continuous-time sinusoid is always periodic. But a discrete-time cosine $\cos(\dw n + \phi)$ may be quasi periodic (i.\,e., not periodic). If someone tries to simply calculate $\Nperiod= \frac{2 \pi}{\dw}$, it may end up with a non-integer period. The condition for periodicity is to be able to write $2 \pi / \dw$ as a ratio of integers, i.\,e.
\[\frac{\Nperiod}{m} = \frac{2 \pi}{\dw},\]
where $\Nperiod$ is the period in samples. After turning an $\Nperiod/$ an irreducible fraction, $\Nperiod$ is the fundamental period.

\bExample \textbf{Meaning of $m$ when determining the fundamental period.}
To understand the role of $m$, consider the signal $\cos( (3\pi/17) n)$. In this case, $\frac{2 \pi}{\dw} = 34/3$ cannot be the period because it is not an integer. However, if one allows for $m=3$ times the number of samples specified by $\frac{2 \pi}{\dw}$, the result is the integer period $\Nperiod=m  \frac{2 \pi}{\dw}=34$ samples. See Application~\ref{app:rat_in_matlab} for a discussion on finding $m$ and $\Nperiod$ using {\matlab}.

Sometimes, it is misleading to guess the period of a discrete-time cosine or sine via the observation of its graph.
\figl{nonperiodicsinusoid} depicts the graph of $x[n]=\sin(0.2n)$ and was obtained with the following code:
\begin{lstlisting}
M=100, w=0.2; %%num of samples and angular freq. (rad)
n=0:M-1; %generate abscissa
xn=sin(w*n); stem(n,xn); %generate and plot a sinusoid
\end{lstlisting}
In this case, the signal seems to have a period around 31 samples at a first glance (because $2 \pi / \dw \approx 31.4$). But, for example, $x[n]$ will never be 0 at the beginning of a cycle for a value of $n$ other than $n=0$. Therefore, in spite of resembling a periodic signal, the angular frequency $\dw$ is such that $2 \pi / \dw = 10 \pi$ is a irrational number and a cycle of numbers will never repeat. In this case, the signal is called \emph{almost} or \emph{quasi} periodic\index{Almost periodic}\index{Quasi-periodic}.\footnote{See, e.\,g., \cite{Giannakis99,Antoni07} to see
the importance of almost periodic signals in random processes.}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/nonperiodicsinusoid}		
	\caption[{Graph of the signal $x[n]=\sin(0.2n)$.}]{Graph of the signal $x[n]=\sin(0.2n)$. Observe carefully that this signal is not periodic. The first non-negative sample of the sine cycle will never exactly repeat its value, as indicated by the `x' marks.\label{fig:nonperiodicsinusoid}}
\end{figure}

It is useful to visualize a discrete-time sinusoid that is periodic with $m>1$. \figl{periodicsinusoid} depicts the graph of $x[n]=\sin( (3\pi/17) n )$ and illustrates the repetition of $m=3$ sine \emph{envelopes} within a period of $\Nperiod=34$ samples.
\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/periodicsinusoid}		
	\caption[{Graph of the signal $x[n]=\sin( (3 \pi/17) n)$.}]{Graph of the signal $x[n]=\sin( (3 \pi/17) n)$. The signal has period $\Nperiod=34$ samples as indicated by the combined marks `x' and `o'. This value of $\Nperiod$ corresponds to $m=3$ cycles of a sine envelope corresponding to $2 \pi / \dw =34/3 \approx 11.3$.\label{fig:periodicsinusoid}}
\end{figure}

\figl{nonperiodicsinusoid} and \figl{periodicsinusoid} illustrate how to distinguish strictly periodic sinusoids from quasi periodic, and help interpreting $m$ in \equl{sinusoidPeriodicityCondition}.
\eExample

%\subsection{Left-sided and right-sided signals}

%\subsection{Symmetric signals}

\subsection{Power and energy signals}
\label{sec:powerEnergySignals}
%\subsubsection{Power and energy of continuous-time signals}

%Before discussing quantization, 
It is important to understand the concepts of power $\calP$ and energy $E$ of a signal. One reason is that, in some cases, the equation to be used for a specific analysis (autocorrelation, for example) differs depending if $\calP$ or $E$ are not finite. 
This section assumes continuous-time but the concepts are also valid for discrete-time signals.

If $E$ is the energy dissipated by a signal during a time interval $\Delta t$, the average power along $\Delta t$ is
\[
\calP = \frac{E}{\Delta t}.
\]
If $p(t)=|x(t)|^2$ is the instantaneous power of $x(t)$, $E$ can be calculated as
\begin{equation}
E = \int_{\langle\Delta t\rangle} {p(t)} \textrm{d}t.
\label{eq:signal_energy}
\end{equation}
If the interval $\Delta t$ is not specified, it is implicitly assumed (by \emph{default}) the whole time axis $]-\infty, \infty[$ and
\[
E = \int_{-\infty}^\infty{p(t)}\textrm{d}t.
\]
In this case, $\calP$ is defined as the limit
\begin{equation}
\calP = \lim_{\Delta t \rightarrow \infty} \left[ \frac{1}{\Delta t} \int_{-\Delta t/2}^{\Delta t/2}{p(t)}\textrm{d}t \right].
\label{eq:power_continuous_time_signals}
\end{equation}
Note that, because the time interval goes to infinite (denominator), $\calP$ is zero unless the energy $E$ (numerator) also goes to infinite. This situation suggests the definition of \emph{power} and \emph{energy} signals, which have \emph{finite} power and energy, respectively. Their characteristics are summarized in Table~\ref{tab:powerenergy}, which also indicates that there is a third category for signals that have neither finite power or energy.

%TC:ignore
\begin{table}
\centering
	\caption{Total energy $E$ and average power $\calP$ for two kinds of signal assuming an infinite time interval.\label{tab:powerenergy}}	
\begin{tabularx}{\textwidth}{ccc>{\centering}X}
\toprule
Category &  $E$ &  $\calP$ & Example(s) \\
\midrule
Power signal & $\infty$ & finite & $\cos(\aw  t)$ and other periodic signals\\
Energy signal & finite & 0 & $e^{-t}u(t)$ and $t^2[u(t)-u(t-5)]$ \\
Neither & $\infty$ & $\infty$  & $t$ \\
\bottomrule
\end{tabularx}
\end{table}
%TC:endignore

The most common power signals are periodic. In this case, the energy $E_T$ in one  period $T$
\[
E_T = \int_{\langle T\rangle} {p(t)} \textrm{d}t
\]
can be used to easily calculate 
\[
\calP = \frac{E_T}{T}
\]
because what happens in one period is replicated along the whole time axis.

The most common energy signals have a finite duration, such as $x(t)=t^2[u(t)-u(t-5)]$. Assuming the signals have finite amplitude, their energy in a finite time interval cannot be infinite. Note that infinite duration signals, such as $x(t)=e^{-t}u(t)$, can also have a finite energy in case their amplitude decay over time.

%\bModel
It is assumed throughout this text that the signals are currents $i(t)$ or voltages $v(t)$ over a resistance $R$, such that the instantaneous power is
\[p(t)=v(t)i(t)=\frac{1}{R}v^2(t) = i^2(t) R.\]
Besides, to deal with signals $x(t)$ representing both currents and voltages without bothering about the normalization by $R$, it is assumed that $R=1$ ohm. Hence, the instantaneous power is $p(t)=x^2(t)$ for any real $x(t)$ and, more generally, $p(t)=|x(t)|^2$ in case $x(t)$ is complex-valued.
%\eModel

Throughout the book, unless stated otherwise, $x(t)$ is assumed to be in volts, $p(t)$ and $\calP$ in watts and $E$ in joules. A dimensional analysis of $p(t)=x^2(t)$ should not be interpreted directly as watts = $\textrm{volts}^2$, but watts = $\textrm{volts}^2$/ohm, where the normalization by 1~ohm is implicit. Two examples are provided in the sequel.

\bExample \textbf{Sinusoid power.}
\label{ex:sinusoid_power}
Sinusoids and cosines can be represented by $x(t)=A \cos (\aw_0 t + \theta)$ and are power signals with average power $\calP=\frac{A^2}{2}$. The phase $\theta$ does not influence the power calculation. The proof follows.

The angular frequency is $\aw_0 = \frac{2 \pi}{T}$ rad/s, where $T$ is the period in seconds. 
\[E_T=\int_{\langle T\rangle} p(t)\textrm{d}t= \int_{\langle T\rangle} x^2(t)\textrm{d}t = A^2 \int_{\langle T\rangle}  \cos^2 (\aw_0 t + \theta) \textrm{d}t.\]
Using the identity $\cos^2 a = \frac{1}{2} (\cos (2a) + 1)$ (see Appendix):
\[E_T=\frac{A^2}{2} \int_{\langle T\rangle} (\cos (2\aw_0 t + 2\theta) + 1) \textrm{d}t = \frac{A^2 T}{2}.\]
The first parcel of the integral is zero, independent of $2 \theta$ because $T$ corresponds exactly to two periods of the cosine with angular frequency $2 \aw_0$, while the second parcel is $T$. The average power is
\begin{equation}
\calP = \frac{E_T}{T} = \frac{A^2}{2},
\label{eq:sinusoid_power}
\end{equation}
which is a result valid for any sinusoid or cosine. This discussion assumed continuous-time signals, but \equl{sinusoid_power} is also valid for discrete-time sinusoids.
\eExample

\bExample \textbf{Power of a DC signal.}
A constant signal $x(t) = K$ (i.\,e., a DC signal) has power $\calP = K^2$ because the energy at any interval $\Delta t$ is $E = K^2 \Delta t$. 
%A DC signal is not periodic because a period must be $T > 0$, but it has few properties of a periodic signal.
\eExample

The root-mean-square (RMS) value\index{RMS value} $\xrms$ of any signal $x(t)$ is the DC value that corresponds to the same power $\calP$ of $x(t)$, i.\,e., $x^2_{\textrm{rms}} = \calP$ or, equivalently, $\xrms = \sqrt{\calP}.$ For example, the RMS value of a cosine $x(t)=A \cos (\aw_0 t + \theta)$ is $\xrms = \frac{A}{\sqrt{2}}$ because a DC signal $y(t) = \frac{A}{\sqrt{2}}$ has the same average power as $x(t)$.

As discussed in Section~\ref{sec:impulseIsNotAFunction},  $\delta(t)$ is a distribution and it is tricky to define the energy or power of a sampled signal, which is the topic of Section~\ref{sec:sampledSignalEnergyPower}.

\section{Modeling the Stages in A/D and D/A Processes}
\label{sec:addaprocesses}

This section discusses mathematical models that are used to conveniently represent in a high-level, the individual signal processing stages in the A/D and D/A processes.

Different electronic circuits can be used when designing an ADC chip to perform the A/D process such as \emph{successive approximation} and \emph{sigma-delta}. If needed, the signal processing stages used in each of these techniques can be modeled with details. For instance, a detailed description may be necessary in case the goal is to optimize the electronic circuits to be more robust to impairments and noise. In the scope of this text, these details are not important and the A/D process is considered ideal. A similar approach is adopted for the D/A process. 

\subsection{Modeling the sampling stage in A/D}
\label{sec:sampling_stage}

The goal here is to extract samples to represent an analog signal $x(t)$.
In \emph{periodic} (or \emph{uniform}) sampling \index{Sampling} of a continuous-time function $x(t)$  with a sampling interval $T_s$, the sampling frequency is
\begin{equation}
\fs = \frac{1}{T_s}.
\label{eq:samplingFrequency}
\end{equation}
For example, digital telephony adopts $\fs = 8000$ Hz, which corresponds to $T_s = 125$~$\mu$s. 

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/impulsetrain}		
	\caption{An impulse train with unitary areas and $T_s = 125$ $\mu$s.\label{fig:impulsetrain}}
\end{figure}

In the A/D context, periodic sampling can be modeled as the multiplication of $x(t)$ by a periodic impulse train $p(t)$.
\figl{impulsetrain} depicts the infinite duration impulse train for sampling at 8~kHz. As conventionally done, the impulses heights indicate their areas.
The sampled signal is denoted as
\[
x_s(t) = x(t) p(t) = \sum_{k=-\infty}^\infty x(k \ts) \delta(t-k \ts),
\]
where $x(k \ts)$ is the impulse area in $x_s(t)$, which coincides with the amplitude of $x(t)$ at $t=k \ts$.
The amplitude of $x_s(t)$ is not defined when $t = k \ts$ (it goes to infinite, and only the area of the impulse is defined). 
And when $t \ne k \ts$, the amplitude of $x_s(t)$ is zero.
The notation $\xsarea{t}$ will sometimes be used to emphasize we mean the area of the impulse at time $t$.


\bExample \textbf{Example of periodic sampling}.
Consider sampling the analog signal in \figl{analogsignal} via multiplying it by the train of impulses in \figl{impulsetrain}. Using the impulse sifting property (see Section~\ref{sec:sifting}) leads to the sampled signal in \figl{sampledanalog}, which is depicted with the original analog signal superimposed.

In this case, the signal segment $x(t)$ in \figl{analogsignal} has $D=6.125$~ms of duration. Given that $\fs = 8$ kHz, this segment is represented by $N=50$ samples. To understand how $N$ is calculated in this case, observe that $x_s(t)$ starts and ends with samples, such that it contains $N-1$ intervals of duration $\ts = 1/\fs$ with a total duration $D = (N-1) \ts = (50-1) (1/8000)= 6.125$~ms.

An alternative representation would be to assume that there are $N$ intervals of duration $\ts$, such that $x_s(t)$ would end after an interval of $\ts$ seconds after its last impulse. One can find both representations in the literature and associated software.
\eExample

\figl{sampledanalog} shows that, in this case, the sampled signal $x_s(t)$ represents very well the original analog signal $x(t)$, in the sense that the envelope of the impulse areas correspond to a good match with respect to the original signal. This is not the case in the following example. Consider now that the sampling interval is increased by a factor of 4 to $T_s = 500 \mu$s. \figl{undersampled} indicates that in this case $x_s(t)$ cannot represent well, for example, the variation between the samples indicated in the figure.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/undersampled}	
	\caption{Example of a sampled signal obtained with a sampling frequency smaller than the required for accurately representing the original signal (shown in dotted lines).	\label{fig:undersampled}}
\end{figure}


In terms of computational cost, it is often convenient to use the smallest possible value of $\fs$. But choosing $\fs$ too small may lead to a sampled signal that does not represent well the original signal as noted in \figl{undersampled}. Section~\ref{sec:sampling_theorem} discusses a theorem that informs the minimum sampling rate for perfect reconstruction of a signal $x(t)$ from its samples $x[n]$ given the maximum frequency of $x(t)$.

%This expression was missed in our previous discussion about sampling.

\subsection{Oversampling}
\label{sec:oversampling}

When the sampling rate $\fs$ is larger than the minimum required sampling rate, the signal is considered to be \emph{oversampled}\index{Oversampling}. Oversampling an analog signal may facilitate the consequent digital signal processing, especially
when real-time is not a requirement. For instance, consider that a signal could be sampled with 8~kHz, but $\fs = 32$~kHz is adopted. If real-time processing is required, instead of processing 8,000 samples per second, now the system must process
32,000. In this case, the \emph{oversampling factor}\index{Oversampling factor} would be $L=32,000/8,000=4$.
Oversampling by a factor of $L$, can also be interpreted as decreasing the sampling interval $\ts=1/\fs$ by $L$. For the
given example, the interval $1/8,000 = 125$~\mus~between neighbor samples is reduced to $31.25$~\mus.

In spite of the potential increase in computational cost of oversampling, it may be the case that higher accuracy can be obtained
or eventually simpler algorithms adopted. It is very common to observe commercial DSP systems adopting $L=2, 4$ or larger.

When simulating DSP in computers, a very large $L$ can be adopted to generate graphs of oversampled discrete-time signals that
look like continuous-time signals.

\bExample \textbf{Mimicking continuous-time using discrete-time signals with a large oversampling factor}.
When representing a signal via a vector, one is implicitly dealing with a discrete-time signal.
But even if a signal $x[n]$ is representing the samples of an analog signal $x(t)$, sometimes
it is desirable to generate graphs that resemble $x(t)$. In such situations, one alternative is
to use a large oversampling factor, as previously used in \codl{snip_signals_rect}.
\codl{snip_signals_oversampling} provides another example.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_oversampling}{snip_signals_oversampling}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/oversampled_cosine}		
	\caption{Example of discrete-time cosines generated with sampling intervals of  $125$~\mus~(top) and 625~ns (bottom) to illustrate
	the better representation achieved by using a smaller sampling interval $\ts$, which corresponds to adopting a larger oversampling factor.\label{fig:oversampled_cosine}}
\end{figure}

\codl{snip_signals_oversampling} generates \ci{x} with a sampling interval $\ts=1/8,000=125$~\mus. 
It also creates an oversampled version \ci{xo} using a new sampling interval of $\ts/200 = 625$~ns.
Instead of using \ci{stem} in \matlab, both graphs were generated with \ci{plot}, assuming the
goal is to depict a continuous-time signal.
However, as can be seen in \figl{oversampled_cosine}, \ci{xo} corresponds to a coarse representation
of a cosine that uses $N=40$ samples (length of \ci{x}), while \ci{xo} is a much better one using $N_o=7801$ samples.
\eExample

There are several ways to create versions with different sample rates from a given signal $x(t)$.
In the specific case of \codl{snip_signals_oversampling}, the signals \ci{x} and \ci{xo} were created
such that their first and last samples correspond to the same time instant, $t=0$ and $t=4.875$~ms, respectively.
In this case, the number $N_o$ of samples in the oversampled \ci{xo} is given by
\begin{equation}
N_o = L (N -1) + 1,
\label{eq:oversampled_length}
\end{equation}
where $N$ is the number of samples in \ci{x} and $L$ is the oversampling factor. This equation
can be understood by considering that \ci{xo} has all $N$ samples of \ci{x} plus the extra $L-1$ samples
between each of the $N-1$ consecutive pairs of samples of \ci{x}. This leads to
$N_o = N + (L-1)(N-1)$, which can be rewritten as \equl{oversampled_length}.

When \equl{oversampled_length} is used for the signals in \codl{snip_signals_oversampling}, it leads to $N_o=7801$ samples, as can be confirmed by the command \ci{length(x)}.

\subsection{Mathematically modeling the whole A/D process}
%\label{sec:}

According to the convention adopted in this text, the following four signals appear during the A/D process along the stages of sampling (SAMPLING) and quantization (QUANTIZATION):
analog, sampled, discrete-time and digital.
Therefore, the A/D process is conveniently represented via the block diagram in \figl{whole_ad}.
%\[x(t) \arrowedbox{SAMPLING} x_s(t) \arrowedbox{S/D} x[n] \arrowedbox{QUANTIZATION} x_q[n],\]

\begin{figure}
	\centering
		\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/whole_ad}
	\caption{Complete process of A/D conversion with intermediate stages and four signals: analog, sampled, discrete-time and digital.\label{fig:whole_ad}}
\end{figure}

\figl{whole_ad} indicates that the sampled signals $x_s(t)$ is converted into the discrete-time signal $x[n]$ via the \emph{sampled to discrete-time} (S/D) conversion\index{Sampled to discrete-time (S/D) conversion}.
The sampling and S/D stages can be combined into the \emph{continuous to discrete-time} (C/D) conversion. We first discuss the S/D conversion and then the C/D.

\subsection{Sampled to discrete-time (S/D) conversion}
\label{sec:sd_conversion}

%$$x_s(t) \rightarrow\fbox{h(t)}\rightarrow x(t).$$

The S/D conversion represents an intermediate stage in the periodic sampling process and is always associated to a given sampling interval $\ts$. The actual signal processing performed by the electronic circuits of an ADC do not generate impulses or sampled signals, but the S/D conversion is a convenient mathematical model for the ideal periodic sampling.
For example, if $x_s(t) = 3\delta(t+ 2 \ts) - 1 \delta(t +  \ts) + 8\delta(t) - 5\delta(t-3\ts)$, the S/D conversion outputs $x[n] = 3\delta[n+ 2] - 1 \delta[n + 1] + 8\delta[n] - 5\delta[n-3]$. 

Recall that $x_s(t)$ exists for all values of $t$ while $x[n]$ is a discrete time signal with $n \in \integers$. 
Another mathematical detail is that the non-zero amplitudes of $x_s(t)$ are $\pm \infty$, but the corresponding areas $\xsarea{t}$ lead to the amplitudes of the corresponding $x[n]$ and are well-defined. For instance, $\xsarea{0}=8$ and leads to the parcel $8\delta[n]$ in discrete-time.
%and equal to the area of the impulses in $x_s(t)$.

\bExample \textbf{Example of S/D conversion}.
\label{ex:sd_conversion}
\figl{sd_conversion} illustrates an example of S/D conversion assuming $\ts=0.2$~s.

\begin{figure}
	\centering
		\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/sd_conversion}
	\caption{Example of S/D conversion assuming $\ts=0.2$~s.\label{fig:sd_conversion}}
\end{figure}

The visible samples of the input are $x_s(t) = 0.5\delta(t) -2.8\delta(t-\ts) + 1.3\delta(t-2\ts)+ 3.5\delta(t-3\ts) -1.7\delta(t-4\ts) + 1.1\delta(t-5\ts) + 4\delta(t-6\ts)$. Their corresponding output values are $x[n] = 0.5\delta[n] -2.8\delta[n-1] + 1.3\delta[n-2]+ 3.5\delta[n-3] -1.7\delta[n-4] + 1.1\delta[n-5]) + 4\delta[n-6]$.
\eExample 

\subsection{Continuous-time to discrete-time (C/D) conversion}
\label{sec:cd_conversion}

It is sometimes convenient to group the sampling and S/D stages into a single operation called continuous to discrete-time (C/D) conversion:
\[x(t) \arrowedbox{C/D} x[n].\]
The only distinction between C/D and A/D conversions is that the former does not incorporate the quantization stage, as indicated in \figl{whole_ad}.

%\begin{figure}
%	\centering
%		\includegraphics[width=7cm,keepaspectratio]{FiguresNonScript/cd_conversion}
%	\caption{Block diagram of C/D conversion, incorporating the sampling and S/D stages.\label{fig:cd_conversion}}
%\end{figure}

\subsection{Discrete-time to sampled (D/S) conversion}
\label{sec:ds_conversion}

The discrete-time to sampled (D/S) conversion is basically the inverse of S/D. 
\figl{ds_conversion} illustrates this, by depicting the inverse of the operation in \figl{sd_conversion} of \exal{sd_conversion}. 
\begin{figure}
	\centering
		\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/ds_conversion}
	\caption{Example of D/S conversion assuming $\ts=0.2$~s. It implements the inverse operation of \figl{sd_conversion}.\label{fig:ds_conversion}}
\end{figure}

In \figl{ds_conversion}, the visible samples of the input are
$x[n] = 0.5\delta[n] -2.8\delta[n-1] + 1.3\delta[n-2]+ 3.5\delta[n-3] -1.7\delta[n-4] + 1.1\delta[n-5]) + 4\delta[n-6]$.
Given that $\ts = 0.2$~seconds, the corresponding output of the D/S conversion is $x_s(t) = 0.5\delta(t) -2.8\delta(t-0.2) + 1.3\delta(t-0.4)+ 3.5\delta(t-0.6) -1.7\delta(t-0.8) + 1.1\delta(t-1) + 4\delta(t-1.2)$.

While the S/D is a stage of the A/D process model, the D/S is part of the D/A.

\subsection{Reconstruction}
\label{sec:reconstruction}

In the context of A/D and D/A processes, the sampling inverse operation
is called \emph{reconstruction}\index{Reconstruction (signal)}, and converts a sampled signal $x_s(t)$ into an analog signal $x(t)$. Reconstruction is also called \emph{interpolation}\index{Interpolation} or filtering. It consists of choosing a function $h(t)$ that is combined with $x_s(t)$ as follows:

\begin{equation}
x(t) = \sum_{n=-\infty}^{\infty} \xsarea{{n \ts}} h(t - n \ts),
\label{eq:reconstruction1}
\end{equation}
where $\xsarea{{n \ts}}$ is the area of the impulse in $x_s(t)$ at time $t=n \ts$.

The complete mathematical description of reconstruction will be discussed only in Section~\ref{sec:samplingRevisited}, after the concepts of \emph{convolution} and filtering are presented.\footnote{\equl{reconstruction1} represents the convolution between $x_s(t)$ and $h(t)$. Convolution will be discussed in Section~\ref{sec:impresponse_convolution} and allows $h(t)$ to be interpreted as the impulse response of a \emph{filter} used for reconstruction.} 

Because $x_s(t)$ is obtained from $x[n]$ via a D/S process, it is convenient to rewrite \equl{reconstruction1} observing that the impulse area $\xsarea{{n \ts}}$ coincides with its corresponding amplitude $x[n]$, such that:
\begin{equation}
x(t) = \sum_{n=-\infty}^{\infty} x[n] h(t - n \ts).
\label{eq:reconstruction}
\end{equation}
\equl{reconstruction} indicates that $x(t)$ is obtained by shifting $h(t)$ by $t=n \ts$, scaling this intermediate result by the corresponding amplitude $x[n]$, and then summing up all these parcels.

Among many alternatives for choosing the reconstruction function $h(t)$, two important ones are the so-called \emph{zero-order holder} (ZOH) $h(t)=\Pi( (t-0.5\ts)/\ts)$ and sinc reconstruction $h(t)=\sinc(t/\ts)$. ZOH is useful for its simplicity, while sincs achieve a perfect reconstruction. They are discussed in the next paragraphs.

\subsubsection{Reconstruction with zero-order holder (ZOH)}

The ZOH consists of choosing $h(t)$ with amplitude 1 from $t=0$ to $\ts$. This can be written as 
$h(t)=\Pi( (t-0.5\ts)/\ts)$ (see Section~\ref{sec:rect_function}).
ZOH reconstruction sustains the amplitude of a given sample $x[n_0]$ (which coincides with the area $\xsarea{{n_0 \ts}}$ of the impulse at time $t=n_0 \ts$ in signal $x_s(t)$) during an interval of $\ts$ seconds until the new sample $x[n_0+1]$ updates this amplitude and so on.

\bExample \textbf{Example of ZOH reconstrucion}.
\equl{reconstruction} implemented with ZOH is illustrated in \figl{zoh_reconstruction} for the signal $x_s(t)$ in \exal{sd_conversion}. In this case, $\ts=0.2$~s and, therefore, $h(t)=\Pi( (t-0.5\ts)/\ts) = \Pi( (t-0.1)/0.2)$.

\begin{figure}
	\centering
		\includegraphics[width=9cm,keepaspectratio]{Figures/zoh_reconstruction}
	\caption{Example of ZOH reconstruction using the signals of \exal{sd_conversion} with $\ts=0.2$~s. In this case, $x[n] = 0.5\delta[n] -2.8\delta[n-1] + 1.3\delta[n-2]+ 3.5\delta[n-3] -1.7\delta[n-4] + 1.1\delta[n-5]) + 4\delta[n-6]$.\label{fig:zoh_reconstruction}}
\end{figure}

For instance, assuming $n_0=0$ in \figl{zoh_reconstruction}, the area of the impulse in $x_s(0)$ is 0.5 such that $x(t)=0.5$ in the interval $t \in [0, \ts[$. Then, in the interval $t \in [\ts, 2 \ts[$ the amplitude of $x(t)$ is $-2.8$, which is the area of the impulse at $x_s(\ts)$. And this ``holding'' process continues for other samples.
\eExample 

\subsubsection{Reconstruction with sincs}
\label{sec:rec_with_sincs}

Reconstruction with sincs consists of choosing $h(t) = \sinc(t / \ts)$ (see \figl{sinc_scaling}).
It is the ideal reconstruction, as will be discussed along with the sampling theorem when presenting \equl{signalReconstructionViaSincs}.

\bExample \textbf{Perfect reconstruction with sinc functions}.
\figl{sinc_reconstruction} provides an example of \equl{reconstruction} using sinc functions. In this case, $\ts=0.2$~s and, therefore, $h(t)=\sinc(t/0.2)$.

%which have zeros at $t = \pm 0.2n, n=1,\ldots,\infty$. Hence, for all time instants $t$ in which $x_s(t)$ is non-zero (at $t=0, 0.2$ and $0.4$), only one sinc is non-zero.

\begin{figure}
	\centering
		\includegraphics[width=11cm,keepaspectratio]{Figures/sinc_reconstruction}
	\caption{Example of D/A of signal $x_q[n]=\delta[n]-3\delta[n-1]+3\delta[n-2]$ (with quantized amplitudes) with D/S using $\ts=0.2$~s and reconstruction using sinc functions.\label{fig:sinc_reconstruction}}
\end{figure}

The resulting signal $x(t)$ has an infinite support, extending from $t=-\infty$ to $\infty$ (\figl{sinc_reconstruction} depicts only the range $t \in [-0.8, 0.8]$).
It can be also noted that even $x_q[n]$
having quantized amplitudes, the reconstruction process creates an analog signal $x(t)$ for which the amplitudes
assume an infinite number of different values.

\figl{sum_sinc_reconstruction} provides more details about the bottom plot in \figl{sinc_reconstruction}. 
%In this example, D/S is based on $\ts=0.2$~s and signal reconstruction used sinc functions. 
%The input signal was $x[n]=\delta[n]-3\delta[n-1]+3\delta[n-2]$ (in spite of being a digital signal $x_q[n]$, the notation $x[n]$ will be adopted here).
Assuming $x_q[n]$ was obtained by sampling a continuous-time signal with the sampling theorem satisfied, this D/C conversion exactly recovers the original signal $x(t)$.

In this case, the canonical $\sinc(t)$ is contracted to $\sinc(t/\ts) = \sinc(t/0.2) = \sinc(5t)$, which has zeros at $t= \pm 0.2, \pm 0.4, \ldots$.
Still as indicated in \equl{reconstruction}, 
 the amplitude $x[n_0]$ at discrete-time $n_0$ 
multiplies the corresponding contracted and time-shifted $\sinc \left( \frac{t-n_0 \ts}{\ts}\right)$ to create a parcel that is represented by a specific color and dashed line in \figl{sum_sinc_reconstruction}.

\begin{figure}
	\centering
		\includegraphics[width=10cm,keepaspectratio]{Figures/sum_sinc_reconstruction}
	\caption{Identification of the individual scaled sinc functions (dashed lines) after D/S with $\ts=0.2$~s and signal reconstruction of $x_q[n]=\delta[n]-3\delta[n-1]+3\delta[n-2]$ in \figl{sinc_reconstruction}.\label{fig:sum_sinc_reconstruction}}
\end{figure}

In this example, the first sinc in \figl{sum_sinc_reconstruction} is centered at $t=0$ (corresponding to $\delta[n]$). The second and third sincs
 are $-3\sinc \left( \frac{t-0.2}{0.2}\right)$ and $3\sinc \left( \frac{t-0.4}{0.2}\right)$, respectively. The summation of these three parcels according to \equl{reconstruction}, results in the analog signal 
\begin{equation}
x(t) = \sinc(t/0.2) - 3\sinc((t-0.2)/0.2) + 3 \sinc((t-0.4)/0.2),
\label{eq:three_sincs}
\end{equation}
which is depicted with a continuous (instead of dashed) line.

The circles in \figl{sum_sinc_reconstruction} indicate the time instants $t=n \ts$ and the $\times$ marks indicate the amplitude in the position of each of the three impulses.
Note that $x(t)$ has an infinite support (duration) but has zero amplitude at $t=n \ts$, except for $n=0, 1$ and 2. These are discrete-time values corresponding to the support of $x[n]$.
\eExample 

%The reconstruction block will be discussed in more details in Section~\ref{sec:signal_reconstruction}, after Fourier analysis and linear systems are introduced.

\subsection{Discrete-time to continuous-time (D/C) conversion}
\label{sec:dc_conversion}

The discrete-time to continuous-time (D/C) conversion is the inverse of C/D and can be represented as
the following block diagram:
\[x[n] \arrowedbox{D/S} x_{s}(t) \arrowedbox{RECONSTRUCTION} x(t)\]
that illustrates the D/C conversion is composed by D/S followed by RECONSTRUCTION.


\subsection{Analog to digital (A/D) and digital to analog (D/A) conversions}

The A/D and D/A processes differ from C/D and D/C, respectively, because the former pair incorporates quantization 
such that the amplitude values can be represented with a finite number of bits and processed or stored in digital hardware.
The distinction between D/C and D/A conversions is that the inputs are, respectively, a discrete-time $x[n]$ and a digital signal $x_q[n]$. For instance, the following block diagram illustrate the signals involved in a D/A process: digital $x_q[n]$, quantized-sampled $x_{s,q}(t)$ and analog $x(t)$:
\[x_q[n] \arrowedbox{D/S} x_{s,q}(t) \arrowedbox{RECONSTRUCTION} x(t).\]

\bExample \textbf{Example of a D/A process}.
\figl{zoh} provides an example of a complete D/A process.

\begin{figure}
	\centering
		\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/zoh}
	\caption{Signal reconstruction of a signal with quantized amplitudes using ZOH.\label{fig:zoh}}
\end{figure}
\figl{zoh} illustrates D/A assuming RECONSTRUCTION using ZOH, a digital signal $x_q[n]$ with amplitudes from the set $\setM=\{-3,-1,1,3\}$ and $\ts=0.2$~s.
\eExample 

Both \figl{zoh} and \figl{zoh_reconstruction} illustrate ZOH reconstruction, but in \figl{zoh} the amplitudes of $x(t)$ are limited to the given finite set (quantized amplitudes) used in $x_q[n]$.

One could denote the ZOH output  in \figl{zoh} as $x_q(t)$ to emphasize the amplitude is quantized, but most practical reconstruction schemes (other than ZOH) generate an analog signal with amplitudes not belonging to $\setM$ and it is 
more general to denote the output of a reconstruction stage as $x(t)$ instead of $x_q(t)$. 

%In general, the D/A and D/C processes use the same signal processing stages and are distinguished only by their
%input signals: digital $x_q[n]$ or discrete-time $x[n]$, respectively.

Having all these concepts defined, \figl{canonicalDSPChain} illustrates a complete processing chain of an input analog signal $x(t)$
that generates the output $y(t)$. The core of the processing is the digital signal processing (DSP)
block, which can implement, for instance, a digital filter. Often, the same sampling frequency value $\fs$ is used
by both A/D and D/A processes.\footnote{But in general, multiple sampling frequencies can be adopted at distinct
stages of a signal processing chain. In this case, one can use multirate signal processing.} Note that the sampled signals $x_s(t)$ and $y_{q,s}(t)$ do not
actually exist within ADC or DAC chips, but are useful abstract models for the
input of the S/D process and output of the D/S process, respectively.

\begin{figure}
	\centering
		\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/canonicalDSPChain}
	\caption{Complete processing chain of an input analog signal $x(t)$
to generate an output $y(t)$ using DSP.\label{fig:canonicalDSPChain}}
\end{figure}

%The reconstruction block will be discussed later on.
%BECAUSE IT IS A LTI AND WE WILL DISCUSSE LATER.

\ignore{	
	Note that \xd is a special case of a discrete-time signal. The adopted notation could make explicit that \xs in Figure is quantized using $x_{s,q}(t)$, but the notation would get cumbersome while it is not hard to see that even if implicit.
	}

\subsection{Sampling theorem}
\label{sec:sampling_theorem}

The \emph{sampling theorem \index{Sampling theorem}} specifies the minimum value that $\fs$ must assume in order to be able to perfectly reconstruct $x(t)$ from $x_s(t)$ or the respective $x[n]$. It is stated here without proof, which is deferred to Chapter~\ref{ch:systems}.

%\newpage
\bTheorem
\label{th:sampling} \textbf{Sampling theorem.}
Assuming the maximum frequency of a real-valued signal $x(t)$ is $f_\tmax$, the sampling frequency must obey 
\begin{equation}
\label{eq:samplingTheoremRealSignals}
\fs > 2 f_\tmax,	
\end{equation}
in order to guarantee the \emph{perfect reconstruction} of $x(t)$ from its sampled version $x_s(t)$ or the corresponding $x[n]$.
If $f_\tmax$ is given in Hz, $\fs$ is the number of real-valued samples per second, also given in Hz (alternatively, it can be denoted in samples per second, or SPS).
\eTheorem

Sampling is so important that it will be further discussed later with the help of Fourier transforms in Section~\ref{sec:samplingRevisited}. But before delving into a more rigorous mathematical description of sampling, it is useful to understand the expression of a periodic impulse train
\[
p(t) = \sum_{k=-\infty}^\infty \delta(t-k T_s),
\]
where $T_s$ is the sampling interval and $k \in \integers$. The expression for $p(t)$ and the sifting property of the impulse allows to model the sampled signal as
\begin{equation}
x_s(t) = x(t) p(t) = \sum_{k=-\infty}^\infty x(k T_s) \delta(t-k T_s).
\label{eq:sampledSignalGeneral}
\end{equation}
The S/D stage can then convert $x_s(t)$ into $x[n]$, which represents $x(t)$.

%of \equl{sampledSignalGeneral}.
% when compared to other options such as using pulses.

%While \equl{sampledSignalGeneral} models sampling, the inverse process of reconstructing $x(t)$ is more involved.
In \equl{samplingTheoremRealSignals}, perfect reconstruction means that given the sample values, specified as $x_s(t)$ or $x[n]$, it is possible to recover the original $x(t)$. But in this case, the reconstruction process is not as simple as the ZOH in \figl{zoh}. The ideal (``perfect'') reconstruction adopts the infinite-duration sinc function as discussed in Section~\ref{sec:rec_with_sincs}. More specifically, $x(t)$ is obtained from \equl{reconstruction} as the following combination of scaled and shifted versions of $h(t)=\sinc(t / \ts)$:
\begin{equation}
x(t) = \sum_{n=-\infty}^{\infty} x[n] \sinc \left( \frac{t-n \ts}{\ts}\right).
\label{eq:sincInterpolation}
\end{equation}

A continuous-time signal $x(t)$ with infinite duration often requires an infinite number of samples to be fully represented. This can be seen by sampling an eternal sinusoidal signal according to the sampling theorem.
For instance, $x(t)=3 \cos(20 \pi t)$ has angular frequency $\aw=20\pi$~rad/s and frequency 10~Hz. Assuming C/D conversion with $\fs=40$~Hz, leads to $x[n] = \ldots + 3\delta[n+4] -3\delta[n+2] + 3\delta[n] -3\delta[n-2] + 3 \delta[n-4] -3 \delta[n-6] + \ldots$,
which corresponds to repeating the amplitude values $3, 0, -3, 0$ from $n=-\infty$ to $\infty$. 
Hence, the original signal $x(t)$ can be perfectly reconstructed from $x[n]$ using \equl{sincInterpolation}.
In this case, the infinite number of sinc parcels in \equl{sincInterpolation} would properly add to perfectly compose the original signal $x(t) = 3 \cos(20 \pi t)$.

The following example aims at providing a more concrete description of reconstructing a sinusoid. Instead of trying to use an eternal sinusoid and infinite sinc parcels, an approximation is used for a specific time interval.

\bExample \textbf{Example of cosine reconstruction from few samples}.
This examples adopts oversampling (see Section~\ref{sec:oversampling}) to generate a high-resolution discrete-time signal 
that resembles the original analog signal.
\codl{snip_signals_cosine_reconstruction} describes all preparation steps for invoking the
function \ci{ak\_sinc\_reconstruction.m}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_cosine\_reconstruction}{snip_signals_cosine_reconstruction}

The low-resolution signal $x[n]$, or \ci{xn} in \codl{snip_signals_cosine_reconstruction}, represents a segment with only 17 samples obtained by sampling a segment from $t=-1.6$ to 1.6~s of the original signal $x(t)=A \cos(2 \pi f_c t)$, with $A=4$~volts and $f_c = 1.25$~Hz. Because $\fs = 5$~Hz is larger than $2 f_c$~Hz, the sampling theorem is obeyed and a perfect reconstruction is possible.

\begin{figure}
	\centering
		\includegraphics[width=0.8\textwidth,keepaspectratio]{Figures/cosine_reconstruction_steps}		
	\caption{Sampling and sinc-based perfect reconstruction of a cosine as implemented
	in function \ci{ak\_sinc\_reconstruction.m}.\label{fig:cosine_reconstruction_steps}}
\end{figure}

The signal \ci{oversampled\_xn} plays the role of the analog signal $x(t)$ within a computer, using a relatively large oversampling factor of 200.
This signal is shown in the top plot of \figl{cosine_reconstruction_steps}. The second plot from the top is depicting the sampled signal $x_s(t)$, while the third shows the discrete-time signal $x[n] = \textrm{S/D} \{ x_s(t) \}$.

The signal \ci{oversampled\_xn} is passed as an input argument to function
\ci{ak\_sinc\_reconstruction.m} such that it can be compared to the reconstructed signal created within this function, as depicted in the bottom plot of \figl{cosine_reconstruction_steps}. The two plots (``original'' and ``reconstructed'') overlap almost  perfectly. But closely observing the bottom plot in \figl{cosine_reconstruction_steps} shows that the reconstruction is
better in the time instants closer to the center $t=0$ than in the endpoints $t = \pm 1.6$~s.
The reason for that can be understood by interpreting \figl{cosine_reconstruction_single_parcel} and \figl{cosine_reconstruction_parcels}.

\begin{figure}
	\centering
		\includegraphics[width=0.7\textwidth,keepaspectratio]{Figures/cosine_reconstruction_single_parcel}		
	\caption{Single sinc parcel from \figl{cosine_reconstruction_parcels} corresponding to the sinc centered at $t=-0.8$~s as dashed line and the reconstructed signal as a solid line.\label{fig:cosine_reconstruction_single_parcel}}
\end{figure}

\figl{cosine_reconstruction_single_parcel} depicts a single sinc while \figl{cosine_reconstruction_parcels} shows all sinc parcels. \figl{cosine_reconstruction_single_parcel} shows the sinc centered at $t=-0.8$~s (or $n=-4$) and the reconstructed signal, obtained by using all sinc parcels.

\begin{figure}
	\centering
		\includegraphics[width=0.7\textwidth,keepaspectratio]{Figures/cosine_reconstruction_parcels}		
	\caption{All individual parcels in dashed lines corresponding to each sinc and the summation
	as reconstructed signal (solid line) of \codl{snip_signals_cosine_reconstruction}.\label{fig:cosine_reconstruction_parcels}}
\end{figure}

Because it has several superimposed curves, the reader is invited to obtain \figl{cosine_reconstruction_parcels} by executing \ci{ak\_sinc\_reconstruction.m} iteratively, step-by-step, to see each individual sinc being plotted. \figl{cosine_reconstruction_parcels} can only depict the final result, with all sincs superimposed.
Basically, this figure shows that the scripts was created such that only nine sincs effectively contribute to reconstructing the cosine, given that six samples of $x[n]$ have amplitude equals zero. The reconstruction at the endpoints is impaired because the infinite number of sincs in \equl{sincInterpolation} are not used by the script. The missing sincs at the left ($t < -1.6$) and right ($t > 1.6$) extrema have more impact at the endpoints than at $t=0$.
\eExample

\bExample \textbf{Reconstruction of a signal composed by sincs}.
This example further investigates the reconstruction of the signal described by \equl{three_sincs}, which is
repeated below for convenience:
\[
x(t) = \sinc(t/0.2) - 3\sinc((t-0.2)/0.2) + 3 \sinc((t-0.4)/0.2).
\]
The signal $x(t)$ is composed by three parcels of the sinc
\[
z(t) = \sinc \left( \frac{t}{0.2} \right).
\]
Finding the frequencies that compose a signal is the topic of Chapter~\ref{ch:transforms}, but it can be anticipated
here that the maximum frequency\footnote{This $f_\tmax$ value is obtained from \equl{sinc_time_transform_Hz} with $F=2.5$~Hz, and observing that the spectrum of $x(t)$ extends up to 2.5~Hz but does not have an impulse at this frequency, indicating that there is no frequency component at 2.5~Hz, and $f_\tmax < 2.5$~Hz.} of $z(t)$ is bounded by $f_\tmax < 2.5$~Hz. Shifting $z(t)$ to $t=0.2$ and $t=0.4$, and then adding these three parcels to obtain \equl{three_sincs}, does not increase the
maximum frequency. Hence, the bound $f_\tmax < 2.5$~Hz also holds for the signal $x(t)$.

In the case of $f_\tmax < 2.5$~Hz, the sampling theorem states that $\fs = 5$~Hz guarantees perfect reconstruction.
\codl{snip_signals_reconstruction_sinc} adopted $\ts = 0.1$~s, which corresponds to $\fs = 1/\ts = 10$~Hz,
which also obeys the theorem.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_reconstruction\_sinc}{snip_signals_reconstruction_sinc}

\figl{sinc_reconstruction_steps} (bottom plot) illustrates good reconstruction, even when using a segment of limited duration (from $t=-0.8$ to 1.2~s).

\begin{figure}
	\centering
		\includegraphics[width=0.8\textwidth,keepaspectratio]{Figures/sinc_reconstruction_steps}		
	\caption{Sampling and reconstruction of
	$x(t) = \sinc(t/0.2) - 3\sinc((t-0.2)/0.2) + 3 \sinc((t-0.4)/0.2)$ using $\ts = 0.1$~s.\label{fig:sinc_reconstruction_steps}}
\end{figure}

\figl{sinc_reconstruction_parcels} is similar to \figl{cosine_reconstruction_parcels} and shows the
sinc parcels used to reconstruct the signal in \figl{sinc_reconstruction_steps}.
\begin{figure}
	\centering
		\includegraphics[width=0.8\textwidth,keepaspectratio]{Figures/sinc_reconstruction_parcels}		
	\caption{Sinc parcels used in the reconstruction of $x(t)$ of \figl{sinc_reconstruction_steps}.\label{fig:sinc_reconstruction_parcels}}
\end{figure}

Both \figl{sinc_reconstruction_steps} and \figl{sinc_reconstruction} are derived using the same signal $x(t)$, but using $\ts=0.1$ and 0.2, respectively. It can be seen from \figl{sinc_reconstruction} that $\ts=0.2$~s has, in this case, the property of representing $x(t)$ with only three non-zero samples.
The reason to have only three non-zero samples is that the impulses in the train $p(t) = \sum_{k=-\infty}^\infty \delta(t-0.2k)$ used for sampling, coincide with $x(t)$ having an amplitude of zero, with the exception of $t=0, 0.2$ and 0.4~seconds, as depicted in \figl{sum_sinc_reconstruction}. However, if $\ts=0.1$~s is adopted, an infinite number of samples is required to represent $x(t)$.
\eExample

\bExample \textbf{Failing to reconstruct signals when adopting $\fs = 2 f_\tmax$}.
%\equl{three_sincs}
%If the sampling theorem is obeyed, one can always perfectly recover $x(t)$ from its samples $x[n]$.
%For instance, knowing that $\ts=0.2$~s and that the sampling theorem was obeyed, the signal
%$x(t) = \sinc(t/0.2) - 3\sinc((t-0.2)/0.2) + 3 \sinc((t-0.4)/0.2)$
%can be fully represented by $x[n]=\delta[n]-3\delta[n-1]+3\delta[n-2]$, as indicated in \figl{sum_sinc_reconstruction}. In this case, the three non-zero
%samples of $x[n]$ allow to recover the infinite duration $x(t)$. 
%
%Consider now the signal $z(t)=x(t-0.1)$, obtained by delaying $x(t)$ by 0.1~s. In this case, the C/D conversion of $z(t)$ would lead to $z[n]$ with infinite duration.
The sampling theorem is a strict inequality $\fs > 2 f_{\textrm{max}}$ but some people state it as a non strict inequality, i.\,e., $\fs \ge 2 f_{\textrm{max}}$, which is incorrect. 

One reason for considering $\fs \ge 2 f_{\textrm{max}}$ is wrongly interpreting that $f_\tmax$ of the signal described by \equl{three_sincs} is $f_\tmax=2.5$~Hz. Because in this case $\fs = 5$~Hz works, it may lead to the incorrect assumption that $\fs \ge 2 f_{\textrm{max}}$ always works.
However, the signal of \equl{three_sincs} has infinite frequency components, which extend from zero up to 2.5~Hz, but it does
not have a discrete component at $2.5$~Hz. Hence, its $f_{\textrm{max}} < 2.5$~Hz and $\fs = 5$~Hz obeys the sampling theorem, and allows perfect reconstruction, as can be seen in \figl{sinc_reconstruction}.

However, if a signal has a discrete component at $f_{\textrm{max}}$, then choosing $\fs = 2 f_{\textrm{max}}$ does not guarantee
reconstruction. A simple example is to consider sampling the signals 
$y(t) = \cos (2 \pi \times 2.5 t)$ and $x(t) = \cos (2 \pi \times 2.5 t - 0.5 \pi)$ using $\fs = 5$~Hz. Both signals have $f_{\textrm{max}} = 2.5$~Hz, but the time
delay created by the phase $0.5 \pi$ makes the sampling instants coincide with the zeros of $x(t)$. In this case, $x_s(t)$ (and, consequently $x[n]$) has amplitudes equal to zero, and reconstruction fails. This is indicated in \figl{failed_cosine_reconstruction}, created with the script \ci{snip\_signals\_failed\_cosine\_reconstruction.m}. This script can be modified to show that $y(t)$ can be eventually reconstructed, but it is not guaranteed. In summary, one must use $\fs > 2 f_{\textrm{max}}$ to guarantee perfect reconstruction.

%\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_failed\_cosine\_reconstruction}{snip_signals_failed_cosine_reconstruction}

\begin{figure}
	\centering
		\includegraphics[width=0.8\textwidth,keepaspectratio]{Figures/failed_cosine_reconstruction}		
	\caption{Example of failing to reconstruct the signal $x(t) = \cos (2 \pi 2.5 t - 0.5 \pi)$ using $\fs = 2 f_{\textrm{max}} = 5$~Hz.\label{fig:failed_cosine_reconstruction}}
\end{figure}


	%In fact, Shannon himself stated the theorem not using the correct strict $fs > 2 f_{\textrm{max}}$. 
As another example, assume a cosine 
	\begin{equation}
	x(t) = A \cos\left(2 \pi f_0 t + \theta \right),
	\label{eq:samplingCosineExample}
	\end{equation}	
where $A=1/\cos(\theta)$. The sampling frequency is (wrongly) chosen as $\fs=2f_0$ such that $x(n) = \cos(\pi n) = (-1)^n$ and $x(t)$ cannot be recovered from $x[n]$ (which is the same for any phase $\theta$), as illustrated by \codl{snip_signals_sampling_inequality}. 
	
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_sampling\_inequality}{snip_signals_sampling_inequality}
%\begin{lstlisting}
%theta=0.8 %define an arbitrary angle
%Fs=20; Ts=1/Fs; %pick Fs in Hz (any value)
%f0=Fs/2; %key thing: Fs should  be greater than 2 f0
%t=0:Ts:99*Ts; %discrete-time axis
%A=1/cos(theta); %amplitude
%x=A*cos(2*pi*f0*t+theta); %you can plot this signal
%\end{lstlisting}
This ambiguity and the previous example demonstrate the need for a strict inequality $\fs > 2 f_{\textrm{max}}$ when interpreting the 
sampling theorem.
\eExample 


\bExample \textbf{Trying to prove that the sampling theorem is wrong}!
A very astute student compared \figl{three_rects} and \figl{sinc_reconstruction}, and observed that
two distinct signals could have generated the samples in $x[n]=\delta[n]-3\delta[n-1]+3\delta[n-2]$.
Hence, she concluded that the sampling theorem was wrong, given that from the samples $x[n]$, one
would not be able to distinguish if the original signal corresponds to 
\equl{three_rects} or \equl{three_sincs}.

Her main mistake was not to check the maximum frequencies of the signals given by \equl{three_rects} or \equl{three_sincs}.
Finding the frequencies that compose a signal is the topic of Chapter~\ref{ch:transforms}, but it can be anticipated
here that the maximum frequency of \equl{three_sincs} is $f_\tmax = 2.5$~Hz (from \equl{sinc_time_transform_Hz}) while the signal corresponding to \equl{three_rects} has an infinite maximum frequency (from \equl{sinc_transform}) and
its conversion to $x[n]$ did not obey the sampling theorem!

\codl{snip_signals_reconstruction_failure} illustrates what happens when one starts with \equl{three_rects},
samples this signal (but not obeying the sampling theorem) and then tries to recover \equl{three_rects}
using sinc interpolation. The generated \figl{failed_reconstruction_steps} shows that reconstruction fails miserably.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_reconstruction\_failure}{snip_signals_reconstruction_failure}

\begin{figure}
	\centering
		\includegraphics[width=0.8\textwidth,keepaspectratio]{Figures/failed_reconstruction_steps}		
	\caption{Reconstruction of signal given by \equl{three_rects} fails because the sampling theorem is not obeyed.\label{fig:failed_reconstruction_steps}}
\end{figure}

\figl{failed_reconstruction_parcels} is similar to \figl{cosine_reconstruction_parcels} and shows the
sinc parcels used to reconstruct the signal.
\begin{figure}
	\centering
		\includegraphics[width=0.8\textwidth,keepaspectratio]{Figures/failed_reconstruction_parcels}		
	\caption{Sinc parcels used in the signal reconstructed as depicted in \figl{failed_reconstruction_steps}.\label{fig:failed_reconstruction_parcels}}
\end{figure}

The main message of this example is the proper interpretation of
the sampling theorem: there is a unique continuous-time signal that has $f_\tmax < \fs/2$ and is represented
by a given sampled signal \xs, or equivalently, $x[n]$. 
In this specific case, \equl{three_sincs} is the only signal $x(t)$ that can lead to
$x[n]=\delta[n]-3\delta[n-1]+3\delta[n-2]$.
\eExample

The perfect reconstruction guaranteed by the sampling theorem, \equl{samplingTheoremRealSignals}, assumes the use of the ideal sinc interpolation of \equl{sincInterpolation}, which is not realizable in practice. Hence, $\fs$ is chosen to be typically $2.5 f_\tmax$ or even larger to alleviate the errors imposed by a non ideal reconstruction. The sampling frequencies adopted for some common signals\footnote{ECG is discussed, for example, at \akurl{http://circ.ahajournals.org/content/104/25/3087.full}{1ecg}.} are shown in Table~\ref{tab:samplingfreqs}.

%TC:ignore
\begin{table}
   \centering
   \caption{Typical sampling frequencies.\label{tab:samplingfreqs}}	
\begin{tabularx}{\textwidth}{llXl}
\toprule
  Signal & $f_\tmax$ & Explanation for $f_\tmax$ & $\fs$ \\
  \midrule
	Telephone speech & 3400 Hz & Band [300,~3400]~Hz suffices for intelligibility & 8 kHz \\
	Audio (CD format) & 20 kHz & Humans can hear up to $\sim$20~kHz & 44.1 kHz \\
	%ADSL modem & 1,104 kHz & Cable attenuation increases with frequency & 2,208 kHz \\
	Electrocardiogram (ECG) & $<250$ Hz &  Clinical studies & 500 Hz \\
	\bottomrule
	\end{tabularx}
\end{table}
%TC:endignore

\ifml
In special cases, when $x(t)$ is complex-valued or a ``passband'' signal (Section~\ref{sec:undersampling}), a lower value of $\fs$ can be adopted and still allow perfect reconstruction.
\else
In special cases, when $x(t)$ is complex-valued (Section~\ref{sec:iqSampling}) or a ``passband'' signal (Section~\ref{sec:undersampling}), a lower value of $\fs$ can be adopted and still allow perfect reconstruction.
\fi

\subsection{Different notations for S/D conversion}

It is convenient to have two different notations for the S/D conversion: a simpler one and an alternative
that better represents cases in which the amplitude of $x[n]$ depends on the sampling interval $\ts$. 
We start this discussion with an example.

\bExample \textbf{Example of S/D conversion of a signal $x_s(t)$}.
\label{ex:CDnotation}
Assume the continuous-time signal $x(t) = 4 e^{-2t} u(t)$ should be sampled with a period $\ts$
to create $x_s(t)$, which is then transformed into a discrete-time signal $x[n]$ via the S/D operation.
The notation $\textrm{S/D} \{ \cdot \}$ will denote the S/D process, which in this case leads to:
\begin{align*}
x[n] &= \textrm{S/D} \left\{x_s(t)\right\} \\
  &= \textrm{S/D} \left\{\sum_{n=-\infty}^{\infty} x(n\ts) \delta(t - n\ts) \right\} \\
	&= \textrm{S/D} \left\{ \sum_{n=-\infty}^{\infty} 4 e^{-2 n \ts} u(n \ts) \delta(t - n\ts) \right\} \\
	&= 4 e^{-2 n \ts} u[n], \numberthis
\label{eq:cdConversionExample2}
\end{align*}
The last step in \equl{cdConversionExample2} is based
on the fact that $\textrm{S/D} \{ u(n \ts) \delta(t - n\ts) \} = u[n]$.
\eExample

When writing signal expressions, the sampling operation is eventually not made
explicit. An alternative notation is discussed in the following example.

\bExample \textbf{Simplified notation for the S/D conversion of a signal $x(t)$}.
\label{ex:simplifiedCD}
This example discusses a simplified notation for the S/D conversion that is sometimes
adopted in the literature.

Assume the continuous-time signal $x(t) = 4 e^{-2t} u(t)$ should be transformed to a 
discrete-time $x[n]$ with a sampling period $\ts$.
This conversion is often denoted as:
\begin{equation}
x[n] = x(t)|_{t=n \ts} = 4 e^{-2 n \ts} u[n],
\label{eq:cdConversionExample1}
\end{equation}
which can be confusing. One could write $x(t)|_{t=n \ts} = 4 e^{-2 n \ts} u(n \ts)$ and, comparing to \equl{cdConversionExample1}, complain that the samples of the continuous-time step function $u(n \ts)$ 
became $u[n]$ ($n\ts$ was ``substituted'' by $n$), while $n\ts$ remained (was not substituted
by $n$) in the exponential $e^{-2 n \ts}$. 

The reason to be careful with the simplified notation $x[n] = x(t)|_{t=n \ts}$ is that, when performing a S/D conversion, the
occurrences of $n\ts$ as part of the \emph{independent variable} (the argument $t$ of $x(t)$, within $(\cdot)$) are converted to $n$, as depicted in 
\figl{sd_conversion}. However, the factor $n \ts$ remains when it influences the \emph{amplitude}
(dependent variable).

%The multiplication by the impulse train is not explicit, but this step should be recalled such that the S/D conversion becomes similar to the one in \figl{cd_conversion}. 

Hence, the notation $x[n]=x(t)|_{t=n \ts}$ for the S/D process in \equl{cdConversionExample1} is somehow incomplete. It does not rely on impulses and, consequently, it does not make explicit the intermediate
step of creating a sampled signal $x_s(t)$.
However, because the $\textrm{S/D} \{ \cdot \}$ notation is cumbersome, the reader should be
also familiar with the widely adopted alternative of \equl{cdConversionExample1}.
\eExample


\section{Relating Frequencies of Continuous and Discrete-Time Signals}
\label{sec:relatingContinuousDiscrete}

When periodic sampling is used
%. It is useful to observe the conversion of $x(t)$ into $x[n]$ via uniform sampling 
with sampling frequency $\fs$, the frequencies 
that show up in $x(t)$ are mapped into frequencies in $x_s(t)$ and $x[n]$.
This section discusses such mappings. We start by observing that angular frequencies
are denoted as $\aw$ and $\dw$, in continuous and discrete-time, respectively.
This notation is important because they have different units and properties.

\subsection{Units of continuous-time and discrete-time angular frequencies}

%It is important to distinguish the units of $\aw$ and $\dw$.
In essence, a continuous-time signal (for instance, $x(t)=\cos(\aw t)$) has radians per second (rad/s) as the unit of the angular frequency $\aw$. Multiplying $\aw$ by $t$, which is given in seconds, leads to an angle in radians. In contrast, the unit of the angular frequency $\dw$ of a discrete-time signal (for instance, $x[n]=\cos(\dw n)$) is given in radians. Because $n$ is dimensionless, $\dw n$ is an angle in radians. Similar to the interpretation of discrete-time $n$ as ``time'', in spite of being an angle, $\dw$ will be interpreted as angular frequency.

The different units of $\aw$ (rad/s) and $\dw$ (rad) will play a fundamental role in discrete-time signal processing: a function $f(\aw)$ of $\aw$ can assume distinct values when $\aw$ is varied in the range $[- \infty, \infty]$, while a
function $f(\dw)$ is periodic if $\dw$ is an angle. More specifically, when the variable $\dw$ is used to denote a discrete-time angular frequency, any function $f(\dw)$ of $\dw$ will have a period of $2 \pi$ rad and be typically evaluated only in a range of $2\pi$, such as $[0, 2\pi[$ or $[-\pi, \pi[$.

In other words, whenever $\dw$ is an angular frequency (that is, an angle), the function $f(\dw) = f(\dw + 2 \pi), \forall \dw$ is periodic and $\dw$ shows up as the argument of cosines or sines. For example, $f(\dw) = \cos(3 \dw) / \cos(5 \dw)$ and $f(\dw) = e^{j 2 \dw}$ are possible functions of an angular frequency $\dw$. 
In fact, it is a common practice to use the
notation $f(e^{j\dw})$ to indicate that $f$ is a function of an angle $\dw$ (see, e.\,g. \equl{fourier_transform}).
Therefore, one will never find something like $f(e^{j\dw})=(3+\dw)/(5+\dw)$ because the periodicity $f(\dw) = f(\dw + 2 \pi)$ is not observed in this case.

\subsection{Mapping frequencies in continuous and discrete-time domains}

% with the intermediate step of creating $x_s(t)$.
In the following paragraphs, the main goal is not to prove, but to motivate the \emph{fundamental equation}
\begin{equation}
\aw = \dw \fs,
\label{eq:freqdiscrete2continuous} %\label{eq:fundamentalExpression}
\end{equation}
where $\fs$ is the sampling frequency assumed to be in Hertz, $\aw$ is the continuous-time angular frequency given in radians per second and $\dw$ is the discrete-time angular frequency given in radians.

To better interpret \equl{freqdiscrete2continuous}, one can apply the S/D conversion to a single sinusoid, as exemplified in the next paragraphs.

%write the expression for a sampled signal $x_s(t)$ with $t = n \ts$, where $\ts=1/\fs$ is the sampling interval, as exemplified in the sequel.

\bExample \textbf{Example of using the fundamental equation for relating angular frequencies of continuous-time and discrete-time cosines.}
%\label{ex:fundamentalExpression}
Assume $x(t) = 10 \cos(24 \pi t )$ is sampled with sampling interval $\ts$ to create the signal $x_s(t)$ via the impulse sifting property of \equl{sampledSignalGeneral}:
%$$
%x_s(t) = x(n \ts) = 10 \cos(24 \pi n \ts) \delta(t - n \ts).
%$$
%This expression depends on $t$ and $n$ and is not adequate. The notation $x_s(t)$ suggests that $t$ is the only independent variable, and the proper expression is
\begin{equation}
x_s(t) = \sum_{n=-\infty}^\infty 10 \cos(24 \pi n \ts) \delta(t - n \ts).
\label{eq:sampled_cosine}
\end{equation}
In this case, the S/D conversion of $x_s(t)$ results in
\[
x[n] = \textrm{S/D} \left\{x_s(t)\right\} = 10 \cos(24 \pi \ts n) = 10 \cos \left( \frac{24 \pi}{\fs}~ n\right).
\]
%As mentioned, the impulses in \equl{sampled_cosine} were necessary to locate the sample values and also to inform that the amplitude of $x_s(t)$ is zero when $t \ne n \ts$. But after the S/D conversion, the continuous-time impulses are not necessary anymore.
Note  that the original angular frequency $\aw=24 \pi$~radians/s was converted to the angular frequency in discrete-time $\dw = 24 \pi / \fs$ radians, which corresponds to $\dw=\aw / \fs$ in \equl{freqdiscrete2continuous}. 

For example, assuming $\fs = 36$~Hz, the cosine with angular frequency $\aw=24 \pi$ rad/s (that is equivalent to the linear frequency $f=12$~Hz in this case) will be mapped to the angle $\dw = 2 \pi / 3$ rad. Using \equl{freqdiscrete2continuous} in the other direction, a discrete-time angular frequency of $\dw = \pi$ rad is always mapped to $\aw = \pi \fs$. In the given example, $\dw = \pi$ rad is mapped into $\aw = 36 \pi$ rad/s or, equivalently, $f=18$~Hz. 
\eExample 

The previous example can be generalized. In summary, \equl{freqdiscrete2continuous}
is valid for all pairs of continuous-time and discrete-time signals that are related by a C/D or D/C conversion obtained with periodic sampling.

\bExample \textbf{Another example of conversion between discrete and continuous-time domains}.
Assume a discrete-time signal $x[n]=10 \cos ( (2 \pi / 7)~n)$. It is possible to indicate that its angular frequency is $\dw = 2 \pi / 7$ rad because the signal repeats itself every $N=7$ samples. However, because $n$ is dimensionless, there is no information about time in this case. If it is stated that $x[n]$ was obtained via sampling at $\fs = 10$~Hz, $x[n]$ would be representing a cosine of angular frequency $\aw = 20 \pi / 7$~rad/s. Instead, if $\fs = 100$~Hz, then $\aw = 200 \pi / 7$~rad/s. 
As indicated in \equl{freqdiscrete2continuous}, the sampling frequency is needed when mapping the discrete-time angular frequency $\dw$ into the corresponding continuous-time angular frequency $\aw$.
\eExample

In summary, $\fs$ in \equl{freqdiscrete2continuous} plays the role of a normalization factor that relates angular frequencies of continuous-time signals and their discrete-time counterparts obtained by C/D conversion.
It leads to two distinct ways of simulating a continuous-time sinusoid using a discrete-time signal.


\bExample \textbf{Two approaches to simulate continuous-time sinusoids}.
\codl{snip_signals_sinusoid_generation} indicates how to use \equl{freqdiscrete2continuous} to create discrete-time sinusoids. The task is to generate a cosine of 600 Hz using {\matlab}. Two distinct approaches are: 
\begin{enumerate}
\item use ``continuous-time'' frequencies $f$ (in Hz) or $\aw$ (in rad/s), with discretized time $t$ in seconds,
or
\item use ``discrete-time'' frequencies $\dw$ (in rad), obtained via \equl{freqdiscrete2continuous}, and using a dimensionless ``time'' $n$.
\end{enumerate}
These two approaches are contrasted in \codl{snip_signals_sinusoid_generation}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_sinusoid\_generation}{snip_signals_sinusoid_generation}

Note that the sequences \ci{x1} and \ci{x2} are essentially the same, and there are only small numerical errors.
In essence, one can simulate discrete-time signals representing the time evolution either with $n$ (an integer) or $t$ (in seconds), but properly using the corresponding angular frequencies $\dw$ (in radians) or $\aw$ (in rad/s), respectively.

\codl{snip_signals_sinusoid_generation} compared signals created with $n=0,1,2,\ldots,$ versus $t=n \ts=0, 125 \times 10^{-6}, 250 \times 10^{-6}, \ldots,$ (given that $\ts=1/\fs=125 \times 10^{-6}$ seconds), with the corresponding angular frequencies being $\dw=2 \pi 600 / 8000 \approx 0.471$~rad or $\aw= 2 \pi 600 \approx 3769.9$~rad/s, respectively.
\eExample

\subsection{Nyquist frequency}
\label{sec:nyquist_frequency}

As indicated by \equl{samplingTheoremRealSignals}, if the sampling theorem is obeyed, the maximum frequency in the original continuous-time signal $x(t)$ is restricted to $f_\tmax < \fs/2	$~Hz, where $\fs/2$ is called the \emph{Nyquist frequency}\index{Nyquist frequency}. 


Using \equl{freqdiscrete2continuous}, one can see that angle $\dw = \pi$ rad will be mapped into $\aw = \dw \fs = \pi \fs$~rad/s, which corresponds to the Nyquist frequency $\fs/2$~Hz.
This is consistent with the fact that $\pi$ represents the highest frequency in discrete-time processing. This can be observed by plotting $x[n]=\cos( \dw n)$ and varying $\dw$ until it reaches $\pi$ rad, which corresponds to a period of $N=2$ samples. Increasing $\dw$ from $\pi$ to $1.5 \pi$, for instance, will slow down the signal (observe the angles: $1.5 \pi = -0.5 \pi$), and increase the period to $N=4$ samples.

%\subsection{The frequency normalization $\dw/\pi$ in {\matlab}}
\subsection{Frequency normalization in Python and {\matlab}}
\label{sec:frequency_normalization}

It is sometimes inconvenient to show graphs with the abscissa using $\dw$ in radians. For example, a graph in the range $[0, \pi]$ would have the last abscissa value as approximately $3.14159$, which could be annoying. To avoid that, the convention adopted by {\matlab} is to use, instead of $\dw$ in rad, a normalized frequency 
\begin{equation}
f_N=\dw / \pi.
\label{eq:matlabNormalizedFrequency}
\end{equation}
This division by $\pi$ maps the discrete-time angular frequency range $[0, \pi]$~rad into the range $[0, 1]$ of a normalized frequency $f_N$.

Given the sampling frequency $\fs$ and the fact that $\dw=\aw/\fs$ (from \equl{freqdiscrete2continuous}), \equl{matlabNormalizedFrequency} can be conveniently written as
\begin{equation}
f_N=  \frac{\dw}{\pi} = \frac{\aw}{\fs \pi} = \frac{2 \pi f}{ \fs \pi} = \frac{f}{\fs/2}.
\label{eq:matlabNormFreq2}
\end{equation}

Having a frequency $f$ in Hz, \equl{matlabNormFreq2} can be used to map it to $f_N$ in the range $[0, 1]$
as required by software such as {\matlab} and Python, and indicated in \equl{matlabNormalizedFrequency}. 
For instance, if $\fs=10$~Hz, a frequency $f$=4~Hz after the sampling process would be represented as $f_N=4/5$ in {\matlab} or Python,
which can be quickly calculated via
\begin{equation}
f_N= \frac{f}{\fs/2},
\label{eq:inHzMatlabNormalizedFrequency}
\end{equation}
i.\,e., $f$ is simply divided by the Nyquist frequency $\fs/2$.

\begin{table}
\centering
\caption{The notation and values of the Nyquist frequency in distinct domains.\label{tab:nyquist_frequency}}
\begin{tabular}{|c|c|c|c|}
\hline
$f$ (Hz)  & $\aw$ (rad/s) & $\dw$ (rad) & {\matlab} normalized $f_N$ ($\dw/\pi$)\\ \hline
$\fs / 2$ & $\pi \fs$     & $\pi$       & 1 \\ \hline
\end{tabular}
\end{table}

\begin{figure}[htb]
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/frequency_axes}		
	\caption{Versions of the Nyquist frequency in continuous and discrete-time.\label{fig:frequency_axes}}
\end{figure}

\tabl{nyquist_frequency} and \figl{frequency_axes} summarize the discussed information about versions of the Nyquist frequency in different domains, including its normalized version in {\matlab}. 
\figl{frequency_axes} is also useful to observe the relation between the frequencies $\aw$, $f$, $\dw$ and $f_N$.


%AKGUIDELINE need to explain that variance is power before using it in quantization
\section{An Introduction to Quantization}

Similar to sampling, quantization is very important, for example, because computers and other digital systems use a limited number $b$ of bits to represent numbers.
%For example, all machines that run Oracles's Java Standard Edition use the native types short, float and double for representing numbers with 16, 32 and 64 bits, respectively.
In order to convert an analog signal to be processed by a computer, it is therefore necessary to quantize its sampled values.

Quantization is used not only when ADCs are involved. Whenever it is necessary to perform an operation representing the
numbers with less bits than the current representation, this process can be modeled as quantization. For instance, an image with the pixels represented by 8 bits can be turned into a binary image with a quantizer that outputs one bit. Or an audio signal stored as a WAV file can be converted from 24 to 16 bits per sample.

\subsection{Quantization definitions}

A \emph{quantizer}\index{Quantization}\index{Quantizer} $Q$ maps input values from a set (eventually with an infinite number of elements) to a smaller set with a finite number of elements. Without loss of generality, one can assume in this text that the quantizer inputs are real numbers. Hence, the quantizer is described as a mapping $Q: \reals \rightarrow \setM$ from a real number $x[n] \in \reals$ to an element $x_q[n]$ of a finite set $\setM$ called \emph{codebook}\index{Codebook}. The quantization process can be represented pictorially by:
\[
x[n] \arrowedbox{Q} x_q[n] \in \setM.
\]
The cardinality of this set is $| \setM | = M$ and typically $M = 2^b$, where $b$ is the number of bits used to represent each output $x_q[n]$. The higher $b$, the more accurate the representation tends to be.

A generic quantizer can then be defined by the quantization levels specified in $\setM$ and the corresponding input range that is mapped to each quantization level. These ranges impose a \emph{partition} of the input space. 
Most practical quantizers impose a partition that corresponds to \emph{rounding}\index{Rounding} the input number to the nearest output level. An alternative to rounding is \emph{truncation}\index{Truncation} and, in general, the quantizer mapping is arbitrary.

The input/output relation of most quantizers can be depicted as a ``stairs'' graph, with a non-linear mapping describing the range of input values that are mapped into a given element of the output set $\setM$.
%(see, e.\,g., \figl{quantizerdelta1}). 

\bExample \textbf{Example of generic (non-uniform) quantizer}.
For instance, assuming the codebook is $\setM = \{-4,-1,0,3 \}$ and a quantizer that uses ``rounding'' to the nearest output level, the input/output mapping is given by \figl{nonuniform_quantization}.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/nonuniform_quantization}
\caption{Input/output mapping for the \emph{non-uniform} quantizer specified by $\setM = \{-4,-1,0,3 \}$.\label{fig:nonuniform_quantization}}
\end{figure}

\tabl{nonuniform_quantizer} lists the input ranges and corresponding quantizer output levels for the example of \figl{nonuniform_quantization}.

\begin{table}
\centering
\caption{Input/output mapping for the quantizer specified by $\setM = \{-4,-1,0,3 \}$ of \figl{nonuniform_quantization}.\label{tab:nonuniform_quantizer}}
\begin{tabular}{|c|c|}
\hline
Input range  & Output level \\ \hline
$]- \infty, -2.5[$ & $-4$ \\ \hline
$[-2.5, -0.5[$ & $-1$ \\ \hline
$[-0.5, 1.5[$ & $0$ \\ \hline
$[1.5, \infty[$ & $3$ \\ \hline
\end{tabular}
\end{table}

Given the adopted rounding, the input \emph{thresholds} (in this case, $-2.5$, $-0.5$ and $1.5$) indicate when the output changes, and are given by the average between two neighboring output levels. For instance, the threshold $-2.5 = (-1 - (-4))/2$ is the average between the outputs $-4$ and $-1$.
\eExample 

The adopted convention is that the input intervals are left-closed and right-open (e.\,g., $[-2.5, -0.5[$ in \tabl{nonuniform_quantizer}).

In most practical quantizers, the distance between consecutive output levels is the same and the quantizer is called \emph{uniform}. In contrast, the quantizer of \tabl{nonuniform_quantizer} is called \emph{non-uniform}.

The \emph{quantization error}\index{Quantization error} is
\[e_q[n] \triangleq  x[n]-x_q[n].\]

\bExample \textbf{Examples of rounding and truncation}.
Assume the grades of an exam need to be represented by integer numbers. Using rounding and truncation, an original grade of 8.7 is quantized to 9 and 8, respectively. In this case, the quantization error is $-0.3$ and $0.7$, respectively. Assuming the original grade can be any real number $x[n] \in [0, 10]$, 
rounding can generate quantization errors in the range $e_q[n] \in [-0.5, 0.5[$ while truncation generates errors $e_q[n] \in [0, 1]$.
\eExample

Unless otherwise stated, hereafter we will assume the quantizer implements rounding.


\subsection{Implementation of a generic quantizer}

A quantizer can be implemented as a list of if/else rules implementing a binary search.
The quantizer of \tabl{nonuniform_quantizer} can be implemented as in \codl{snip_signals_nonuniform_quantization}. 

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_nonuniform\_quantization}{snip_signals_nonuniform_quantization}

An alternative implementation of quantization is by performing a ``nearest-neighbor'' search: find the output level $x_q[n]$ minimum squared error between the input value $x[n]$ and all output levels (that play the role of ``neighbors'' of the input value). This is illustrated in \codl{snip_signals_nonuniform_quant2}.

%The Euclidean distance  to compute the squared error between the input $x$ and each quantization level ,  choosing the level that minimizes the error.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_nonuniform\_quant2}{snip_signals_nonuniform_quant2}

\subsection{Uniform quantization}

Unless otherwise stated, this text assumes \emph{uniform} quantizers with a \emph{quantization step} or \emph{step size} $\Delta$. %which indicates the variation (e.g., in volts) corresponding to a variation of one in the integer value.

In this case, all the steps in the quantizer ``stairs'' have both height and width equal to $\Delta$. 
In contrast, note that the steps in \figl{nonuniform_quantization} are not equal.
Uniform quantizers are very useful because they are simpler than the generic non-uniform quantizer. 

A uniform quantizer is defined by the number $b$ of bits and only two other numbers: $\Delta$ and $\hat X_{\textrm{min}}$, where $\hat X_{\textrm{min}}$ is the minimum value of the quantizer output $x_q[n] \in \setM$, where
\begin{equation}
\setM = \{\hat X_{\textrm{min}}, \hat X_{\textrm{min}}+\Delta,\hat X_{\textrm{min}}+2\Delta,\ldots,\hat X_{\textrm{min}}+(M-1)\Delta \}. 
\label{eq:uniform_quantizer_levels}
\end{equation}
For instance, assuming $b=2$ bits, $\hat X_{\textrm{min}} = -4$ and $\Delta=3$, the output levels are $\setM = \{-4, -1, 2, 5\}$.

The value of $\Delta$ is also called the least significant bit (LSB)\index{Least significant bit (LSB)} of the ADC, because it represents the value (e.\,g., in volts) that corresponds to a variation of a single bit. Later, this is also emphasized in \equl{deltaAsLSB} in the context of modeling the representation of real numbers in fixed-point as a quantization process.

\subsection{Granular and overload regions}

There are three regions of operation of a quantizer: the granular \index{Granular region of a quantizer} and two overload \index{Overload region of a quantizer} (or saturation\index{Saturation region of a quantizer}) regions. An overload region is reached when the input $x[n]$ falls outside the quantizer output \emph{dynamic range}. The granular is between the two overload regions. 

\figl{quantizerdelta1} depicts the input/output relation of a 3-bits quantizer with $\Delta=1$. 
In \figl{quantizerdelta1}, because $\Delta=1$, operation in the granular region corresponds to the rounding operation \ci{round} to the nearest integer. For example, \ci{round(2.4)=2} and \ci{round(2.7)=3}. When $\Delta \ne 1$, the quantization still corresponds to rounding to the nearest $\hat X \in \setM$, but $\hat X$ is not restricted to be an integer anymore.

\begin{figure}
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{Figures/quantizerdelta1}		
	\caption{Input/output relation of a 3-bits quantizer with $\Delta=1$.\label{fig:quantizerdelta1}}
\end{figure}

\figl{quantizerdelta05} depicts the input/output relation of a 3-bits quantizer with $\Delta=0.5$. Close inspection of Figures~\ref{fig:quantizerdelta1} and \ref{fig:quantizerdelta05} shows that the error $e_q[n]$ is in the range $[-\Delta/2, \Delta/2]$ within the granular region, but can grow indefinitely when the input falls in one of the overload regions.

\begin{figure}
	\centering
		%\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/quantizerdelta05_V2}		
		\includegraphics[width=\figwidth,keepaspectratio]{Figures/quantizerdelta05}		
	\caption{Input/output relation of a 3-bits quantizer with $\Delta=0.5$.\label{fig:quantizerdelta05}}
\end{figure}

%\subsection{Designing a quantizer}

\subsection{Design of uniform quantizers}

There are several strategies for designing a uniform quantizer. Some are discussed in the following paragraphs.

\subsubsection{Designing a uniform quantizer based on input's statistics}

Ideally, the chosen output levels in $\setM$ should match
the statistics of the input signal $x[n]$.
A reasonable strategy is to observe the \emph{histogram} (see \figl{simple_histogram}, for an histogram example) of the quantizer input and pick reasonable values to use in $\setM$.
%\equl{delta_typical_quantizer}.
% or \equl{delta_typical_quantizer_symmetric}. 
For example, if the input data follows a Gaussian distribution, the sample mean $\mu$ and variance $\sigma^2$ can be estimated and the dynamic range assumed to be $X_{\textrm{min}}=\mu - 3 \sigma$ and $X_{\textrm{max}}= \mu + 3 \sigma$ to have the quantizer covering approximately 99\% of the samples.

In case the input data has approximately a uniform distribution, the quantizer can be designed based on the dynamic range\index{Dynamic range} $[X_{\textrm{min}}, X_{\textrm{max}}]$ of its input $x[n]$, where $X_{\textrm{min}}$ and $X_{\textrm{max}}$ are the minimum and maximum amplitude values assumed by $x[n]$.

One should notice that \emph{outliers}\index{Outlier} (a sample numerically distant from the rest of the data, typically a noisy sample) can influence too much a design strictly based on $X_{\textrm{min}}$ and $X_{\textrm{max}}$. This suggests always checking the statistics of $x[n]$ via a histogram.


\subsubsection{Designing a uniform quantizer based on input's dynamic range}

Even if the input signal does not have a uniform distribution, it is sometimes convenient to adopt the suboptimal strategy of taking into account only the input dynamic range $[X_{\textrm{min}}, X_{\textrm{max}}]$ of $x[n]$. Among several possible strategies, a simple one is to choose $\hat X_{\textrm{min}} = X_{\textrm{min}}$ and
\begin{equation}
\Delta = \left| \frac{X_{\textrm{max}} - X_{\textrm{min}}}{M} \right|.
\label{eq:delta_simple_quantizer}
\end{equation}
In this case, the minimum quantizer output is $\hat X_{\textrm{min}} = X_{\textrm{min}}$, but the maximum value does not reach $X_{\textrm{max}}$ and is given by $\hat X_{\textrm{max}} = X_{\textrm{max}} - \Delta$. The reason is that, as indicated in \equl{uniform_quantizer_levels}, $\hat X_{\textrm{max}} = \hat X_{\textrm{min}}+(M-1)\Delta$. 

To design a uniform quantizer in which $\hat X_{\textrm{max}} = X_{\textrm{max}}$, one can adopt
\begin{equation}
\Delta = \left| \frac{X_{\textrm{max}} - X_{\textrm{min}}}{M-1} \right|.
\label{eq:delta_typical_quantizer}
\end{equation}
%which leads to $\hat X_{\textrm{max}} = X_{\textrm{max}}$.

\bExample \textbf{Design of a uniform quantizer}.
\label{ex:uniform_quantizer_design}
For example, assume the quantizer should have $b=2$ bits and the input $x[n]$ has a dynamic range given by $X_{\textrm{min}}=-1$~V and $X_{\textrm{max}}=3$~V. Using \equl{delta_simple_quantizer} 
leads to $\Delta = (3 - (-1)) / 4 = 1$~V and the quantizer output levels are $\setM = \{-1, 0, 1, 2\}$. Note that $\hat X_{\textrm{max}} = X_{\textrm{max}} - \Delta = 3 - 1  =2$.
Alternatively, 
\equl{delta_typical_quantizer} can be used to obtain $\Delta = (3 - (-1)) / (4 - 1) \approx 1.33$~V and the quantizer output levels would be $\setM = \{-1, 0.33, 1.66, 3\}$ with $\hat X_{\textrm{max}} = X_{\textrm{max}} = 3$.
\eExample

\bExample \textbf{Forcing the quantizer to have an output level representing ``zero''}.
One common requirement when designing a quantizer is to reserve one quantization level to represent zero (otherwise, it would output a non-zero value even with no input signal). The levels provided by \equl{delta_typical_quantizer} can be refined by counting the number $M_{\textrm{neg}}$ of levels representing negative numbers and adjusting $\hat X_{\textrm{min}}$ such that $\hat X_{\textrm{min}} + \Delta M_{\textrm{neg}} = 0$.  \codl{snip_signals_quantizer} illustrates the procedure.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_quantizer}{snip_signals_quantizer}
%\begin{lstlisting}
%Xmin=-1; Xmax=3; %adopted minimum and maximum values
%b=2; %number of bits of the quantizer
%M=2^b; %number of quantization levels
%delta=abs((Xmax-Xmin)/(M-1)); %quantization step
%QuantizerLevels=Xmin + (0:M-1)*delta %output values
%isZeroRepresented = find(QuantizerLevels==0); %is 0 there?
%if isempty(isZeroRepresented) %zero is not represented yet
%    Mneg=sum(QuantizerLevels<0); %number of negative
%    Xmin = -Mneg*delta; %update the minimum value
%    NewQuantizerLevels = Xmin + (0:M-1)*delta %new values
%end
%\end{lstlisting}

Considering again \exal{uniform_quantizer_design}, \codl{snip_signals_quantizer} would convert the original set $\setM = \{-1, 0.33, 1.66, 3\}$ into $\setM = \{-1.33, 0, 1.33, 2.67\}$, which has one level dedicated to represent zero.
%Notice the new value of $\hat X_{\textrm{max}}$ is $X_{\textrm{max}} - \Delta$ but one level is dedicated to represent zero.
\eExample

\bExample \textbf{Designing a quantizer for a bipolar input}.
\label{ex:bipolar_quantizer}

Assume here a bipolar signal $x[n]$, for instance with peak values $X_{\textrm{min}}=-5$ and $X_{\textrm{max}} = 5$). 
If it can be assumed that $X_{\textrm{max}} = |X_{\textrm{min}}|$, \equl{delta_typical_quantizer} simplifies to
\[
\Delta = \frac{2X_{\textrm{max}}}{M-1}.
\]
%\label{eq:delta_typical_quantizer_symmetric
%\end{equation}
Furthermore, one quantization level can be reserved to represent zero, while $M/2 = 2^{b-1}$ and $M/2-1 = 2^{b-1}-1$ levels represent negative and positive values, respectively. Most ADCs adopt this division of quantization levels when operating with bipolar inputs.
% starting from $-X_{\textrm{max}}$ and $M/2-1$ represent positive values with the maximum being $X_{\textrm{max}}-\Delta$. 
For example, several commercial 8-bits ADCs can output signed integers from $-128$ to 127, which corresponds to the integer
range $-2^{b-1} = -2^{8-1} = -128$ to $2^{b-1}-1 = 2^{7}-1 = 127$. These integer values can be multiplied by the quantization
step $\Delta$ in volts to convert the quantizer output into a value in volts.

\codl{quantizer} illustrates a {\matlab} function that implements a conventional quantizer with $\hat X_{\textrm{min}} = -2^{b-1} \Delta$ and saturation. The \ci{round} function is invoked to obtain the integer that represents the number of quantization levels as
\begin{equation}
x_i = \textrm{round} \left( \frac{x}{\Delta} \right),
\label{eq:quantization_bipolar}
\end{equation}
and later $x_q = x_i \times \Delta$.

\includecode{MatlabOctaveFunctions}{quantizer}

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_quantizer\_use}{snip_signals_quantizer_use}
%\begin{lstlisting}
%delta=0.5; %quantization step
%b=3; %number of bits
%x=[-5:.01:4]; %define input dynamic range
%[x_hat, passos] = ak_quantizer(x,delta,b); %quantize
%plot(x,x_hat), grid	%generate graph
%\end{lstlisting}
The commands in \codl{snip_signals_quantizer_use} can generate a quantizer input-output graph such as \figl{quantizerdelta05} using the function \ci{ak\_quantizer.m}.
\eExample

\ignore{
Here, the motivation for this sequence of operation it its mathematical convenience. 
In practice, for successive approximation ADC chips for example, the mappings are not exactly in the described order. In this case, the ADC chip tests whether or not $x_b$ is a good representation of the input $x$ by converting $x_b$ into $x_q = x_i \Delta$ and comparing $x_q$ with $x$.
%
Decoding is the inverse operation. It should be noted that, in some cases, the value of $\Delta$ is not important and it is assumed $\Delta = 1$. 
}

\subsection{Design of optimum non-uniform quantizers}

The optimum quantizer, which minimizes the quantization error, must be designed strictly according to the input signal statistics (more specifically, the probability density function of $x[n]$). The uniform quantizer is the optimum only when the input distribution is uniform. For any other distribution, the Lloyd's algorithm\footnote{Lloyd's paper is listed at \akurl{http://en.wikipedia.org/wiki/Lloyd's_algorithm}{1qua}.} can be used to find the optimum set $\setM$ of output levels and the quantizer will be non-uniform. For instance, if the input is Gaussian, the optimum quantizer allocates a larger number of quantization levels around the Gaussian mean than in regions far from the mean.

\bExample \textbf{Optimum quantizer for a Gaussian input}.
\label{ex:gaussian_quantization}
This example illustrates the design of an optimum quantizer when the input $x[n]$ has a Gaussian distribution with variance
$\sigma^2=10$. It is assumed that the number $b$ of bits is three such that the quantizer has $M=2^3=8$ output levels.
The following code illustrates the generation of $x[n]$ and quantizer designer using Lloyd's algorithm.
\begin{lstlisting}
clf
N=1000000; %number of random samples
b=3; %number of bits
variance = 10;
x=sqrt(variance)*randn(1,N); %Gaussian samples
M=2^b;
[partition,codebook] = lloyds(x,M); %design the quantizer
\end{lstlisting}

The obtained results are listed in \tabl{gaussian_quantizer} and \figl{gaussian_quantization}.

\begin{table}
\centering
\caption{Input/output mapping for a generic quantizer designed for a Gaussian input with variance $\sigma^2=10$.\label{tab:gaussian_quantizer}}
\begin{tabular}{|c|c|}
\hline
Input range  & Output level \\ \hline
$]- \infty, -5.5[$ & $-6.8$ \\ \hline
$[-5.5, -3.3[$ & $-4.2$ \\ \hline
$[-3.3, -1.6[$ & $-2.4$ \\ \hline
$[-1.6, 0[$ & $-0.8$ \\ \hline
$[0, 1.6[$ & $0.8$ \\ \hline
$[1.6, 3.3[$ & $2.4$ \\ \hline
$[3.3, 5.5[$ & $4.2$ \\ \hline
$[5.5, \infty[$ & $6.8$ \\ \hline
\end{tabular}
\end{table}

Note that the two intervals close to 0 have length 1.6 while the interval $[3.3, 5.5[$ has a longer length $5.5-3.3=2.2$. In general, the
intervals corresponding to regions with less probability are longer such that more output levels can be concentrated in
regions of high probability.
This is illustrated in \figl{gaussian_quantization}.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/gaussian_quantization}
\caption{Theoretical and estimated Gaussian probability density functions with thresholds represented by dashed lines and the output levels indicated with circles in the abscissa.\label{fig:gaussian_quantization}}
\end{figure}

Because Lloyd's algorithm has a set of samples as input, this number has to be large enough such that the
input distribution is properly represented. In this example, \ci{N=1000000} random samples were used.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/gauss_quantization}
\caption{Input/output mapping for the quantizer designed with a Gaussian input and outputs given by $\setM=[-6.8, -4.2, -2.4, -0.8, 0.8, 2.4, 4.2, 6.8]$.\label{fig:gauss_quantization}}
\end{figure}

\figl{gaussian_quantization} depicts the mapping for the designed non-uniform quantizer.
\eExample

\bExample \textbf{Optimum quantizer for a mixture of two Gaussians}.
This example is similar to \exal{gaussian_quantization}, but the input distribution here is a mixture of two Gaussians instead of
a single Gaussian. More specifically, the input $x[n]$ has a probability density function given by
\begin{equation}
f(x) = 0.8 \calN(-4, 0.5) + 0.2 \calN(3, 4),
\label{eq:mixture_of_gaussians}
\end{equation}
where the notation $\calN(\mu,\sigma^2)$ describes a Gaussian with average $\mu$ and variance $\sigma^2$.

\begin{figure}[!htb]
  \begin{center}
    \subfigure[Thresholds (dashed lines) and output levels (circles).]{\label{fig:gaussian_mixture_quantization}\includegraphics[width=6.5cm]{Figures/gaussian_mixture_quantization}}
    \subfigure[Input/output mapping.]{\label{fig:gaussian_mixture_stairs}\includegraphics[width=6.5cm]{Figures/gaussian_mixture_stairs}}
  \end{center}
  \caption{Results for the quantizer designed with the mixture of \equl{mixture_of_gaussians} as input.\label{fig:mixture_quantization}}
\end{figure}

\figl{mixture_quantization} illustrates results obtained with the Lloyd's algorithm (see script \ci{figs\_signals\_quantizer.m}). 
\figl{mixture_quantization} indicates that three quantizer levels are located closer to the Gaussian with average 
$\mu=-4$, while the other five levels are dedicated to the Gaussian with $\mu=3$.
Because the input distribution is the mixture of Gaussians of \equl{mixture_of_gaussians}, quantization steps are smaller around the input $x=-4$, and larger around $x=3$. The largest step height occurs in a region close to $x=-1$ due to the relatively low probability in this region.
\eExample

\subsection{Quantization stages: classification and decoding}

Recall that the complete A/D process to obtain a digital signal $x_q[n]$ to represent an analog signal $x(t)$ can be modeled as:
\[
x(t) \arrowedbox{SAMPLING} x_s(t) \arrowedbox{S/D} x[n] \arrowedbox{Q} x_q[n].
\]
Note the convention adopted for the quantizer output: $x_q[n]$ is represented by \emph{real} numbers, as in \codl{quantizer}.
This is convenient because one wants to perform signal processing with $x_q[n]$, for instance, calculating the quantization error with $e_q[n]=x[n]-x_q[n]$.
However, in digital hardware, a \emph{binary} representation $x_b[n]$ better describes the actual output of an ADC. 
Hence, a more faithful representation of the job actually done by the ADC should be denoted as
\[
x(t) \arrowedbox{SAMPLING} x_s(t) \arrowedbox{S/D} x[n] \rightarrow\boxed{\tilde{Q}_c}\rightarrow x_b[n],
\]
where $\tilde{Q}_c$ generates binary codewords, not real numbers. 
Hence, to
enable better representing an ADC output and related variables,
the quantization is broken into the two stages depicted in \figl{quantizers}: \emph{classification} and \emph{decoding}, denoted as $\tilde Q_c$ and $\tilde Q_d$, respectively.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/quantization}
	\caption{A quantizer $Q$ is composed by the classification and decoding stages, denoted as $\tilde Q_c$ and $\tilde Q_d$, respectively.\label{fig:quantizers}}
\end{figure}

The output $\tilde Q_c \{ x[n] \}$
of the classification stage is represented by a binary codeword $x_b[n]$ with $b$ bits.
The \emph{decoding}\index{Decoding (quantization)} corresponds to obtaining a
real number using $x_q[n] = \tilde Q_d \{ x_b[n] \}$.
The next section discusses the decoding stage.
%As shown in \figl{quantizers}, 
%the two quantization stages allow to explicitly denote the binary representation $x_b[n]$. G
%Given that $x_b[n]$ represents an element in quantizer output set $\setM$, it can be 
%easily represented in any other numeral system, such as hexadecimal or decimal. Conveniently, 


\subsection{Binary numbering schemes for quantization decoding}

%As mentioned, in practice, the ADC outputs binary codewords $x_b[n]$ that represent the respective outputs $x_q[n]$.
The decoding mapping $\tilde Q_d \{ \cdot \}$ in \figl{quantizers} can be arbitrary, but in most cases simple and well-known binary numbering schemes are adopted.
When assuming a quantizer that uses bipolar inputs (see \equl{quantization_bipolar}), decoding often relies on a
binary numbering scheme that is able of representing negative numbers. Otherwise, an \emph{unsigned} numbering scheme is adopted.
In both cases, the numbering scheme allows to convert $x_b[n]$ into an integer $x_i[n]$, represented in the decimal system.

As the final decoding step, the integer $x_i[n]$ is multiplied by the quantization step $\Delta$, as indicated in \exal{bipolar_quantizer} and repeated below:
\begin{equation}
x_q[n] = \tilde Q_d \{ x_b[n] \} = x_i[n] \times \Delta.
\label{eq:decoding}
\end{equation}
%The following examples indicate how \equl{decoding} is useful.

There are various binary numeral systems that differ specially on how negative numbers are represented. Some options are briefly discussed in the sequel.\footnote{Binary numeral systems are also discussed at \akurl{https://www.intel.com/programmable/technical-pdfs/653864.pdf}{1bin}.} 
%Concentrating now on the first stage of \blol{quantizerDecoding}, where the actual decoding occurs, 
\tabl{outputcoding} provides an example of practical binary numbering schemes, also called \emph{codes}. The first column indicates values for $x_b$ while the others indicate $x_i$ values. For example, note in \tabl{outputcoding} that $x_b[n]=100$ corresponds to 7 in Gray code and $-4$ in two's complement.

%TC:ignore
\begin{table}
\centering
\caption{Examples of binary numbering schemes used as output codes in A/D conversion for $b=3$ bits.\label{tab:outputcoding}}
\begin{tabularx}{\textwidth}{*{6}{>{\centering$}X<{$}}}\toprule
%\multirow{2}{*}
\multicolumn{1}{X}{Binary number} & \multicolumn{1}{X}{Unsigned integer} & \multicolumn{1}{X}{Two's complement} & \multicolumn{1}{X}{Offset} & \multicolumn{1}{X}{Sign and magnitude} & \multicolumn{1}{X}{Gray code} \\\midrule 
	000 & 0 & 0 & -4 & 0 & 0\\
	001 & 1 & 1 & -3 & 1 & 1\\ 
	010 & 2 & 2 & -2 & 2 & 3\\ 
	011 & 3 & 3 & -1 & 3 & 2\\ 
	100 & 4 & -4 & 0 & \text{``$-0$''} & 7\\ 
	101 & 5 & -3 & 1 & -1 & 6\\ 
	110 & 6 & -2 & 2 & -2 & 4\\ 
	111 & 7 & -1 & 3 & -3 & 5 \\ \bottomrule
\end{tabularx}
\end{table}
%TC:endignore

%In practice, the hardware (A/D and glue logic) outputs, for example, $x_b=100$ (A/D of $b=3$ bits) and the mapping to $x_q$ . 
%For convenience, the output coding procedure is assumed to will be split into two steps: a) finding the integer number $x_i = x_q / \Delta$ that represents $x_q$ and b) representing $x_i$ by a binary codeword using, for example, two-complement or unsigned numbers with an offset. 

Three of the codes exemplified in \tabl{outputcoding} will be discussed in more details because they are very important from a practical point of view: \emph{unsigned integer}, \emph{offset}\index{Offset binary output coding} and \emph{two's complement} output coding.

The unsigned representation, sometimes called \emph{standard-positive binary coding}, represents numbers in the range $[0,2^b-1]$. It is simple and convenient for arithmetic operations such as addition and subtraction. However, it cannot represent negative values. An alternative is the offset representation, which is capable of representing numbers in the range $[-2^{b-1},2^{b-1}-1]$, as used in \codl{quantizer}. This binary numbering scheme can be obtained by subtracting the ``offset'' $2^{b-1}$ from the unsigned representation. The two's complement is a popular alternative that is also capable of representing numbers in the range $[-2^{b-1},2^{b-1}-1]$ and allows for easier arithmetics than the offset representation, as studied in textbooks of digital electronics. Note that the offset representation can be obtained by inverting the most significant bit (MSB) of the two's complement. For this reason it is sometimes called \emph{offset two's complement}.

\bExample \textbf{Negative numbers in two's complement}.
To represent a negative number in two's complement, one can first represent the absolute value in standard-positive binary coding, then invert all bits and sum 1 to the result. For example, assuming $x_i[n]=-81$ should be represented with $b=8$ bits, the first step would lead to $0101 0001$, which corresponds to $81$. Inverting these bits leads to $1010 1110$ and summing to 1 gives the final result $x_b[n]=1010 1111$ corresponding to $-81$.

Note that two's complement requires \emph{sign extension}\index{Sign extension} (see, e.\,g., \akurl{http://en.wikipedia.org/wiki/Two\%27s_complement}{1two}) by extending the MSB. For example, if $x_i[n]=-81$ were to be represented with $b=16$ bits, the 
first step would lead to $0000 0000$ $0101 0001$, corresponding to $81$. After inverting the bits to obtain $1111 1111$ $1010 1110$ and summing 1, the correct codeword to represent $-81$ would be $x_b[n]=1111 1111$ $1010 1111$ (instead of $0000 0000$ $1010 1111$). Comparing the representation of $-81$ in 16 and 8 bits, one can see that the MSB of the 8-bits representation was extended (repeated) in the first byte of the 16-bits version.
\eExample

Distinguishing $x_i[n]$ and $x_b[n]$ may seem an abuse of notation because they are the same number described in decimal and binary numeral systems, respectively. However, the notation is useful because there are many distinct ways of mapping $x_b[n]$ to $x_i[n]$ and vice-versa.

\tabl{outputcoding} covers the binary numbering schemes adopted in most ADCs. These schemes can be organized within a class of number representation called \emph{fixed-point}. But the representation of real numbers within computers typically use a more flexible representation belonging to another class called \emph{floating-point}. The fixed and floating-point representations
are discussed in Appendix~\ref{app:fixed_floating_point}.

\subsection{Quantization examples}

Some quantization examples are provided in the following paragraphs.

\bExample \textbf{ECG quantization}.
An ECG signal was obtained with a digital Holter that used a uniform quantizer with 8 bits represented in two's complement and quantization step $\Delta=0.15$~mV. Hence, when represented as decimal numbers, the quantized outputs $x_i[n]$ vary from $-128$ to 127.
Assuming three quantizer input samples as $x[0]=2.52$, $x[1]=-14.8$ and $x[2]=7.3$, all in mV, the
quantized values are given by $x_i[n]= Q \{ x[n] \} = \textrm{round}(x[n]/\Delta)$, which lead to $x_i[0]=17$, $x_i[1]=-99$ and $x_i[2]=49$.
The corresponding binary codewords $x_b[n]= {\tilde Q}_c \{ x[n] \}$ are $x_b[0]=0001~0001$, $x_b[1]=1001~1101$ and $x_b[2]=0011~0001$.
Using \equl{decoding}, the quantizer outputs are $x_q[0]=2.55$, $x_q[1]=-14.85$ and $x_q[2]=7.35$ mV, and the quantization error $x[n]-x_q[n]$ is $e_q[0]=-0.03$, $e_q[1]=0.05$ and $e_q[2]=-0.05$ mV.
\eExample 

\bExample \textbf{Quantization using 16 bits}.
\label{ex:quantization2}
Assume the quantizer of an ADC with $\Delta=0.5$ and $b=16$ bits is used to quantize the discrete-time signal $x[n]$ at time $n_0$.  The input amplitude is $x[n_0]=12804.5241$ and the quantizer output is $Q\{x[n_0]\}=12804.5$. It is also assumed that the classification quantizer stage $\tilde Q_c$ outputs $x_b[n_0]=\tilde Q_c\{x[n_0]\}=0110~0100~0000~1001$ in two's complement, which in decimal is $x_i[n_0]=25609$. The decoding then generates
\[
x_q[n_0] = x_i[n_0] \times \Delta = 25609 \times 0.5 = 12804.5,
\] 
using \equl{decoding}.
\eExample


%\subsection{Number representation in quantization processes}
%The two quantizers $Q$ and $\tilde Q$ can be related by
%\[
% x[n] \arrowedbox{$Q$} x_q[n] \equiv x[n] \arrowedbox{$\tilde Q_b$} x_b[n] \arrowedbox{DECODING} x_q[n],
%\]
%where an additional \emph{decoding} step converts the binary codeword into $x_q[n]$. 

\bExample \textbf{The quantization step may be eventually ignored}.
As mentioned, the \emph{quantization step} or \emph{step size} $\Delta$ indicates the variation (e.g., in volts) between two neighbor output values of a uniform quantizer.

%is the second stage of decoding:
%In summary, when modeling a full quantizer $Q$ for the purpose of a theoretical study, for example, the quantizer $\tilde{Q}$ of a practical A/D chip should be followed by a decoding procedure. 
%With the mentioned assumption and the normalization by $\Delta$ in order to implement the model of , as indicated below:
%x[n] \arrowedbox{$\tilde{Q}$} 
%\begin{equation}
%x_b[n] \arrowedbox{DECODING stage 1} x_i[n] \arrowedbox{$\times~\Delta$} x_q[n].
%\label{eq:quantizerDecoding}
%\end{equation}
In many stages of a digital signal processing pipeline of algorithms, the value of $\Delta$ is not important. For example, speech recognition is often performed on samples $x_i[n]$ represented as integers instead of $x_q[n]$. In such cases, it is
convenient to implicitly assume $\Delta=1$. 

For example, considering \exal{quantization2} and an original sample value in analog domain $x(t_0)=12804.5$~volts, this sample may be represented and processed in digital domain as $x_i[n_0]= 25609$. This corresponds to an implicit assumption that $\Delta=1$ and $x_q[n_0] = x_i[n_0]$, instead of minding to adopt the actual $\Delta = 0.5$ of \exal{quantization2}.

If at some point of the DSP pipeline, it becomes necessary to relate $x(t)$ and $x_q[n]$, such as when the average power in both domains should match, the scaling factor $\Delta$ is then incorporated in the calculation. But many signal processing stages can use $x_i[n]$ instead of $x_q[n]$.
\eExample 

\bExample \textbf{The step size of the ADC quantizer is often unavailable}.
When the quantization is performed in the digital-domain, such as when representing a real number in fixed point, the value of the step $\Delta$ is readily available. However, the value of the step $\Delta_{\textrm{AD}}$ in the quantization process that takes place within an ADC chip is often not available.

For example, as explained in Application~\ref{app:recording_sound}, the {\matlab} function's \ci{readwav} returns $x_i[n]$ (when the option \ci{'r'} is used in Matlab) or a scaled version of $x_i[n]$, where the scaling factor is not $\Delta_{\textrm{AD}}$ but $2^{b-1}$. The value of $\Delta_{\textrm{AD}}$ that the sound board's ADC used to quantize the analog values obtained via a microphone for example, is often not registered in audio files (e.\,g., `wav'), which store only the samples $x_b[n]$ and other information such as $\fs$. Application~\ref{app:sound_board_quantizer} discusses more about $\Delta_{\textrm{AD}}$ in commercial sound boards.
%In other cases $x_q[n]$ is composed by $x_q$ (eventually real numbers).

File formats developed for other applications may include the value of $\Delta_{\textrm{AD}}$ in their headers. For example, Application~\ref{app:ecgcoding} uses an ECG file that stores the information that
$\Delta_{\textrm{AD}} = 1/400$~mV. The analog signal power can then be estimated as 3.07~mW by normalizing the digital samples with $x_i[n] \times \Delta_{\textrm{AD}}$.

If the correct value of $\Delta_{\textrm{AD}}$ is not informed, it is still possible to relate the analog and digital signals in case there is information about the analog signal dynamic range.

%Binary codewords and integer numbers $x_i$ are related by a code, as the ones listed in~\tabl{outputcoding}. A code is assumed to exist here, and the task of this subsection is different: to find how pairs of values $x_i$ and $x_q$ can be mapped via a single scaling factor $\Delta$. It is a simple matter of relating two ``scales'', as in Celsius and Fahrenheit in thermometry, and an example helps the explanation.

\begin{figure}
	\centering
		\includegraphics[width=4cm,keepaspectratio]{FiguresNonScript/conversion_ad}
		%\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/conversion_ad}
	\caption[Example of conversion using proportion when the dynamic ranges of both analog and digital signal are available]{Example of conversion using proportion when the dynamic ranges of both analog and digital signal are available. In this case the \emph{step size} is found as $\Delta=2.5$ V and the value $A$ (or $x_q$) is related to the integer representation $D$ (or $x_i$) as $x_q=\Delta x_i-5$.\label{fig:conversion_ad}}
\end{figure}

As an example of using proportion to relate $x_q$ and $x_i$, assume a 2-bits ADC that adopts unsigned integers as indicated in~\tabl{outputcoding}. The task is to relate the integer codewords $x_i[n] \in \{0,1,2, 3\}$ (corresponding to 00, 01, 10 and 11, respectively) to $x_q[n]$ values, which can represent volts, amperes, etc. It is assumed that the original $x(t)$ is given in volts and continuously varies in the range $[-5, 2.5]$.
The relation between $x_i$ and $x_q$ can be found by proportion as:
\[
\frac{x_i-0}{3-0} = \frac{x_q-(-5)}{2.5-(-5)},
\]
which gives
\[
x_q  = \left(\frac{7.5}{3}\right) x_i -5 = 2.5 x_i - 5
\]
and, consequently, an estimate of $\Delta=2.5$~V. In this case, the ADC associates the code 00 to $-5$~V, 01 to $-2.5$~V, 10 to $0$~V and 11 to $2.5$~V, as illustrated in \figl{conversion_ad}.
\eExample 

%and the next subsection discusses the quantizer $Q$ more formally.


%AK_GUIDELINE need to define before quantization is studied
\section{Correlation: Finding Trends}
\label{sec:correlation}

It is often useful to infer whether or not random variables are related to each other via \emph{correlation}\index{Correlation}. For example, height and weight are quantities that present positive correlation for the human population. However, happiness and height are uncorrelated. There are measures other than correlation to detect similarities among variables, such as the \emph{mutual information}, but correlation is simple and yet very useful. Correlation is called a second-order statistics because it considers only pairwise or quadratic dependence. It does not detect non-linear relations nor causation, that is, correlation cannot deduce cause-and-effect relationships. But correlation helps to indicate \emph{trends} such as one quantity increasing when another one decreases.

The correlation $C$ for two complex-valued random variables $\rvx$ and $\rvy$ is defined as 
\[
C = \textrm{cor}(\rvx,\rvy) =  \ev[\rvx \rvy^*],
\]
where $*$ denotes the complex conjugate.
The \emph{correlation coefficient}\index{Correlation coefficient}, which is a normalized version of the covariance (not correlation), is defined as
\[\rho = \frac{\textrm{cov}(\rvx,\rvy)}{\sigma_x \sigma_y},\]
where the \emph{covariance}\index{Covariance} is given by
%vx vy* - vx my - vy*mx + mx my*
\begin{equation}
\textrm{cov}(\rvx,\rvy) = \ev[ (\rvx - \mu_x) (\rvy - \mu_y)^*].
\label{eq:covarianceBetweenRVs}
\end{equation} 

Note that, when one wants to fully identify the statistical moments of two complex random variables, it is convenient to separately identify the interactions among their real and imaginary parts and organize the partial results of \equl{covarianceBetweenRVs} as a $2 \times 2$ matrix. 
\ifml
Real-valued random variables are assumed hereafter.
\else
Complex-valued variables are briefly discussed in Appendix~\ref{sec:properCircularRV} and real-valued random variables are assumed hereafter.
\fi
When the random variables are real-valued, \equl{covarianceBetweenRVs}
simplifies to
\[
\textrm{cov}(\rvx,\rvy) = \ev[\rvx \rvy] - \mu_x \mu_y.
\]

Two real-valued random variables are called \emph{uncorrelated}\index{Uncorrelated random variables} if, and only if, $\textrm{cov}(\rvx,\rvy) = 0$, which is equivalent to
\begin{equation}
\ev[\rvx \rvy] = \mu_x \mu_y.
\label{eq:uncorrelatedness}
\end{equation}

As mentioned, $\rho$ is calculated by extracting the means $\ev[\rvx]=\mu_x$ and $\ev[\rvy]=\mu_y$, i.\,e., using $\textrm{cov}(\rvx,\rvy)$ instead of $\textrm{cor}(\rvx,\rvy)$, and is restricted to $-1 \le \rho \le 1$ when the random variables are real-valued.
Application~\ref{app:correlationBeautyCream} gives an example of using correlation to perform a simple data mining.

\subsection{Autocorrelation function}
\label{sec:autocorrelationFunction}

Autocorrelation functions are extension of the correlation concept to signals. \tabl{acf_definitions} summarizes the definitions that will be explored in this text.

%TC:ignore
\begin{table}
 \centering
 \caption{Autocorrelation functions and their respective equation numbers.\label{tab:acf_definitions}}
 \begin{tabularx}{\textwidth}{lc}
 \toprule
Type of process or signal & Equation \\ \midrule
General stochastic process & (\ref{eq:autocorrelation}) \\ 
Wide-sense stationary stochastic process & (\ref{eq:wss_autocorrelation}) \\ 
Deterministic continuous-time energy signal $x(t)$ & (\ref{eq:autocorrelation_energy}) \\ 
Deterministic continuous-time power signal $x(t)$ & (\ref{eq:autocorrelation_power}) \\ 
Deterministic discrete-time energy signal $x[n]$ (unbiased estimate) & (\ref{eq:discrete_time_unbiased_acf}) \\ 
Deterministic discrete-time power signal $x[n]$ & (\ref{eq:discrete_time_power_acf}) \\ \bottomrule
\end{tabularx}
\end{table}
%TC:endignore

The existence of distinct autocorrelation definitions for power and energy signals 
illustrate the importance of distinguishing them, as discussed in Section~\ref{sec:powerEnergySignals}.

\subsubsection{{\akadvanced} Definitions of autocorrelation for random signals}

%There are correlation functions defined for random signals generated by 
An \emph{autocorrelation function} (ACF) defined for stochastic processes (see Appendix~\ref{app:stochasticprocesses}) is
\begin{equation}
R_{X}(s,t) = \ev[\calX(s) \calX^*(t)],
	\label{eq:autocorrelation}
\end{equation}
which corresponds to the correlation between random variables $\calX(s)$ and $\calX(t)$ at two different points $s$ and $t$ in time of the same random process where $\calX^*(t)$ is the complex conjugate of $\calX(t)$.
The purpose of the ACF is to determine the strength of relationship between amplitudes of the signal occurring at two different time instants.

For wide-sense stationary (WSS) processes, the ACF is given by
\begin{equation}
R_{X}(\tau) = \ev[\calX(t+\tau) \calX^*(t)],
\label{eq:wss_autocorrelation}
\end{equation}
where the time difference $\tau=s-t$ is called the lag time. 
%For random variables $\calX(s)$ and $\calX(t)$ at different time instants $s$ and $t$ of some random process $\calX$, the correlation function is

%Look at http://cnx.org/content/m10676/latest/ for more information.
%\subsubsection{Different definitions of autocorrelation}

%The definition of \equl{autocorrelation} is the most common in signal processing, but there are others. Choosing one depends on the application and the model adopted for the signals. In statistics, for example,\footnote{See, e.g., \akurl{http://en.wikipedia.org/wiki/Autocorrelation}{1aut}.} the definition of 
%\begin{equation}
%R_{X}(s,t) = \frac {\ev[(\calX(s)-\mu_s) (\calX(t)-\mu_t)]} {\sigma_s \sigma_t}
%\end{equation}
%incorporates the subtraction of means and normalization by the standard deviations, and also
%assumes a real-valued signal.

The random process formalism is very powerful and allows for representing complicated processes. But in many practical cases only one realization $x(t)$ (or $x[n]$) of the random process $\calX(t)$ is available. The alternative to deal with the signal without departing from the random process formalism is to assume that $\calX(t)$ is \emph{ergodic}\index{Ergodicity}. In this case, the statistical (or \emph{ensemble}\index{Ensemble average}) average represented by $\ev[ \cdot]$ is substituted by averages over time. 

The definitions of autocorrelation given by \equl{autocorrelation} and \equl{wss_autocorrelation} are the most common in signal processing, but others are adopted in fields such as statistics. Choosing one depends on the application and the model adopted for the signals.

%Look at http://cnx.org/content/m10676/latest/ for more information.

When dealing with a simple signal, one can naturally adopt one of the definition of autocorrelation for deterministic signals discussed in the sequel.

\subsubsection{Definitions of autocorrelation for deterministic signals}

%In this case, the realization $x(t)$ is assumed to represent the statistics of $\calX(t)$ and \equl{autocorrelation_power} (that is, the expression used for non-random power signals) can be used instead of the expression \equl{autocorrelation} for general random processes.

A definition of ACF tailored to a deterministic (non-random) energy signal $x(t)$, which does not use expected values as \equl{autocorrelation}, is
\begin{equation}
R_{X}(\tau)  = \int_{-\infty}^{\infty} {x(t+\tau)x^*(t)} \textrm{d}t = \int_{-\infty}^{\infty} {x^*(t-\tau)x(t)} \textrm{d}t,
\label{eq:autocorrelation_energy}
\end{equation}
such that 
\begin{equation}
R_X(0)= \int_{-\infty}^{\infty} |x(t)|^2 \textrm{d}t = E,
\label{eq:autocorrelationOriginIsEnergy}
\end{equation}
where $E$ is the signal energy.
It should be noted that this definition of autocorrelation cannot be used for power signals. Power signals have infinite energy and using \equl{autocorrelation_energy} for $\tau=0$ leads to $R_X(0)=\infty$ in the case of power signals, which is uninformative.
%Study the subject and find out why.

If $x(t)$ is a deterministic power signal, an useful ACF definition is
\begin{equation}
R_{X}(\tau)  = \lim_{T \rightarrow \infty} \frac 1 {2T} \int_{-T}^{T} {x(t+\tau)x^*(t)} \textrm{d}t,
\label{eq:autocorrelation_power}
\end{equation}
such that 
\begin{equation}
R_X(0)= \lim_{T \rightarrow \infty} \frac 1 {2T} \int_{-T}^{T} |x(t)|^2 \textrm{d}t = \calP,
\label{eq:autocorrelationOriginIsPower}
\end{equation}
where $\calP$ is the signal average power. \equl{autocorrelation_power} can also be used when $x(t)$ is a realization of an ergodic stochastic process.

%In this case, $R_{X}(\tau)$ for $\tau=0$ corresponds to the average power of $x(t)$. In contrast, $R_X(0)$ corresponds to the signal energy when using \equl{autocorrelation_energy}.

Similarly, there are distinct definitions of autocorrelation for deterministic discrete-time signals. For example, when $x[n]$ is a finite-duration real signal with $N$ samples, the (unscaled or not normalized) autocorrelation can be written as
\begin{equation}
\hat R_X[i] = \sum_{n=i}^{N-1} x[n] x[n-i],
\label{eq:xcorrFiniteDuration}
\end{equation}
which has a minus in $n-i$ that does not lead to reflecting the signal.\footnote{This operation should not be confused with convolution, which is discussed in Chapter~\ref{ch:systems}.}

As an alternative to use expressions more similar to the ones for signals with infinite duration, the (unscaled or not normalized) autocorrelation can be expressed as
\[
R_X[i] = \sum_n x[n+i] x^*[n],~i=-(N-1),\ldots,-1,0,1,\ldots,N-1
\]
and computed assuming a zero value for $x[n]$ when its index is out of range. This corresponds to assuming the signal is extended with enough zeros (\emph{zero-padding}\index{Zero-padding}) to the right and to the left. For example, assuming $x[n]=\delta[n]+2\delta[n-1]+3\delta[n-2]$, which can be represented by the vector $[1,2,3]$, its autocorrelation would be $[3,8,14,8,3]$, for the lags $i=-2,-1,0,1,2$, respectively.

%TC:ignore
\begin{table}
 \centering
 \caption{Example of autocorrelation for a real signal $[1,2,3]$ ($n=0,1,2$). \label{tab:autocorrelation}}
 \begin{tabularx}{\textwidth}{rXr}
 \toprule
 lag $i$ & valid products $x^*[n]x[n+i]$ & $R_X[i]$ \\\midrule
  $-2$ & $x^*[2] x[0]$ &  3 \\
 $-1$ & $x^*[1] x[0] + x^*[2] x[1] $ & 8 \\
 $0$ & $x^*[0] x[0] + x^*[1] x[1] + x^*[2] x[2] $ & 14 \\
 $1$ & $x^*[0] x[1] + x^*[1] x[2] $ & 8 \\
 $2$ & $x^*[0] x[2]$ &  3 \\\bottomrule
\end{tabularx}
\end{table}
%TC:endignore

Note in \tabl{autocorrelation} that the number of products decreases as $|i|$ increases. More specifically, when computing $R_X[i]$ there are only $N-|i|$ ``valid'' products. To cope with that, the normalized (and statistically unbiased) definition is
\begin{equation}
R_X[i] = \frac 1 {N-|i|} \sum_n x[n+i] x^*[n],~~~ i=-(N-1),\ldots,N-1.
\label{eq:discrete_time_unbiased_acf}
\end{equation}
In {\matlab}, the unbiased estimate for the signal $[1,2,3]$ can be obtained with:
\begin{lstlisting}
x=[1,2,3];xcorr(x,'unbiased')
\end{lstlisting}
which outputs \co{[3,4,4.67,4,3]}. The unscaled estimate of \tabl{autocorrelation} can be obtained with \ci{xcorr(x,'none')} or simply \ci{xcorr(x)} because it is the default.

\begin{table}
 \centering
 \caption{Example of calculating the unscaled autocorrelation for a complex-valued signal $[1+j,2,3]$ ($n=0,1,2$), where $j=\sqrt{-1}$. \label{tab:complex_autocorrelation}}
 \begin{tabularx}{\textwidth}{rXr}
 \toprule
 lag $i$ & valid products $x^*[n]x[n+i]$ & $R_X[i]$ \\ \midrule
 $-2$ & $x^*[2] x[0]$ &  3+3j \\
 $-1$ & $x^*[1] x[0] + x^*[2] x[1] $ & 8+2j \\
 $0$ & $x^*[0] x[0] + x^*[1] x[1] + x^*[2] x[2] $ & 15 \\
 $1$ & $x^*[0] x[1] + x^*[1] x[2] $ & 8-2j \\
 $2$ & $x^*[0] x[2]$ &  3-3j \\\bottomrule
\end{tabularx}
\end{table}

Another observation of interest is that for real signals, $R_X(\tau) = R_X(-\tau)$. In general, for complex-valued signals, $R_X(\tau) = R_X^*(-\tau)$, which is called \emph{Hermitian symmetry}\index{Hermitian symmetry}. \tabl{complex_autocorrelation} provides an example. It can also be noted that, for a given lag $i$, the subtraction of the indexes of all parcels in valid products is $(n+i)-n=i$.

\bExample \textbf{Software implementation of autocorrelation}.
The definitions of $R_X[i]$ used a generic summation $\sum_n$ over $n$. To be more concrete, an example of {\matlab} code to calculate the unscaled autocorrelation $R_X[i]$ is given in \codl{snip_signals_unscaled_autocorrelation}. It can be seen that the property $R_X(\tau) = R_X^*(-\tau)$ is used to obtain the autocorrelation values for negative $\tau$.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_unscaled\_autocorrelation}{snip_signals_unscaled_autocorrelation}
%\begin{lstlisting}
%%Calculate the unscaled autocorrelation R(i) of x
%x=[1+j 2 3] %define some vector x to test the code
%N=length(x);
%R=zeros(1,N); %space for i=0,1,...N-1
%R(1)=sum(abs(x).^2); %R(0) is the energy
%for i=1:N-1 %for each positive lag
%    temp = 0; %partial value of R
%    for n=1:N-i %vary n over valid products
%        temp = temp + x(n+i)*conj(x(n));
%    end
%    R(i+1)=temp; %store final value of R
%end
%R = [conj(fliplr(R(2:end)))] %append complex conjugate
%\end{lstlisting}

The function \ci{xcorr} in {\matlab} uses a much faster implementation based on the fast Fourier transform (FFT), to be discussed in Chapter~\ref{ch:transforms}. When comparing the results of the two methods, the discrepancy is around $10^{-14}$, which is a typical order of magnitude for numerical errors when working with {\matlab}.
\eExample 

When infinite duration power signals can be assumed, it is sensible to define
\begin{equation}
R_X[i] = \lim_{N \rightarrow \infty} \frac 1 {2 N} \sum_{n= -N}^N x[n+i] x^*[n].
\label{eq:discrete_time_power_acf}
\end{equation}

\subsubsection{Examples of some signals autocorrelations}
\label{sec:examplesAutocorrelation}
Two examples of autocorrelation are discussed in the sequel.

\bExample \textbf{Autocorrelation of white signal}.
\label{ex:autocorrelationWhite}
A signal is called ``white''\index{White signal} when it has an autocorrelation $R[i] = A \delta[i]$ in discrete-time, or $R(\tau) = A \delta(\tau)$ in continuous-time, where $A \in \Re$ is an arbitrary value. In other words, a white signal has an autocorrelation that is nonzero only at the origin, which corresponds to having samples that are uncorrelated and, consequently, statistically independent.

This nomenclature will be clarified in Chapter~\ref{ch:frequency} but it can be anticipated that such signals have their power uniformly distributed over frequency and the name is inspired by the property of white light, which is composed by a mixture of color wavelengths.

As mentioned, the samples of a white signal are independent and, if they are also identically distributed according to a Gaussian PDF, the signal is called \emph{white Gaussian noise} (WGN)\index{White Gaussian noise (WGN)}. More strictly, a WGN signal can be modeled as a realization of a wide-sense stationary process. In this case, WGN denotes the stochastic process itself.
Using {\matlab}, a discrete-time realization of WGN can be obtained with function \ci{randn}, as
illustrated in Section~\ref{sec:randomSignals}.
\eExample

\bExample \textbf{Autocorrelation of sinusoid}.
%From: http://www.latex-community.org/viewtopic.php?f=4&t=2142
% \allowdisplaybreaks and \displaybreak 
%\bExample \textbf{Autocorrelation of a sine or cosine.}
%\label{ex:AutocorrelationOfSinusoid}
Using \equl{autocorrelation_power}, the autocorrelation of a sinusoid $x(t)=A \sin(\aw t+\phi)$ can be calculated as follows:
\begin{align}
R(\tau) &= %\nonumber \\  %note that phantom below just to put a place holder for align: &\mathrel{\phantom{=}} 
\lim_{T \rightarrow \infty} \left[ \frac 1 {2T} \int_{-T}^{T}{A\sin(\aw t + \phi) A\sin(\aw t + \aw \tau + \phi)} \textrm{d}t \right]. \nonumber
\end{align}
We now assume $a=\aw t+\phi$ for simplicity and use \equl{sinaplusb} to expand $\sin(\aw t + \phi + \aw \tau)$:
\begin{align}
R(\tau) &=\lim_{T \rightarrow \infty} \left[ \frac {A^2} {2T} \int_{-T}^{T}{\sin(a) \left[ \sin(a) \cos(\aw \tau) + \sin(\aw \tau) \cos(a) \right] } \textrm{d}t \right]  \nonumber \\
&=A^2 \lim_{T \rightarrow \infty} \left[ \frac {\cos(\aw \tau)} {2T}  \int_{-T}^{T}{\sin^2(a)} \textrm{d}t + \frac {\sin(\aw \tau)} {2T} \int_{-T}^{T}{\sin(a)\cos(a)} \textrm{d}t \right]  \nonumber \\
&= A^2 \lim_{T \rightarrow \infty} \left[ \frac {\cos(\aw \tau)}{4T}  \int_{-T}^{T}{[1-\cos(2a)]} \textrm{d}t + \frac{\sin(\aw \tau)}{4T} \int_{-T}^{T}{\sin(2a)} \textrm{d}t \right]  \nonumber \\ 
&=A^2 \lim_{T \rightarrow \infty} \left[ \frac {\cos(\aw \tau)}{2T}  \frac{1}{2}\int_{-T}^{T} \textrm{d}t \right]  \nonumber \\ 
&=\frac {A^2 \cos(\aw \tau)}{2}. \label{eq:sinusoidcorrelation}
\end{align}
%%%%
\ignore{
& = & \lim_{T \rightarrow \infty} \left[ \frac 1 {2T} \int_{-T}^{T}{A\sin(\aw t + \phi) A\sin(\aw t + \aw \tau + \phi)} \textrm{d}t \right]\\
 & = & \lim_{T \rightarrow \infty} \left[ \frac {A^2} {2T} \int_{-T}^{T}{\sin(a) \left[ \sin(a) \cos(\aw \tau) + \sin(\aw \tau) \cos(a) \right] } \textrm{d}t \right]\\
 & = & \lim_{T \rightarrow \infty} \left[ \frac {A^2 \cos(\aw \tau)} {2T}  \int_{-T}^{T}{\sin^2(a)} \textrm{d}t + \frac {\sin(\aw \tau)} {2T} \int_{-T}^{T}{\sin(a)\cos(a)} \textrm{d}t \right] \\
 & = & \lim_{T \rightarrow \infty} \left[ \frac {A^2 \cos(\aw \tau)}{4T}  \int_{-T}^{T}{[1-\cos(2a)]} \textrm{d}t + \frac{\sin(\aw \tau)}{4T} \int_{-T}^{T}{\sin(2a)} \textrm{d}t \right]\\ 
 & = & \lim_{T \rightarrow \infty} \left[ \frac {A^2 \cos(\aw \tau)}{2T}  \frac{1}{2}\int_{-T}^{T} \textrm{d}t \right]\\ 
 & = & \frac {A^2 \cos(\aw \tau)}{2} \label{eq:sinusoidcorrelation}\\
}
The third equality used \equl{squaredSine} and \equl{sin2a}, and it was simplified because the integrals of both $\cos(2a)$ and $\sin(2a)$ are zero given the integration interval is an integer number of their periods. The final result indicates that the autocorrelation of a sinusoid or cosine does not depend on the phase $\phi$ and is also periodic in $\tau$, with the same period $2 \pi / \aw$ that the sinusoid has in $t$. Therefore, the frequencies contained in the realizations of a stationary random process can be investigated via the autocorrelation $R(\tau)$ of this process.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/cosineAutocorrelation}
\caption[A sinusoid of period \ci{N=8} samples and its autocorrelation, which is also periodic each 8 lags]{A sinusoid of period \ci{N=8} samples and its autocorrelation, which is also periodic each 8 lags. The cosine corresponding to $R_x[l]$ has amplitude $A^2/2=4^2/2=8$.\label{fig:cosineAutocorrelation}}
\end{figure}

A simulation with {\matlab} can help understanding this result.
\figl{cosineAutocorrelation} was generated with \codl{snip_signals_sinusoid_autocorrelation}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_sinusoid\_autocorrelation}{snip_signals_sinusoid_autocorrelation}
%\begin{lstlisting}
%numSamples = 48; %number of samples
%n=0:numSamples-1; %indices
%N = 8; %sinusoid period
%x=4*sin(2*pi/N*n); %sinusoid (try varying the phase!)
%[R,l]=xcorr(x,'unbiased'); %calculate autocorrelation
%subplot(211); stem(n,x); xlabel('n');ylabel('x[n]');
%subplot(212); stem(l,R); xlabel('Lag l');ylabel('R_x[l]');
%\end{lstlisting}
It can be seen that the \ci{x} and \ci{R} are a sine and cosine, respectively, of the same frequency in their corresponding domains (\ci{n} and \ci{l}, respectively).

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/autocorrelationFiniteDurationEffects}
\caption{The a) unbiased and b) raw (unscaled) autcorrelations for the sinusoid of \figl{cosineAutocorrelation} with a new period of \ci{N=15} samples.\label{fig:autocorrelationFiniteDurationEffects}}
\end{figure}

\figl{autocorrelationFiniteDurationEffects} was obtained by changing the sinusoid period from \ci{N=8} to \ci{N=15} and illustrates the effects of dealing with finite-duration signals. Note that both the unbiased and unscaled versions have diminishing values at the end points.
\eExample

Using \equl{discrete_time_power_acf} and a calculation similar to the one used in \equl{sinusoidcorrelation},
%Example~\ref{ex:AutocorrelationOfSinusoid}, 
one can prove that the autocorrelation of $x[n]=\sin (\dw n + \phi)$ is $R_X[i]=\cos(\dw i)/2$.

\subsection{Cross-correlation}\index{Cross-correlation}
The cross-correlation function (also called correlation) is very similar to the ACF but uses two distinct signals, being defined for deterministic energy signals as
\[
R_{xy}(\tau) \triangleq \int_{-\infty}^{\infty} x(t+\tau) y^*(t) \textrm{d}t = \int_{-\infty}^{\infty} x(t) y^*(t-\tau) \textrm{d}t.
\]
Note the adopted convention with respect to the complex conjugate. 

In discrete-time, the deterministic cross-correlation for energy signals is
\[
R_{xy}[l] \triangleq \sum_{n=-\infty}^{\infty} x[n+l] y^*[n] = \sum_{n=-\infty}^{\infty} x[n] y^*[n-l].
\]

Application~\ref{app:crosscorrelation} illustrates the use of cross-correlation to align two signals.

Before concluding this section, it is convenient to recall that the autocorrelation of a signal inherits any periodicity that is present in the signal. Sometimes this periodicity is more evident in the autocorrelation than in the signal waveform. The next example illustrates this point by discussing the autocorrelation of a sinusoid immersed in noise.

\bExample \textbf{The power of a sum of signals is the sum of their powers in case they are uncorrelated}.
\label{ex:autocorrelationAWGN}
Assume that a sinusoid $x[n]$ is contaminated by noise $z[n]$ such that the noisy version of the signal is $y[n]=x[n]+z[n]$. 
The signal $z[n]$ is a WGN (see Section~\ref{sec:examplesAutocorrelation}) that is added to the signal of interest $x[n]$ and, therefore, called additive white Gaussian noise (AWGN)\index{AWGN}. 
%The AWGN which is further studied in Section~\ref{AAAA-AQUI-sec:awgn}. The name reminds that $z[n]$ is added to the signal, t
%The PDF of its samples is Gaussian and its frequency spectrum is ``white''.
If $x[n]$ and $z[n]$ are uncorrelated, such that $R_{xz}[l]=\ev[x[n+l] z[n]] = 0, \forall l$, the autocorrelation $R_y[l]$ of $y[n]$ is given by
\begin{eqnarray*}
R_y[l] & = & \ev[y[n+l]y[n]] = \ev[(x[n+l]+z[n+l])(x[n]+z[n])] \\
& = & R_x[l] + R_{zx}[l] + R_{xz}[l] + R_z[l] \\
& = & R_x[l] + R_z[l].
\end{eqnarray*}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{Figures/noisePlusSine}
\caption{Sinusoid of amplitude 4 V immersed in AWGN of power 25 W. The bottom graph is a zoom showing the first 100 samples.\label{fig:noisePlusSine}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{Figures/xcorrNoisePlusSine}
\caption[Autocorrelations of sine plus noise]{Bottom graph: autocorrelation of the sine plus noise in \figl{noisePlusSine}, top: autocorrelation of the sine and middle: autocorrelation  of the noise.\label{fig:xcorrNoisePlusSine}}
\end{figure}

\codl{snip_signals_noisy_sinusoid} illustrates a practical use of this result. A sine with amplitude 4~V and power $4^2/2=8$~W is contaminated by AWGN with power of 25~W. All signals are represented by 4,000 samples, such that the estimates are relatively accurate.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_noisy\_sinusoid}{snip_signals_noisy_sinusoid}
%\begin{lstlisting}
%%Noise plus sinusoid
%A=4; %sinusoid amplitude
%noisePower=25; %noise power
%f=2; %frequency in Hz
%n=0:3999; %"time", using many samples to get good estimate
%Fs=20; %sampling frequency in Hz
%x=A*sin(2*pi*f/Fs*n); %generate discrete-time sine
%randn('state', 0); %Set randn to its default initial state
%noise=sqrt(noisePower)*randn(size(x)); %generate noise
%clf, subplot(211), plot(x+noise) %plot noise
%maxSample=100; %determine the zoom range
%subplot(212),stem(x(1:maxSample)+noise(1:maxSample)) %zoom
%maxLags = 20; %maximum lag for xcorr calculation
%[Rx,lags]=xcorr(x,maxLags,'unbiased'); %signal only
%[Rn,lags]=xcorr(noise,maxLags,'unbiased'); %noise only
%[Ry,lags]=xcorr(x+noise,maxLags,'unbiased');%noisy signal
%subplot(311), stem(lags,Rx); ylabel('R_x[l]');
%subplot(312),stem(lags,Rn);ylabel('R_n[l]'); 
%subplot(313),stem(lags,Ry);xlabel('Lag l');ylabel('R_y[l]');
%\end{lstlisting}

The signal-to-noise ratio (SNR)\index{Signal-to-noise ratio (SNR)} is a common metric
consisting of the ratio between the signal and noise power values and often denoted in dB (seem Appendix~\ref{app:decibel}).
\figl{noisePlusSine} illustrates the fact that it is hard to visualize the sinusoid because of the relatively low signal-to-noise ratio $\snrdb=10 \log_{10}(8/25) \approx -5$~dB. The waveform does not seem to indicate periodicity.

\figl{xcorrNoisePlusSine} shows the autocorrelations of $y[n]$ and its two parcels. For the lag $l=0$, the estimated values are $R_x[0]=8$, $R_z[0]=24.92$ and $R_y[0]=33.17$. The theoretical values are 8 W (the sine power), 25 W (noise power) and $8+25=33$ W (sum of the parcels), respectively. The bottom graph clearly exhibits periodicity and the noise disturbs only the value of $R_y[l]$ at $l=0$. In summary, two assumptions can simplify the analysis of random signals: that the ACF of the noise is approximately an impulse at the origin ($l=0$) and that the signal and noise are uncorrelated.\eExample

%The concept of correlation is useful for quantization, as provided in the next sections.


\subsubsection{{\akadvanced} Random processes cross-correlation and properties}

Some important properties of the cross-correlation are:
\begin{itemize}
	\item $R_{xy}(\tau)=R_{xy}^*(-\tau)$ (Hermitian symmetry),
	\item $R_{xy}(\tau)=R_{yx}^*(-\tau)$ (swapping arguments is also Hermitian),
	\item $|R_{xy}(\tau)| \le \sqrt{R_{xx}(0) R_{yy}(0)}$, (maximum is not necessarily at $\tau=0$ but is bounded).
\end{itemize}

When considering random processes, the two random variables are obtained from distinct processes:
\begin{equation}
R_{XY}(s,t) = \ev[\calX(s) \calY^*(t)]
\end{equation}
or, assuming stationarity (see Section~\ref{sec:appendix_correlation}) with $s=t+\tau$:
\begin{equation}
R_{XY}(\tau) = \ev[\calX(t+\tau) \calY^*(t)].
	\label{eq:crosscorrelation}
\end{equation}

\bExample \textbf{The AWGN channel}.
\label{ex:AWGNchannelIntroduction}
Thermal noise is ubiquitous and WGN is a random process model often present in telecommunications and signal processing applications.  WGN was briefly introduced in Examples~\ref{ex:autocorrelationWhite},~\ref{ex:autocorrelationAWGN} and Application~\ref{app:autocorrelationWhiteSines}. 
\ifml
In telecommunications, distinct systems that share the property of having WGN added at the receiver are called AWGN channel models.
\else
In communications  (as discussed in Section~\ref{sec:channelModels}), distinct models that share the property of having WGN added at the receiver are called AWGN channel models.
\fi

\figl{awgnChannelSimple} illustrates a continuous-time AWGN, where the received signal $r(t) = s(t) + \nu(t)$ is simply the sum of the transmitted signal $s(t)$ and WGN $\nu(t)$.

\begin{figure}[htbp]
\centering
\includegraphics[width=5cm,keepaspectratio]{FiguresTex/awgnChannelSimple}
\caption{Continuous-time version of the AWGN channel model.\label{fig:awgnChannelSimple}}
\end{figure}

Because $s(t)$ and WGN $\nu(t)$ are often assumed to be uncorrelated, as discussed in
\exal{autocorrelationAWGN}, the power of $r(t)$ is the sum of the powers of $s(t)$ and $\nu(t)$.
\eExample


Having learned about correlation (cross-correlation, etc.), it is possible to study a linear model for quantization, which circumvents dealing with the quantizer non-linearity.

\section{{\akadvanced} A Linear Model for Quantization}

As discussed, the quantization process is non-linear and the quantization error values depend on the quantizer input. But the following \emph{linear model for the quantization error} has proved to be a good approximation in several applications.

%\bModel
In this model, the error $\rve_q = \rvx - \rvx_q$ is assumed to be a uniformly distributed random variable with support $[-\Delta/2, \Delta/2]$ and zero-mean. This is sometimes called \emph{quantization noise}\index{Quantization noise}.
Two assumptions are important:
\begin{enumerate}
	\item The signal has enough variation to span all levels of the quantizer stair. The model fails, for example, if the input signal is a constant (DC) value.
	\item There is no correlation between the error $\rve_q$ and the signal to be quantized $\rvx$. 
	%The concept of correlation will be explored later in this chapter.
\end{enumerate}
The main assumption is that, for a quantizer with a step of $\Delta$, the quantization noise $\rve_q$ is modeled by assuming it is a random variable $\rve_q$ with uniform PDF and support $[-\Delta/2, \Delta/2].$ Because it is zero-mean, the power of $\ev[\rve_q^2]$ is solely the variance of the uniform PDF given by \equl{variance_uniformrv}, which in this case is
\begin{equation}
\sigma^2 = \Delta^2 / 12,
%\label{eq:variance_uniformrv}
\end{equation}
as indicated by \equl{variance_uniformrv}.
%There are more sophisticated models, but this one is adequate in many applications.
%\eModel

Assuming that 
\[
\Delta = \frac{\hat X_{\textrm{max}} - \hat X_{\textrm{min}}}{2^b},
\]
the power $\calP_n$ of the quantization noise is
\begin{equation}
\calP_n = \sigma^2 = \frac{\Delta^2}{12} = \frac{ (\hat X_{\textrm{max}} - \hat X_{\textrm{min}})^2} {2^{2b} 12},
\label{eq:quantizationNoisePower}
\end{equation}



%Assume a constant value $x=2.1$, the error would be... The correct 

It is a good idea to practice using the model by calculating the \emph{quantization SNR}\index{Quantization SNR} for sinusoids and cosines, uniformly and normally distributed random signals. The quantization SNR is obtained by using the quantization noise power in the denominator of the SNR expression (the numerator is the signal power, as usual). The following example illustrates the result for quantizing a Gaussian input signal.

\bExample \textbf{Quantization SNR of a Gaussian signal and the 6 dB per bit rule of thumb.}
\label{ex:6dbrule}
If the signal to be quantized $x(t)$ has samples distributed according to a Gaussian $\calN(3,4)$ with mean $\mu=3$ V and variance $\sigma^2=4$ W, a reasonable alternative (see \equl{delta_typical_quantizer}) is to adopt
$\hat X_{\textrm{min}} = \mu - 3 \sigma$ and $\hat X_{\textrm{max}} = \mu + 3 \sigma$.
% because this interval is responsible by more than 99\% of the probability of a Gaussian PDF.
In this case and using \equl{delta_typical_quantizer}, 
\[
\Delta = \frac{\hat X_{\textrm{max}} - \hat X_{\textrm{min}}}{M-1} = \frac{3+6-(3-6)}{2^b-1} = \frac{12}{2^b-1},
\]
where $b$ is the number of bits of the quantizer. Using the linear model of quantization:
\[
\snr = \frac{\calP_s}{\calP_n} = \frac{3^2+4}{\Delta^2/12} = \frac{13 \times 12}{12^2/(2^b-1)^2} = \frac{13(2^b-1)^2}{12} \approx 1.083 (2^b-1)^2,
\]
where $\calP_s$ and $\calP_n$ are the signal and noise power, respectively. The $\snr_{\dB}$ is
\begin{align*}
10 \log_{10}  \snr &= 10 [\log_{10} 1.0833 + 2\log_{10} (2^b-1)]\\
&\approx 0.3463 + 20 b \log_{10}2\\
&\approx 0.3463 + 6.021 b, 
\end{align*}
where the last step assumed that $2^b \gg 1$.

The result $\snr_{\dB} \approx 6 b + \textrm{cte.}$ is a well-known ``rule of thumb''. The constant (cte.) may vary, but the $\snr_{\dB}$ typically increases by 6 dB for each extra bit in the quantizer. It is common to use this approximation to suggest, for example, that an ADC of 12 bits has approximately quantization $\snr_{\dB} = 6 \times 12 = 72$~dB, while an ADC of 16 bits has $\snr_{\dB} = 6 \times 16 = 96$~dB.
\eExample


\section{{\akadvanced} Power and Energy in Discrete-Time}
\label{sec:powerDiscreteTime}
The concepts of power and energy are better defined for continuous-time than for discrete-time signals and vectors.
Dealing with the concept of power of a discrete-time signal $x[n]$ requires some caution because the discrete-time $n$ does not have the
dimension of time as the continuous-time t, which can be given in seconds, for instance. 
Sometimes one has a set of values, but the index that helps locating each individual value cannot be interpreted as
discrete-time. In such cases, to distinguish from $x[n]$ (with $n$ representing discrete-time), the notation $x_i$ is used.
The key concept here is that $i$ simply identifies the $i$-th element of the set and is not interpreted as ``time''. Hence, as will 
be discussed in this section, taking an average over $|x_i|^2$ values will not be interpreted as power (energy divided by time).

\subsection{Power and energy of discrete-time signals}

The following definition of average power will be adopted in this text
\begin{equation}
\calP  \triangleq \lim_{N \rightarrow \infty} \frac{1}{2N+1} \sum_{n=-N}^{N} |x[n]|^2.
\label{eq:power_discrete_time_signals}
\end{equation}
and interpreted as Watts given that $x[n]$ is assumed to be in volts. This is a sensible definition, as will be discussed in Section~\ref{sec:cont_disc_power}.

Accordingly, the energy $E$ of a discrete-time signal is
\begin{equation}
E  \triangleq \sum_{n=-\infty}^{\infty} |x[n]|^2.
\label{eq:energy_discrete_time_signals}
\end{equation}
such that $\calP = E/N$ when the support of $x[n]$ are $N$ samples.

%The index $n$ is associated to time, and when the 

%in \equl{power_discrete_time_signals} could be interpreted as, instead of an estimate in Watts, obtaining an average energy in Joules because the denominator $N$ is not in seconds. This ambiguity must be  resolved by the context (see \equl{power_energy_contellation} for an example) for the adequate interpretation of power for discrete-time signals. 

%AK TO DO - THE DEFINITION IS USEFUL BECAUSE THE POWER COINCIDES WHEN USING ZERO-ORDER.... %\section{Revisiting the Reconstruction (Interpolation) of Sampled Signals}
%\subsection{Power and energy of vectors}

A similar situation occurs when dealing with vectors.

\subsection{Power and energy of signals represented as vectors}

Here, $x[n]$ denotes the $n$-th element of a vector $\bx$. The order of these
 elements is assumed to correspond to a time evolution, such that a finite-dimension vector is
equivalent to a finite-duration discrete-time sequence.
Hence, the energy $E$ of vector $\bx$ is its squared Euclidean norm:
\begin{equation}
E=\sum_{n=1}^N |x[n]|^2=||\bx||^2,
\label{eq:vector_energy}
\end{equation} 
where $N$ is the dimension of $\bx$. Accordingly, the power of such finite-dimension vector is
\begin{equation}
\calP = \frac{E}{N} = \frac{1}{N}\sum_{n=1}^N |x[n]|^2.
\label{eq:vector_power}
\end{equation}

\subsection{{\akadvanced} Power and energy of vectors whose elements are not time-ordered}

In contrast to the previous equations, there are cases in which the vector elements are
not indexed according to a time evolution.

It should be noticed that the average of the squared norms of several vectors should be interpreted as their average energy, not power. For example, assuming there are $M$ vectors $\bx_1,\ldots,\bx_M$ of dimension $N$, an average energy is obtained with
\begin{equation}
\overline E = \frac{1}{M} \sum_{i=1}^M E_i = \frac{1}{M} \sum_{i=1}^M ||\bx_i||^2,
\label{eq:vectors_average_energy}
\end{equation}
and interpreted in joules. 

\equl{vector_power} and \equl{vectors_average_energy} are similar due to the connection between vectors and finite-duration discrete-time signals. The distinction that allows to observe their results as ``power'' or ``energy'' relies on interpreting the index $i$ as ``time'' or not.
In \equl{vector_power}, an average of instantaneous power values $|x[n]|^2$ along the discrete-time $n$ leads to an estimate of power. But when taking an average $\overline E$ over vectors of energy $E_i$ in \equl{vectors_average_energy}, the result is (average) energy. 

\ifml
\else
This distinction is important in situations such as  \equl{pam_energy_contellation}, when $\overline E$ is the average energy of a constellation.
\fi
%for an example) for the adequate interpretation of power for discrete-time signals. 

%To make sure the concepts are clear, applying \equl{vector_power} to each vectors $\bx_i$ and taking the average of their power $\calP_i$ leads to
%\[
%\calP=\frac{1}{M}\sum_{i=1}^M\calP_i = \frac{1}{M N}\sum_{i=1}^M \sum_{j=1}^N  |x_i(j)|^2 = \frac{\overline E}{N}.
%\]
%\] of their powers, where $x_i(j)$ is the $j$-th element of $\bx_i$ is given
%\[
%\calP_i = \frac{1}{N}\sum_{j=1}^N |x_i(j)|^2,
%\] or equivalently, $\calP = \overline E / N$, and interpreted in Watts.


\subsection{Power and energy of discrete-time random signals}

If $x[n]$ represents a random signal, with samples $x[n_0]$ corresponding to outcomes of a
%continuous (unquantized)
random variable $\rvx$,  an alternate definition is
\[\calP \triangleq \ev[|\rvx|^2] = \ev [| x[n] |^2],\]
%\ev [\Vert x[n]\Vert^2],\]
where $\ev [\cdot]$ is the expectation operator.

%Unless otherwise stated, it is assumed real (not complex) signals, and there is no need for taking the magnitude $| \cdot |$ in the expressions for $\calP$.

%\bTheorem \textbf{Power of random signals}.
The power of a random signal $x[n]$ (or $x(t)$) can be decomposed into two parcels as follows:
\begin{equation}
\calP = \ev [\rvx^2] = \sigma_x^2 + \mu_x^2,
	\label{eq:powerasexpectedvalue}
\end{equation}
where the variance $\sigma_x^2$ and the squared-mean $\mu_x^2$ correspond to the powers of the AC and DC components of a real $x[n]$, respectively.
The proof is derived in \equl{variance_alternative}. 

%from the definition of variance:
%$\sigma_x^2 \triangleq \ev [(\rvx-\mu_x)^2] = \ev [\rvx^2 - 2 \rvx \mu_x + \mu_x^2]$. Observing that $\mu_x$ is a constant and $\ev$ is a linear operator, one can write $\sigma_x^2 = \ev[\rvx^2] - 2 \ev[\rvx] \mu_x + \mu_x^2 = \ev[\rvx^2] - \mu_x^2$.
%\eTheorem

Most of the signals in telecommunications and other applications have zero mean ($\mu = 0$). In this case, \equl{powerasexpectedvalue} shows that the power $\calP = \sigma_x^2$ coincides with the variance of the random signal and the standard deviation $\sigma_x$ with its RMS value. % = \xrms$ 

\subsection{{\akadvanced} Relating Power in Continuous and Discrete-Time}
\label{sec:cont_disc_power}

The goal here is to relate the power of a discrete-time $x[n]$ to the power of a continuous-time $x(t)$ where these signals are related by an A/D or D/A conversion. A possible processing chain relating these signals with their associated power in parenthesis is:
%\begin{equation}
%\underset{(\calP_c)}{x(t)} \arrowedbox{sampling} \underset{(\calP_s)}{x_s(t)} \arrowedbox{S/D} \underset{(\calP_d)}{x[n]}.
%\label{eq:powersFromCtoD}
%\end{equation}
\begin{equation}
{\underset{({\calP}_c)}{x(t)}} \arrowedbox{sampling} x_s(t) \arrowedbox{S/D} {\underset{({\calP}_d)}{x[n]}}  = x(n\ts).
\label{eq:powersFromCtoD}
\end{equation}
Another processing chain of interest is the reconstruction of a continuous-time signal:
%\begin{equation}
%\underset{(\calP_d)}{x[n]} \arrowedbox{D/S} \underset{(\calP_s)}{x_s(t)} \arrowedbox{$h(t)$} \underset{(\calP_c)}{x(t)}
%\label{eq:powersFromDtoC}
%\end{equation}
\begin{equation}
x(n\ts) = \underset{(\calP_d)}{x[n]} \arrowedbox{D/S} x_s(t) \arrowedbox{h(t)} \underset{(\calP_c)}{x(t)}
\label{eq:powersFromDtoC}
\end{equation}
which will be discussed in Section~\ref{sec:signal_reconstruction}.

Special interest lies on systems that have \emph{equivalence between power in discrete and continuous-time}, such that
\begin{equation}
\calP_d=\calP_c,
\label{eq:energyConservationContinuousDiscreteTime}
%\label{eq:samePowerContinuousDiscrete}
\end{equation}
where $\calP_d$ and $\calP_c$ are given by \equl{power_discrete_time_signals} and \equl{power_continuous_time_signals}, respectively.\footnote{This is not the same, but similar to the energy-conservation property of Fourier transforms (e.\,g., \equl{parsevalEnergy}, or its version for periodic signals \equl{parsevalPower}).}

\equl{energyConservationContinuousDiscreteTime} is further discussed in Section~\ref{sec:energyConservationThroughSampling}, but here it is derived\footnote{\equl{energyConservationContinuousDiscreteTime} can also be obtained by assuming the zero-order hold (ZOH) reconstruction of \figl{zoh}.} using the rectangle method to approximate the integral of $p(t)=|x(t)|^2$ as a sum of rectangles with bases $\ts$ and heights $|x[n]|^2$,
 as follows:
\begin{align*}
\calP_c &= \lim_{N \rightarrow \infty} \left[ \frac{1}{(2N+1)\ts} \int_{-N \ts}^{N \ts}{|x(t)|^2}\textrm{d}t \right]  \\
   &\approx \lim_{N \rightarrow \infty} \left[ \frac{1}{(2N+1)\ts} \sum_{n=-N }^{N }{\ts|x(n\ts)|^2} \right] \\
   &= \lim_{N \rightarrow \infty} \left[ \frac{1}{(2N+1)} \sum_{n=-N }^{N }{|x[n]|^2} \right] \\	
	&= \calP_d. \numberthis
\end{align*}

%Not needed:
%Considering the D/A process, when $x_s(t)= \sum_{k=-\infty}^{\infty}{x[k] \delta(t-k \ts)}$ is converted to $x(t)$ using zero-order hold (ZOH), 
%the expression relating $x(t)$ and $x[n]$ is
%$$
%x(t) = \sum_{k=-\infty}^{\infty}{x[k] h(t-k \ts)}.
%$$
%In this case, 
%$\calP_c = \calP_d$. To see that, note the integral can be written as
%\[
%\calP_c = \frac{1}{N} \int_{0}^{T}{|x(t)|^2 \textrm{d}t} \approx \frac{1}{N \ts} \sum_{k=0}^{N-1} \left( |x[k]|^2 \ts \right) = \calP_d
%\]
%$$
%P_c = \lim_{\Delta t \rightarrow \infty} \left\{ \frac{1}{\Delta t} \int_{-\Delta t/2}^{\Delta t/2}{|x(t)|^2 \textrm{d}t} \right\} = \lim_{N \rightarrow \infty} \left\{ \frac{1}{N T} \sum_{k=-N/2}^{N/2} \left( x^2[k] T \right) \right\}
%$$

%An equivalent reasoning is: the energy $E_d$ in discrete-time can be defined as $E_d =  \sum_{k=0}^{N-1}{|x[k]|^2}$ while its counterpart in continuous-time with ZOH is
%\begin{equation}
%E_c \approx \ts \sum_{k=0}^{N-1}{x^2[k]} = \ts \, E_d = \frac{E_d}{\fs},
%\label{eq:energy_discretetime}
%\end{equation}
%such that $E_c$ is given in Joules and $\calP_c = E_c / (N \, \ts) = \calP_d$, as before.

In summary, when the sampling theorem is obeyed, the signal processing chains (filtering, amplification, etc.) associated to the A/D and D/A processes are assumed here not to alter the power of $x(t)$ and $x[n]$, such that \equl{energyConservationContinuousDiscreteTime} holds.

%transforms in Section~\ref{sec:samplingRevisited}. But before delving into a more rigorous mathematical description of sampling, it is useful to understand the expression of a periodic impulse train
%\[
%p(t) = \sum_{k=-\infty}^\infty \delta(t-k T_s),
%\]
%where $T_s$ is the sampling interval and $k \in \integers$. The expression for $p(t)$ and the sifting property of the impulse allows to model the sampled signal as
%\begin{equation}
%x_s(t) = x(t) p(t) = \sum_{k=-\infty}^\infty x(k T_s) \delta(t-k T_s).
%\label{eq:sampledSignalGeneral}
%\end{equation}

\section{Applications}
\label{sec:applications_signal}

Some applications of the main results in this chapter are discussed in this section.

\bApplication \textbf{Recording signals with a sound board and the Audacity software}.
\label{app:recording_sound}
%criar uma funcao ak\_algo que grave som no matlab e no octave, que seja portavel no linux e windows.
It is relatively easy to record sound using a microcomputer. However, most softwares that capture sound are not very useful for DSP applications, because they assume the user is not interested in ``low-level'' details such as the number of bits per sample. But there many alternatives that do provide this kind of information. Two free and open-source (FOSS) softwares for manipulation sound files are Audacity~\akurl{http://audacity.sourceforge.net/}{1aud} and  Sox~\akurl{http://sourceforge.net/projects/sox/}{1sox}. While Sox is very useful for converting among file formats and working from a command line, Audacity is adopted here because it has a graphical user interface (GUI) that allows, for example, to monitor recording and avoid saturating the ADC, which would distort the sound due to clipping of its amplitudes.

\figl{audacity} shows a short segment of audio recorded with the default options of sampling frequency $\fs=44.1$~kHz and number $b=32$ bits per sample in floating-point, as indicated by the letter A in the figure. The menu \ci{Edit - Preferences - Quality} of Audacity allows to change these values. Another option to change $\fs$ is the ``Project rate'' in letter B of \figl{audacity}. The level meters indicated with letter C are activated during recording and playback and, in this case, suggest that the signal amplitude was clipped due to ADC saturation. Alternatively, this can be visualized using menu \ci{View - Show Clipping}. Each time a new recording starts (by clicking the button indicated by letter D), the audio track has the $\fs$ and $b$ imposed by the current default options.
Most sound boards have two channels and can record in stereo but here it is assumed that only one channel is of interest and the files are mono.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthLarge,keepaspectratio]{FiguresNonScript/audacity}
	\caption{Example of sound recorded at $\fs=44.1$~kHz with the Audacity sound editor.\label{fig:audacity}}
\end{figure}

Audacity can save a signal in a variety of file formats, such as MP3, using the menu \ci{Export}. Our goal is to later read the saved file in another software ({\matlab}, etc.), so MP3 should be avoided and of special interest here are ``wav'' (actually the WAVE format, an instance of Microsoft's RIFF file format) and raw (header-less). 

The ``wav'' format is just a wrapper for many codecs. In other words, within a ``wav'' file one can find uncompressed data requiring hundreds of kilobits to represent each second (kbps) of audio as well highly compressed data requesting less than five~kbps. Unless the file size should be minimized, for increased portability it is better to use an uncompressed ``PCM'' format. Due to its adoption in digital communications, the result of A/D conversion is sometimes called pulse-coded modulation (PCM)\index{PCM}. Hence, PCM can be seen as a codec but its output is equivalent to a signal simply sampled at $\fs$ and quantized (or encoded) with $b$ bits/sample. If the adopted quantizer is uniform (see \equl{uniform_quantizer_levels}), the PCM is called linear. The linear PCM is the best format with respect to portability but there are also two popular non-linear PCMs.

Because the probability distribution of long segments of speech signals is approximately Laplacian, not uniform, the quantizer used in digital telephony is non-uniform. These quantizers are based on non-linear curves (approximately logarithmic) called A-law and $\mu$-law. \figl{audacity_wavoptions} shows some options when the user chooses \ci{Export - Other compressed files (in ``type'')} and then \ci{Options}.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/audacity_wavoptions}
	\caption{Some Audacity options for saving an uncompressed WAVE file. The two non-linear PCMs are indicated.\label{fig:audacity_wavoptions}}
\end{figure}

Hence, use Audacity to record some sound with $\fs=8$~kHz and export it as a file named \ci{myvoice.wav} in the ``WAV (Microsoft) signed 16 bits'' format. After that, read it with {\matlab} using:
\begin{lstlisting}
[x,Fs,wmode,fidx]=readwav('myvoice.wav');
b = fidx(7); % num of bits per sample
\end{lstlisting}
It can be observed that \co{Fs=8000} and \co{b=16}, as recorded.
Note that by default the \ci{readwav} function outputs samples in floating-point and normalizes them to be within the range $[-1,1]$. If the actual integer values are of interest, Matlab allows to use
\begin{lstlisting}
[x2,Fs,wmode,fidx]=readwav('myvoice.wav','r'); % get raw samples
b = fidx(7); % num of bits per sample
\end{lstlisting}
Using the previous commands and comparing \ci{min(x), max(x)} and \ci{min(x2), max(x2)}, in the case of a specific audio file, the (native) integer values $-8488$ (min) and 5877 (max) were normalized to $-0.2590$ and 0.1794, respectively, when not using the option \ci{'r'}. The normalization consists in dividing the native integer values by $2^{b-1}$, which takes in account that these values are originally within the range $[-2^{b-1},2^{b-1}-1]$. For example, in this case $b=16$ and $5877/2^{15} \approx 0.1794$.

In case the file had used A-law non-linear PCM, {\matlab} could potentially give error messages.\\
%\co{Data compression format (CCITT a-law) is not supported.}\\
%and Octave:\\
%\co{error: wav read: sample format 0x6 is not supported}

%\ref{app:fileHeaders}
Now, it is suggested to get more familiar with headerless files using Audacity to save a sound file as ``raw''.
It may be useful to check Appendix \ref{app:files} for more details on how the information is organized in binary files.
After recording in Audacity, choose \ci{Export - Other compressed files (in ``type'')} as in \figl{audacity_wavoptions}, but this time select the header ``RAW (header-less)'' instead of ``WAV (Microsoft)''. For the encoding, select ``Signed 16 bit-PCM'', as before, and name the file \ci{'myvoice\_raw.wav'}. In this case, it would be wiser to use another file extension and name it \ci{'myvoice.raw'}, for example. But the purpose of using ``wav'' is to make clear that the extension by itself cannot guarantee a file format is the expected one. 

In this particular example, the file sizes are $29,228$ and $29,184$ for the WAVE and raw formats, respectively. In fact, in spite of a WAVE possibly having a sophisticated structure with several sections (chunks), most of them have a single chunk and one header consisting of the first 44 bytes, which is the difference between the two sizes given that both have the same $29184/2=14592$~samples of 2 bytes each.

Using the command \ci{readwav} for the raw file would generate error messages in {\matlab}. Based on Appendix~\ref{app:matlab}, the following code properly reads the samples:
\begin{lstlisting}
fp=fopen('myvoice_raw.wav','rb'); %open for reading in binary
x=fread(fp,Inf,'int16'); %read all samples as signed 16-bits
fclose(fp);  %close the file
\end{lstlisting}
As a sanity check, one can read the samples of the WAVE file, skip its header and compare with the result of \ci{readwav} with \codl{snip_signals_wavread} on Matlab.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_signals\_wavread}{snip_signals_wavread}

The advantage of using WAVE is that the header informs $\fs$, $b$, whether its mono or stereo, etc. Also, the WAVE format takes care of endianness (see Appendix~\ref{app:endianness}).
Not using \ci{readwav}, write code in {\matlab} to open a WAVE file (with only 1 chunk) and extract $\fs$, $b$ and the samples as integers. This code can be used by Octave users to mimic the option \ci{'s'} (raw) in Matlab's \ci{readwav}. 
\ifdefined\akAmazonBook
\else
It may be useful to read Appendix~\ref{app:fileHeaders} and use the companion code \ci{laps\_dump.c}, which can be compiled with most C compilers. A short description of the WAVE header is provided at \akurl{https://ccrma.stanford.edu/courses/422/projects/WaveFormat/}{1wav}.
\fi
\eApplication 

\bApplication \textbf{Recording sound with Matlab}. \href{https://colab.research.google.com/github/aldebaro/dsp-telecom-book-code/blob/master/PythonNotebooks/ch1ap2_RecordingSound.ipynb?authuser=2}{Colab's notebook version}
This application discusses how to record sound directly with Matlab, which has several functions to deal with sound recording and playback. You can check \ci{soundsc, audiorecorder, wavplay} and \ci{wavrecord}, for example. Some functions work only on Windows.

Octave has functions such as \ci{record} and \ci{sound} and its support to sound is more natural on Linux. There are solutions such as \akurl{http://www.playrec.co.uk}{1rec} to record and play sound on Octave running on Windows, but the installation is not trivial.

The following code was used in Matlab to record 5 seconds of one (mono) channel sound at a sampling rate of 11,025 Hz, using 16 bits to represent each sample:
\begin{lstlisting}
r=audiorecorder(11025,16,1); %create audiorecorder object
record(r,5); %record 5 seconds and store inside object r
\end{lstlisting}

One can use \ci{play(r)} to listen the recorded sound or \ci{y = getaudiodata(r, 'int16')} to obtain the samples from the audiorecorder object. However, if one of these commands immediately follows \ci{record(r,5)}, the error can be generated:\\
\co{??? Cannot retrieve audio data while recording is in progress.}\\
This means the software was still recording when it tried to execute the second command. An alternative is to use \ci{recordblocking} as in \codl{snip_signals_recordblocking}.

\includecodepython{MatlabOnly}{snip\_signals\_recordblocking}{snip_signals_recordblocking}
%\begin{lstlisting}
%r=audiorecorder(11025,16,1); %create audiorecorder object
%recordblocking(r,5); %record 5 seconds and store inside r
%play(r) %playback the sound
%y = getaudiodata(r, 'int16'); %extract samples as int16
%plot(y); %show the graph
%\end{lstlisting}

Note that \ci{y} in the above example is an array with elements of the type int16, i.\,e., 2 bytes per sample. This saves storage space when compared to the conventional real numbers stored in double (8 bytes) each, but limits the manipulations. For example, the command \ci{soundsc(y,11025)} generates an error message if \ci{y} is int16. In such cases, a conversion as \ci{y=double(y)} can be used before invoking \ci{soundsc} (use \ci{whos y} to check that the storage has quadruplicated).

To write \ci{y} to a 16-bits per sample WAV file and read it back, use \codl{snip_signals_wavwrite} but the command \ci{double(z)./double(y)} shows that the normalization used by \ci{wavwrite} made \ci{z} approximately three times \ci{y}.
The Voicebox toolbox (\akurl{http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html}{1voi}) has functions \ci{readwav} and \ci{writewav} that are smarter than Matlab's with respect to avoiding the normalization.

\includecodelong{MatlabOnly}{snip\_signals\_wavwrite}{snip_signals_wavwrite}


Some sound boards allow full-duplex operation, i.\,e., recording and playing at the same time. Typically the sampling frequency must be the same for both operations. On Windows one can try the \ci{wavplay} function with the option ``async'' as exemplified in \codl{snip_signals_realtimeLoopback}.

\includecodepython{MatlabOnly}{snip\_signals\_realtimeLoopback}{snip_signals_realtimeLoopback}

%\begin{lstlisting}
%Fs = 11025;   %define sampling rate
%wavplay(outputSignal, Fs, 'async');  %playback
%inputSignal  = wavrecord(5*Fs, Fs, 'int16'); %record 5 s
%\end{lstlisting}

\codl{snip_signals_realtimeLoopback} shows the acquired signal (from the ADC) in both time and frequency domains. In this code, the call to \ci{wavplay} is non-blocking but samples are lost in the sense that \ci{inputSignal} is not a perfect cosine.
Using a loopback cable, as in Application~\ref{app:latency}, allows to evaluate the system.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./FiguresNonScript/cosineLoopBack}
\caption{Cosine obtained with \codl{snip_signals_realtimeLoopback}  and a loopback cable connecting the soundboard DAC and ADC.\label{fig:cosineLoopBack}}
\end{figure}

\figl{cosineLoopBack} was obtained with a loopback. Note from the top plot that approximately 300 samples are a transient and after that one can see the cosine at $f_c=1500$~Hz, which is mapped to $1500/(\fs/2) \approx 0.27$ ($\fs=11025$~Hz) in the normalized axis of the bottom plot. In order to get this kind of system running, it is important to reduce the volume (DAC gain) to avoid saturation of the signals.

\ignore{
Note that if one records sound (for example, with Audacity), saves to a wav file and then reads this file into 
Matlab with the \ci{readwav} function, the amplitudes will be normalized to be within the range $[-1, 1]$ by \ci{readwav}. 
}
\ignore{
The Voicebox toolbox (\url{http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html}) has a function called \ci{readwav} that provides support for not using this normalization.
The reason for not appreciating this forced normalization is that one may want to recover the actual level of the signal (in volts, for example). 
}

As an exercise, digitize signals at different sampling frequencies and plot them with the axes properly normalized.
Another interesting exploration is to obtain a sound signal \ci{inputSignal}, digitized at a given rate (e.\,g., $\fs=11,025$ Hz) and represented by int16. Convert it to double with \ci{x=double(inputSignal)} in order to easily manipulate the signal and describe what is the result of each of the commands in \codl{snip_signals_digitize_signals}.

\includecodepython{MatlabOnly}{snip\_signals\_digitize\_signals}{snip_signals_digitize_signals}
%\begin{lstlisting}
%fs=22050; %sampling frequency
%r = audiorecorder(fs, 16, 1);%create audiorecorder object
%recordblocking(r,5);%record 5 s and store inside object r
%y = getaudiodata(r,'int16'); %retrieve samples as int16
%x = double(y); %convert from int16 to double
%soundsc(x,fs); %play at the sampling frequency
%soundsc(x,round(fs/2));%play at half of the sampling freq.
%soundsc(x,2*fs); %play at twice the sampling frequency
%w=x(1:2:end); %keep only half of the samples
%soundsc(w,fs); %play at the original sampling frequency
%z=zeros(2*length(x),1); %vector with twice the size of x
%z(1:2:end)=x;%copy x into odd elements of z (even are 0)
%soundsc(z,fs); %play at the original sampling frequency
%\end{lstlisting}

What should be the sampling frequency for vectors \ci{w} and \ci{z} in \codl{snip_signals_digitize_signals} to properly listen the audio?
\eApplication

\bApplication \textbf{Real time sound processing with Matlab's DSP System Toolbox}.
%Exemplo mostrando Matlab amostrando em tempo real
Matlab's DSP System Toolbox has extended support to interfacing with the
sound board. \codl{snip_signals_realtimeWithDspSystem} provides a simple example
that illustrates recording audio.

%http://www.mathworks.com/help/dsp/ref/dsp.audiorecorder-class.html

\includecodepython{MatlabOnly}{snip\_signals\_realtimeWithDspSystem}{snip_signals_realtimeWithDspSystem}

\codl{snip_signals_realtimeWithDspSystem} indicates how to plot the signal
in frequency domain or to perform digital filtering. The code may drop samples
depending on the computer's speed. Matlab's documentation inform how to control
the queue and buffer lengths, and also obtain the number of overruns.
\eApplication

\bApplication \textbf{Estimating latency using the sound board, Audacity and a loopback cable}.
\label{app:latency}
The goal here is to practice dealing with digital and analog signals, interfacing Audacity and {\matlab}.
%A sound board can be used to generate and acquire analog signals. 
An audio cable and a single computer (with the sound system working) is all that is needed for many interesting experiments. The user is invited to construct or purchase a cable with the proper connectors for his/her computer. In most cases, a ``3.5~mm male to 3.5~mm male audio cable'' is the required one, as indicated in \figl{audioCableLoopback}. A single channel (mono) cable may suffice but stereo cables have almost the same cost and can be used in more elaborated experiments.\footnote{Note that some computers do not have a sound input connector (microphone or line in) and an external (e.\,g., USB-based) sound board would be required.}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/audioCableLoopback}
	\caption[Setup for loopback of the sound system using an audio cable.]{Setup for loopback of the sound system using an audio cable, which is connected to the microphone input (or, alternatively, to the line in) and to the speaker output (or to the line out).\label{fig:audioCableLoopback}}
\end{figure}

The task is to estimate the \emph{latency} or \emph{channel delay}, which is the time interval between the signal is transmitted and its arrival at a receiver after passing through a channel. In this specific case, the channel is composed of the sound board hardware (buffers, etc.) and the used device drivers (low-level software that interfaces with the hardware) and application software (Audacity in this case). In sound processing, latency is especially important when \emph{overdubbing}, i.\,e., recording a track while playing back others. A detailed description of testing the latency with Audacity can be found at
\akurl{http://manual.audacityteam.org/man/Latency_Test}{1lat}. 

\begin{figure}
\centering
\includegraphics[width=\figwidth]{./FiguresNonScript/SoundBoardOptions}
\caption{Example of options provided by Windows and the sound board. All the enhancements for both recording and playback devices should be disabled.\label{fig:SoundBoardOptions}}
\end{figure}

To have a better control of the sound board, it is important to disable all special effects and enhancements for both recording and playback devices, such as automatic gain control for the input ADC signal.
\figl{SoundBoardOptions} provides screenshots from Windows but users of other operating systems should be able to find how to choose the best sound options. 

After making sure the best configuration for your sound system was chosen, the task now is to generate some samples of a periodic train of impulses. Instead of impulses, the Audacity menu ``Generate - Click Track'' provides a dialog window with other options. But here the suggestion is to use {\matlab} and create a signal with \ci{N} discrete-time impulses $\delta[n]$. Note that the analog signal corresponding to $\delta[n]$ will never be the theoretical continuous-time $\delta(t)$. For example, assuming zero-order reconstruction (see \figl{zoh}), the amplitude of $\delta[n]$ would be held constant during the whole sampling interval $\ts$.
Aware of this limitation, \codl{snip_signals_impulse_train} generates a train of $N$ discrete-time impulses and saves it to a WAVE file.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_impulse\_train}{snip_signals_impulse_train}
%\begin{lstlisting}
%Fs = 44100; %sampling frequency
%Ts = 1/Fs; %sampling period
%Timpulses = 0.25; %interval between impules in seconds
%L=floor(Timpulses/Ts); %number of samples between impulses
%N = 4; %number of impulses
%impulseTrain=zeros(N*L,1); %allocate space with zeros
%b=16; %number of bits per sample
%amplitude = 2^(b-1)-1; %impulse amplitude, max signed int
%impulseTrain(1:L:end)=amplitude; %generate impulses
%wavwrite(impulseTrain,Fs,b,'impulses.wav') %save WAVE RIFF
%\end{lstlisting}

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/impulsesAudacity}
	\caption{Audacity window after reading in the 'impulses.wav' file.\label{fig:impulsesAudacity}}
\end{figure}

Opening the generated file with Audacity should lead to \figl{impulsesAudacity}. Note the amplitudes have been normalized and the first impulse barely appears. In this case, as indicated by letter F in \figl{impulsesAudacity}, the selection region starts approximately at 1~s. The interface is friendly and the letters C and D indicate how
to switch between zooming the signal and enabling the cursor, respectively.
After a segment is selected, letter E indicates how to easily zoom it to fit the selection. Instead of seconds (in letter F), it is sometimes convenient to use ``samples''. Using the play button indicated with letter A plays the file.

\begin{figure}
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/latencyAudacity}
	\caption{Audacity window after simultaneously recording and playing 'impulses.wav' with a loopback.\label{fig:latencyAudacity}}
\end{figure}

At this point, a feature of Audacity that is useful for overdubbing can be used to simultaneously activate the DAC and ADC: when recording, Audacity also plays all the signals that are ``open'' (in this case, the ``impulses'' signal). With the audio cable connected in loopback, start recording (and playback) simply using button (letter B), stopping it after a second. The final situation should be similar to \figl{latencyAudacity}.

From the code used to generate 'impulses.wav' it can be seen that the impulses are separated by $\fs/4=11025$ samples (the first one is at $n=1$, the second at $n=11026$ and so on). This information was used to impose the start selection (letter F in \figl{impulsesAudacity}) at sample 11,026 in \figl{latencyAudacity} (it is irrelevant here, but recall that the first index in {\matlab} is 1 but 0 in Audacity). The end of the selection was located approximately at the start of the second impulse of the recorded signal (bottom plot, identified as ``Audio Track''). In this case, the number of samples of this selection indicate that the latency was approximately $2102 \times \ts \approx 47.66$~ms.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/estimatedImpulseResponse}
	\caption{Zoom of the response to the second impulse in \figl{impulsesAudacity}.\label{fig:estimatedImpulseResponse}}
\end{figure}

At this point it may be useful to export the recorded signal as a WAVE file to be read in {\matlab}. First, you can close the window with the ``impulses'' signal (otherwise Audacity will ask if the two files should be merged) and use the ``Export'' menu. Assuming the output file name was \ci{impulseResponses.wav}, the command
\ci{h=readwav('impulseResponses.wav')} can be used to generate the zoom of the second impulse response in
 \figl{estimatedImpulseResponse}. The concept of impulse response is very important, as discussed in Chapter~\ref{ch:systems}.
 
Because the maximum absolute amplitude occurs at $n=13037$ in \figl{estimatedImpulseResponse} and the corresponding impulse is located at $n=11026$, another estimate of the latency is $(13037-11026)\ts \approx 45.6$~ms. A detail is that
for creating \figl{estimatedImpulseResponse}, the \ci{`s'} option of Matlab's \ci{readwav} was used to avoid normalization and, consequently, the minimum signal value is $-2^{b-1}=-32768$ not $-1$.
\eApplication

\bApplication \textbf{PC sound board quantizer}.
\label{app:sound_board_quantizer}
Given a system with an ADC, typically one has to know beforehand or conduct measurements to obtain the quantizer step size $\Delta$. This is the case when using a personal computer (PC) sound board. For a sound board, the value of $\Delta$ depends if the signal was acquired using the microphone input or the line-in input of the sound board. The microphone interface is designed for signals with peak value of $X_{\textrm{max}}=10$ to 100 mV while the peak for the line-in is typically 0.2 to 2~V. Note that the voltage ranges of line inputs and microphones vary from card to card. See more details in \akurl{http://www.epanorama.net/links/pc_sound.html}{1pcs}. For the sake of this discussion, one can assume a dynamic range of $[-100, 100]$ mV and a ADC of 8 bits per sample, such that $\Delta = 200/(2^8-1) \approx 0.78$ mV. The following example illustrates how to approximately recover the analog signal for visualization purposes.
Assume the digital dynamic range is [0, 255] and the digital samples are $D=[13, 126, 3, 34, 254]$. If one simply uses \ci{stem(D)}, there is no information about time and amplitude. \codl{snip_signals_amplitude_normalization} shows the necessary normalizations to visualize the abscissa in seconds and the ordinate in volts, which in this case corresponds to \co{A=1000*[-89.70,   -1.56,  -97.50,  -73.32,   98.28]}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_amplitude\_normalization}{snip_signals_amplitude_normalization}
%\begin{lstlisting}
%D=[13 126 3 34 254]; %signal as 8-bits unsigned [0, 255]
%n=[0:4]; %sample instants in the digital domain
%Fs=8000; %sampling frequency
%delta=0.78e-3; %step size in volts
%A=(D-128)*delta; %subtract offset=128 and normalize by delta
%Ts=1/Fs; %sampling interval in seconds
%time=n*Ts; %normalize abscissa
%stem(time,A); %compare with stem(n,D)
%xlabel('time (s)'); ylabel('amplitude (V)');
%\end{lstlisting}

Now assume a PC computer with a sound board that uses a 
16 bits ADC and supports at its input a dynamic range of $-185$ to $185$~mV. The quantizer is similar to the one depicted in \figl{quantizerdelta05}, but the quantization step should be $\Delta=2 \times 185 \times 10^{-3}/2^{16} \approx 5.6$~\muV. It is assumed here that $\Delta = 5.6$ \muV~and the quantizer is uniform from 
$-2^{15}\Delta$ to $(2^{15}-1)\Delta$. In this case, the $M=65,536=2^{16}$ levels are organized as 32,767 positive levels, 32,768 negative levels and one level representing zero. The assumed coding scheme is the offset code of \tabl{outputcoding} with 32,768 as the offset. Hence, the smallest value $-2^{15}\Delta$ is mapped to the 16-bits codeword ``0000 0000 0000 0000'', $(-2^{15}+1)\Delta$ to ``0000 0000 0000 0001'' and so on, with $(2^{15}-1)\Delta$ being coded as ``1111 1111 1111 1111''. 

If at a specific time $t_0$ the ADC input is $x(t_0)=x=0.003$ V, the ADC output is $x_i=536$, which corresponds to $x_q=0.0030016$ V and leads to a quantization error $e=x-x_q \approx -1.6 \times 10^{-6}$ V. These results can be obtained in {\matlab} with
\begin{lstlisting}
delta=5.6e-6, b=16 %define parameters for the quantizer
format long %see numbers with many decimals
x=3e-3; [xq,xi]=ak_quantizer(x,delta,b), error=x-xq
\end{lstlisting}
Based on similar reasoning, calculate the outputs $x_i$ of the quantizer, their respective $x_q$ values and the quantization error for $x \in \{-300,-100,0,20,180\}$ mV.

If you have access to an oscilloscope and a function generator, try to estimate the value of $\Delta$ of your sound board, paying attention to the fact that some software/hardware combination use automatic gain control (AGC). You probably need to disable AGC to better control the acquisition.

It is not trivial, but if you want to learn more about your sound board, try to evaluate its performance according to the procedure described at \akurl{http://www.baudline.com/solutions/full_duplex/index.html}{1bau}.
\eApplication

\bApplication \textbf{Using \ci{rat} in {\matlab} to find the period of discrete-time sinusoids}.
\label{app:rat_in_matlab}
The {\matlab} function \ci{rat} for rational fraction approximation can be used for finding $m$ and $N$.  But care must be exercised because \ci{rat} approximates the input argument within a given tolerance. The code below illustrates how this function can be used to obtain $m$ and $N$:
\begin{lstlisting}
w=3*pi/5 %define some angular frequency (rad)
[m,N]=rat(w/(2*pi)) %find m and N
\end{lstlisting}
In this case, the result is \co{m=3, N=10}, as expected. However, note that \ci{w=0.2,[m,N]=rat(w/(2*pi))} returns \co{m=113, N=3550}, which is not precise (recall that if $\dw=0.2$ the sinusoid is non-periodic). Modifying the previous command to use a smaller tolerance \ci{w=0.2,[m,N]=rat(w/(2*pi),1e-300)} gives much larger values for \co{m,N}, which clearly indicates that the user must be aware that \ci{rat} uses approximations.
Make sure you can generate discrete-time sinusoids with distinct values of $m$ and $N$ and understand the roles played by these two values.
\eApplication

\bApplication \textbf{Power of the sum of two signals}.
\label{app:power_of_sum_signals}
Assume a signal $z[n]=x[n]+y[n]$ is generated by summing two real signals (similar result can be obtained for complex-valued signals) $x[n]$ and $y[n]$ with power $\calP_x$ and $\calP_y$. The question is: What is the condition for having $\calP_z = \calP_x + \calP_y$?

Assuming the two signals are random and using expected values (a similar result would hold for deterministic signals):
\begin{equation}
\calP_z = \ev[\rvz^2] = \ev[(\rvx+\rvy)^2] = \calP_x + \calP_y + 2 \ev[\rvx \rvy].
\label{eq:power_of_sum_correlated}
\end{equation}

If $\rvx$ and $\rvy$ are uncorrelated, i.\,e., $\ev[\rvx \rvy]=\ev[\rvx]\ev[\rvy]$ and at least one signal is zero-mean, \equl{power_of_sum_correlated} simplifies to
\begin{equation}
\calP_z = \calP_x + \calP_y.
\label{eq:power_of_sum_uncorrelated}
\end{equation}

This is a useful result for analyzing communication channels that model the noise as additive.
%as, e.\,g., in \equl{snr_sum_uncorrelated}. 
These models assume the noise is uncorrelated to the transmitted signal and \equl{power_of_sum_uncorrelated} applies.
\eApplication

\bApplication \textbf{Estimate the PDF of speech signals.} Via a normalized histogram, estimate the PDF of a speech signal with a long duration. After this estimation, you should convince yourself that uniform quantizers are not adequate for speech signals. In fact, when using a non-linear quantizer based on the A-law or $\mu$-law, it is possible to use only 8 bits to achieve the subjective quality of a linear PCM with 12 bits.
Observe whether or not your histogram approaches a Laplacian density, as suggested by previous research in speech coding.
\eApplication

\bApplication \textbf{A simple application of correlation analysis}.
\label{app:correlationBeautyCream}
A company produces three distinct beauty creams: A, B and C. The task is the analysis of correlation in three  databases, one for each product. The contents of each database can be represented by two vectors $\bx$ and $\by$, with 1,000 elements each.  Vector $\bx$ informs the age of the consumer and $\by$ the number of his/her purchases of the respective cream (A, B or C) during one year, respectively. \figl{examplecorrelations} depicts scatter plots corresponding to each product.

\begin{figure}
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{Figures/examplecorrelations}		
	\caption[Scatter plot of customer age versus purchased units for three products.]{Scatter plot of customer age versus purchased units for three products. These two variables present positive correlation for product B, negative for C and are uncorrelated for product A.\label{fig:examplecorrelations}}
\end{figure}

The empirical (the one calculated from the available data) covariance matrices and means were approximately the following:
$
\bC_a=
\begin{bmatrix}
    4.08 & -0.002 \\[-1ex]
    -0.002 & 0.98 \\
    \end{bmatrix}
$
and
$\mu_a = [30.0,    6.05]$,
$
\bC_b=
\begin{bmatrix}
    4.00 & 0.99 \\[-1ex]
    0.99 & 1.00 \\
    \end{bmatrix}
$
and
$\mu_b = [30.1,   12.0]$,
$
\bC_c=
\begin{bmatrix}
    4.16 & -1.46 \\[-1ex]
    -1.46 & 2.02 \\
    \end{bmatrix}
$
and
$\mu_c = [30.13,   7.99]$. The correlation coefficients are $\rho_a=-0.0011$, $\rho_b=0.4924$ and $\rho_c=-0.5031$.
      
The plots and correlation coefficients indicate that when age increases, the sales of product B also increases (positive correlation). In contrast, the negative correlation of $\rho_b$ indicates that the sales of product C decreases among older people. The sales of product A seem uncorrelated with age. The script \ci{figs\_signals\_correlationcoeff.m} allows to study how the data and figures were created. Your task is to learn how to generate two-dimensional Gaussians with arbitrary covariance matrices.

Note that the correlation analysis was performed observing each product sales individually. You can assume the existence of a unique database, where each entry has four fields: age, sales of A, B and C. What kind of analysis do you foresee? For example, one could try a marketing campaign that combines two product if their sales are correlated. Or even use data mining tools to extract association rules that indicate how to organize the marketing.
\eApplication

\bApplication \textbf{Playing with the autocorrelation function of white noise and sinusoids.}
\label{app:autocorrelationWhiteSines}
Using \ci{randn} in {\matlab}, generate a vector corresponding to a realization of a WGN process (see \exal{autocorrelationWhite}): \ci{x=randn(1,1000)}. Check whether or not it is
Gaussian by estimating the FDP (use \ci{hist}). Plot its
autocorrelation with the proper axes. Generate a new signal that is uniformly distributed: \ci{y=rand(1,1000)-0.5;} and
plot the same graphs as for the Gaussian signal. What does it
happen with the autocorrelation if you add a DC level (add a
constant to \ci{x} and \ci{y})? And what if you multiply by a number (a
``gain'')? Generate a cosine \ci{T=0.01; t=0:T:10-T;
z=cos(2*pi*10*t);} of 10 Hz with a sampling frequency of 100 Hz.
Take a look at the autocorrelation for lags from $m=-30,\ldots,30$ with
\ci{[c,lags]=xcorr(z,30,'biased');plot(lags,c)}. Compare this last
plot with a zoom of the cosine: \ci{plot(z(1:30))}. Note that they
have the same period. In fact, an autocorrelation $R$ of $x$
incorporates all the periodicity that is found in $x$ as indicated by \equl{sinusoidcorrelation}.
Make sure you can use \equl{sinusoidcorrelation} to predict the plots you obtain with \ci{xcorr} when the signal is a sinusoid.
\eApplication

\bApplication \textbf{Using autocorrelation to estimate the cycle of sunspot activity}.
The international sunspot number (also known as the Wolfer number) is a quantity that simultaneously measures the number and size of sunspots. A sunspot is a region on the Sun's surface that is visible as dark spots. 
The number of sunspots correlates with the intensity of solar radiation: more sunspots means a brighter sun. 
This number has been collected and tabulated by researchers for around 300 years. They have found that sunspot activity is cyclical and reaches its maximum around every 9.5 to 11 years (in average, 10.4883 years).\footnote{See, e.\,g., \akurl{http://www.nationmaster.com/encyclopedia/Sunspots}{1sun} for more information. Note also that Matlab has a script \ci{sunspots.m} that works in the frequency domain.}
The autocorrelation can provide such estimate as indicated by the script below. Note that we are not interested in $R(0)$, which is always the maximum value of $R(\tau)$. The lag of the largest absolute value of $R(\tau)$ other than $R(0)$ indicates the signal fundamental period. Because theoretically no other value can be larger than $R(0)$, the task of automatically finding the second peak (not the second largest sample), which is the one of interest, is not trivial. The code snippet below simply (not automatically) indicates the position of the second peak for the sunspot data.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_signals\_peak\_detection}{snip_signals_peak_detection}
%\begin{lstlisting}
%load sunspot.dat; %the data file
%year=sunspot(:,1); %first column
%wolfer=sunspot(:,2); %second column
%%plot(year,wolfer); title('Sunspot Data') %plot raw data
%x=wolfer-mean(wolfer); %remove mean
%[R,lag]=xcorr(x); %calculate autocorrelation
%plot(lag,R); hold on;
%index=find(lag==11); %we know the 2nd peak is lag=11
%plot(lag(index),R(index),'r.', 'MarkerSize',25);
%text(lag(index)+10,R(index),['2nd peak at lag=11']);
%\end{lstlisting}
\figl{sunspotautocorrelation} shows the graph generated by the companion script \ci{figs\_signals\_correlation.m}. It complements the previous code snippet, showing how to extract the second peak automatically (this can be useful in other applications of the ACF). Your task is to study this code and get prepared to work with ``pitch'' estimation in Application~\ref{app:pitch}.

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/sunspotautocorrelation}		
	\caption{Autocorrelation of the sunspot data.\label{fig:sunspotautocorrelation}}
\end{figure}

An important aspect of the sunspot task is the interpretation of $R(\tau)$. As discussed, when the autocorrelation has a peak, it is an indication of high similarity, i.\,e., periodicity.
In the sunspot application, the interval between two lags was one year. If the ACF is obtained from a signal sampled at $\fs$ Hz, this interval between lags is the sampling period $\ts=1/\fs$ and it is relatively easy to normalize the lag axis. The next example illustrates the procedure.
\eApplication

\bApplication \textbf{Using autocorrelation to estimate the ``pitch''}.
\label{app:pitch}
This application studies a procedure to record speech, estimate the average fundamental frequency $F0$ (also erroneously but commonly called \emph{pitch}) via autocorrelation and play a sinusoid with a frequency proportional to $F0$. 

One can estimate the fundamental frequency of a speech signal by looking for a peak in the delay interval corresponding to the normal pitch range in speech.\footnote{Adult male speakers have a pitch range typically in the range [110,~130] Hz, while females have pitch in [200,~230] Hz. You may get an idea if you like Hollywood: it has been said that Sean Connery, Mel Gibson, Barbra Streisand and Julia Roberts have an average pitch of 158, 108, 228 and 171 Hz, respectively.} The following script illustrates the procedure.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_fundamental\_frequency}{snip_signals_fundamental_frequency}
%\begin{lstlisting}
%Fs=44100; %sampling frequency
%Ts=1/Fs; %sampling interval
%minF0Frequency=80; %minimum F0 frequency in Hz
%maxF0Frequency=300; %minimum F0 frequency in Hz
%minF0Period = 1/minF0Frequency; %correponding F0 (sec)
%maxF0Period = 1/maxF0Frequency; %correponding F0 (sec)
%Nbegin=round(maxF0Period/Ts);%number of lags for max freq.
%Nend=round(minF0Period/Ts); %number of lags for min freq.
%if 0 %record sound or test with 300 Hz cosine
%    r = audiorecorder(Fs, 16, 1);%object audiorecorder
%    disp('Started recording. Say a vowel a, e, i, o or u')
%    recordblocking(r,2);%record 2 s and store in object r
%    disp('finished recording');
%    y=double(getaudiodata(r, 'int16'));%get recorded data
%else
%    y=cos(2*pi*300*[0:length(y)-1]*Ts); %test with cosine
%end
%subplot(211); plot(Ts*[0:length(y)-1],y);
%xlabel('time (s)'); ylabel('Signal y(t)')
%[R,lags]=xcorr(y,Nend,'biased'); %ACF with max lag Nend
%subplot(212); %autocorrelation with normalized abscissa
%plot(lags*Ts,R); xlabel('lag (s)');
%ylabel('Autocorrelation of y(t)')
%firstIndex = find(lags==Nbegin); %find index of lag
%Rpartial = R(firstIndex:end); %just the region of interest
%[Rmax, relative_index_max]=max(Rpartial);
%%Rpartial was just part of R, so recalculate the index:
%index_max = firstIndex - 1 + relative_index_max;
%lag_max = lags(index_max); %get lag corresponding to index
%hold on; %show the point:
%plot(lag_max*Ts,Rmax,'xr','markersize',20);
%F0 = 1/(lag_max*Ts); %estimated F0 frequency (Hz)
%fprintf('Rmax=%g lag_max=%g T=%g (s) Freq.=%g Hz\n',...
%    Rmax,lag_max,lag_max*Ts,F0);
%t=0:Ts:2; soundsc(cos(2*pi*3*F0*t),Fs); %play freq. 3*F0
%\end{lstlisting}

\figl{sinusoidautocorrelation} was generated using the previous script with the signal $y(t)$ consisting of a cosine of 300 Hz instead of digitized speech (simply change the logical condition of the ``if'').

\begin{figure}
	\centering
		\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/sinusoidautocorrelation}		
	\caption[Autocorrelation of a cosine of 300 Hz]{Autocorrelation of a cosine of 300 Hz. The autocorrelation is also periodic. The period in terms of lags is 1/300 s.\label{fig:sinusoidautocorrelation}}
\end{figure}
The code outputs the following results:\\
\co{Rmax=0.49913 lag\_max=147 T=0.00333333 (sec) Frequency=300 Hz}\\
Note that the autocorrelation was normalized \ci{xcorr(y,Nend,'biased')}, which led to $R(0) \approx 0.5$ Watts, coinciding with the sinusoid power $A^2/2$, where $A=1$ V is the sinusoid amplitude.

As commonly done, in spite of dealing with discrete-time signals, the graphs assume the signals are approximating a continuous-time signal and ACF. Hence, the abscissa is $t$, not $n$.

Some PC sound boards heavily attenuate the signals around 100 Hz. Therefore, the last command multiplies the estimated $F0$ by 3, to provide a more audible tone. Modify the last line of the code to use $F0$ instead of $3 F0$ and observe the result of varying F0 with your own voice. 
Then try to improve the code to create your own F0 estimation algorithm.
Find on the Web a {\matlab} F0 (or pitch) estimation algorithm to be the baseline (there is one at
\akurl{http://www.ee.ucla.edu/~spapl/shareware.htm}{1pit}) and compare it with your code. Use the same input files for a fair comparison and superimpose the pitch tracks to spectrograms to better compare them. If you enjoy speech processing, try to get familiar with Praat~\akurl{http://www.fon.hum.uva.nl/praat}{1pra} and similar softwares and compare their F0 estimations with yours.
\eApplication 

\bApplication \textbf{Using cross-correlation for synchronization of two signals or time-alignment}.
\label{app:crosscorrelation}
Assume a discrete-time signal $x[n]$ is transmitted through a communication channel and the receiver obtains a delayed and distorted version $y[n]$. The task is to estimate the delay imposed by the channel. The transmitter does not ``stamp'' the time when the transmission starts, but uses a predefined preamble sequence $p[n]$ that is known by the receiver. The receiver will then guess the beginning of the transmitted message by searching for the preamble sequence in $y[n]$ via cross-correlation. Before trying an example that pretends to be realistic, some simple manipulations can clarify the procedure.

Assume we want to align the signal $x[n]=\delta[n]+2\delta[n-1]+3\delta[n-2]$ with $y[n]=3\delta[n]+2\delta[n-1]+\delta[n-2]+\delta[n-3]+2\delta[n-4]+2\delta[n-5]$. Intuitively, the signal $x[n-3]$ is a good match to $y[n]$. Alternatively, $y[n+3]$ matches $x[n]$. The {\matlab} command \ci{xcorr(x,y)} for cross-correlation can help finding the best lag $L$ such that $x[n+L]$ matches $y[n]$. The procedure is illustrated below:
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_cross\_correlation}{snip_signals_cross_correlation}
The result is $L=-3$. If the order is swapped to \ci{xcorr(y,x)} as below, the result is $L=3$.
\begin{lstlisting}
[c,lags]=xcorr(y,x); 
max(c) %show the maximum cross-correlation value
maxlag=lags(find(c==max(c))) %max cross-correlation y,x lag
\end{lstlisting}

It should be noticed that the cross-correlation is far from perfect with respect to capturing similarity between waveforms. For example, if $y[n]$ is changed to \ci{y=[(4:-1:1) x]}, the previous commands would indicate the best lag as $L=1$. The reader is invited to play with simple signals and find more evidence of this limitation.
As a rule of thumb, the cross-correlation will work well if one of the signals is a delayed version of the other, without significant distortion. However, in situations such as reverberant rooms where one of the signals is composed by a sum of multi-path (with distinct delays) versions of the other signal, more sophisticated techniques should be used.

Another aspect is that, in some applications, the best similarity measure is the absolute value of the cross-correlation (i.\,e., \ci{L = lags(find(abs(c)==max(abs(c))))} instead of \ci{L = lags(find(c==max(c)))}). For example, this is the case when $x[n]$ can be compared either to $y[n]$ or $-y[n]$.

\codl{snip_signals_time_delay} illustrates the delay estimation between two signals $x[n]$ and $y[n]$. The vector \ci{y}, representing $y[n]$, is obtained by delaying \ci{x} and adding Gaussian noise to have a given SNR.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_time\_delay}{snip_signals_time_delay}
%\begin{lstlisting}
%Fs=8000; %sampling frequency
%Ts=1/Fs; %sampling interval
%N=1.5*Fs; %1.5 seconds
%t=[0:N-1]*Ts;
%if 1
%    x = rand(1,N)-0.5; %zero mean uniformly distributed
%else
%    x = cos(2*pi*100*t); %cosine
%end
%delayInSamples=2000;
%timeDelay = delayInSamples*Ts %delay in seconds
%y=[zeros(1,delayInSamples) x(1:end-delayInSamples)];
%SNRdb=10; %specified SNR
%signalPower=mean(x.^2);
%noisePower=signalPower/(10^(SNRdb/10));
%noise=sqrt(noisePower)*randn(size(y));
%y=y+noise;
%subplot(211); plot(t,x,t,y);
%[c,lags]=xcorr(x,y); %find crosscorrelation
%subplot(212); plot(lags*Ts,c);
%%find the lag for maximum absolute crosscorrelation:
%L = lags(find(abs(c)==max(abs(c)))); 
%estimatedTimeDelay = L*Ts
%\end{lstlisting}

\figl{delayestimation} illustrates the result of running the previous code. The random signals have zero mean and uniformly distributed samples. The estimated delay via \ci{xcorr(x,y)} was -0.25 s. The negative value  indicates that \ci{x} is advanced with respect to \ci{y}. The command \ci{xcorr(y,x)} would lead to a positive delay of 0.25 s.

\begin{figure}
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{Figures/delayestimation}		
	\caption[{First graph shows signals $x(t)$ and $y(t-0.25)$ contaminated by AWGN at a SNR of 10 dB.}]{First graph shows signals $x(t)$ and $y(t-0.25)$ contaminated by AWGN at an SNR of 10 dB. The signal $y(t)$ can be identified by a smaller amplitude in the beginning, where its samples are due to noise only. The second graph shows their cross-correlation $R_{XY}(\tau)$ indicating the delay of -0.25 s. \label{fig:delayestimation}}
\end{figure}

After running the code as it is, observe what happens if \ci{x=rand(1,N)}, i.\,e., use a signal with a mean different than zero (0.5, in this case). In this case, the correlation is affected in a way that the peak indicating the delay is less pronounced. Another test is to use a cosine (modify the \ci{if}) with \ci{delayInSamples} assuming a small value with respect to the total length \ci{N} of the vectors. The estimation can fail, indicating the delay to be zero. Another parameter to play with is the SNR. Use values smaller than 10 dB to visualize how the correlation can be useful even with negative SNR.

It is important to address another issue: comparing vectors of different length. Assume two signals $x[n]$ and  $y[n]$ should be aligned in time and then compared sample-by-sample, for example to calculate the error $x[n]-y[n]$. There is a small problem for {\matlab} if the vectors have a different length.
Assuming that \ci{xcorr(x,y)} indicated the best lag is positive ($L>0$), an useful post-processing for comparing $x[n]$ and $y[n]$ is to delete samples of $x[n]$. If $L$ is negative, the first samples of $y[n]$ can be deleted. \codl{snip_signals_time_aligment} illustrates the operation and makes sure that the vectors representing the signals have the same length.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_signals\_time\_aligment}{snip_signals_time_aligment}
%\begin{lstlisting}
%x=[1 -2 3 4 5 -1]; %some signal
%y=[3 1 -2 -2 1 -4 -3 -5 -10]; %the other signal
%[c,lags]=xcorr(x,y); %find crosscorrelation
%L = lags(find(abs(c)==max(abs(c)))); %lag for the maximum
%if L>0
%	x(1:L)=[]; %delete first L samples from x
%else
%	y(1:-L)=[]; %delete first L samples from y
%end
%if length(x) > length(y) %make sure lengths are the same
%	x=x(1:length(y));
%else
%	y=y(1:length(x));
%end
%plot(x-y); title('error between aligned x and y');
%\end{lstlisting}
Elaborate and execute the following experiment: record two utterances of the same word, storing the first in a vector \ci{x} and the second in \ci{y}. Align the two signals via cross-correlation and calculate the mean-squared error (MSE) between them (for the MSE calculation it may be necessary to have vectors of the same length, as discussed).
\eApplication


\section{Comments and Further Reading}

Signal processing books sometimes target two distinct audiences. The most introductory treatments are typically adopted for ``Signals and Systems'' courses, while more advanced textbooks target ``Digital Signal Processing'' courses.
This division can be observed in the two classic books \cite{Oppenheim96} and \cite{Oppenheim09}. It is also used in the Schaum's Outline Series (well-known for the many fully solved problems).

Regarding the name digital or discrete-time: the jargon is such that people call digital signal processing (DSP)\index{DSP} many operations that should be considered discrete-time signal processing given the amplitude is not quantized.\footnote{This is a possible reason for the change in the book title ``Digital signal processing''~\cite{Oppenheim75} to ``Discrete-time signal processing''~\cite{Oppenheim09}.} 

There are many great books on DSP and three of them are (always check for new editions): \cite{Diniz10,Lyons10,Mitra10}. Youtube has several channels with videolectures about signal processing such as
the ones by Prof. Barry Van Veen~\akurl{https://www.youtube.com/user/allsignalprocessing}{1you}.

%Because it is not wise to fight against a well established jargon and.
%\footnote{In France, for example, DSP has been called ``Traitement num\'erique du signal''. As a consequence, in March 2008, the attendant of an Air France international flight said in English they would show movies in a ``numerical video on-demand'' system, instead of digital video.}

When reading DSP and telecommunication books, keep in mind that the taxonomy of signals adopted here is not the only one used. For example, books such as~\cite{Peebles86,Lathi98} define digital signals as the ones with quantized amplitudes. Their alternative definition considers that $x_q(t)$, a continuous-time signal, is digital because of its quantized amplitudes.

Topics such as sampling and quantization have been widely investigated.
The sampling theorem is part of a broad area. Only periodic sampling is discussed here. There are many other sampling theorems, addressing issues such as non-uniform sampling and non-bandlimited signals. See, e.\,g.,~\cite{Marvasti01,Benedetto01}. In~\cite{Peebles86} a whole chapter is dedicated to sampling. As a side note: the sampling theorem is related to the work by Harry Nyquist and, therefore, called Nyquist theorem by some authors. 
%However, Nyquist did not explicitly consider the problem of sampling and reconstruction of continuous signals. 
However, the credits for the theory related to the sampling theorem should also go to C. Shannon, V. Kotelnikov, E. Whittaker and others. See, for example,~\cite{Butzer92,Luke99}, for historical information. 

As discussed in \cite{Candan21}, to be more pedagogical, signal processing textbooks typically do not adopt rigorous mathematical treatment of continuous-time impulses. This was the approach adopted in this text.

Quantization is also part of a vast area known as source coding. A classical and good book is~\cite{Jayant84}. A more recent book is~\cite{Anderson07}.

Regarding quantization, it is important to warn the experienced reader that in this text, unless otherwise stated (e.\,g., when discussing PAM decoding), it is assumed the quantizers are uniform and \emph{mid-tread}. Non-uniform and \emph{mid-riser} quantizers (see~\cite{Jayant84}) are not discussed. 


%Also, the quantizers are assumed to reserve one output level to represent a zero output. These assumptions are sensible because this kind of quantizer is found in the vast majority of the ADCs.

%The unit adopted for $\ev[ \rvx^2]$ is sometimes Joule instead of Watt (see, e.\,g.,~\cite{Ciofficn}). This is related to the model used for the D/A conversion. Section~\ref{app:dc_conversion} will discuss this issue.

%\ignore{
%Links to the electronics of sound cards and jacks:
%http://www.hobby-hour.com/electronics/computer_microphone.php
%http://en.kioskea.net/contents/pc/carte-son.php3
%http://en.wikipedia.org/wiki/TRS_connector
%http://en.wikipedia.org/wiki/Sound_card
%http://www.epanorama.net/documents/pc/soundcard_tips.html
%}

