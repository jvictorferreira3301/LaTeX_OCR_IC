\section{To Learn in this Chapter}

%This chapter discusses:
\begin{itemize}
\item Basic channel models and an introduction to the techniques used to mitigate the channel effects.
\item Nyquist pulses such as the raised cosine for combating intersymbol interference (ISI). A shaping pulse is called a \emph{Nyquist pulse}\index{Nyquist pulse} when it leads to zero-ISI.
\item Equalization and system identification.
\item Concepts related to synchronization techniques.
\end{itemize}


%This section concerns two situations:
%\begin{itemize}
%\item How to represent the signals when simulating a communication system, especially with respect to the choice of sampling frequency and oversampling factor.
%\item Given a digitized communication signal, how to analyze it.
%\end{itemize}

%
%
%AK-LATER Example using same constellation but different basis functions, gives the same result (as in Cioffi).
%
%Typically, the Tx is easier to implement than the Rx. This is the case for a PAM system.
%This section starts the discussion about the Rx.
%

%\section{Noise and Interference}


%\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/awgn_channel_continuous_time_p2}
%\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/flat_channel}

In practice, communication systems have to deal with several impairments: noise, interference, channel fading, etc. As discussed in Section~\ref{sec:sinr}, while \emph{noise}\index{Noise} is the term used for nature-made signals such as thermal noise, \emph{interference}\index{Interference} is typically adopted to describe signals created by humans but other than the signal(s) of interest. For example, in a binder of copper pairs used for DSL, one user receives interference signals from other users due to crosstalk.

%There are several effective techniques to mitigate interference. For example, in DSL, it is possible to mitigate the mentioned crosstalk signals using MIMO techniques. 

In most channel models, added to the interference, there is thermal (or background) noise, which is %harder to mitigate and 
typically modeled as AWGN, for example, with a unilateral PSD of $\no = -140$~dBm/Hz.

Interference is not going to be considered here, and this chapter aims at studying how to deal with a channel that: 
\begin{itemize}
\item implements a linear time-invariant (LTI) or time-variant  filter $h_c(t)$ that may alter the spectrum of the transmitted signal $s(t)$,
\item adds noise to the signal $g(t)$ at the output of $h_c(t)$ such as white Gaussian noise (WGN).
\end{itemize}

\section{Channel Models}
\label{sec:channelModels}

%\subsection{Features of additive white Gaussian noise (AWGN) channels}
%\label{sec:awgn}


The WGN model is based on two assumptions: a) the noise \textbf{amplitudes} are distributed according to a \textbf{Gaussian} and b) the noise \textbf{PSD} is ``\textbf{white}'', i.\,e., its power is equally distributed over all frequencies such that the PSD is flat. Gaussian does not imply white nor vice-versa. It is possible to have a Poisson white noise, for example, or a non-white (``\emph{colored}'') Gaussian noise.\index{Colored noise}
AWGN models add an extra assumption: the WGN noise is \textbf{added} to a given signal of interest.

The acronym AWGN denotes the \textbf{channel} but it is sometimes used to denote the \textbf{noise} signal itself. In other words, if WGN is added to the received signal, it may be called ``AWGN signal''. However, it is more pedagogical to reserve the acronym AWGN to denote the channel model. 

AWGN has been already briefly presented in \exal{AWGNchannelIntroduction}.
Before further discussing AWGN channels, the more general \emph{LTI Gaussian channel} is the subject of the next section.
% in the context of AWGN channel.
%This special channel is detailed in the next section.

\subsection{LTI Gaussian channel}

In practice, the channels have rather complicated responses (non-linear, time-varying, etc.). Fortunately, for many applications, a simple LTI model followed by AWGN provides a good match to the actual system under certain conditions. The combined effect of a LTI system denoted by its impulse response $h_c(t)$ with WGN $\nu(t)$ added to the LTI system output leads to a received signal given by
\begin{equation}
r(t) = s(t) \conv h_c(t) + \nu(t),
\label{eq:filtered_awgn_channel}
\end{equation}
which is depicted in \figl{frequency_selective_channel} and is called here \emph{LTI Gaussian channel}\index{LTI Gaussian channel}. 
This channel is called \emph{frequency-selective} channel\index{Frequency-selective channel} when its frequency response $H_c(f)=\calF \{ h_c(t) \}$ varies with $f$
Another key aspect of the LTI Gaussian channel is that, because it is time-invariant, it is reasonable to eventually assume that it is known by both transmitter and receiver.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresTex/frequency_seletive_channel}
\caption{Block diagram of a LTI Gaussian channel.\label{fig:frequency_selective_channel}}
\end{figure}

Typically the channel model of \figl{frequency_selective_channel} can be classified according to the following features of the channel: a) equivalent to a lowpass or bandpass filter, b) 
ideal ``brick-wall'' magnitude flat-frequency response or \emph{frequency-selective} (non-flat) filter\index{Frequency-selective system} and c) WGN at its output or not. The possible combinations of these three binary features 
%lead to $2^3=8$ possible combinations and 
require remedies that are widely used in digital communications, as illustrated in \tabl{channel_medicines}.

\begin{table}
\centering
\caption{Some channel features related to \equl{filtered_awgn_channel} and associated remedies.\label{tab:channel_medicines}}
\begin{tabular}{|l|c|}
\hline
\emph{Channel feature} & \emph{Remedy}\\ \hline
Flat (in frequency) over BW & Concentrate transmit signal spectrum within BW\\ \hline
Non-flat & Use equalizer  or multicarrier modulation \\ \hline
AWGN & Use matched filter to maximize SNR \\ \hline %and eliminate out-of-band frequencies 
Bandpass & Use frequency upconversion to locate signal within BW\\ \hline
\end{tabular}
\end{table}

Assuming $h_c(t)$ is an ideal bandpass channel with center frequency $f_c$, \tabl{channel_medicines} indicates that the transmit signal should also have its spectrum approximately centered at $f_c$ by using frequency upconversion. 

When $h_c(t)$ has a flat magnitude frequency response as, for example, the ideal lowpass in \figl{lowpass_spec}, it does not require equalization but most of the power of the transmit signal PSD should be confined within the channel bandwidth (BW). However, even with a flat magnitude, if $h_c(t)$ has a nonlinear phase, it may be still necessary to perform equalization. Note that \tabl{channel_medicines} just indicates some of the possible remedies. For example, multicarrier transmission such as OFDM has been widely used to avoid the conventional equalizers in non-flat (or frequency-selective) channels. As will be discussed in Chapter~\ref{ch:multicarrier}, the idea in OFDM is to divide the non-flat channel in many small frequency bands (sometimes called ``subchannels'') that can be considered approximately flat within their respective (small) bandwidth and, therefore, avoid complicated equalization (in fact, a 1-tap filter per subchannel is often used as equalizer in OFDM).

Whenever the channel has ideal frequency response (flat magnitude and linear phase) but is bandlimited, the channel $\BW$ limits the maximum symbol rate $\rsym$ for achieving zero ISI, as will be discussed in this chapter. It should be noted that, even if $\rsym$ is bounded by a maximum value, if there was no noise ($\nu(t)=0$ in \figl{frequency_selective_channel}), the number $b$ of bits could be increased by using constellations with smaller spacing among symbols, and the rate $R = \rsym b$ could be made as large as desired.\footnote{It is also possible to increase $R$ by using
higher transmit power but there is often a practical limit.}
 Hence, generally speaking, $\BW$ limits $\rsym$, while the WGN power limits $b$.

%For example, assuming $\fs=8$~kHz in \exal{qam_modulation} in Chapter~\ref{ch:transforms}, $\tsym = 0.004$ seconds, $\rsym=250$ bauds and $R=1$ kbps.

%During the study of a), the important Nyquist criterion is discussed. For the lowpass channel, PAM is used, while for the bandpass channel, QAM is discussed. The PSD of line codes is investigated as a related topic. When studying AWGN in b), equations for the theoretical probability of error are derived and the matched filter is presented as the optimal solution in the AWGN case. The concepts of MLE and MAP decoding are presented.
%
%%First, some motivation is presented.
%The previous section provided an introduction based solely on the transmitter of a binary ($M=2$ symbols) digital communication.
%This section provides an introduction to digital communication and defines the nomenclature.
%

%\ignore{
%Therefore, the adopted approach for studying the Rx is to gradually increase the level of difficulty. First the channel will simply add white Gaussian noise (AWGN channel). Besides, the adopted approach also concentrates on PAM. Important concepts such as matched filtering and calculating the error probability are presented in the specific case of PAM. Later, in Section~\ref{sec:modulation_schemes}, other modulation schemes are discussed and it will be shown that the fundamental concepts developed for PAM also apply.

%
%There are many models and techniques adopted in digital communication. The approach used here is to focus on baseband systems and first study the noiseless ($n(t)=0$) and unlimited-bandwidth ($h_c(t)=\delta(t)$). 
%In fact, Application~\ref{app:InterpretingDigitalModulation} in Chapter~\ref{ch:transforms} already developed 
%
%After practicing how to apply this theory to digital communication, the text focuses on evaluating the performance on AWGN and how to minimize ISI due to the limited bandwidth of the channel $h_c(t)$.

%Before that, oversampling is discussed.

%\section{Oversampling Factor and Number of Samples to Represent a Signal}
%\label{sec:oversampling}

Assuming \equl{filtered_awgn_channel} specifies the channel of interest, it is pedagogical to study the effect of filtering and AWGN separately.
Note that the optimal strategy may very well be to jointly combat the noise $\nu(t)$ and the non-idealities of $h_c(t)$. 
%However, it is much simpler, and often adopted in practice, to study them individually. 

%\section{Distortionless Channel}

By assuming that $h_c(t)$ is ideal, one can derive the \emph{matched filter} as the optimal receiver for AWGN. Similarly, by discarding the influence of $\nu(t)$, one can design pulses to combat ISI due to the channel \emph{dispersion}. Or in the case of a frequency-selective channel, one can assume an ideal equalizer can perfectly compensate the channel and use an AWGN model to study the system under this assumption of perfect equalization.

\subsection{Flat-fading channel}

A special case of the LTI Gaussian channel is the so-called \emph{flat-fading channel}\index{Flat-fading channel}, depicted in \figl{flat_channel}. This simpler channel is completely described by three parameters: $\kappa$, $\phi$ and $t_0$, which are the channel gain, phase and delay, respectively. Assuming, for example, that $\kappa$ is a random variable with values that change over time allows to model effects that occur in wireless communications, for example.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresTex/flat_channel}
\caption{Flat-fading channel block diagram.\label{fig:flat_channel}}
\end{figure}

As depicted in \figl{flat_channel}, the flat-fading impulse response is $h_c(t)=\kappa e^{j \phi} \delta(t-t_0)$. Hence, the output $g(t)$ is related to the channel input $s(t)$ as
\begin{equation}
g(t) = \kappa e^{j \phi} s(t-t_0)
\label{eq:flatFadingOutput}
\end{equation}
and a simple scalar gain $1/(\kappa e^{j \phi})$ can be used as an equalizer for this channel.

The following sections discuss AWGN models, which are simpler than the flat-fading because the impulse response in \figl{frequency_selective_channel} can be modeled as $h_c(t) = \delta(t)$ and, consequently, these channels have infinite bandwidth and are \emph{memoryless}\index{Memoryless channel}.

\subsection{Discrete-time scalar AWGN}

There are AWGN versions for continuous and discrete-time processing.
The discrete-time AWGN channels can be further divided based on the fact that, at each channel use, a scalar (real or complex number) or a vector of dimension $D$ is transmitted. 
\figl{awgn_types} depicts some AWGN channels. An extra degree of freedom when modeling
the discrete-time channels is whether the numbers represent symbols (at $\rsym$) or samples (at $\fs$).

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/awgn_types}
\caption{Mind map with a possible taxonomy of AWGN channels.\label{fig:awgn_types}}
\end{figure}

Complex numbers, such as the symbols in QAM modulation, can be modeled either as scalars or vectors of dimension $D=2$. 
In other words, when $D=2$, the fourth channel type in \figl{awgn_types} is equivalent to the 
third. For convenience, complex numbers will be treated as scalars in the sequel.%, with vectors having dimension $D>2$.

The scalar AWGN channel adds a noise sample $\rvn$ to every transmitted scalar $x$, which can represent a sample or a symbol. A distinction between having samples or symbols is that the assumed time interval between consecutive values of the corresponding sequences is $\ts$ or $\tsym$, respectively. 

As mentioned, the input $x$ is a real or complex number, such that the channel output is $\rvr=x+\rvn$, where
%A complex symbol is useful in modulations such as QAM while a real symbol suffices for PAM, for example.
%When $x$ is a scalar, 
the random variable $\rvn$ is i.\,i.\,d. and distributed according to a Gaussian PDF $\calN(0,\sigma^2)$ with zero mean and variance $\sigma^2$. 

\bExample \textbf{Generating discrete-time real-valued scalar WGN}.
For example, a realization with 100 samples of a scalar discrete-time WGN can be easily obtained using {\matlab} with a command such as \ci{sigma2=0.5,N=100,noise=sqrt(sigma2)*randn(1,N)}, where $\sigma^2=0.5$~W is the noise power.
\eExample 

For a complex-valued $x$, both real and imaginary parts of $\rvn$ are often assumed i.\,i.\,d. and distributed according to $\calN(0,\sigma^2)$, such that the total power is $\ev[|\rvn|^2]=2 \sigma^2$. 
More strictly, in this case, $\rvn$ is a \emph{circularly-symmetric complex Gaussian}\footnote{See Appendix~\ref{sec:properCircularRV} for a brief discussion about circular random variables. \figl{wgnExpansionHistogram} motivates the term circularly-symmetric, given that the 2-d pdf is ``circular''.}% and invariant to a rotation.
\index{Circularly-symmetric Complex Gaussian} random variable distributed according to $\calC \calN(\bu,\Sigma)$ with zero mean $\bu=[0,0]^T$ and covariance matrix $\Sigma=\sigma^2 \bI$, where $\bI$ is a $2 \times 2$ identity matrix. 

\bExample \textbf{Generating discrete-time complex-valued WGN}.
\codl{snip_digi_comm_gaussian_noise} is an example of code\footnote{When generating the imaginary component of \ci{noise}, the value \ci{1j} was adopted instead of simply using \ci{j} because \ci{j} is often used as index and if previously defined with a value different than $\sqrt{-1}$, the simulation would go wrong. This is a potential problem when the code is large and it is always a good idea to use \ci{1j} or \ci{1i} to avoid this pitfall.} to generate discrete-time complex-valued WGN.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_gaussian\_noise}{snip_digi_comm_gaussian_noise}
%\begin{lstlisting}
%sigma2=0.5; %power of each component (real and imaginary)
%N=10000; %number of samples
%noise=sqrt(sigma2)*randn(1,N)+1j*sqrt(sigma2)*randn(1,N);
%mean(abs(noise).^2) %estimate the noise power
%\end{lstlisting}
Note that after running \codl{snip_digi_comm_gaussian_noise}, the resulting power is approximately $\ev[|\rvn|^2]=2 \sigma^2 = 1$~W, with 0.5~W per dimension.
\eExample

\subsection{Discrete-time vector AWGN}
\label{sec:vectorAWGNChannel}

As the name indicates, the vector AWGN channel adds a noise vector $\nuv$
to each transmitted vector $\bx$ (with symbols or samples as its elements), as depicted in \figl{awgn_vector_channel}.
%The channel simply adds noise ${\mathbf \nu}$ to $\bx$ (that are symbols or samples). 

\begin{figure}[htbp]
\centering
\includegraphics[width=5cm,keepaspectratio]{FiguresTex/awgn_vector_channel}
\caption{AWGN vector channel, where $\bs$, $\br$ and $\nuv$ are vectors of dimension $D$.}
\label{fig:awgn_vector_channel}
\end{figure}

%The model assumes that
%\[
%\bx \arrowedbox{vector AWGN channel} \br=\bx+ {\mathbf \nu},
%\]
%and the channel simply adds noise ${\mathbf \nu}$ to $\bx$ (that are symbols or samples). 
%In many cases, $\bx,\bn,\br \in \CC$ are complex (e.\,g., in QAM modulation) and represented by two-dimensional vectors.
The $D$ elements of vector $\nuv$ are i.\,i.\,d. random variables, each one distributed according to a Gaussian PDF with zero mean and variance $\sigma^2$. Hence, the average power is $\ev\{|\nuv|^2\} = D \sigma^2$.

As discussed in the context of signals (not vectors) in Application~\ref{app:power_of_sum_signals}, the output of the vector AWGN channel has power given by the sum of the powers of signal and the noise: 
\begin{equation}
\ev\{|\br|^2\}= \ev\{|\bs|^2\}+\ev\{|\nuv|^2\}
\label{eq:awgn_power}
\end{equation}
because $\bs$ and $\nuv$ are assumed to be orthogonal.

%\section{Representing Continuous-Time AWGN as Vector Channel}
%
%In order to estimate error probability and other metrics, it is of interest to convert waveform signal models into the vector model discussed in Section~\ref{sec:vectorAWGNChannel}. 
%%Hence, converting a continuous-time AWGN to a vector channel is particularly useful. 
%This representation is somehow similar, but should be distinguished from the one in 
%%Section~\ref{sec:discreteAndContinuousAWGN}, 
%Section~\ref{sec:conversion_awgn},
%which included an anti-aliasing filtering operation. Here, the development is based on finding a vector to represent the continuous-time WGN over the transmission of a single symbol via the waveform $s(t)$. 
%
%The transmit Note that is obtained from a set of orthonormal basis functions $\{\varphi(t)\}$. There is no filtering operation involved
%
%For example, assume the following block diagram with a received signal $r(t)$ contaminated by continuous-time AWGN:
%\begin{align*}
%&m[n] \arrowedbox{$p(t)$} \fbox{AWGN} \rightarrow r(t) \arrowedbox{$h_\textrm{Rx}(t)$} y(t)\ldots\\
%&\ldots \arrowedbox{sampling and normalization at each $\tsym$}  r[n].
%\end{align*}
%As illustrated, the signal $r(t)$ is then filtered and sampled, creating a random variable $r[n]$. Under some reasonable assumptions, it is possible to model $r[n]$ as being contaminated by a discrete-time AWGN $z[n]$ such that $r[n] = m[n] + z[n]$, where $m[n]$ is the original transmitted symbol. Hence, it is important to relate these two versions of AWGN as discussed in the sequel.
%
%%For example, when dealing with PAM systems, for which $D=1$, the equivalent AWGN is scalar, but when dealing for example with the $D=2$ dimensions of QAM, the equivalent model is a vector AWGN.
%
%%A random vector $\bn$ often represents the noise in the discrete-time AWGN channel that deals with vectors and is, therefore, called AWGN vector channel\index{Vector channel}.
%
%%This result states that when the continuous-time AWGN with bilateral PSD level $\no / 2$ is simultaneously filtered with $P$ \emph{orthonormal} functions and the output is sampled to form a vector $\bn$ of $P$ elements, the noise samples in $\bn$ are independent and have variance $\sigma^2=\no / 2$.
%%\footnote{Proof Eq. 1.49 in Cioffi, but I should change the limits of the integrals to $[0, T]$.} Proof is similar to the one used in Theorem~\ref{th:filteringAWGN}.
%
%%If $D$ is the dimension of the signal space (e.\,g., $D=1$ for PAM and $D=2$ for QAM), 
%%Note that continuous-time AWGN is inherently infinite dimensional ($D=\infty$).
%%When $D>1$, the noise energy per dimension is still $\overline{\sigma^2} = \frac{\sigma^2_{total}}{N} = \frac {\sum_{t=1}^N \sigma^2} N = \sigma^2 = \no / 2$. 
%
%%The sum in the numerator was used because the elements of the noise vector $\bn$ are uncorrelated. 

\subsection{Continuous-time AWGN channel}
\label{sec:continuousTimeAWGN}

The continuous-time AWGN channel can be represented as in \figl{awgn_channel_continuous_time_p1},
where the noise $\nu(t)$ is a realization of a WGN continuous-time WSS stochastic process.

\begin{figure}[htbp]
\centering
\includegraphics[width=5cm,keepaspectratio]{FiguresTex/awgn_channel_continuous_time_p1}
\caption{Continuous-time AWGN channel, which has the implicit assumption that $s(t)$ is both power and bandwidth-limited.\label{fig:awgn_channel_continuous_time_p1}}
\end{figure}

It is important to note that the continuous-time AWGN model assumes that the \textbf{input signal} $s(t)$ is both power and bandwidth-limited: its maximum power is $\calP_s$ and, in the baseband case, its support in frequency is from $-\BW$ to BW assuming a bilateral representation. Hence, an alternative view to 
the AWGN in \figl{awgn_channel_continuous_time_p1} is to explicitly indicate the restricted bandwidth as in \figl{awgnChannels}, which depicts baseband and bandpass versions.

\begin{figure}[htbp]
\centering
  \subfigure[Lowpass]{\label{fig:awgnChannelsBaseband}\includegraphics[width=5.5cm]{FiguresTex/awgnChannel2}}
    \subfigure[Bandpass]{\label{fig:awgnChannelsPassband}\includegraphics[width=6.5cm]{FiguresTex/awgnChannel3}}
  \caption[Lowpass (baseband) and bandpass continuous-time AWGN channels.]{Lowpass (baseband) and bandpass continuous-time AWGN channels with the restriction in bandwidth made explicit. The input
	signal $s(t)$ is not altered because it is bandwidth-limited.}
  \label{fig:awgnChannels}
\end{figure}

The continuous-time AWGN channels in \figl{awgnChannels} should not be confused with the more general case of the LTI Gaussian channel of \figl{frequency_selective_channel}. While, in \figl{frequency_selective_channel} any LTI system can be used, the systems in \figl{awgnChannels} must
be ideal and are depicted just in case one wants to make explicit that the input signal $s(t)$ is band-limited. In other words, two descriptions are equivalent:
\begin{enumerate}
	\item The AWGN corresponds to having an impulse response $h_c(t)=\delta(t)$ in \figl{frequency_selective_channel}. The channel has infinite bandwidth, but $s(t)$ has a limited bandwidth BW, or
	\item $h_c(t)$ is an ideal lowpass or bandpass filter, with a unitary gain (flat frequency response) and bandwidth BW, which does not alter the $s(t)$.
\end{enumerate}
In both cases, the essential fact is that the channel does not alter the band-limited $s(t)$ up to the point that it is added to $\nu(t)$.
 
The continuous-time AWGN model is very important for deriving theoretical results and, therefore, studying how to better implement practical systems. When using a computer to simulate a digital communication with DSP, the continuous-time AWGN is often mapped to a discrete-time AWGN model. This is discussed in Section~\ref{sec:conversion_awgn}. 

Note that, when simulating the AWGN channel using  discrete-time processing, its bandwidth is not greater than $\pi$~rad or, equivalently, $\fs/2$~Hz, which is an inherent limitation of discrete-time processing.
%All the discussion is based on PAM signals, which are discussed in the sequel.
Besides, when simulating AWGN, there are no synchronization issues (after the channel, it is remains well-defined where each symbol starts and ends) and there is no \emph{intersymbol interference} (ISI). 
%Even then, important concepts such as matched filtering and error probability calculations can be studied. 
This allows the transmission over AWGN to be treated as ``one-shot'', i.\,e., modeling the channel effect over a single symbol without concerns on the influence of neighbor symbols, given that there is no ISI (detailed in Section~\ref{sec:nyquistZeroISI}). A zero-ISI transmission basically means that one symbol does not impacts the demodulation of other symbols. This way, the optimal receiver is a \emph{symbol-by-symbol} detector that can make its decision based on a single use of the channel not on a sequence of uses (or sequence of symbols). On the other hand, when ISI exists, the decision may have to take in account a sequence of symbols and the optimum solution becomes more complicated, as discussed in Section~\ref{sec:mapMLESequence}.

%There are different situations where AWGN is used such as: a) continuous-time, b) discrete-time and c) multicarrier transmission. 
%The first two are described in this chapter while the latter is discussed in Chapter~\ref{ch:multicarrier}.
%AK-PUTBACK The latter will be discussed in Chapter~\ref{ch:multicarrier} while the other two are described in the sequel.

%in the case of PAM transmission scalar symbols are transmitted and the adequate model is a discrete-time scalar AWGN. Alternatively, if there are more than one dimensions involved as 
%QAM that has $N=2$ dimensions, a vector of dimension $N$ is transmitted at each channel use and the adequate model is an AWGN vector channel. %Each model will be discussed in the next subsections.

\subsection{LTI channel with non-white additive noise}

In some situations of practical interest, the additive noise at the receiver may have a
non-white PSD $S_{\nu}(f)$. For example, in DSL systems, the interference among distinct
lines is often significant at a given receiver and it has a non-white PSD. 
In such cases, if its amplitudes are distributed according to a Gaussian,
the noise is called \emph{additive Gaussian noise} (AGN)\index{Additive Gaussian noise (AGN)}.
%not AWGN because it does not have a flat PSD.

As discussed in Section~\ref{sec:arModelingPSD}, parametric spectral estimation techniques
can be used to estimate a filter $H(f)$ that, when excited by WGN, generates an output with a PSD 
that approximates $S_{\nu}(f)$. In some cases $H(f)$ can then be incorporated to the processing
chain, creating a new system with a white noise that is equivalent to the original 
system with non-white noise, as represented in \figl{nonWhiteNoise}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/nonWhiteNoise}
\caption{Two equivalent representations of non-white noise, where $H(f)$ is such that
outputs a signal with PSD $S_{\nu}(f)$ when its input is WGN.\label{fig:nonWhiteNoise}}
\end{figure}

It is possible to transform the system in \figl{nonWhiteNoise} into an equivalent AWGN system
by moving the filter $H(f)$ to process the signal after the summation. Note that the Fourier
transform $R(f) = \calF \{ r(t) \}$ can be written as $R(f)=S(f) + W(f)H(f)$, which is 
equivalent to $R(f)=\left(\frac{S(f)}{H(f)} + W(f)\right)H(f)$, as indicated in \figl{equivalentAWGN}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/equivalentAWGN}
\caption{Signal processing trick to convert any AWN system into an equivalent AWGN.\label{fig:equivalentAWGN}}
\end{figure}

The trick depicted in \figl{equivalentAWGN} allows to transform any LTI system
contaminated by non-white noise into an \emph{equivalent AWGN}\index{Equivalent AWGN}.
 This allows to use results available for AWGN such as matched filtering, discussed
in the next section, and 
 often simplifies the analysis of systems with non-white noise.

%\section{Matched Filter: A Brief Introduction}
%\label{sec:matchedFilter}
\section{Optimal Receivers for AWGN: Matched Filters or Correlators}
\label{sec:optimalReceiversAWGN}

%Intuitively, using a receiver that makes a decision based on the amplitude value of a single sample does not seem to be the optimal approach. One can then ask a fundamental question:\\
%- What is the optimal receiver scheme for the AWGN?


Correlative decoding as depicted in \figl{correlative_decoding} is an optimal scheme when
the channel is AWGN. As discussed in Section~\ref{sec:convolutionViaCorrelation}, the correlations
in \figl{correlative_decoding} can be interpreted as convolutions. This view gives additional
insight on the optimal decoding for AWGN and leads to the so-called matched filtering scheme, which
will be discussed in the sequel.

The following matched filter discussion relies on basic concepts:
\begin{itemize}
	\item Equivalence of matched filtering via convolution and correlative decoding via crosscorrelation;
	\item orthogonal expansion of WGN;
	\item SNR at the output of MF.
\end{itemize}
These three concepts allow to prove the optimality of the MF, as the one that maximizes
the SNR.

%In this case, the orthonormal functions $\{ \varphi_i(t) \}$ used in correlative decoding
%are interpreted shaping pulses at the transmitter and filters at the receiver. For simplicity, the following discussion assumes $D=1$ and $\varphi_1(t)=p(t)$ is a shaping pulse
%with unitary energy.

%\subsection{Optimality of the matched filter for AWGN channels}
\subsection{Motivation to matched filtering}

As discussed in Section~\ref{sec:thermalNoise}, the thermal noise $\calP=\no \BW$ depends
on the bandwidth (BW) of the receiver analog front end. To minimize this noise power, the receiver typically tries to use a filter with a BW that is not larger than the BW of the signal of interest.

This receiver filter, denoted by its impulse response $h_{\textrm{Rx}}(t)$, could be designed aiming at an ideal lowpass or bandpass filter, with a flat passband over BW, or yet the Gaussian filter of \equl{gaussianFilterImpulseResponse}, etc. However, as it will be proved later, a \emph{matched filter}\index{Matched filter} (MF) is the optimal solution when the channel is AWGN and one imposes a linear filtering as the mandatory receiver architecture. Due to the equivalence of matched filtering and correlative decoding, this
discussion will also indicate that correlative decoding is optimum for AWGN.

When the noise is colored as in \figl{nonWhiteNoise}, the receiver 
(or its mathematical analysis) can first use a filter that ``whitens'' 
the noise, and then a matched filter, which is ``matched'' to the
system that combines the channel and the noise-whitening
filter. Such matched filter is called 
\emph{whitened matched filter} (WMF).\index{Whitened matched filter}

\subsection{Relation between matched filtering and correlative decoding}

In a general case of correlative decoding as in \figl{correlative_decoding}, the transmit signal is composed by a linear  combination of $D$ orthonormal functions 
$\{ \varphi_i(t) \}$. Here, as adopted in Section~\ref{sec:correlativeDecoding}, 
$D=1$ for a PAM and the assumed continuous-time model is given by the block diagram:
\begin{align*}
m \arrowedbox{$p(t)$} \fbox{AWGN} \rightarrow r(t) \arrowedbox{$h_\textrm{Rx}(t)$} y(t) \arrowedbox{decision at each $\tsym$}  r
%&m \arrowedbox{$p(t)$} \fbox{AWGN} \rightarrow r(t) \arrowedbox{$h_\textrm{Rx}(t)$} y(t) \ldots\\
%&\ldots \arrowedbox{decision at each $\tsym$}  r
\end{align*}
where $p(t)$ is the shaping pulse.

As indicated by \equl{mfNoisySignal}, the correlative decoder performs the inner product
of the received noisy signal $r(t)$ with $p(t)$. \equl{convViaCorr} suggests that this correlation
can be implemented as a convolution, 
%Therefore, for simplicity, consider in the following discussion: a) $t_0$ to be the instant the noise-free parcel $m p(t)$ of the received signal $r(t)$ starts to convolve with the impulse response $h_\textrm{Rx}(t)$ of the receiver filter, and b) that the decision has to be made at $t_0 + \tsym$ (or after $L$ samples in the discrete-time case, where $L$ is the oversampling factor).
%Under these requirements, 
Consequently, if the correlative decoder is assumed to be optimum, the best linear filter at the receiver is the filter ``matched'' to $p(t)$ with impulse response\footnote{The filter name is due to the ``matching'' between
its impulse response and the transmitter shaping pulse as if they were soul mates!} 
\begin{equation}
h_\textrm{Rx}(t)=p^*(\tsym-t).
\label{eq:matchedFilter}
\end{equation}

%In case where $p(t)$ is the shaping pulse, which is assumed to have unitary energy. This result will be further discussed in the sequel.

%\subsection{Understanding the matched filtering operation}

%It is not obvious at this point that using $p(t)$ is the optimum scheme but hopefully this will be clarified in Section~\ref{sec:MFOptimalityProofSketch}.

In other words, instead of using a correlator to calculate $\langle r(t),p(t) \rangle$ it is possible to use a filter $h_\textrm{Rx}(t)$ to obtain the same result. The ``trick'' (see Section~\ref{sec:convolutionViaCorrelation} and Application~\ref{app:correspondencesInnerProduct}) involves three steps:
\begin{enumerate}
	\item Assume that $h_\textrm{Rx}(t) = p(-t)$ is needed to make a convolution mimic correlation. 
	\item Assuming $p(t)$ is a right-sided signal, $p(-t)$ must be delayed to make the filter causal. This delay is assumed here to be the symbol period $t=\tsym$. In theoretical developments, a non-causal filter is not a problem and the delay may not be considered. 
	\item The third step is to take the complex conjugate.
% that dictates the rate $1/\tsym$ the symbols must be estimated at the receiver. Therefore, when $p(t)$ is a real signal, the matched filter is $h_\textrm{Rx}(t) = p(-t + \tsym)$.
When dealing with complex-valued signals, it is useful to recall from \equl{innerprod_signals} that, by definition, the inner product adopts the conjugate of the second signal. Hence, when $p(t)$ is complex, the MF uses the conjugate $p^*(t)$ and it is given by \equl{matchedFilter}.
\end{enumerate}

\figl{matchedFilterExample} depicts an example of an arbitrary $p(t)$. The value $\tsym=5$~s was chosen (instead of, for example, $\tsym=3$~s) to be larger than the support of $p(t)$, which is $[0, 3]$~s. This emphasizes that, according to the adopted notation, the result of the convolution should be obtained at multiples of $\tsym$.
% assuming the convolution started at $t=0$.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/matchedFilterExample}
\caption{Example of matched filter when the support of $p(t)$ is $[0, 3]$~s and $\tsym=5$~s.\label{fig:matchedFilterExample}}
\end{figure}

Assuming $r(t)=p(t)$ and $h_\textrm{Rx}(t) = p^*(-t + \tsym)$, the convolution at time instant $\tsym$ is
\begin{equation}
\left\{ r(t) \conv h_\textrm{Rx}(t) \right\} |_{t=\tsym} = \langle p(t),p(t)\rangle
\label{eq:convCorrGiveSameResult}
\end{equation}
as desired.

\bExample \textbf{Checking results with {\matlab}}.
If $p(t)$ is represented by a complex row vector $\bp$ in {\matlab}, the vector that implements matched filtering could be obtained with \ci{h=conj(fliplr(p))} as in \codl{snip_systems_convolution_correlation}. For example, \co{p=[2 -1 3+j]} would correspond to \co{h=[3-j -1 2]}. 
\eExample

In summary, there are two equivalent alternatives to optimal reception of signals contaminated by AWGN: matched filtering and correlative demodulation\index{Correlative demodulation}. Mathematically, as indicated by \equl{convCorrGiveSameResult}, their results are equivalent. But their actual implementations differ, especially when done with analog electronics instead of digital signal processing.

The next section discusses another important topic: the representation of AWGN as a linear
combination of orthogonal basis functions.

%\subsection{Relating the continuous-time and vector AWGN channels}
\subsection{Orthogonal expansion of white Gaussian noise}
\label{sec:orthogonalExpansionWGN}

Many developments in communications require the representation of transmit and receive deterministic signals using orthogonal basis functions. These expansions can be obtained via procedures such as Gram-Schmidt, discussed in Section~\ref{sec:Gram-Schmidt}.
The next paragraphs discuss the expansion of WGN using orthonormal basis functions, which is also of interest especially in the context of the correlative decoder of \figl{correlative_decoding}.

For example, after transmission over a continuous-time AWGN channel, the noise $\nu(t)$ contaminates the transmitted signal $s(t)$ and a correlative decoding scheme will operate on both parcels of the received signal $r(t) = s(t) + \nu(t)$. Assuming $r(t) = \nu(t)$ is only noise, the output of \figl{correlative_decoding} can be represented as in \figl{correlative_decodingNoise}.

\begin{figure}[htbp]
\centering
%AK-TODO: tem um estranho "branco" essa figura. Na versao da Fatima isso nao aparece
\includegraphics[width=\figwidth]{FiguresTex/correlative_decodingNoise}
%\includegraphics[width=\figwidthSmall]{FiguresTex/correlative_decoding}
\caption[Correlative decoding when the received signal is only WGN $\nu(t)$.]{Correlative decoding when the received signal is only WGN $\nu(t)$.  The output vector $\overline \rvn = [\rvn_1, \ldots, \rvn_D]$, updated at the symbol rate,  is composed by i.\,i.\,d. Gaussian random variables.\label{fig:correlative_decodingNoise}}
\end{figure}

The goal of this section is to obtain the statistical properties of the output vector $\overline \rvn$ in \figl{correlative_decodingNoise}.
Hence, the representation of WGN $\nu(t)$ with bilateral PSD level $\no / 2$~W/Hz using a set of orthogonal functions $\{\varphi_i(t)\}$ with unitary energy $E_{\varphi} = \int_{-\infty}^{\infty}{|\varphi(t)|^2 } dt=1$ (i.\,e., a orthonormal set) is discussed in the sequel. The first topic is the representation of $\nu(t)$ for a single time instant using a vector  $\overline{\rvn}$. This is slightly different but related to the discussion in 
\exal{discretizingWhiteNoiseViaUnitaryEnergy}, where white noise is filtered by $h(t)$ with unitary energy. Complementing \exal{discretizingWhiteNoiseViaUnitaryEnergy}, the evolution of  $\overline{\rvn}$ over time is considered in Application~\ref{app:xcorrWGN}.

%\subsubsection{One-shot orthogonal expansion of WGN}

For the following discussion, one key fact is that due to its white PSD, the WGN noise $\nu(t)$ cannot be completely represented by a linear combination of the functions $\{\varphi_i(t)\}$ because it is not restricted to the space spanned by a finite set of $\{\varphi_i(t)\}$. Therefore, $\nu(t)=\nu_r(t)+\nu_i(t)$ can be decomposed into two parcels where $\nu_r(t)$ is the \emph{relevant} part: the projection of $\nu(t)$ onto the space spanned by $\{\varphi_i(t)\}$. The parcel $\nu_i(t)$ is outside this space and, consequently, \emph{irrelevant} to the demodulation of $s(t)$ because it will be eliminated (``filtered out'') by the correlative decoding scheme.

The relevant parcel can be written as:
\begin{equation}
\nu_r(t) = \sum_{i=1}^D \rvn_i \varphi_i(t),
\label{eq:wgnComposedByLinearCombination}
\end{equation}
where $\rvn_i = \langle \nu(t), \varphi_i(t) \rangle$. The vector $\overline{\rvn} = [\rvn_1, \rvn_2, \ldots, \rvn_D]$ can be interpreted as a symbol representing $\nu(t)$ and its elements $\rvn_i$ are independent Gaussian variables 
with variance $\sigma^2=\no / 2$~W and zero mean.
A sketch of the proof goes as follows.

%\subsubsection{Filtering and sampling continuous-time AWGN}
%\label{sec:filteringAWGN}
%\bTheorem \textbf{Filtering and sampling continuous-time AWGN}	
%\label{th:filteringAWGN}
%In summary, when the continuous-time WGN $\nu(t)$ with bilateral PSD level $\no / 2$ is 
%filtered by an LTI with impulse response $h_\textrm{Rx}(t)=
%decomposed by $\varphi(t)$ of unitary energy $E_{\varphi} = \int_{-\infty}^{\infty}{|\varphi(t)|^2 } dt$, and the filter output is sampled to form a sequence $r[n]$, the values $r[n]$ are i.i.d. samples of Gaussian with variance $\sigma^2=\no / 2$ and zero mean. 
%\footnote{Proof Eq. 1.49 in Cioffi, but I should change the limits of the integrals to $[0, T]$.} 

The mean is zero because any signal with a white spectrum, such as WGN, has $\ev[\nu(t)]=0$. Hence,
%Sampling the output of the filter is mathematically equivalent to using a correlator that obtains the inner product between $\nu(t)$ and $\varphi(t)$. Assuming a correlator, the mean is
\begin{equation}
\ev[\rvn_i]=\ev[\langle \nu(t), \varphi_i(t) \rangle] = \ev \left[ \int_{-\infty}^{\infty} \nu(t)\varphi_i(t) dt \right]=\int_{-\infty}^{\infty} \ev[\nu(t)]\varphi_i(t) dt=0, \textrm{~~}\forall i.
\label{eq:wgnMeanIsZero}
\end{equation}

To prove statistical independence, one should recall that, if Gaussian random variables are uncorrelated they are independent. The correlation between two elements $\rvn_i$ and $\rvn_j$ of $\overline \rvn$, both obtained at a given time instant $t$ is:
%Taking two samples $r[k]$ and $r[l]$ and checking their correlation:
\begin{align*}
\ev[\rvn_i \rvn_j] &= \ev \left[ \int_{-\infty}^{\infty}{\nu(t)\varphi_i(t) dt} \int_{-\infty}^{\infty}{\nu(t)\varphi_j(t) dt} \right] \\
 &= \ev \left[ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}{\nu(t)\varphi_i(t) \nu(s)\varphi_j(s) dt ds} \right] \\
 &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} { \ev \left[ \nu(t) \nu(s) \right] \varphi_i(t) \varphi_j(s) dt ds} \\
 &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} { R(t-s)  \varphi_i(t) \varphi_j(s) dt ds} \\
 &=  \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} { \frac{\no}{2} \delta(t-s) \varphi_i(t) \varphi_j(s) dt ds} \\
 &=   \frac{\no}{2} \int_{-\infty}^{\infty} {  \varphi_i(t) \varphi_j(t) dt} \\
\stepcounter{equation}\tag{\theequation} \label{eq:uncorrelated} &= \frac{\no}{2} \delta[i-j]. 	
\end{align*}	
where \equl{squaredintegral} was used in the second line. The expression
$\ev[\rvn_i \rvn_j] = \frac{\no}{2} \delta[i-j]$ indicates that the samples are uncorrelated and is interpreted as $\ev[\rvn_i \rvn_j] = 0, i \ne j$ and $\ev[\rvn_i^2]=\frac{\no}{2}$. 

If a function $\varphi_i(t)$ has non-unitary energy $E_{\varphi} \ne 1$, then the elements of $\overline \rvn$ continue to be uncorrelated, but their variance (or power, in this case) is scaled by $E_{\varphi}$, i.\,e., 
\begin{equation}
\ev[\rvn_i^2] = E_{\varphi} \frac{\no}{2}.
\label{eq:filteredAWGNNotUnitaryEnergy}
\end{equation}

Because $\rvn_i$ is a Gaussian random variable, it is described by its mean and variance, informed by \equl{wgnMeanIsZero} and \equl{uncorrelated}, respectively.

In summary, when WGN is passed through a bank of correlators composed of orthonormal  functions $\{\varphi_i(t)\}$, the elements of the output random (see Section~\ref{sec:randomSignals}) vector $\overline \rvn$ are i.\,i.\,d. and distributed according to $\calN(\mu,\sigma^2)$, where $\mu=0$ and $\sigma^2 = \no/2$. 
The fact that $\sigma^2 = \no/2$ can be understood under the light of \equl{discretizingWhiteNoiseViaUnitaryEnergy} given the connection between correlation and convolution. And \equl{uncorrelated} is obtained because $\{\varphi_i(t)\}$ are orthogonal.

\bExample \textbf{Orthogonal expansion of WGN}.
For example, if $\no = -140$~dBm/Hz and $\{\varphi_i(t)\}$ are orthonormal in \figl{correlative_decodingNoise}, each element $\rvn_i$ of $\overline \rvn$ will have power (in discrete-time) $\calP = \no/2 \approx -143$~dBm. This is valid for any set $\{\varphi_i(t)\}$ and depends only on their orthonormality. The output of a correlator can be depicted as
\begin{align*}
\nu(t) \rightarrow \fbox{$h(t)=\varphi_i^*(\tsym-t)$} \rightarrow x(t) \arrowedbox{A/D}  \rvn_i
\end{align*}
where $\rvn_i$ is sampled at $\fs = \rsym$ (e.\,g., $\rsym=200$~bauds).
Note that this sampling operation can provoke aliasing of the
spectrum of $x(t)$. 

As discussed in \exal{discretizingWhiteNoiseViaUnitaryEnergy}, 
the PSD of $x(t)$ depends on the Fourier transform of $\varphi_i(t)$ and, in general,
is not flat. Hence, the values of $\rvn_i$ over discrete-time $n$ are a realization of a random process $\calX_i[n]$ that, in general, is not white, but the elements of $\overline \rvn$ are uncorrelated at each sample instant.
\eExample


\subsection{SNR at the output of a continuous-time matched filter}
\label{sec:equivalentAWGNAtMFOutput}

%The discussion in Application~\ref{app:correspondencesInnerProduct} 
%and Section~\ref{sec:discreteAndContinuousAWGN} are 
%is now put in the context of matched filtering.

The goal here is to obtain the SNR at the output of a continuous-time MF,
which processes the received signal $r(t)$ and is followed by a sampler at rate $\rsym$.
%When the MF is operating at the output of a continuous-time channel, it 
Extending \equl{convCorrGiveSameResult} to a sequence $y[n]$ of MF outputs, its values are given by
\[
y[n] = \left\{ r(t) \conv h_\textrm{Rx}(t) \right\} |_{t=n \tsym},
\]
where $n \tsym$ are the sampling instants. In the receiver processing chain, the value
$y[n]$ is the input to the ``decision block'' that outputs the $n$-th received symbol.
 The SNR is defined here as $\snr \defeq \ev[|y[n]|^2]/\sigma^2$, where $\sigma^2$ is the power 
of the noise parcel at the receiver. The numerator and denominator of the SNR are calculated
in the sequel.

Because matched filtering and correlative decoding lead to the same outputs,
the values of $y[n]$ are determined by the results in Section~\ref{sec:correlativeDecoding}. 
One distinction is that, while Section~\ref{sec:correlativeDecoding}
(e.\,g. \equl{mfNoisySignal}) assumed a pulse with unitary energy $E_p = || p(t) ||^2 = \int_{-\infty}^{\infty} |p(t)|^2 dt = 1$ for both transmitter and receiver, here the pulse at the transmitter is 
modeled as
\[
p_{\textrm{Tx}}(t) = \sqrt{E_{\textrm{Tx}}} \textrm{~} p(t),
\]
where $p(t)$ has unitary energy and, consequently, $E_{\textrm{Tx}}$ is the energy of $p_{\textrm{Tx}}(t)$. Similarly, the matched filter is
\[
h_{\textrm{Rx}}(t) = \sqrt{E_{\textrm{Rx}}} \textrm{~} p^*(\tsym - t)
\]
and has energy $E_{\textrm{Rx}}$. One motivation to distinguish the energies is to observe
that $E_{\textrm{Rx}}$ does not impact the SNR, while $E_{\textrm{Tx}}$ does.

In fact, with respect to the mathematical model, the matched filtering (or correlation) operation is insensitive to a scaling factor at the receiver (although it may be important
 in the context of the corresponding electronic circuit, for example).
In contrast, $E_{\textrm{Tx}}$ impacts the transmit power and, consequently, the performance
 of the MF-based system. For example, $E_{\textrm{Tx}}$ can be increased by using a pulse with larger amplitude or longer support (duration).

%As discussed in Section~\ref{sec:correlativeDecoding}, 
%because the MF is a LTI system,  ... each parcel of 
Adapting \equl{mfNoisySignal} to take in account distinct values of $E_{\textrm{Tx}}$  and $E_{\textrm{Rx}}$, the MF output when the received signal is $r(t)=m[n] \sqrt{E_{\textrm{Tx}}} p(t) + \nu(t)$ can be written as
\begin{equation}
y[n]=\langle r(t), \sqrt{E_{\textrm{Rx}}} \textrm{~} p(t) \rangle = 
 \sqrt{E_{\textrm{Rx}}} \sqrt{E_{\textrm{Tx}}} m + \sqrt{E_{\textrm{Rx}}} \langle \nu(t), p(t) \rangle.
%\label{eq:}
\end{equation}
The parcel $\sqrt{E_{\textrm{Rx}}} \sqrt{E_{\textrm{Tx}}} m$ is the ``signal of interest''
and, as suggested by \equl{vectors_average_energy}, has average power 
\begin{equation}
\ev[|\sqrt{E_{\textrm{Rx}}} \sqrt{E_{\textrm{Tx}}} m|^2] = E_{\textrm{Tx}} E_{\textrm{Rx}} \overline E_c,
\label{eq:mfOutputSignalOfInterest}
\end{equation}
where $\overline E_c$ is the average constellation energy.

As indicated by \equl{filteredAWGNNotUnitaryEnergy}, when the parcel $\nu(t)$, with PSD level $\no/2$, is filtered by the MF $h_\textrm{Rx}(t)$ and sampled, the output is a discrete-time noise signal with variance (or power)
\begin{equation}
\sigma^2 = E_{\textrm{Rx}} \frac{\no}{2}.
\label{eq:mfOutputNoiseParcel}
\end{equation}

Therefore, the SNR at the output of a continuous-time MF is given by 
\begin{equation}
\snr_{\textrm{MF}} = \frac{E_{\textrm{Tx}} \overline E_c}{\no/2}.
\label{eq:matchedFilterSNR}
\end{equation}
\equl{matchedFilterSNR} is known as the \emph{matched-filter bound}\index{Matched-filter bound} because it represents an upper limit on the SNR values. When the system has ISI, the SNR is lower than this bound. In other words, \equl{matchedFilterSNR} was derived assuming the channel is AWGN and, consequently, the ISI is zero. When studying equalization methods, which aim at compensating the influences of a dispersive channel, this bound illustrates the SNR that would be achieved if the equalizer eliminates ISI.

In retrospective, when the channel is a continuous-time AWGN and the receiver uses matched filtering, the overall system can be modeled by an equivalent discrete-time AWGN with
the signal of interest parcel given by \equl{mfOutputSignalOfInterest} and the noise
parcel by \equl{mfOutputNoiseParcel}.

In case $E_{\textrm{Tx}}=1$ and $E_{\textrm{Rx}}=1$, 
\equl{mfOutputSignalOfInterest} and \equl{mfOutputNoiseParcel} indicate that the power of the parcels of interest and noise of $y[n]$ are $\overline E_c$ and $\no/2$, respectively. As always when dealing with the
power of discrete-time signals (see Section~\ref{sec:powerDiscreteTime}), the reader should not be confused by the units in this case: both power values should be interpreted in Watts, in spite of $\overline E_c$ and $\no/2$ being originally defined with units of Joules and Watts/Hz, respectively.

\subsection{Discrete-time matched filtering}

A MF can also be implemented in discrete-time as modeled by the following block diagram:
\begin{align*}
&m[n'] \arrowedbox{$\uparrow L$} x[n] \arrowedbox{$p[n]$} \fbox{AWGN} \rightarrow r[n] \arrowedbox{$h_\textrm{Rx}[n]$} y[n] \ldots\\
&\ldots \arrowedbox{decision at each $L$ samples ($\tsym$)}  \hat m[n'].
\end{align*}
%\begin{align*}
%&m \arrowedbox{$p[n]$} \fbox{AWGN} \rightarrow r[n] \arrowedbox{$h_\textrm{Rx}[n]$} y[n] \ldots\\
%&\ldots \arrowedbox{decision at each $L$ samples ($\tsym$)}  \tilde m.
%\end{align*}
where $m[n']$ and $\hat m[n']$ are the transmit and receive symbols, respectively, with $n'$ and $n$ indicating the distinct sample rates.
The oversampling factor is $L=\tsym/\ts$ and the matched filter is
%plays an important role on the SNR at the output of the MF.
\[
h_\textrm{Rx}[n]=p^*[L-n].
\]

In this case, the  input of the AWGN is a discrete-time signal with an associated sampling interval $\ts$. The output of the MF can be modeled as another discrete-time AWGN, but with $\tsym$ as the time interval between samples. 

The upsampled signal $x[n]$ has $L-1$ zeros between each pair of symbols and power $\overline E_c / L$. Because the samples of $x[n]$ are uncorrelated, \equl{powerOutputLTI} can be used with $p[n]$ interpreted as an impulse response with energy $E_{\textrm{Tx}}$, such that the power for the signal of interest at the AWGN input is $\ev[|m p[n]|^2] = \overline{E}_c E_{\textrm{Tx}} / L$ (which is valid even if $p[n]$ has more than $L$ samples).

%Having derived the optimal filter for the AWGN channel, i
%In fact, 
%one can observe the improvement that 

%The power of the signal of interest $m p[n]$ at the AWGN input is $E_c E_p / L$
%
%
%without matched filtering is easy to understand when the pulse $p[n]$ has $L$ samples or less, such that the pulse does not overlap over distinct symbols. Under this assumption, the signal power at the input of the AWGN is $\ev[|m p[n]|^2] = E_c E_p / L$.
%In general, $p[n]$ has a support longer than $L$ samples. Even in this case, the power is $E_c E_p / L$ as the following modified diagram helps to understand:
%\begin{align*}
%&m[n'] \arrowedbox{$\uparrow L$} x[n] \arrowedbox{$p[n]$} \fbox{AWGN} \rightarrow r[n] \arrowedbox{$h_\textrm{Rx}[n]$} y[n] \ldots\\
%&\ldots \arrowedbox{decision at each $L$ samples ($\tsym$)}  \tilde m[n'].
%\end{align*}
%The upsampled signal $x[n]$ has $L-1$ zeros between each pair of symbols and power $E_c / L$. Because the samples of $x[n]$ are uncorrelated, \equl{powerOutputLTI} can be used with $p[n]$ interpreted as an impulse response such that the power $E_c E_p / L$ for the signal at the AWGN input is also observed even if $p[n]$ has more than $L$ samples.

Assuming $\sigma^2$ is the noise power of the discrete-time AWGN, the SNR without (or before) the matched filter is
\[
\snr_\textrm{in} = \frac{\overline{E}_c E_{\textrm{Tx}}}{L \sigma^2}.
\]

Following the same reasoning for a continuous-time AWGN, which led to \equl{matchedFilterSNR}, a MF after a discrete-time AWGN generates an output SNR given by
\[
\snr_\textrm{MF} = \frac{\overline E_c E_{\textrm{Tx}}}{\sigma^2},
\]
which is simply another way to write \equl{matchedFilterSNR}.
In the discrete-time case, the matched filter improves the SNR by a factor of $L$ because $\snr_\textrm{MF} = L \textrm{~} \snr_\textrm{in}$. 

In Section~\ref{sec:mfsimulations}, this will be verified via simulations that compare the SNR of a discrete-time AWGN with and without MF.

%Note that, the operation $\langle\br,\bp \rangle $, which corresponds to filtering the received signal $\br$ by the vector $\bp$ and sampling, 

\subsection{Characteristics of the matched filter}

In frequency domain, the MF is not-necessarily flat over its passband. In fact, it is designed to have a frequency response that matches the PSD of the signal of interest as it arrives at the receiver. As discussed in Section~\ref{sec:PSDsOfDigitalSignals} (see, e.\,g., \tabl{psdExpressions}), in many cases of interest the PSD of the digitally modulated signal
at the transmitter is imposed by the shaping pulse $p(t)$. The AWGN channel does
not modify the ``shape'' of the transmit PSD. 
Hence, the MF has frequency response imposed by the Fourier transform $P(f)$ of $p(t)$ according to
the sequential use of Eqs.~(\ref{eq:fourierTimeReversal}), (\ref{eq:fourierTimeShift}) and (\ref{eq:fourierComplexConj}), which results in:
\begin{equation}
H_\textrm{Rx}(f)=P^*(f)e^{-j2\pi f \tsym}. 
%\label{eq:}
\end{equation}
Note that the MF has the same magnitude of $P(f)$, i.\,e., $|H_\textrm{Rx}(f)|=|P(f)|$. 

%For a given frequency component $f_0$ of the received signal, the MF will give a small gain in case $|P(f_0)|$ is small and provide a relatively large gain if $|P(f_0)|$ is large.

%\subsection{When to use a matched filter}

The MF produces an output voltage that is proportional to its impulse response energy.
In other words, the performance of a receiver using matched filtering on AWGN depends on the input signal energy and the noise PSD, not on the specific shape of the transmitted and received waveforms. 

Intuitively, increasing the energy of $p(t)$ used by the transmitter improves performance 
(it requires more transmit power but the SNR at the receiver should increase). The receiver can use a normalized version of $p(t)$ because, as indicated in \equl{mfNoisySignal}, any
gain at the receiver scales both received signal and noise. This issue is further discussed in the next section.

%AK-PUTBACK
\ignore{
%\subsection{Artificial neural network for AWGN}
%
Design experiment in which all $L$ (oversampling) samples of each sample are inputs to a neural network with four outputs, each one indicating a symbol.
It should outperform the previous scheme. But should lead to the same result as a linear processing based on matched filtering. Weka? Claudomir?
%
Present GMM classifier? Bayes' error?
}
%\subsection{The matched filter for AWGN}

Matched filtering provides improvement in performance over a receiver that makes a decision based on a single sample.
% as in \codl{waveformSimulationOfPAMInAWGN}. 
The SNR in both cases will be compared in the sequel.

\subsection{Simulations with matched filtering}
\label{sec:mfsimulations}
The simulations discussed in this section assume a MF following a discrete-time AWGN:
\begin{align*}
&m \arrowedbox{$p[n]$} \fbox{AWGN} \rightarrow r[n] \arrowedbox{$p^*[L-n]$} \fbox{$\div E_p$}\ldots\\
&\ldots \arrowedbox{decision at each $L$ samples}  \tilde m,
\end{align*}
where $E_p$ is the energy\footnote{$E_p$ was previously denoted as $E_{\textrm{Tx}}$, but in this section there is no need to distinguish the energies at transmitter and receiver.} of $p[n]$. The normalization by $E_p$ conveniently restores the amplitude of the signal passed to the decision block to the same amplitude level used by transmitter.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/mfWaveformsNoNoise}
\caption[{Matched filtering (MF) without noise for 4-PAM ($m \in \{-1,1,-3,3\}$).}]{Matched filtering (MF) without noise for 4-PAM ($m \in \{-1,1,-3,3\}$). The $\Diamond$ symbols indicate the sampling instant, in which the amplitude can be seen to coincide with the transmitted symbol $m$.\label{fig:mfWaveformsNoNoise}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/mfWaveformsNoNoiseWithReceivedSignal}
\caption{Version of \figl{mfWaveformsNoNoise} that also shows the received (Rx) signal.\label{fig:mfWaveformsNoNoiseWithReceivedSignal}}
\end{figure}

\figl{mfWaveformsNoNoise} illustrates an example of matched filtering when the input signal is a 4-PAM signal. This figure depicts how the output of the MF evolves along the convolution. Because of the normalization by $E_p$ and the fact that there is no noise, \figl{mfWaveformsNoNoise} shows that the MF output coincides with the transmitted symbol when $n$ is a multiple of $L$. \figl{mfWaveformsNoNoiseWithReceivedSignal} is a version of \figl{mfWaveformsNoNoise} that simply includes the noiseless received signal. The motivation is to prepare the reader to interpret the situation where noise is present. In the noiseless case, sampling the received signal at any instant would lead to the correct amplitude of the corresponding transmitted symbol and it seems that MF is not needed. However, the situation changes drastically when the received signal is noisy.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/mfWaveformsNoisyWithReceivedSignal}
\caption[{Matched filtering with AWGN noise at $\snr_{\textrm{in}}=0.72$~dB for 4-PAM ($-1,1,-3,3$).}]{Matched filtering with AWGN noise at $\snr_{\textrm{in}}=0.72$~dB for 4-PAM ($-1,1,-3,3$). In spite of the relatively low input SNR, none of the 10 symbols are interpreted erroneously with MF.\label{fig:mfWaveformsNoisyWithReceivedSignal}}
\end{figure}

\figl{mfWaveformsNoisyWithReceivedSignal} was obtained with the sample-based simulation in \codl{snip_digi_comm_matched_filtering}, which corresponds to $\snr_{\textrm{in}}=0.72$~dB, and the noiseless case depicted in \figl{mfWaveformsNoNoiseWithReceivedSignal} used the same code but \ci{noisePower=0}.

\lstinputlisting[caption=Sample-based simulation of matched filtering,label=code:snip_digi_comm_matched_filtering]{./Code/MatlabOctaveCodeSnippets/snip_digi_comm_matched_filtering.m}
%\begin{lstlisting}[caption={Sample-based simulation of matched filtering.},label=code:snip_digi_comm_matched_filtering]
%randn('state',0); rand('state',0); %reset both generators
%numberOfSymbols=10; %number of symbols to be transmitted
%M = 4; %modulation order (number of symbols)
%noisePower=6; %noise power
%oversampling=8; %oversampling factor L
%p = ones(1,oversampling); %shaping pulse
%Ep = sum(p.^2); %calculate the pulse energy
%h_mf = fliplr(p); %matched filter (MF) impulse response
%constellation=[-3 -1 1 3]; %4-PAM constellation, energy=5
%symbolIndicesTx=floor(4*rand(1,numberOfSymbols));%indices
%transmittedSymbols=constellation(symbolIndicesTx+1);%symb.
%s = zeros(1,oversampling*numberOfSymbols); %pre-allocate
%s(1:oversampling:end) = transmittedSymbols; %upsampled s
%s = filter(p,1,s); %PAM signal
%n = sqrt(noisePower)*randn(size(s)); %Gaussian noise
%r = s + n; %add noise
%rfiltered = filter(h_mf,1,r); %matched filtering operation
%rfiltered = rfiltered/Ep; %normalize by pulse energy
%initialSample = oversampling; %output after symbol period
%receivedSymbols=rfiltered(oversampling:oversampling:end); 
%symbolIndicesRx = pamdemod(receivedSymbols, M); %demodul.
%SER=sum(symbolIndicesTx~=symbolIndicesRx)/numberOfSymbols
%clf; stem(rfiltered); hold; stem(s,'rx') %plot signals
%stem(r,'g+') %plot received signal
%sampleInstants=[initialSample:oversampling: ...
%    length(rfiltered)]; %indicate sampling instants
%stem(sampleInstants,rfiltered(sampleInstants),'kd')
%\end{lstlisting}

\figl{mfWaveformsNoisyWithReceivedSignal} visually indicates why MF provides a better result than simply sampling at any time instant the received signal. Many samples of the received signal pass the decision threshold and would lead to errors if used alone for decision. In contrast, in spite of the relatively low input SNR, none of the 10 symbols in \figl{mfWaveformsNoisyWithReceivedSignal} are interpreted erroneously when MF is used. Another alternative to observe the advantage of MF is to check the relation $\snr_\textrm{MF} = L \textrm{~} \snr_\textrm{in}$, which will be done in the sequel.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/serberMatchedFilterSimulationOfPAMInAWGN}
\caption[{Matched filtering result. Note the better results when compared to \figl{serberWaveformSimulationOfPAMInAWGN}.}]{Matched filtering result. Note the better results when compared to \figl{serberWaveformSimulationOfPAMInAWGN}. The matched filtering at the receiver led to $P_e=0$ for all SNRs larger than 10 dB.\label{fig:serberMatchedFilterSimulationOfPAMInAWGN}}
\end{figure}

\figl{serberMatchedFilterSimulationOfPAMInAWGN} illustrates the matched filtering using the code \ci{ak\_matchedFilterSimulation OfPAMInAWGN.m} under the conditions of \figl{serberWaveformSimulationOfPAMInAWGN}, which corresponds to $\snr_\textrm{in} \approx 5/0.5 = 10$ and $L=8$. After matched filtering, the SNR is $\snr_\textrm{MF} = L \textrm{~} \snr_\textrm{in} \approx 80$, which corresponds to approximately 19 dB, an improvement of 9~dB over $\snr_\textrm{in}$. \figl{berMFWaveformComparisonOfPAMInAWGN_L8} illustrates this result, putting together the results of \figl{serberWaveformSimulationOfPAMInAWGN} and \figl{serberMatchedFilterSimulationOfPAMInAWGN} in terms of BER. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/berMFWaveformComparisonOfPAMInAWGN_L8}
\caption[{Comparison of \figl{serberWaveformSimulationOfPAMInAWGN} and \figl{serberMatchedFilterSimulationOfPAMInAWGN} in terms of BER.}]{Comparison of \figl{serberWaveformSimulationOfPAMInAWGN} and \figl{serberMatchedFilterSimulationOfPAMInAWGN} in terms of BER. For these two figures, the oversampling was $L=8$.\label{fig:berMFWaveformComparisonOfPAMInAWGN_L8}}
\end{figure}

\figl{berMFWaveformComparisonOfPAMInAWGN_L20} uses the same simulation parameters as \figl{berMFWaveformComparisonOfPAMInAWGN_L8}, but the oversampling is increased to $L=20$. This does not change $\snr_\textrm{in}$, but $\snr_\textrm{MF} = L \textrm{~} \snr_\textrm{in} \approx 200$, which corresponds to approximately 23 dB, an improvement of 13~dB over $\snr_\textrm{in}$. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/berMFWaveformComparisonOfPAMInAWGN_L20}
\caption[{Results increasing the oversample to $L=20$, which corresponds to $\snr_\textrm{MF} \approx \snr_\textrm{in} + 13$ dB.}]{Results increasing the oversample to $L=20$, which corresponds to $\snr_\textrm{MF} \approx \snr_\textrm{in} + 13$ dB. This result should be compared to \figl{berMFWaveformComparisonOfPAMInAWGN_L8} with $L=8$.\label{fig:berMFWaveformComparisonOfPAMInAWGN_L20}}
\end{figure}

The results clearly indicate that matched filtering outperforms decisions based on a single sample. 

\bExample \textbf{Making sample and symbol-based MF simulations equivalent}.
A sample-based MF simulation can be cast as an equivalent symbol-based one.
%From \equl{mfout2} and 
Taking in account a normalization by $E_p$ after the matched filter, one can simulate the sampled output as
\[\rvr = m + \tilde \rvn\]
where $\tilde \rvn$ is a zero-mean Gaussian with variance $\tilde \sigma^2 = \sigma^2 / E_p$. This variance is a result of the division of the filtered noise with variance $E_p \sigma^2$ by $E_p$. The SNR is the same $E_p \overline E_c / \sigma^2$ as previously calculated.

A symbol-based simulation of the receiver using matched filtering is illustrated in \codl{snip_digi_comm_symbol_matched_filtering}. Note that the simulated noise \ci{n} has variance \ci{noisePower/Ep}, which corresponds to the variance \ci{noisePower} of the noise before matched filtering.

\lstinputlisting[caption=Symbol-based simulation of matched filtering. ,label=code:snip_digi_comm_symbol_matched_filtering]{./Code/MatlabOctaveCodeSnippets/snip_digi_comm_symbol_matched_filtering.m}
%\begin{lstlisting}[caption={Symbol-based simulation of matched filtering.},label=code:snip_digi_comm_symbol_matched_filtering]
%numberOfSymbols=400; %number of symbols to be transmitted
%noisePower=4; %noise power
%constellation=[-3 -1 1 3]; %4-PAM constellation, energy=5
%L=8; p=ones(1,L); %define a pulse 
%Ep = sum(p.^2); %pulse energy
%symbolIndicesTx=floor(4*rand(1,numberOfSymbols));%indices
%transmittedSymbols=constellation(symbolIndicesTx+1);%symb.
%%Gaussian noise:
%n = sqrt(noisePower/Ep)*randn(size(transmittedSymbols));
%receivedSymbols = transmittedSymbols + n; %add noise
%symbolIndicesRx = pamdemod(receivedSymbols, M);%demodulate
%SER=sum(symbolIndicesTx~=symbolIndicesRx)/numberOfSymbols
%\end{lstlisting}
The implementation in \codl{snip_digi_comm_symbol_matched_filtering} is simpler and faster than \codl{snip_digi_comm_matched_filtering}. Both lead to equivalent estimates of SER.
\eExample 

%where the transmitted symbols have the original amplitude after proper normalization by the energy $E_p = \int_0^\infty |p(t)|^2 dt$ of the shaping pulse and the noise has smaller power because its white PSD was filtered by the matched filter $|P(f)|^2$.

%REMEMBER: split the signals and filter individually.

\subsection{Proof sketch of the optimality of matched filtering for AWGN}
\label{sec:MFOptimalityProofSketch}

One way of understanding the optimality of the matched filter is to use linear algebra and make an analogy between finite-duration signals and vectors.
For the sake of this discussion, one can assume that $p[n]$, $r[n]$ and $h_\textrm{Rx}[n]$ have non-zero samples from $n=0$ to $L-1$. In this case, the convolution
\[y[n]=\sum_{k=-\infty}^\infty r[k] h_\textrm{Rx}[n-k] = \sum_{k=0}^{L-1} r[k] h_\textrm{Rx}[n-k]\]
at the sampling instant $L$ can be written as
\begin{equation}
y[L]= \sum_{k=0}^{L-1} r[k] h_\textrm{Rx}[L-k].
\label{eq:mfout}
\end{equation}
It simplifies notation to write $h_\textrm{Rx}[L-k] = a[k]$, leading to
\[
y[L]= \langle r[k],a[k]\rangle.
\]

\ignore{
In the case of adopting a matched filter, according to \equl{mfparcels}, for a given transmitted symbol $m$, the signal at the output of the optimal matched filter $\ba=\bp$ has value 
\begin{equation}
\rvy = m\langle\bp,\bp \rangle  + \langle\bn,\bp \rangle  = m E_p + \langle\bn,\bp \rangle 
\label{eq:mfout2}
\end{equation}
and one can write
%$E_a^2 = E_p^2 E_c$ and 
\[
\snr_\textrm{MF} = \frac{\ev[ m^2 E_p^2]}{\ev[\langle\bn,\bp \rangle ^2]} = \frac{E_p^2 \overline E_c}{\sigma^2 E_p} = L \textrm{~} \snr_\textrm{in}.
\]
%where $\snr_\textrm{in}=\frac{E_c E_p}{\sigma^2}$ is the SNR before the matched filter.
}

As done before, because the subject is the detection of a single symbol, it is convenient to consider only the samples associated to one symbol and represent the shaping pulse by a vector $\bp$. The same can be done with $h_\textrm{Rx}[n]$ and $r[n]$, which are here represented by vectors $\bh$ and $\br$, respectively. 
The imposed architecture for the filtering operation can be represented by the blocks:


Now it is relatively easy to argue that $h_\textrm{Rx}[n]=p^*[L-n]$ 
%(or, alternatively, \ci{h=conj(fliplr(p))}) 
is the optimal filter under the restrictions.
When the Tx sends a scaled pulse $m \bp$, the multiplication by the symbol $m$ does not change the direction of $\bp$, but only its norm. However, the added noise does change the direction of $\bp$, creating the received signal $\br = m\bp + \bn$. The optimal filtering problem is to find a vector $\ba$ that has an inner product $\langle\br,\ba \rangle $ that minimizes the effect of $\bn$.

By the linearity of the inner product, one can write 
\begin{equation}
\langle\br,\ba \rangle  = \langle m \bp + \bn, \ba \rangle  = m\langle\bp,\ba \rangle  + \langle\bn,\ba \rangle .
\label{eq:mfparcels}
\end{equation}
%As indicated in Section~\ref{sec:filteringGaussianNoise}, 
Because the elements of $\bn$ are uncorrelated, $\langle\bn,\ba \rangle$ has power given by $\sigma^2 E_a$, where $E_a$ is the squared norm of $\ba$.

%Doing a similar analysis 
Working now with the parcel of $\br$ in \equl{mfparcels} that corresponds to the signal of interest $m \bp$, the matched filter output is $m \langle\bp,\ba \rangle $.
In average, taking all symbols in account, the power of $m \langle\bp,\ba \rangle $ (i.\,e., the input $m \bp$ after filtering) is $\ev[\langle m \bp,\ba \rangle ^2]=\ev[m^2]\langle\bp,\ba \rangle ^2=\overline E_c \langle\bp,\ba \rangle ^2$.
%where $E_c$ is the average constellation energy. 
Combining the two results, the SNR at the filter output is
\[
\snr_\textrm{MF} = \frac{\overline E_c \langle\bp,\ba \rangle ^2}{\sigma^2 E_a} = \frac{\overline E_c\langle\bp,\frac{\ba}{\sqrt{E_a}} \rangle ^2}{\sigma^2} =
\frac{\overline E_c\langle\bp,\overline \ba \rangle ^2}{\sigma^2},
\]
which is maximized when $\langle\bp,\overline \ba \rangle $ is maximum. Note that $\overline \ba$ is a unitary norm vector at the direction of $\ba$ and confirms that scaling the receiver filter does not alter the SNR.

The Cauchy-Schwarz's inequality (see, Section \ref{sec:Cauchy-Schwarz}) states that the magnitude of the inner product is maximized when the vectors are colinear. Therefore, $\overline \ba = \bp / \sqrt{||p||}$. Adopting $\ba=\bp$ (e.\,g., for convenience) will not change the SNR.

%Using the optimal matched filter $\ba=\bp$ (\ci{h = conj(fliplr(\bp))}), 

Recall that $h_\textrm{Rx}[L-k] = a[k]$. Hence, choosing $\ba=\bp$ corresponds to adopting the impulse response \ci{h = conj(fliplr(\bp))} for the optimal matched filter, which corresponds to $h[n]=p^*[L-n]$.

%Note that, the values that the convolution assumes at the sampling interval coincides with an inner product, so the matched filter is equivalent to using a \emph{correlator}\index{Correlator}, which calculates the equivalent inner product.

The next section discusses digital transmission impacted by ISI.
% assuming the channel has a flat frequency response or that it has been already equalized.

\section{Receivers to Combat ISI and Their Analysis}
\label{sec:nyquistZeroISI}

Section~\ref{sec:optimalReceiversAWGN} assumed AWGN, which is a relatively simple channel that allows the transmitted signal to arrive at the receiver without distortion,
with the added noise being the only impairment. 

Here, 
the goal is to briefly study the effects of sending a signal through a frequency-selective
channel (or, equivalently, a dispersive channel when considering the time domain), such as e.\,g. the $h_c(t)$ composing the LTI Gaussian channel of \equl{filtered_awgn_channel}.
Dispersive channels modify their input signals with respect to energy, spectrum, etc. 
To achieve a good performance in digital communication it is necessary to understand
the effects of a frequency-selective channel and mitigate them. 

With respect to the performance analysis of transmission systems in the presence of ISI, 
one strategy is to find an equivalent AWGN model. This allows the analysis to use concepts
such as optimal matched filtering and deal with the effect of ISI as if it was non-white
noise.

\subsection{Intersymbol interference (ISI)}

With a dispersive channel impulse response $h_c(t) \ne \delta(t)$, one filtered symbol at the receiver can have a duration that exceeds its own interval of duration $\tsym$ and extend to the time interval of neighbors symbols. This is called intersymbol interference (ISI)\index{Intersymbol interference (ISI)}. For example, the impulse response of an average \emph{plain old telephony system} (POTS) channel has a duration of approximately 10 ms. In this case, if one uses $\rsym=2500$ bauds ($\tsym=0.4$ ms), a single symbol will influence approximately 25 neighboring symbols (i.\,e. the ISI will extend over approximately 25 symbols).

For example, assume a PAM with pulses of 100\% duty cycle as in \figl{pamExampleFullDutyCycleWithBits}. In this case, at the transmitter, a pulse corresponding to a new symbol begins just after the end of the pulse corresponding to the previous symbol. But the band limited channel can alter this harmony as indicated in \figl{pamExampleDistortedFullDutyCycle}.

\subsection{Pulse response}

A channel that creates ISI is conveniently modeled by its \emph{overall pulse response}\index{Overall pulse response}.
In the sequel, PAM is assumed for simplicity, but all development is valid for QAM (in this case the
signals would be complex-valued) and even higher dimensions. 

Note that extending the PAM generation of \blol{linecodegeneration} to include the channel and receiver leads to the so-called \emph{ISI-channel model}\index{ISI-channel model}:


The shaping pulse at the transmitter is denoted here as $p_t(t)$. Also for simplicity, it is
assumed that $p_t(t)$ incorporates the impulse response of the transmitter filter $h_{\textrm{Tx}}(t)$ used e.\,g. in \equl{basicDigicomm}. Similarly, the receiver system impulse response $p_r(t)$ models all filtering processes that take place at the receiver. 

\blol{isiModel} indicates that achieving zero ISI when dealing with $y(t)$ imposes requirements on the overall pulse response
\begin{equation}
p(t) = p_t(t) \conv h_c(t),
\label{eq:overallPulse}
\end{equation}
which is also called simply \emph{pulse response} and corresponds to the cascade of all LTI systems used for modeling the complete chain composed of transmitter and channel\footnote{As discussed in Section~\ref{sec:isiOtherModels}, $p(t)$ is sometimes considered to be the shaping pulse $p_t(t)$.}
% Still about nomenclature: authors such as \cite{Ciofficn} call \emph{pulse response}\index{Pulse response} the result of $p_t(t) \conv h_c(t)$, which does not account for $p_r(t)$.}

Using \equl{overallPulse}, \blol{isiModel} can be depicted as:
\begin{equation}
m[n] \arrowedbox{ D/C } m_s(t) \arrowedbox{$p(t)$} r(t).
\label{eq:isiModelRepeated}
\end{equation}
%Now the pulse is $p(t) = p_t(t) \conv p_r(t)$ and, because both parcels are controlled by the system designer, \blol{isiModel} can be simplified to
%\begin{equation}
%m_s(t) \arrowedbox{$p(t)$} y(t),
%\label{eq:isiModelSimplified}
%\end{equation}
From \blol{isiModelRepeated} and ignoring the receiver LTI $p_r(t)$, given that $m_s(t) = \sum_{n=-\infty}^{\infty} m[n] \delta(t-n \tsym)$, zero ISI is achieved when $r(n \tsym) = m[n]$. In words,
if there is no ISI, when sampling $r(t)$ for the $n$-th symbol $m[n]$, this sample $r(n \tsym)$ depends only on $m[n]$. Any signal $p(t)$ that leads to this situation is known as a zero-ISI \emph{Nyquist pulse}\index{Nyquist pulse}.

The pulse response energy $E_p$ is an important parameter when studying the ISI impact
\begin{equation}
E_p = ||p(t)||^2 = \left< p(t),p(t) \right> = \int_{-\infty}^{\infty} p(t)p^*(t) dt.
\label{eq:}
\end{equation}

In the sequel, examples of overall pulses $p(t)$ and the associated ISI
are discussed in the context of PAM.

%Examples of ISI using PAM are provided in the next section.

\subsection{Examples of intersymbol interference (ISI) in PAM}

%The previous sections focused on channels with unlimited bandwidth. The only impairment was AWGN, which does not impose the effect of filtering the transmitted signals. 

%First, consider a very simple example on how a dispersive channel distorts the transmit signal: consider a binary PAM with oversampling of $L=4$ samples and channel channel $h[n]=\delta[n]-0.5\delta[n-1]$, as depicted in \figl{exampleOfISI}. The signal $y[n]$ is severely distorted, with most amplitudes $\pm 0.5$ instead of the original $\pm 1$ of $x[n]$. 
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/exampleOfISI}
%\caption[Example of transmission over dispersive channel.]{Example of transmission over dispersive channel. The plot on top is the signal $x[n]$, which is transmitted through the channel $h[n]=\delta[n]-0.5\delta[n-1]$, generating the output $y[n]=x[n]-0.5x[n-1]$ (bottom) as the sum of the two top plots.\label{fig:exampleOfISI}}
%\end{figure}

The following three examples illustrate how $p(t)$ impacts a PAM signal,
eventually creating ISI.% and what should be done to make it zero.

\bExample \textbf{Example of continuous-time Nyquist pulse shorter than $\tsym$}.
\begin{figure}[!htb]
  \begin{center}
    \subfigure[]{\label{fig:isiExample1a}\includegraphics[width=7cm]{Figures/isiexample1}}
    \subfigure[]{\label{fig:isiExample1b}\includegraphics[width=7cm]{Figures/isiexample1zoom}}
  \end{center}
  \caption{Example of achieving zero ISI by using a pulse $p(t)$ shorter than the symbol duration $\tsym$. \figl{isiExample1b} is a zoomed version of the middle graphic in \figl{isiExample1a}.\label{fig:isiExample1}}  
\end{figure}
\figl{isiExample1} suggests a first solution to avoid ISI: constrain the duration of $p(t)$ to be shorter than the symbol duration $\tsym$. The drawback of this strategy is that the shorter the pulse, the larger its BW, as suggested by \equl{sinc_transform}.
\eExample

\bExample \textbf{Example of continuous-time Nyquist pulse longer than $\tsym$}.
\begin{figure}[!htb]
  \begin{center}
    \subfigure[]{\label{fig:isiExample2a}\includegraphics[width=7cm]{Figures/isiexample2}}
    \subfigure[]{\label{fig:isiExample2b}\includegraphics[width=7cm]{Figures/isiexample2zoom}}
  \end{center}
  \caption{Example of achieving zero ISI by using a Nyquist pulse $p(t)$.\label{fig:isiExample2}}  
\end{figure}

\figl{isiExample2} illustrates that a properly chosen pulse can achieve zero ISI: the amplitude of the pulse simply has to be zero at time instants $t=n \tsym, n \ne 0$. 
This is the distinctive property of a Nyquist pulse\index{Nyquist pulse}.
When compared with \figl{isiExample1}, the Nyquist pulse illustrated in \figl{isiExample2} can have its bandwidth reduced by using a pulse with support longer than $\tsym$.
\eExample

\bExample \textbf{Example of continuous-time non-Nyquist pulse}.
\begin{figure}[!htb]
  \begin{center}
    \subfigure[]{\label{fig:isiExample3a}\includegraphics[width=7cm]{Figures/isiexample3}}
    \subfigure[]{\label{fig:isiExample3b}\includegraphics[width=7cm]{Figures/isiexample3zoom}}
  \end{center}
  \caption{Example for which the ISI is non-zero because $p(t)$ is not a Nyquist pulse.\label{fig:isiExample3}}  
\end{figure}

In contrast to \figl{isiExample1} and \figl{isiExample2}, \figl{isiExample3} illustrates the consequence of using a pulse with support longer than $\tsym$ and that is not a Nyquist pulse.
In this case, taking in account only the power at the sampling instants (the ``ISI power''), one calculates 0.834 Watts for the specified samples. The power from this interference would add to the power of the background noise in case they are not correlated. 
\eExample 

\subsection{Some practical aspects concerning the ``pulse'' to study ISI}
\label{sec:isiOtherModels}

The section discusses a model\footnote{The nomenclature is based on \cite{Ciofficn} (chapter about
equalization).} to study the performance of a communication system that faces ISI.
First, some aspects of practical interest are discussed.

%\subsubsection{Some practical aspects}

The overall pulse $p(t)$ determines the ISI of a system but, as indicated in 
\equl{overallPulse}, it  has the inconvenience of depending on the channel $h_c(t)$,
which is not under control of the communication system designer.
This can be circumvented by assuming that the channel was equalized and the combined effect of channel and equalizer is an ideal lowpass filter $h_c(t)$. Besides, the spectrum $P_t(f) = \calF \{p_t(t)\}$ has to be assumed confined within the channel bandwidth $\BW$.\footnote{Fourier theory explains that if $p(t)$ has finite support, its spectrum $P(f)$ cannot have and vice-versa. In other words, if $p(t)$ has finite duration, $P(f)$ has infinite bandwidth and if $P(f)$ is band limited then $p(t)$ has infinite duration. But in practice, functions can have negligible values outside ranges and, for practical purposes, be considered to have finite support in both time and frequency domains.} Under these two assumptions, $h_c(t)$ does not distort the band-limited $s(t)$ and, consequently, $r(t)=s(t)$ in \blol{isiModel}. This way one gets rid of the channel $h_c(t)$ in the model but should remember that the assumption is valid only if the transmitted signal is contained within the channel bandwidth, i.\,e., $P_t(f) \approx 0$ for $f > \BW$.


%\section{Matched Filtering when the Channel is not AWGN}

%In other words, 
%$h_{\textrm{Rx}}(t)$ tries to match the spectrum of $h_{\textrm{Tx}}(t) \conv h_c(t)$, where $h_{\textrm{Tx}}(t)$ and $h_c(t)$ are the transmit filter and channel impulse responses.

With $h_c(t)$ eliminated, the pulse response \equl{overallPulse} $p(t) = p_t(t)$ becomes simply the
transmitter shaping pulse. It is also convenient to take in account the filter $p_r(t)$ at the receiver
and, with an abuse of notation, call $p(t)$ the signal (that is not anymore the channel pulse response):
\begin{equation}
p(t) = p_t(t) \conv p_r(t).
\label{eq:pulseWithEqualizedChannel}
\end{equation}
For zero ISI, the signal $p(t)$ must be a Nyquist pulse and the issue now is how
to design $p_t(t)$ and $p_r(t)$ such that their convolution is a Nyquist pulse.
When matched filtering is adopted, \equl{matchedFilter} in this context leads
to $p_r(t) = p_t^*(\tsym-t)$. This issue is further discussed in
Section~\ref{sec:matchedFilterAndSquareRootCosine}, which discusses the \emph{square-root raised cosine} shaping pulse.

There are applications that cannot afford using matched filtering because, for example, the channel
cannot be equalized. In these cases, it may be convenient to use a Nyquist pulse
at the transmitter (not a square-root version). For example, high-speed transmission in
optical fibers may adopt the raised cosine discussed in Section~\ref{sec:raised_cosines} (a Nyquist pulse) at the transmitter
and a (non-necessarily ``matched'') Gaussian filter (see \equl{gaussianFilterImpulseResponse}) at the receiver.

%Based on the model depicted in \blol{isiModel}, the solution to achieving zero ISI is to design $p(t)$ such that there is no ISI at the times that $y(t)$ is sampled. The samples are obtained at the symbol rate $1/ \tsym$ and the goal is to have $y(n \tsym) = m[n]$. The point is that, in contrast to the pulses $p_t(t)$ and $p_r(t)$, which are controlled by the system designer, the channel $h_c(t)$ is imposed by nature.

%In some models, a channel with flat frequency response is considered. 
%If the frequency response is flat, the design can focus on accommodating the transmit signal PSD within the channel bandwidth and, simultaneously, avoiding ISI. This task corresponds to choosing the shaping pulse $p(t)$ and the \emph{Nyquist criterion} guides this choice assuming a flat channel. Hence, if the channel has a non-flat frequency response, it needs to be equalized otherwise the assumptions that lead to the Nyquist criterion will not hold. 

%\subsubsection{ISI model}

The next sections will discuss the property shared by pulses that achieve zero ISI.

\subsection{Nyquist Criterion for Zero ISI}
As mentioned, a deleterious effect of channel dispersion is ISI, which can be generated by any bandlimited channel (with flat or non-flat frequency response). 
This section presents a criterion to guide the search for solutions to combat the ISI provoked by an ideal lowpass (or already equalized frequency-selective) channel. A PAM system is assumed. First, the continuous-time model of \equl{linecodegeneration} is adopted and later the discrete-time of \equl{linecodegenerationDiscreteTime}. The most important aspects are still valid when the channel is bandpass and frequency upconversion is used at the transmitter.

\subsubsection{Zero ISI with a continuous-time model}


First thing to note is that the convolution of $p(t)$ with each impulse leads to the PAM expression
\[
y(t) = \sum_{n=-\infty}^{\infty} m[n] p(t-n \tsym).
\]
Aiming at a specific symbol $m[n_0]$, the sample to be observed is
\[
y(n_0 \tsym) = \sum_{n=-\infty}^{\infty} m[n] p(n_0 \tsym -n \tsym)
\]
and the associated ISI is
\begin{equation}
\textrm{ISI}{(n_0)} = \sum_{n \ne n_0} m[n] p(n_0 \tsym -n \tsym)
\label{eq:isi_pam}
\end{equation}
such that
\[
y(n_0 \tsym) = m[n_0]p(0) + \textrm{ISI}{(n_0)}.
\]
If $p(0)=1$, then a zero ISI transmission leads to $y(n_0 \tsym) = m[n_0]$, as desired. Otherwise a scaling factor needs to be used.\footnote{In practice, scaling factors are always
necessary and often provided by closed-loop control systems (such as an automatic gain control system) within the transceiver circuitry.}

\equl{isi_pam} indicates that $p(t)$ should be zero at multiples of $\tsym$ other than $t=0$  to achieve zero ISI. In other words, for being able to perfectly recover the transmitted symbol, $p(t)$ has to have the following property:
\begin{equation}
p(n \tsym) = \left\{ \begin{array}{l} 1,~n = 0 \\ 0,~n \ne 0 \end{array} \right.
\label{eq:nyquistPulses}
\end{equation}
When a shaping pulse obeys \equl{nyquistPulses} it is called a \emph{Nyquist pulse}\index{Nyquist pulse}.
The question now is what are the features a pulse must obey to be a Nyquist pulse?

Instead of dealing only with $p(t)$, for the next development it is useful to adopt the auxiliary signal 
\[
p_s(t) = \sum_{n=-\infty}^{\infty} p(t) \delta(t-n \tsym),
\]
which corresponds to sampling $p(t)$ via an impulse train of period $\tsym$.
Dealing with $p_s(t)$ helps because \equl{nyquistPulses} does not impose any restriction on the values that $p(t)$ assumes at instants other than the multiples of $\tsym$. Hence, it is sensible to look at properties of $p_s(t)$ instead of $p(t)$.
Interpreting \equl{nyquistPulses}, the property of interest for $p_s(t)$ can be stated as having $p_s(t) = \delta(t)$. 
%With that in mind, one can visualize the desired $p(t)$ and $p_s(t)$ in the frequency domain as follows.

The Fourier pair $\delta(t) \Leftrightarrow 1$ indicates that the property $p_s(t) = \delta(t)$ is equivalent to having $P_s(f) = 1$ or some other constant.\footnote{Again: scaling factors are useful to get the right analytical results but in practice they depend on
the electronics.}
Because $P(f)$ determines $P_s(f)$, the question now is what property of the shaping
pulse $P(f)$ leads to $P_s(f) = 1$.
In other words, the Nyquist criterion can be derived by interpreting the desired property $p_s(t) = \delta(t)$ in the frequency domain. 

The spectrum of $P_s(f) = \calF \{ p_s(t) \}$ is obtained by convolving $P(f)$ with $\calF \{ \sum_{n=-\infty}^{\infty} \delta(t-n \tsym) \}$, the Fourier transform of the periodic impulse train. From \equl{fourierPropertyImpulseTrain}, one has
\begin{equation}
P_s(f) = \frac{1}{\tsym} \sum_{k=-\infty}^{\infty} P \left( f - k \frac{1}{\tsym} \right).
\label{eq:nyquistCondition}
\end{equation}
To have $P_s(f)$ with a constant value over all frequencies, \equl{nyquistCondition}
suggests that the replicas of $P(f)$ should gracefully compose this constant level when
summed up and, there should be no gaps (frequencies for which $P_s(f)=0$) among the 
replicas.

Assuming $P(f)$ is the spectrum of a bandlimited baseband pulse with bandwidth $\BW$
and support from $-\BW$ to $\BW$, 
it suffices to observe the interaction between $P(f)$ and its first replica at the right (positive frequencies),
located at $P(f-\rsym)$. The support of $P(f-\rsym)$ is from $\rsym-\BW$ to $\rsym+\BW$ and,
as indicated in \figl{nyquistCriterion}, to avoid a gap of $P_s(f)=0$ between the two replicas, it is mandatory to have
$\BW \ge \rsym-\BW$. Therefore, for a baseband real signal, the Nyquist criterion for zero ISI is
\begin{equation}
\rsym \le 2 \BW.
\label{eq:nyquistZeroISI}
\end{equation}
This criterion indicates that a channel with bandwidth $\BW$ limits the symbol rate to $2 \BW$. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresTex/nyquistCriterion}
\caption{Explanation of the Nyquist criterion: the replicas of $P(f)$ for $k=0$ and 1 in \equl{nyquistCondition} cannot have a gap between them.\label{fig:nyquistCriterion}}
\end{figure}

\bExample \textbf{Reasoning that leads to the Nyquist criterior for zero ISI}.
To illustrate \equl{nyquistZeroISI}, 
\figl{nyquistCriterionExample} shows the replicas of $P(f)$, spaced by $\rsym$, that compose $P_s(f)$. In this case, the bandwidth $\BW=200$ Hz of $P(f)$ is not large enough, and the gaps (where $P(f)=0$) between the replicas indicate that it is not possible to obtain $P_s(f) = 1$. 
\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/nyquistCriterionExample}
\caption{Spectrum of $P_s(f)$ corresponding to a $p(t)$ with a triangular spectrum $P(f)$ of $\BW=200$ Hz and $\rsym=500$ bauds.\label{fig:nyquistCriterionExample}}
\end{figure}
In fact, changing the triangular spectrum by a rectangular spectrum $P(f)$ with $\BW = \rsym/2$, one can achieve $P_s(f)=1, \forall f$, which is the pulse with smallest $\BW$ that achieves zero ISI. Several other options for $p(t)$ can achieve zero ISI. \eExample

As further discussed in \tabl{fsAndRsymComplex}, for an analytic signal representing a
bandlimited complex envelope (e.\,g., the one in \figl{complexEnvelope}) with
bandwidth $\BW$ and $P(f)$ with support from $0$ to $\BW$, \equl{nyquistZeroISI} becomes
\begin{equation}
\rsym \le \BW.
\label{eq:nyquistZeroISIComplexEnvelope}
\end{equation}

\subsubsection{Zero ISI with a discrete-time model}

The discrete-time model of \blol{linecodegenerationDiscreteTime} is adopted now and the emphasis is on showing some code, while equations were prioritized when discussing the continuous-time model. Assuming $L$ is the oversampling in \blol{linecodegenerationDiscreteTime}, the desired property for the shaping pulse $p[n]$ to achieve zero ISI is:
\[
p[n L] = \left\{ \begin{array}{l} 1,~n = 0 \\ 0,~n \ne 0 \end{array} \right.
\]
and the periodic impulse train is $x[n]=\sum_{k=-\infty}^{\infty} \delta[n-k L]$. 

Some delay $N_0$ can be assumed, for example, to avoid non-causal pulses. In this case, the desired property is
\[
p[n L + N_0] = \left\{ \begin{array}{l} 1,~n = 0 \\ 0,~n \ne 0 \end{array} \right.
\]
and the receiver should find the correct instant to start sampling at the symbol rate.

Because $x[n]$ has period $L$, a natural tool to observe its spectrum is the discrete-time Fourier series. But it is convenient to use the DTFT for the spectra of both signals: $p[n]$ and $x[n]$. The DTFT of $x[n]$ is another impulse train
\[
X(e^{j\dw})=\frac{2 \pi}{L}  \sum_{k=-\infty}^{\infty} \delta\left(\dw+k \frac{2 \pi}{L}\right)
\]
(recall that continuous-time impulses can be used to represent the spectrum of a periodic signal when one is interested in representing it as a Fourier transform instead of a Fourier series).
%AK-IMPROVE I am using p_s[n] for the pulse that is the notation for sampled signals
The spectrum of $p_s[n] = p[n] \times x[n]$ is
\begin{equation}
P_s(e^{j\dw})=  P(e^{j\dw}) \conv X(e^{j\dw}) = \frac{2 \pi}{L}  \sum_{k=0}^{L-1} P(e^{j(\dw+k \frac{2 \pi}{L})}).
\label{eq:nyq_criterion}
\end{equation}

\bExample \textbf{Example of Nyquist pulse with support not longer than oversampling $L$}.
%It is now convenient to observe how these equations are implemented in {\matlab}.
The function \ci{ak\_plotNyquistPulse.m} can be used to observe that \equl{nyq_criterion} leads to a constant value when the pulse obeys the criterion. \figl{pulse1_spectrum} shows the obtained spectra for a pulse \ci{p=[1 1 1 1]} with $L=4$, using \codl{snip_digi_comm_ak_plotNyquistPulse}.

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{Figures/pulse1_spectrum}
\caption[Spectra for the pulse \ci{p=[1 1 1 1]}.]{Spectra for the pulse \ci{p=[1 1 1 1]}. From top to bottom: $P(e^{j\dw})$, the $L=4$ replicas of $P(e^{j\dw})$ that compose $P_s(e^{j\dw})$ and $P_s(e^{j\dw})$ itself. All the graphs are periodic and the shown interval is $\dw=[0,2 \pi]$ rad. \label{fig:pulse1_spectrum}}
\end{figure}

\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_ak\_plotNyquistPulse}{snip_digi_comm_ak_plotNyquistPulse}
%\begin{lstlisting}
%N0 = 1; %sample to start obtaining the symbols
%p=[1 1 1 1]; %shaping pulse
%L=4; %oversampling factor
%ak_plotNyquistPulse(p,L) %graphs in frequency domain
%\end{lstlisting}
Note the pulse has non-zero samples only for $n=0,1,2,3$ and the oversampling is $L=4$. Hence, it should be expected no ISI in this case. The top and bottom plots in \figl{pulse1_spectrum} show $P(e^{j\dw})$ and $P_s(e^{j\dw})$, respectively. The plot in the middle illustrates that, according to \equl{nyq_criterion}, for $L=4$ there are four replicas of $P(e^{j\dw})$ that sum up to compose $P_s(e^{j\dw})$.

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{Figures/pulse1_timedomain}
\caption{Assuming the transmitted symbols are \ci{symbols=[3 -1 3 1]}, \ci{p=[1 1 1 1]} and $L=4$, from top to bottom: $p[n]$, the shifted and scaled versions of $p[n]$ corresponding to each symbol and the transmitted signal $s[n]$.\label{fig:pulse1_timedomain}}
\end{figure}

Besides \figl{pulse1_spectrum}, it is instructive to analyze the pulse \ci{p=[1 1 1 1]} (or $p[n]$) in time-domain. This is the goal of \figl{pulse1_timedomain}, which shows $p[n]$ on the top graph. Assuming the transmitted symbols are \ci{symbols=[3 -1 3 1]}, the bottom plot shows the result of the convolution between the upsampled symbols and $p[n]$, which corresponds to the transmitted signal. This graph shows the sampling instants, where the symbols can be recovered, as specified by \ci{N0 = 1}. The middle graph shows the effect of shifting $p[n]$ and scaling it by the corresponding symbol. For example, the parcel corresponding to the first symbol is $3 p[n]$. Note that for $n=0, 4, 8, 12$, there is only one parcel that has a non-zero value, as requested for zero ISI.
\eExample 

\bExample \textbf{Example of Nyquist pulse with support longer than oversampling $L$}.
As previously discussed, it is possible to use a pulse with duration longer than $L$ samples and still achieve zero ISI. This is useful in practice because a longer shaping pulse (think it as a FIR filter) can have sharper transitions from passband to stopband, which will determine the effective filter's bandwidth. To exemplify this possibility, it is assumed a pulse \ci{p=[0 -1 2 1 1 -2 0]}, oversampling $L=3$ and ak-here
\figl{pulse2_spectrum} and \figl{pulse2_timedomain} were then obtained with \codl{snip_digi_comm_ak_plotNyquistPulse2}.

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{Figures/pulse2_spectrum}
\caption{Equivalent to \figl{pulse1_spectrum} with the pulse \ci{p=[0 -1 2 1 1 -2 0]}.\label{fig:pulse2_spectrum}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{Figures/pulse2_timedomain}
\caption{Equivalent to \figl{pulse1_timedomain} with the pulse \ci{p=[0 -1 2 1 1 -2 0]}.\label{fig:pulse2_timedomain}}
\end{figure}

\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_ak\_plotNyquistPulse2}{snip_digi_comm_ak_plotNyquistPulse2}
%\begin{lstlisting}
%N0 = 4; %sample to start obtaining the symbols
%p=[0 -1 2 1 1 -2 0]; %shaping pulse
%L=3; %oversampling factor
%ak_plotNyquistPulse(p,L) %graphs in frequency domain
%\end{lstlisting}
It is not that evident, but \figl{pulse2_timedomain} indicates that the ISI is zero. Its bottom graph informs that the four symbols $3, -1, 3, 1$ were perfectly recovered at samples $n=3, 6, 9, 12$. The value \ci{N0=4} leads to the first symbol being obtained at $n=3$ (\ci{N0-1}). The bottom graph of \figl{pulse2_spectrum} shows that $P_s(e^{j\dw})$ is a constant over $\dw$, with a numerical error of $\pm 0.0002$.

Careful observation of the middle graph of \figl{pulse2_spectrum} indicates that, considering only the magnitude, the $L=3$ replicas of $P(e^{j\dw})$ do not sum to a constant. Due to the delay imposed by \ci{N0}, the phase must be taken in account, which was not necessary in \figl{pulse1_spectrum}.
\eExample 

As discussed in Application~\ref{app:zeroISIPulses}, there is considerable flexibility for designing zero ISI pulses. But, in practice, two requirements are imposed when interpreting the pulse as a filter: 1) a relatively small bandwidth and 2) a large attenuation at the stopband. The raised cosines are popular options and are discussed in the sequel.

\subsection{Cursor, precursor ISI and postcursor ISI}

The cursor was briefly mentioned in Section~\ref{sec:correlativeDecoding}.
Generally, given a signal waveform, a reference time instant is sometimes 
 called \emph{cursor}\index{Cursor}. The segments of the signal before and after this point are called \emph{precursor}\index{Precursor} and \emph{postcursor}\index{Postcursor}, respectively. 
The next paragraphs, apply the concept of cursor in the specific scenario of a digital receiver,
in which it is also called synchronization delay, timing synchronization delay, or delay due to timing synchronization.

Considering again the transmitted and received signals in \figl{pamExampleFullDutyCycleWithBits} and 
 \figl{pamExampleDistortedFullDutyCycle}, respectively, note that it is not trivial to find the instant to sample the signal at the receiver (or the cursor)
given a distorted signal, as in \figl{pamExampleDistortedFullDutyCycle}.

Before defining the cursor more accurately, it is useful to observe in \figl{isi_pulse_response}, two examples
of channel pulse responses to a single symbol (``one-shot'') of a PAM signal $x(t)$.
 In this case $p(t)$ in case (a) spans approximately $2.5 \tsym$ and potentially causes less
ISI than case (b). 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{FiguresNonScript/isi_pulse_response}
\caption{Two examples of channel pulse responses $p(t)$: the second one, (b), potentially leads to stronger ISI due to its more dispersive channel than (a).\label{fig:isi_pulse_response}}
\end{figure}

The timing references in \figl{isi_pulse_response}, with vertical lines separated by $\tsym$
have the transmitted signal as reference. In the context of ISI analysis, it
is sometimes convenient to have the timing specified by the receiver 
synchronizer as indicated in \figl{isi_cursor_rx}.

\begin{figure}[htbp]
\centering
\includegraphics[width=5.5cm]{FiguresNonScript/isi_cursor_rx}
\caption{Pulse response of \figl{isi_pulse_response}(b) with the timing reference
provided by the estimated cursor at $t^c$.\label{fig:isi_cursor_rx}}
\end{figure}

\figl{isi_cursor_rx} indicates the \emph{cursor time}\index{Cursor time} $t^c$ (or simply cursor), which is typically estimated by the receiver's synchronization algorithm. 
For a given reference symbol $m$,
the cursor is defined here as the time delay between the instant $m$ was generated at
the transmitter and the time estimated by the receiver to obtain the amplitude value that will
represent $m$ and be used for making the decision regarding the symbol.
Considering distinct symbols $m[n]$ as the reference (time ``0''), the estimated 
cursor values $t^c[n]$ may vary, and $t^c = \ev\left[t^c[n]\right]$ is their average.

Assuming the signal that will be used at receiver to make the decisions is already
at the symbol rate $\rsym$, the samples of the pulse response $p(t)$ separated by
$\tsym$ from $t^c$ may impact the decision of other symbols. In \figl{isi_cursor_rx},
three samples before the cursor $t^c$ (precursor samples) may impact the three symbols
that are transmitted before the reference symbol, while the sample at time $t^c + \tsym$
impacts the symbol transmitted just after the reference symbol. This is an important aspect
of ISI that can be summarized as follows:
\begin{itemize}
	\item \emph{Precursor}: interference provoked by future symbols, which can be modeled as
	anticausal with respect to the reference symbol, and it is created by the parcels of $p(t)$ 
	before the cursors of the future symbols.
	\item \emph{Postcursor}: interference provoked by past symbols and created by the parcels of $p(t)$ 
	after the cursors of the past symbols.
\end{itemize}
An example using $p(t)$ from \figl{isi_pulse_response}(a) is described in the next paragraphs.

Because the channel is LTI, it is relatively easy to observe its response to a sequence of symbols
and interpret the final $p(t)$ as the summation of the contribution of each symbol. \figl{isi_cursors} illustrates a case with four PAM symbols composing $m_s(t)$ and the corresponding
responses that should be added to compose $p(t)$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{FiguresNonScript/isi_cursors}
\caption[Illustration of ISI at cursor $t^c$ corresponding to the symbol transmitted at $t=2\tsym$.]{Illustration of ISI at cursor $t^c$ corresponding to the symbol transmitted at $t=2\tsym$ (third symbol, in red). The adopted pulse response is from \figl{isi_pulse_response}(a).\label{fig:isi_cursors}}
\end{figure}

Observing \figl{isi_cursors}, suppose that the automatic synchronization procedure chooses $t^c[2]$ 
(or $t^c$ for simplicity) as the indicated
cursor to obtain the sample that represents the symbol transmitted at time $2 \tsym$. 
In this example, the signal at $t^c$ is the summation of three parcels: 
the parcel of interest, the precursor ISI (which in this case is provoked solely
by the next symbol sent at $t=3\ts$) and the 
postcursor ISI (provoked by the previous symbol, transmitted at $t=\tsym$). In this case, only the two neighboring symbols are interfering
with the reference symbol but, in general, due to the channel dispersion, it is possible that the symbol of interest suffers interference from many other symbols. As mentioned, the influence of ``future'' symbols 
 can be conveniently modeled using noncausal signals and systems.

%\section{Channel Identification and Equalization}
%\subsection{Channel identification (discovery)}

%\bExample \textbf{WGN through correlative decoder}. The proof of \equl{filteredAWGNNotUnitaryEnergy} did not take in account the Fourier transform of $\varphi_i(t)$.
%\eExample
\section{Raised Cosine Shaping Pulses (Filters)}
\label{sec:raised_cosines}

When designing a communication system, the channel bandwidth is typically specified and the task is to design a transmit signal with a spectrum that fits within the bandwidth and can successfully convey information to the receiver. As discussed, the Nyquist criterion for zero ISI indicates reasonable values for the symbol rate and the desired property for the shaping pulse.

The most widely used set of functions that satisfy the Nyquist criterion, or Nyquist pulses, are the raised cosine pulses (or filters), given by:
\begin{equation}
p(t) = \sinc \left( \frac{t}{\tsym} \right) \left[ \frac{\cos \left( \frac{r \pi t}{\tsym} \right) }{1 - \left( \frac{2 r t}{\tsym} \right)^2} \right],
\label{eq:raisedCosine}
\end{equation}
%$0 \le r le 1$
where $0 \le r \le 1$ is the roll-off factor\index{Roll-off of a raised cosine}, which controls the pulse bandwidth (more strictly, the transition band in frequency domain when interpreting $p(t)$ as the impulse response of a filter). Note that when $r=0$, \equl{raisedCosine} becomes the $\sinc$ function.

Also note that \equl{raisedCosine}
has singularities at $t=0$ and $t=\pm \tsym / (2r)$. Using L'Hospital's rule leads to
$
p_{\textrm{sr}}(t)|_{t=0} = 1
$
and
$
p_{\textrm{sr}}(t)|_{t=\pm \tsym / (2r)} = \frac{\pi}{4} \sinc \left( \frac{1}{2r } \right)
$.

Recalling that the oversampling is $L=\tsym/\ts$ and noting that $t$ is always normalized by $\tsym$ in \equl{raisedCosine}, it is straightforward to rewrite \equl{raisedCosine} to obtain its discrete-time version. At the sampling instants $t=n \ts$, one has $t / \tsym = (n \ts) / \tsym = n / L$, which leads to
\begin{equation}
p[n] = \sinc \left( \frac{n}{L} \right) \left[ \frac{\cos \left( \frac{r \pi n}{L} \right) }{1 - \left( \frac{2 r n}{L} \right)^2} \right].
\label{eq:raisedCosineDiscreteTime}
\end{equation}

When using a raised-cosine pulse in practice, the pulse needs to be truncated. For example, one may only consider the time interval from $-5 \tsym$ to $5 \tsym$ ($p(t)$ is assumed here to be symmetrical with respect to $t=0$). After truncation, a delay is added to make the filter causal.

\begin{figure}[htbp]
\centering
  \subfigure[Time domain]{\label{fig:raisedCosineTimeDomain}\includegraphics[width=6.5cm]{Figures/raisedCosineTimeDomain}}
    \subfigure[Frequency domain]{\label{fig:raisedCosineFrequencyDomain}\includegraphics[width=6.5cm]{Figures/raisedCosineFrequencyDomain}}
  \caption[{Raised cosines with $\rsym=1000$ bauds and roll-off $r=0, 0.5$ and $1$.}]{Raised cosines with $\rsym=1000$ baud and roll-off $r=0, 0.5$ and 1.}
  \label{fig:raisedCosines}
\end{figure}
%AK-LATER - need to check about r>1 I got the equations from Cioffi. They may be valid only for the range 0 to 1.

\figl{raisedCosineTimeDomain} shows raised cosines with $\rsym=1000$ bauds and roll-off $r=0, 0.5$ and 1, while \figl{raisedCosineFrequencyDomain} shows these pulses in the frequency domain. The spectrum of a raised cosine pulse is given by
\begin{equation}
P(\aw) = \left\{ {
\begin{array}{cl} 
{\tsym,} & {\lvert\aw\rvert \le \frac{\pi}{\tsym}(1-r)} \\ 
{\frac{\tsym}{2}\left[1 - \sin(\frac{\tsym}{2r}(\lvert\aw\rvert-\frac{\pi}{\tsym})) \right]}, & {\frac{\pi}{\tsym}(1-r) \le \lvert\aw\rvert \text{ and}}\\
& {\lvert\aw\rvert\le \frac{\pi}{\tsym}(1+r)} \\ 
{0,} & {\frac{\pi}{\tsym}(1+r) \le \lvert\aw\rvert} \\
\end{array} 
} \right.
\label{eq:raisedCosineSpectrum}
\end{equation}
%from: Raised-cosine (page 171 - Cioffi) - chap about equalization
%Note in \figl{raisedCosineFrequencyDomain} that $P(f)|_{f=0} = \tsym$ if $0 \le r \le 1$, which is the most used range for $r$.

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{Figures/pulse3_spectrum}
\caption{Equivalent to \figl{pulse1_spectrum} with a raised cosine pulse with $r=0.5$.\label{fig:pulse3_spectrum}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{Figures/pulse3_timedomain}
\caption{Equivalent to \figl{pulse1_timedomain} with a raised cosine pulse with $r=0.5$.\label{fig:pulse3_timedomain}}
\end{figure}

\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_rcosine\_timedomain}{snip_digi_comm_rcosine_timedomain}

\figl{pulse3_spectrum} and \figl{pulse3_timedomain} show graphs obtained with \codl{snip_digi_comm_rcosine_timedomain}.
Application~\ref{app:raisedCosinesGeneration} discusses how to generate raised cosine filters in more details and it may be worth a peek. When using the \ci{rcosine} function in Matlab (Octave misses it and here the companion \ci{ak\_rcosine} is assumed but the syntax is the same), a confusing aspect is that the fifth input parameter, the group delay, is accounted as the number of symbols (at $\rsym$ rate) not of samples (at $\fs$ rate). For example, in \codl{snip_digi_comm_rcosine_timedomain} this delay $D_{\textrm{sym}}$ of 2 symbols, in combination with an oversampling of $L=3$, gives a filter with a delay (in samples) of $D_s=D_{\textrm{sym}} \times L=6$ and consequently the order of the filter is 12 (the filter has 13 taps\index{Filter tap} or coefficients). Note that, in this case, $D_s$ was calculated assuming \ci{length(p)} is an odd number.
%The value \ci{N0} is given by $1+D_s$ because {\matlab} does not use 0 for the first sample. 

In summary, the FIR raised cosine is symmetric and has a constant group delay in the passband. This delay can be accounted as $D_{\textrm{sym}}$ in number of symbols or $D_s$ in number of samples.

The roll-off can be written
\[r=(\BW-\BW_{\textrm{min}})/\BW_{\textrm{min}},\]
where $\BW_{\textrm{min}}=\rsym/2$ is the minimum bandwidth at a given symbol rate. Hence, the required bandwidth when using a raised cosine is
\[
\BW=\BW_{\textrm{min}} (1+r)
\]
or, equivalently,
\[\BW=\frac{\rsym}{2} (1+r)~~~~~~\textrm{(in Hz)},\]
%from (Sklar - eq 3.80)
when $\rsym$ is in bauds.

%Sklar does not explain the Nyquist criterion, but it is good in saying that Nyquist pulses are sincs multiplied by something (pp. 137). Also, it mentions the tradeoff: a compact spectrum optimized bandwidth utilization but is very susceptible to ISI degradation induced by timing errors.

Note that when dealing with raised cosines, $\BW$ should not be understood as the 3-dB bandwidth, but the null-to-null bandwidth that includes passband and transition bands, from DC to the beginning of the stopband imposed by the first zero of the sinc. \equl{raisedCosineSpectrum} describes the spectrum mathematically.
%and the following discussion provides intuition about it.
Hence, in continuous-time, the passband $\BW_{\textrm{min}}$ of a raised cosine is determined by $\rsym$ (or, equivalently, $1/\tsym$) because $\rsym$ determines the sinc of $p(t)$ with spectrum given by \equl{sincTimeDomain}. 
The value of $r$ controls the cosine parcel and, together with $\rsym$, determines the transition band with width $r \BW_{\textrm{min}}$.

In discrete-time signals, the parcel corresponding to the sinc in \equl{raisedCosineDiscreteTime} has spectrum with bandwidth $\pi / L$ rad as given by \equl{sincTimeDomain}, such that
\[
\BW= \frac{\pi}{L} (1+r) ~~~~~~\textrm{(in rad)},
\]
as indicated, for example, in \figl{compareRaisedCosines}, where $\BW = \pi/3 (1+1) \approx 2.09$~rad.

While $\rsym$ and $r$ control $\BW$, the number of coefficients of the shaping pulse (or, equivalently, the order of this FIR filter) determines its attenuation in the stopband and group delay. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{Figures/compareRaisedCosines}
\caption{Comparison of raised cosine filters with $r=1, L=3$ and different orders.\label{fig:compareRaisedCosines}}
\end{figure}

\figl{compareRaisedCosines} compares raised cosine pulses with $r=1, L=3$ and different orders, obtained with the command \ci{p1=rcosine(1,L,[],r,N)}, where \ci{N=1,5} and 20.  For example, in \figl{compareRaisedCosines}, $L=3$ and according to \equl{sincTimeDomain}, the sinc parcel of the raised cosine imposes a $\BW_{\textrm{min}}$ of $\pi/3 \approx 1.05$ rad. Because $r=1$, the cosine part of \equl{raisedCosineDiscreteTime} creates a transition band of an extra 1.05 rad such that $\BW \approx 2$ rad.

\section{Matched Filtering and the Square-Root Raised Cosine}
\label{sec:matchedFilterAndSquareRootCosine}

In many situations the receiver should use a filter similar to the one with impulse response $p_t(t)$ used in the transmitter. When discussing receivers for AWGN in Section~\ref{sec:optimalReceiversAWGN}, it was concluded that the optimal filter is a matched filter $p_t^*(-t)$ delayed by $\tsym$. Hence, disregarding the delay by $\tsym$ and channel $h_c(t)$, \equl{pulseWithEqualizedChannel} can be written with $p_t(t)=p_{\textrm{sr}}(t)$ and $p_r(t)=p_{\textrm{sr}}^*(-t)$. In this case, the overall impulse response of \blol{isiModelRepeated} is 
\begin{equation}
p(t) = p_{\textrm{sr}}(t) \conv p_{\textrm{sr}}^*(-t).
\label{eq:timeDomainSquareRootRaisedCosine}
\end{equation}
Note that the goal is to have $p(t)$ as a Nyquist pulse, not $p_{\textrm{sr}}(t)$. As indicated in Section~\ref{sec:fourierProperties}, $\calF \{ p_{\textrm{sr}}^*(-t) \} = P_{\textrm{sr}}^*(f)$, therefore $P(f)=|P_{\textrm{sr}}(f)|^2$ and assuming that $P(f)$ is a non-negative ``normal'' raised cosine given by \equl{raisedCosine}, a sensible choice for $p_{\textrm{sr}}(t)$ is to have
\begin{equation}
P_{\textrm{sr}}(f) = P^{1/2}(f).
\label{eq:squareRootRaisedCosine}
\end{equation}
Hence, $p_{\textrm{sr}}(t)$ is called a ``square-root'' raised cosine (RRC) and $p(t)$ is a Nyquist pulse (achieves zero ISI). There are distinct definitions for this pulse and the one used in the companion code \ci{ak\_rcosine.m} is assumed here, which is compatible with the expression adopted in Matlab's \ci{rcosine} function. A RRC can be designed with \ci{ak\_rcosine(1,L,'fir/sqrt',r,P)} or using Matlab, with the same syntax: \ci{rcosine(1,L,'fir/sqrt',r,P)}.

%The RRC spectrum is approximately given by
%\[
%P_{\textrm{sr}}(\aw) = \left\{ {
%\begin{array}{*{20}cc} 
%{\sqrt \tsym,} & {\lvert\aw\rvert \le \frac{\pi}{\tsym}(1-r)} \\ 
%{\sqrt \frac{\tsym}{2}\left[1 - \sin(\frac{\tsym}{2r}(\lvert\aw\rvert-\frac{\pi}{\tsym})) \right]^{1/2}}, & {\frac{\pi}{\tsym}(1-r) \le \lvert\aw\rvert \text{ and}}\\
%&{\lvert\aw\rvert \le \frac{\pi}{\tsym}(1+r)} \\ 
%{0,} & {\frac{\pi}{\tsym}(1+r) \le \lvert\aw\rvert} \\
%\end{array} 
%} \right.
%\]
%In spite of the similarity of this expression with \equl{raisedCosineSpectrum} with respect to the bandwidth, when using truncated FIR approximations, the bandwidth of the RRC is slightly higher than
%for the corresponding normal RC (see, e.\,g., \exal{offsetsPAM}).

An expression for the RRC pulse in time domain is
%\begin{equation}
%p(t) = \frac{4 r}{\pi \sqrt \tsym} 
%\frac{\cos \left((1+r) \frac{\pi t}{\tsym} \right) +
%	    \frac{\tsym \sin \left( (1-r) \frac{\pi t}{\tsym} \right)}{4 r t}}
%{1-\left( \frac{4 r t}{\tsym} \right) ^2}.
%\end{equation}
\begin{equation}
p_{\textrm{sr}}(t) = \frac{ \sin \left( (1-r) \frac{\pi t}{\tsym} \right) +
\frac{4 r t}{\tsym} \cos \left( (1+r) \frac{\pi t}{\tsym} \right) }
{ \frac{\pi t}{\tsym}  \left[ 1 - \left( \frac{4 r t}{\tsym} \right)^2 \right] },
\label{eq:sqrtRC}
\end{equation}
which has singularities at $t=0$ and $t=\pm \tsym / (4r)$. Using L'Hospital's rule leads to
\[
p_{\textrm{sr}}(t)|_{t=0} = 1 - r + \frac{4r}{\pi}
\]
and
\[
p_{\textrm{sr}}(t)|_{t=\pm \tsym / (4r)} = \frac{r}{\sqrt{2}} \left[ \sin \left( \frac{\pi}{4r }\right)  \left(1 + \frac{2}{\pi}\right) +
\cos \left( \frac{\pi}{4r }\right)  \left(1 - \frac{2}{\pi}\right)  \right],
\]
respectively.
Similar to the conversion from \equl{raisedCosine} to \equl{raisedCosineDiscreteTime}, a discrete-time version of \equl{sqrtRC} can be obtained by substituting $t / \tsym$ by $n/L$.

As mentioned, the RRC\index{Square-root raised cosine (RRC)} in \equl{sqrtRC} does not lead to zero ISI. Only the cascade of two of them achieves this property. Consider what motivates a RRC: if eventually two Nyquist pulses given by \equl{raisedCosine} are cascaded (one used at the Tx and its matched version at the Rx, for example), the overall effect is not zero ISI! But having a RRC at the Tx and its matched filter at the Rx would lead to zero ISI in case the channel is ideal (or a perfect equalization is assumed).

%\[H(f) = \left\{ {\begin{array}{ll} {aa} & s \\ {bb} & j \\ \end{array} } \right\}\]

%More $E_b/\no$ may not alleviate ISI. Understand Fig. 3.18 in Sklar.


% Note the model used in the ISI problem (equalization) described in eq. 3.83 in Sklar. Sometimes $H_t$ and $H_r$ together correspond to a Nyquist pulse (zero ISI). In these cases, the equalizer ``simply'' inverts the channel. But, as explained in the last paragraph of section 3.4.1, sometimes the transmitter filter $H_t$ is a Gaussian pulse (for bandwidth effiency) and then the equalizer needs to compensate ISI besides the channel.

\section{Eye Diagrams}

%AK-LATER Vide \url{C:\ak2009\Classes\TransmissaoDigital\MatlabBasicExercises}

An eye diagram is used to observe the impact of noise and, especially, of ISI. The received signal is divided into segments of equal sizes. All the signal segments are plotted in
the same figure, with persistence (e.\,g., using the command \ci{hold on} on {\matlab}) so that they are overlapping. The length of the section should be a multiple of $\tsym$. 

The \codl{ex_eye_diagram} can be used to observe the incremental composition of an eye diagram for a 4-PAM modulation.

\includecodelong{MatlabOctaveBookExamples}{ex\_eye\_diagram}{ex_eye_diagram}


\begin{figure}[htbp]
\centering
    \subfigure[]{\label{fig:pulseForEyeDiagram}\includegraphics[width=6.5cm]{Figures/pulseForEyeDiagram}}
   \subfigure[]{\label{fig:eyeDiagramPulse}\includegraphics[width=6.5cm]{Figures/eyeDiagramPulse}}
  \caption{a) Signals corresponding to the b) eye diagram obtained with a 4-PAM shaped with a square pulse of non-zero samples and oversampling $L=3$.}
  \label{fig:eyeDiagramPulseAll}
\end{figure}

\figl{eyeDiagramPulseAll} exemplifies an eye diagram generated with a code similar to \codl{ex_eye_diagram} and using a square wave as shaping pulse. The shaping pulse has three non-zero samples and the oversampling is $L=3$. The time range of this diagram is $5 \ts$, which corresponds to six samples as identified by the location of the \ci{'x'} marks. Few modifications were done in \codl{ex_eye_diagram}: the variable \ci{increment} was decreased from $3 L$ to $2L$ and \ci{firstSample} was made equal to 2 to better centralize the diagram. Only 100 symbols were used to make easier tracking the construction of this diagram.  The first segment (related to the first three symbols) of this diagram is identified with red circles. In this case, as can be seen in \figl{pulseForEyeDiagram}, the first three symbols are $3, 3$ and $-3$. The eye diagram starts with the second sample of the first symbol (\ci{firstSample=2}) and its first segment ends with the first sample of the symbol $-3$.

\begin{figure}[htbp]
\centering
    \subfigure[]{\label{fig:rcosineForEyeDiagramr1}\includegraphics[width=6.5cm]{Figures/rcosineForEyeDiagramr1}}
   \subfigure[]{\label{fig:eyeDiagramRcosiner1}\includegraphics[width=6.5cm]{Figures/eyeDiagramRcosiner1}}
  \caption{Eye diagram obtained with a 4-PAM shaped with a raised cosine with roll off $r=1$.}
  \label{fig:eyeDiagramRcosiner1All}
\end{figure}

\begin{figure}[htbp]
\centering
    \subfigure[]{\label{fig:rcosineForEyeDiagramr0}\includegraphics[width=6.5cm]{Figures/rcosineForEyeDiagramr0}}
   \subfigure[]{\label{fig:eyeDiagramRcosiner0}\includegraphics[width=6.5cm]{Figures/eyeDiagramRcosiner0}}
  \caption{Eye diagram obtained with a 4-PAM shaped with a raised cosine with roll off $r=0$.}
  \label{fig:eyeDiagramRcosiner0All}
\end{figure}

\figl{eyeDiagramRcosiner1All} and \figl{eyeDiagramRcosiner0All} use raised cosines with $r=1$ and $r=0$, respectively, and were generated using the same data as \figl{eyeDiagramPulseAll}. Contrasting the three figures, it can be noted that, when using a raised cosine, the transmitted value is perfectly recovered only at the correct sample instants. For example, while all three samples corresponding to the second transmitted symbol (equal to $+3$) are the same in \figl{eyeDiagramPulseAll}, this value $+3$ is observed only at the proper sample instant in \figl{eyeDiagramRcosiner1All} and \figl{eyeDiagramRcosiner0All}. 
Therefore, eye diagrams indicate what happens when there is a synchronization error at the receiver with respect to the correct moment to extract a symbol. The horizontal opening of the eye diagram indicates the robustness of the system to eventual synchronization errors or errors in the so-called \emph{timing phase}. When comparing \figl{eyeDiagramRcosiner1All} and \figl{eyeDiagramRcosiner0All}, one can see that a raised cosine with $r=1$ consumes twice the bandwidth of a raised cosine with $r=0$, but the former is more robust against timing phase errors.

In summary, shaping pulses as the one used in \figl{eyeDiagramPulseAll} are very effective against synchronization errors but may consume too much bandwidth, while the the roll-off factor $r$ of raised cosines allows to trade off bandwidth and complexity of the synchronization block at the receiver. Another interpretation is that the slope of the inside eye lid indicates the sensitivity to jitter (variations with respect to the assumed periodicity) in the timing phase.

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{Figures/eyeDiagramAWGN}
\caption{Eye diagrams for 2-PAM (symbols $\pm 1$) using raised cosines with $r=0$ and $r=1$ under AWGN with $\snr=15$ dB or noise-free transmission.\label{fig:eyeDiagramAWGN}}
\end{figure}

Besides synchronization, an eye diagram is also useful to infer about the noise at the receiver.
\figl{eyeDiagramAWGN} uses binary transmission (2-PAM with symbols $\pm 1$), $L=3$ and raised cosines with $r=0$ and $r=1$ under two scenarios: AWGN with 15 dB of SNR and noise-free transmission. It can be noted that the additive noise impacts the vertical opening of the eye and, even at the proper sampling instants, the received signal is not perfectly recovered as in the noise-free transmission.

\figl{eye_2500MBD_2PAM} and \figl{eye_12500MBD_4PAM} provide examples of eye
diagrams from actual measurements.

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{FiguresNonScript/eye_2500MBD_2PAM}
\caption{Eye diagram obtained for an 2-PAM signal transmitted using 2.5 Gbauds over an optical link.\label{fig:eye_2500MBD_2PAM}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{FiguresNonScript/eye_12500MBD_4PAM}
\caption{Eye diagram obtained for an 4-PAM signal transmitted using 12.5 Gbauds over an optical link.\label{fig:eye_12500MBD_4PAM}}
\end{figure}


%\color[rgb]{0,0,0}
\section{Distinguishing Nyquist Criterion and Sampling Theorem}

\equl{nyquistZeroISI} is similar to $\fs > 2 f_{\textrm{max}}$, posed by the sampling theorem, especially in case one interprets $f_{\textrm{max}}$ as the bandwidth $\BW$ of the signal. 
%But note that for zero ISI, $\BW$ upper bounds $\rsym$.
%Do not confuse the Nyquist criterion for zero ISI with the Nyquist sampling theorem. They are different things. 

But the reader should distinguish the criterion for zero ISI from the sampling theorem.
For example, assume a channel of bandwidth $\BW = 200$~Hz is used to transmit a signal with $f_{\textrm{max}} = \BW$ and the symbol rate is $\rsym=100$~bauds. Because $\rsym \le 2\BW$ is obeyed, a zero ISI operation is possible even with the receiver ``sampling'' at the symbol rate $\rsym$, that is, its ADC using $F_s^{\textrm{Rx}} = 100$~Hz. However, this sampling frequency is below the required $F_s^{\textrm{Rx}} > 2 W = 400$~Hz of the sampling theorem. 
%As explained in Sklar, page 141,
This is not surprising because in digital communications one is not concerned with reconstructing the waveform unambiguously and can afford aliasing. Recall that the goal is ``simply'' unambiguous detection.\footnote{The notation $F_s^{\textrm{Rx}}$ emphasizes that the ADC is being used as part of a digital communication system, and not for creating a sampled version that will allow perfect signal reconstruction.}

Decreasing the sampling frequency $F_s^{\textrm{Rx}}$ reduces the computational cost at the receiver. Hence, a lower-bound on $F_s^{\textrm{Rx}}$ is of interest and can be obtained as follows: assuming the symbols are independent, at least one signal sample must be used to represent the respective symbol value, such that $F_s^{\textrm{Rx}} \ge \rsym$. In other words, the minimum $F_s^{\textrm{Rx}}$ at the receiver corresponds to an oversampling factor of $L=1$, i.\,e., using only one sample to represent a symbol. 
However, for improved performance, this minimum value is sometimes exceeded in favor of $L>1$.
\tabl{fsAndRsym} summarizes the discussion.

\begin{table}
\centering
\caption[{Contrasting the Nyquist criterion for zero ISI and the sampling theorem for real-valued signals.}]{Contrasting the Nyquist criterion for zero ISI and the sampling theorem for \textbf{real-valued} signals. It is assumed that the signals are baseband and $f_{\textrm{max}} = \BW$.\label{tab:fsAndRsym}}
\begin{tabular}{|l|l|c|}
\hline
\emph{Context} & \emph{Equation} & \emph{Interpretation}\\ \hline
Sampling theorem (\equl{samplingTheoremRealSignals}) & $\fs > 2 \BW$ & At least $\fs$ to reconstruct the original signal \\ \hline
%Zero-ISI Nyquist criterion & $\rsym \le 2 \BW$ & At most $\rsym$ for zero ISI \\ \hline
Zero-ISI Nyquist criterion & $\rsym \le 2 \BW$ & BW Hz allows a maximum of $2\BW$ bauds\\ \hline
Processing at receiver & $F_s^{\textrm{Rx}} \ge \rsym$ & At least one sample representing a symbol\\ \hline
\end{tabular}
\end{table}

\tabl{fsAndRsym} can be expanded when one considers complex-valued signals. The alternative
 definitions of bandwidth discussed in Section~\ref{sec:bandwidthDefinitions} are important here,
together with \equl{samplingTheoremComplexSignals}. The results for both real and complex-valued
signals are very similar when one considers that an analytic signal of bandwidth BW has a 
double-sided bandwidth that is half of the one considered for real-valued signals.

\begin{table}
\centering
\caption[{Contrasting the Nyquist criterion for zero ISI and the sampling theorem for analytic complex-valued signals.}]{Contrasting the Nyquist criterion for zero ISI and the sampling theorem for analytic complex-valued signals. The signals are baseband with support from 0 to $f_{\textrm{max}} = \BW$.\label{tab:fsAndRsymComplex}}
\begin{tabular}{|l|l|c|}
\hline
\emph{Context} & \emph{Equation} & \emph{Interpretation}\\ \hline
Sampling theorem (\equl{samplingTheoremComplexSignals}) & $\fs > \BW$ & At least $\fs$ to reconstruct the original signal \\ \hline
%Zero-ISI Nyquist criterion & $\rsym \le \BW$ & At most $\rsym$ for zero ISI \\ \hline
Zero-ISI Nyquist criterion & $\rsym \le \BW$ & BW Hz allows a maximum of $\BW$ bauds\\ \hline
Processing at receiver & $F_s^{\textrm{Rx}} \ge \rsym$ & At least one sample representing a symbol\\ \hline
\end{tabular}
\end{table}

%transmitted over passband channels. 
For example, assuming a complex envelope with negligible energy outside the frequency range $[0, F_\tmax]$ and calling its bandwidth $\BW=F_\tmax$, \equl{nyquistZeroISI} can be written as
\begin{equation}
\rsym \le \BW.
\label{eq:nyquistZeroISIComplexSignals}
\end{equation}
For the results in \tabl{fsAndRsymComplex}, two things to keep in mind are that each sample or symbol corresponds to two
real values and BW is half of the double-sided bandwidth.

%\tabl{fsAndRsymComplex} is a version of \tabl{fsAndRsym}, but considering complex-valued signals.


%When the task is performing simulation, not a real-time receiver, oversampling allows to represent the spectra of signals involved in the simulation and should ideally obey the sampling theorem or at least minimize the distortion due to aliasing.

% However, as illustrated in \figl{flat_psd_pam_awgn}, with independent symbols and $\fs = \rsym$ ($L=1$ or no oversampling), the PSD is flat in frequency and there is no control over the spectrum of the transmitted and received signals, for example. Hence, from a frequency-domain perspective, 

%Therefore, for simulations, the value of $\rsym$ is typically chosen to maximize the bit rate given the constraint $\rsym \le 2 \BW$ imposed by the available channel bandwidth $\BW$.
 %(from the Nyquist theorem for zero intersymbol interference, to be discussed in Section~\ref{sec:nyquistZeroISI}). 

%AK-IMPROVE: %\section{Frequency-selective AWGN} e %\section{Wireless Communication Channels: Rayleigh and Rician Fading}

%AK-TODO finish scripts, do MMSE
\section{Equalization and System Identification}
\label{sec:equalization}

Many practical channels are frequency-selective and there are several techniques used in digital communication systems to circumvent the corresponding effects. 
%There are many distinct strategies to deal with dispersive channels. 
One of them is to first equalize the channel and then treat it as an AWGN channel. Another one is to use multicarrier modulation, which is discussed in Chapter~\ref{ch:multicarrier}.

The approach discussed in the sequel is to use a LTI system as an equalizer. Two well-known equalizers are the so-called zero-forcing (ZFE) and minimum mean square error (MMSE) equalizers.

Some of the main characteristics that allow to characterize an equalizer are:
\begin{itemize}
	\item Approach: ZFE (aims at inverting the channel and completely minimize ISI), MMSE (maximize SNR), etc.;
	\item linear or non-linear;
	\item trained (estimation aided by training sequence) or blindly estimated;
	\item having finite (FIR) or infinite (IIR) impulse response;
	\item symbol-spaced (or baud-spaced, in which the sampling period
	$\ts = \tsym$ coincides with the symbol period $\tsym$) or fractionally-spaced (in which 
	$\ts < \tsym$).
\end{itemize}

The equalizer design is typically based either only on the output $y(t)$ of the unknown system or on a pair $\{x(t),y(t)\}$ of input $x(t)$ and corresponding output $y(t)$. The first case leads to a \emph{blind} estimation\index{Blind estimation} problem, which is harder than the second. In the second case, assuming the telecommunications context, the input $x(t)$ used for equalization (or system identification) is called \emph{training sequence}, \emph{probing sequence}, \emph{pilot} or \emph{preamble}, depending on the specific scenario.

One approach to equalization is to first identify (or ``discover'') the channel and then use the estimated
channel to obtain the equalizer. This is the \emph{indirect} equalizer design, while the
\emph{direct} method estimates the equalizer without explicitly identifying the system.
Starting by the indirect methods, the next section discusses system identification.

\subsection{System identification using least squares (LS)}
\label{sec:ls_estimation}
%\bApplication
%\textbf{System identification using least squares (LS).}

System identification is the process of estimating a model for an unknown system $\calH$. 
There are many tools and techniques for system identification.\footnote{For example, see  \akurl{http://www.mathworks.com/products/sysid}{7sid}.}
Here, the classic \emph{least-squares} (LS) method\index{Least-squares system identification} is discussed, which is part of the \emph{linear} algorithms group. Non-linear algorithms are typically more involved. 

It is also assumed discrete-time processing and that a training sequence is used. Hence, a discrete-time version of the input $x[n]$ is known at the receiver. Besides, the system is assumed to be LTI with (unknown) finite-length impulse response $h[n]$. There is AWGN $\nu[n]$ at the output, such that the output sequence is $y[n] = x[n] \conv h[n] + \nu[n]$. The task is to obtain the LS estimate $h^\dag[n]$ of $h[n]$ defined as
\begin{equation}
h^\dag[n] = \arg \min_{\hat h[n]} \sum_{n} (y[n] - x[n] \conv \hat h[n])^2.
\label{eq:ls_estimation}
\end{equation}
Note that $\nu[n]$ is ignored in this optimization criterion.

%AK: When \bx is put in italics, it gets weird. So I use \bmx to avoid italics.

Assuming finite-length sequences, it is convenient to use a matrix notation where $y[n],x[n],h[n]$ and $\nu[n]$ are represented as column vectors $\by,\bx,\bh$ and $\bn$, respectively.  The convolution $x[n] \conv h[n]$ can be represented as $\bH \, \bx$ or $\bX \, \bh$ (recall that convolution is commutative), where $\bH$ and $\bX$ are convolution matrices, obtained e.\,g. with \ci{convmtx} as discussed in Section~\ref{sec:convolutionAsMatrix}. 

Hence, the convolution can be stated as $\by = \bX \, \bh + \bn$, which is often more convenient than using $\by = \bH \, \bx + \bn$ because $\bX$ is known by the system designer and this can avoid a matrix inversion.
\equl{ls_estimation} can then be written in matrix notation as
\begin{equation}
{\bh}^\dag = \arg \min_{\hat \bh} ||\by - \bX \, \hat \bh||^2.
\label{eq:ls_problem}
\end{equation}

Alternatively, a least squares problem such as \equl{ls_problem} can be cast as the
solution of a set of linear equations
\begin{equation}
\by = \bX \, \bh^\dag.
\label{eq:ls_problem_equationsSet}
\end{equation}
%Before discussing the solution of \equl{ls_problem}, it is useful to develop some intuition
%considering there is no noise (all elements of $\bn$ are zero).
%Because $\bn$ is AWGN, it can be discarded when looking for a solution. 
Assuming 
%the problem is to find the best estimate of $\bh$, where $\by = \bX \, \bh$ and 
that $\bX$ is a square and invertible matrix, the solution to \equl{ls_problem_equationsSet}
would be simply ${\bh}^\dag = {\bX}^{-1}  \, \by$. In contrast, when $\bX$ is not square, the inverse ${\bX}^{-1}$ does not exist and a \emph{pseudoinverse}\index{Pseudoinverse} must be used.

It can be shown that the solution of \equl{ls_problem} in the least-squares sense is
\begin{equation}
{\bh}^\dag = \bX^{+} \,  \by,
\label{eq:ls_solution_general}
\end{equation}
where $\bX^+$ is the \emph{Moore-Penrose pseudoinverse} matrix of $\bX$ (see Appendix~\ref{sec:pseudoinverse} for a brief discussion on pseudoinverses).

In most practical cases, for a robust estimation,
\equl{ls_problem_equationsSet} is an overdetermined system, where the number of equations is larger than the number of unknowns (elements of $\hat \bh$). 

The number of elements of $\hat \bh$ and $\bx$ are denoted here as $N_h$ and $N_x$, respectively. The convolution matrix $\bX$ is assumed to be a ``tall'' matrix with full rank and
size $N_y \times N_h$, where $N_y = N_h + N_x - 1$. In this case, $\bX^{+}$ is given by \equl{pseudoInverseOver} such that \equl{ls_solution_general} corresponds to
\begin{equation}
{\bh}^\dag = \bX^{+} \,  \by = \left( \bX^H \, \bX \right)^{-1}  \bX^H \,  \by.
\label{eq:ls_solution}
\end{equation}

A figure of merit for assessing channel estimation techniques is the normalized mean squared error (NMSE), which in dB is given by:
\begin{equation}
\textrm{NMSE (dB)} = 10 \log_{10} \left( \frac{||\bh-\bh^\dag||^2}{||\bh||^2} \right).
\label{eq:nmse_dB}
\end{equation}

\bExample \textbf{Channel identification with least-squares}.
\codl{snip_systems_channel_estimation} presents a very simple example of LS applied to channel estimation when noise (AWGN) is present.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_channel\_estimation}{snip_systems_channel_estimation}
%\begin{lstlisting}
%x=transpose(1:5); %input, column vector
%h=transpose(-4:1:1) %channel, column vector
%y=conv(x,h); %pass input signal through the channel
%y=y + 0.2*randn(size(y)); %add noise
%X=convmtx(x,length(h)); %prepare for convolution as matrix
%h_hat = pinv(X)*y %LS estimate via M-P pseudoinverse
%\end{lstlisting}
The SNR between the noiseless channel output signal and the added noise impacts the estimation accuracy.
Decreasing the AWGN power in \codl{snip_systems_channel_estimation} leads to a more accurate estimation, as expected. Increasing \ci{Nx} also decreases \ci{MSE}.
\eExample

%\subsubsection{LS channel estimation without matrix multiplication}
%\subsubsection{Training sequence desired characteristics}
\subsubsection{Synchronization together with channel estimation}

As mentioned, modeling the system as $\by = \bX \, \bh + \bn$ instead of $\by = \bH \, \bx + \bn$ allows to pre-compute $\left( \bX^H \, \bX \right)^{-1}  \bX^H$ and implement \equl{ls_solution} with a simple matrix multiplication. But the method can be further refined
to enable performing synchronization together with channel estimation.

In telecommunications, the system designer may choose a training sequence, but often cannot 
assume that the receiver knows when the training sequence starts. 
%Therefore, $x[n]$ can be chosen such that $\bX^H \, \bX$ is square and has inverse.
 To help synchronization, $\bx$ can be chosen such that 
\begin{equation}
(\bX^H \, \bX)^{-1}=\alpha \bI,
\label{eq:desiredAutocorrelationProperty}
\end{equation}
where $\bI$ is the identity matrix and $\alpha$ a scalar.

The property suggested by \equl{desiredAutocorrelationProperty} can be 
achieved by choosing $\bx$ (and $N_h$) to have $\bX^H \, \bX=(1/\alpha) \bI$. Note also that
the element $(p,q)$ of $\bX^H \, \bX$ is the inner product $\langle x[n-p], \, x[n-q] \rangle$ between shifted versions of $x[n]$ and can be interpreted as an autocorrelation value. \figl{autocorrelationSignalAndMatrix} illustrates this fact by comparing the autocorrelation $R_x[k]$ of $\bmx$ with a 3-d plot of $\bX^H \, \bX$. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{./Figures/autocorrelationSignalAndMatrix}
\caption{Autocorrelation $R_x[k]$ (left plot) of a M-sequence $\bmx$ with $2^m-1$ samples, $m=5$. The corresponding $\bX^H \, \bX$ gets closer to an identity matrix as $R_x[k]$ gets closer to an impulse.\label{fig:autocorrelationSignalAndMatrix}}
\end{figure}

\figl{autocorrelationSignalAndMatrix} used a \emph{maximum length sequence}\index{Maximum length sequence}  \akurl{http://en.wikipedia.org/wiki/Maximum_length_sequence}{7max} or M-sequence, which are known to have an autocorrelation with a large peak.
As suggested by the color bar dynamic range, the values of $\bX^H \, \bX$ in \figl{autocorrelationSignalAndMatrix} coincide with $R_x[k]$ and the peak value is $2^m-1$, which corresponds to 31 in \figl{autocorrelationSignalAndMatrix}. 

There are alternatives to M-sequences such as the ones that have (approximately) a periodic impulse train as autocorrelation. The impulse response can be properly estimated if it fits between two impulses (i.\,e., there are enough zeros between them) of the autocorrelation. GSM uses such a training sequence.

Assuming that \equl{desiredAutocorrelationProperty} applies, \equl{ls_solution} simplifies to
\begin{equation}
{\bh}^\dag = \alpha \bX^{H} \,  \by.
\label{eq:ls_simplified_solution}
\end{equation}
Similar to the interpretation suggested by
\figl{autocorrelationSignalAndMatrix}, $\bX^{H} \,  \by$ in \equl{ls_simplified_solution}
corresponds to the cross-correlation between $\bx$ and $\by$. To obtain synchronization,
the receiver can continuously calculate this cross-correlation and search for a peak
that identifies the beginning of the training sequence as seen by the receiver. After that,
\equl{ls_simplified_solution} can be applied to estimate the channel.
A simple implementation of this procedure is discussed in the sequel.

\bExample \textbf{LS channel identification and synchronization}.
Assuming that \equl{desiredAutocorrelationProperty} applies, 
\equl{ls_simplified_solution} can be implemented as in \codl{snip_systems_Msequence_chan_est}.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_Msequence\_chan\_est}{snip_systems_Msequence_chan_est}

Instead of using \ci{xcorr(x)} to estimate the cross-correlation, 
\codl{snip_systems_Msequence_chan_est} uses convolution to obtain \ci{z}.
As discussed in Section~\ref{sec:convolutionViaCorrelation}, the two alternatives
are equivalent.

In the last lines, \codl{snip_systems_Msequence_chan_est} emphasizes that obtaining \ci{z2}
provides an alternative interpretation of \ci{z}. Using notation of sequences instead of vectors: while \ci{z} was obtained with $(x[n] \conv h[n]) \conv x[-n]$, using the commutative
and associative convolution properties, the operations were rearranged to obtain \ci{z2} via
$(x[n] \conv x[-n]) \conv h[n] = R[n] \conv h[n]$, where $R[n]$ is the autocorrelation of $x[n]$.

The procedure suggested by \codl{snip_systems_Msequence_chan_est} fails when the
peak of the impulse response is not its first sample. For example, 
the reader is invited to modify \codl{snip_systems_Msequence_chan_est} to
use \ci{h=[2;3;4]} and observe that \codl{snip_systems_Msequence_chan_est} is restricted to extract the samples after the impulse response peak.
In more elaborated schemes, after the cursor is chosen, equalizers for the pre-cursor and 
post-cursor parts of the channel impulse response can be used.
\eExample

%AK: Below is code about GSM. Maybe recover it.
\ignore{
Note in \codl{snip_systems_Msequence_chan_est} that if \ci{R(maxIndex)} were a non-zero value with all neighboring samples equal to zero (\ci{R} is a perfect autocorrelation), the impulse response would start from sample \ci{maxIndex}. The code mimics what is done in practice: searching for the maximum of the absolute values of \ci{z}.
%
It remains to relate the autocorrelation property with \equl{ls_simplified_solution}. Note that the last line indicates that \ci{h\_hat} requires only few samples of $\ci{z}$ and one can model a convolution limited to these samples. For the example, \ci{maxIndex=4095}, and from a convolution property one could observe that all the following quantities are equal:
\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_Msequence\_step2}{snip_systems_Msequence_step2}
%\begin{lstlisting}
%z(maxIndex)*maxR
%z2(4094+1)*maxR
%R(4094+1)*h(0+1)+R(4093+1)*h(1+1)+R(4092+1)*h(2+1)
%\end{lstlisting}
\noindent where 1 was added to all indices because {\matlab} starts sequences at $n=1$. Applying a similar reasoning to \ci{z(4096)} and \ci{z(4097)}, the remaining samples from \ci{z(maxIndex:maxIndex+M)}, one concludes that only the values of \ci{R} in the range $[4093, 4097]$ are needed to obtain \ci{h\_hat}. Executing \codl{snip_systems_Msequence_step3} after \codl{snip_systems_Msequence_step2} helps to observe that the pseudoinverse is not necessary in this case.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_Msequence\_step3}{snip_systems_Msequence_step3}
%\begin{lstlisting}
%Rsegment = R(maxIndex-M:maxIndex+M)/maxR; %segment of R
%X=convmtx(Rsegment,length(h)); %convolution matrix
%z3=X*h; %convolution center of z3 is equal to center of z
%h_hat2 = z3(M+1:2*M+1) %equal to h_hat
%A=X'*X %note it is approximately the identity matrix
%\end{lstlisting}
}

\bExample \textbf{More complete example of LS channel identification and synchronization}.
\codl{ex_systems_LS_channel_estimation} provides a more complete example of LS estimation.
For example, it allows to investigate the impact of the AWGN SNR and input length.

\includecodelong{MatlabOctaveBookExamples}{ex\_systems\_LS\_channel\_estimation}{ex_systems_LS_channel_estimation}

\codl{ex_systems_LS_channel_estimation} also compares the solutions provided by three
methods. The first one corresponds to using \equl{ls_solution} and the other two to \equl{ls_simplified_solution}, which assume that \equl{desiredAutocorrelationProperty} applies.
The third method differs from the previous two because it does not assume complete knowledge
about synchronization.
Also, there are three different options for the training sequence. For example, using
\ci{inputSequenceChoice=1} makes the second and third estimation methods fail because
\equl{desiredAutocorrelationProperty} would not be observed.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/ls_channel_estimation}
\caption{Comparison between least-squares estimations using methods 1, 2 and 3 from \codl{ex_systems_LS_channel_estimation}.\label{fig:ls_channel_estimation}}
\end{figure}

\figl{ls_channel_estimation} shows a plot obtained after running \codl{ex_systems_LS_channel_estimation} with inputs
\ci{inputLengthLog2=6}, \ci{inputSequenceChoice} and \ci{SNRdB = 20}. The channel is a bandpass Chebyshev type I filter, the SNR is 20~dB and the input is an M-sequence (\ci{inputSequenceChoice=3}) with $2^6-1=63$ samples. In this case, the comparisons between the correct \ci{h} and the estimates \ci{h\_est}, \ci{h\_est2} and \ci{h\_est3} leads to, respectively (results may slightly change due to the random number generators):
\begin{verbatim}
SNR = 20 dB and 63 training samples
NMSE: -18.7647     -7.05488     -7.05488 (dB)
Spectral distortions: 0.12043      1.2796      1.2796 (dB)
\end{verbatim}
The second and third methods obtained the same result because the ``synchronization''
worked properly in this case, and \ci{guessFirstIndex} was indeed the best option.
The first method significantly outperforms the others because the training sequence is
relatively short and \equl{desiredAutocorrelationProperty} is not accurate. The average spectral distortion over the passband\footnote{Calling \ci{ak\_spectralDistortion.m} with a threshold of 10~dB excluded the stopband from the calculation in this case.} 
increased from 0.12043 to 1.2796~dB when going from method 1 to 2 (or 3). In fact, as illustrated in \figl{ls_channel_estimation}, the frequency response estimated by the two latter methods have peaks of more than 3~dB over
the passband.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/ls_channel_estimation_longtrain}
\caption{Same as \figl{ls_channel_estimation} but with training sequence increased from 63 to 2047 samples.\label{fig:ls_channel_estimation_longtrain}}
\end{figure}

Increasing the training sequence length from 63 to 2047 samples (by changing \ci{inputLengthLog2} from 6 to 11) leads to \figl{ls_channel_estimation_longtrain} and the following result:
\begin{verbatim}
SNR = 20 dB and 2047 training samples
NMSE: -30.767     -26.1736     -26.1736 (dB)
Spectral distortions: 0.035982      0.1119      0.1119 (dB)
\end{verbatim}
In this case, a relatively long M-sequence is used and all three
methods had performance limited by the SNR. The longer training sequence leads to a better approximation of \equl{desiredAutocorrelationProperty} for Methods 2 and 3.
The average spectral distortion of all three methods was below 1~dB over the passband.

%But the third method has a large error in time-domain, which indicates a synchronization problem.
%In this case, \ci{guessFirstIndex} was not the best option.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/ls_channel_estimation_snrinf}
\caption{Same as \figl{ls_channel_estimation} but with SNR increasing from 20~dB to infinite.\label{fig:ls_channel_estimation_snrinf}}
\end{figure}

Using a training sequence length of 63 and infinite SNR leads to \figl{ls_channel_estimation_snrinf} and the following result:
\begin{verbatim}
SNR = Inf dB and 63 training samples
NMSE: -292.1716     -7.321442     -7.321442 (dB)
Spectral distortions: 3.5922e-15      1.2649      1.2649 (dB)
\end{verbatim}
In this case, the first method obtained \ci{h\_est} virtually equals to \ci{h}, where
the NMSE of $-292.1716$~dB indicates there were only numerical errors.
However, the other two methods did not improve given that the training sequence is relatively short.

%But the third method has a large error in time-domain, which indicates a synchronization problem.

Note that synchronization was not a problem even for method 3. In case, \ci{guessFirstIndex} were not the best option, method 3 would be outperformed by method 2.
In fact, it is not trivial to implement \equl{ls_simplified_solution} and better algorithms than the ones in \codl{ex_systems_LS_channel_estimation} should be used to achieve a more robust estimation in practice.
%Hence, synchronize the signals A relevant problem is to such that the sequences are properly converted into vectors that allow the assumptions to be valid. 
This issue is further discussed in Application~\ref{app:GSM_data} in the context of GSM.
%\eApplication
\eExample

Given that the channel has been estimated with LS or other method, it remains the task of designing the equalizer.
The next subsection discusses one well-known criterion.

\subsection{Zero-forcing (ZF) equalization criterion}

When dealing with a frequency-selective channel $h_c(t)$, it is tempting to use an equalizer to try to completely remove any effect of $h_c(t)$. Theoretically, this is possible using an equalizer designed with the zero-forcing criterion. If $H_c(s)$ is the Laplace transform of $h_c(t)$, the zero-forcing equalizer (ZFE) corresponds to a system function $1/H_c(s)$. However, if $H_c(s_0)=0$ at some value $s_0$, then $1/H_c(s_0) \rightarrow \infty$, which creates problems in practice. For example, for all frequencies larger than the cutoff frequency $f_c$ of the ideal lowpass filter, even with an infinite gain, the equalizer cannot recover the input signal because the components with frequency $f > f_c$ were already zeroed. But the ZF criterion does not capture this condition
and would still try to invert the channel.

Another aspect is that the zeros in $H_c(s)$ become the poles in the ZFE. Hence, if $H_c(s)$ is minimum phase\index{Minimum phase}, i.\,e., its inverse $1/H_c(s)$ is causal and stable. For example, assuming causal systems, if all the zeros of $H_c(s)$ are at the left-half side of the the $s$-plane, then the equalizer $1/H_c(s)$ is stable. But if the channel is not minimum phase, the ZFE may be unstable (or noncausal). Similarly, for discrete-time and causal signals, zeros of $H_c(z)$ outside the unit circle would lead to an unstable ZFE system $1/H_c(z)$. 

\bExample \textbf{Impact of minimum phase in equalization}.
\codl{snip_digi_comm_min_phase_channel} creates a minimum phase discrete-time channel and the function \ci{deconv} can perfectly recover the channel input signal:
\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_min\_phase\_channel}{snip_digi_comm_min_phase_channel}
%\begin{lstlisting}
%zeros=[0.3*exp(j*pi/2) 0.3*exp(-j*pi/2) ...
% 0.8*exp(j*pi/4) 0.8*exp(-j*pi/4)];
%h=poly(zeros);
%n=0:1000-1; %abscissa
%x=4*cos(pi/8*n) + 6*cos(0.7*pi*n+pi/5); %input signal
%y=conv(x,h); %filter
%xRecovered=deconv(y,h); %try to deconvolve
%plot(x-xRecovered) %compare the signals
%\end{lstlisting}
In contrast, the FIR filter designed with \codl{snip_digi_comm_not_min_phase_chan} has zeros 
\co{[-16.79 -1.0 -0.06]} and, therefore, is not minimum phase as can be seen with the command \ci{zplane(h)}.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_not\_min\_phase\_chan}{snip_digi_comm_not_min_phase_chan}
Observing the signal \ci{xRecovered} shows that it diverges and has amplitudes going to infinite.
\eExample

In practice, the equalizer should work only for frequencies within the channel passband.
Hence, one important requirement is to design the communication system such that the transmitted signal has a PSD with most of its power contained within the system bandwidth. This allows the receiver to, taking that in account, filter the out-of-band signal and equalize within the frequency band of interest.

One of the most used ZFEs are the discrete-time FIR equalizers estimated
via least squares. This specific class of equalizers will be discussed in the sequel.

\subsection{Least squares FIR equalizer based on estimated channel}

Assuming estimated channel FIR coefficients stored in vector $\hat {\bh}$, the ZFE would be
an IIR filter. However, it is sometimes advantageous to also use a FIR to perform equalization
due to properties such as stability and simpler design.
In the sequel, the least squares (LS) method is used to design a FIR equalizer $\bw$.

Denoting the elements of $\hat {\bh}$ and $\bw$ as sequences $\hat h[n]$ and $w[n]$,
the ZF criterion suggests seeking $w[n]$ that leads to $\hat h[n] \conv w[n] = \delta[n-\Delta]$, where $\Delta$ is the delay the signal suffers after passing through the
(estimated) channel and equalizer. The design of $\bw$ can be cast as a LS problem
 written in matrix notation as
\begin{equation}
\hat \bH \, \bw = \be_{\Delta},
\label{eq:ls_equalizer}
\end{equation}
where $\hat \bH$ is the convolution matrix obtained from $\hat \bh$ and $\be_{\Delta}$ is a vector with zeros but the $\Delta$-th element, which is one, representing $\delta[n-\Delta]$.

Similar to \equl{ls_problem_equationsSet} and its solution \equl{ls_solution}, the 
solution to \equl{ls_equalizer} uses the pseudoinverse $\hat \bH^+ = \left(\hat \bH^H \, \hat \bH \right)^{-1}  \hat \bH^H$ and is given by
\begin{equation}
{\bw}^\dag = \hat \bH^+ \,  \be_{\Delta} = \left(\hat \bH^H \, \hat \bH \right)^{-1}  \hat \bH^H \,  \be_{\Delta}
\label{eq:ls_solution2}
\end{equation}
with a corresponding MSE given by $||\hat \bH {\bw}^\dag - \be_{\Delta}||^2$.

The optimal value of $\Delta$ in \equl{ls_solution2} depends on $\hat \bH$ and
can be found by trial-and-error.\footnote{The search for the best delay $\Delta$
can be avoided by constructing a matrix that allows to simultaneously evaluate the options. See, e.\,g., \cite{Johnson11}, page 279.} An educated guess can be $(N_c-1)/2$, where $N_c$ is the
number of rows of $\hat \bH$ (i.\,e., the number of elements in the convolution
output). The number of elements of $\be_{\Delta}$ is also $N_c$, such that the
suggested guess corresponds to positioning the impulse approximately in the
middle of $\be_{\Delta}$.

\bExample \textbf{Least squares equalizer}.
\codl{snip_channel_ls_equalizer} provides an example of using \equl{ls_solution} to design
an equalizer.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_channel\_ls\_equalizer}{snip_channel_ls_equalizer}
The output of \codl{snip_channel_ls_equalizer} with \ci{Nw=10} and \ci{h=[0.1; 0; 0; 0.5; 3; 0.2; -0.1]} is:
\begin{verbatim}
minMSE = 2.9126e-07    bestDelta = 11     guessBestDelta = 8
\end{verbatim}
which indicates that the best $\Delta$ was 11, with a $\textrm{MSE}=2.9126 \times 10^{-7}$, while rounding
$(N_c-1)/2$ suggested $\Delta=8$. The reader is invited to vary \ci{Nw} and/or \ci{h}, observing how the performance changes. For example, one obtains
\begin{verbatim}
minMSE = 0.0022       bestDelta = 2     guessBestDelta = 8
\end{verbatim}
when keeping \ci{Nw=10} and modifying the first element of \ci{h} from 0.1 to 5.1.
%As in \equl{ls_solution}, \codl{snip_channel_ls_equalizer} assumes that \ci{Hhat} is full rank.
\eExample

\subsection{FIR equalizer: LS direct estimation with training sequence}

\equl{ls_problem_equationsSet} and \equl{ls_equalizer} can be depicted via block diagrams
as follows:

and
\begin{equation}
\hat h[n] \arrowedbox{$w[n]$} \delta[n],
%\label{eq:}
\end{equation}
respectively. In both cases, the sequence within the boxes should be estimated.

Instead of first estimating the channel and then the equalizer, it is often more
convenient and accurate to directly obtain the equalizer. As previously, the LS solution 
can be obtained by solving a set of linear equations.

The LS equalizer can be obtained directly by considering as goal to implement:
\begin{equation}
x[n] \arrowedbox{$h[n]$} y[n] \arrowedbox{$w[n]$} x[n],
%\label{eq:}
\end{equation}
where $w[n]$ should be estimated to transform the observed received signal $y[n]$ back into the training sequence $x[n]$, according to the ZFE criterion.

The equivalent system is $\bx = \bY \, \bw$ and 
%if $\bY$ is full rank 
the LS solution uses the pseudoinverse given by
\begin{equation}
{\bw^\dag} = \left( \bY^H \, \bY \right)^{-1}  \bY^H \,  \bx,
\label{eq:equalizer_direct_solution}
\end{equation}
where $\bx$ is the training sequence.

\bExample \textbf{Least squares equalizer}.
\codl{snip_channel_ls_equalizer_direct} provides an example of using \equl{equalizer_direct_solution} to design an equalizer.
\includecodelong{MatlabOctaveCodeSnippets}{snip\_channel\_ls\_equalizer\_direct}{snip_channel_ls_equalizer_direct}
The output of \codl{snip_channel_ls_equalizer_direct} is:
\begin{verbatim}
minMSE = 1.4257e-06     bestDelta = 11
\end{verbatim}
Decreasing \ci{Nw} from 10 to 5 increases \ci{minMSE} to $0.0011$.
\eExample


%\subsection{MMSE equalization}

%\subsection{Equalization using adaptive filter with LMS}

\subsection{Probabilistic approach to LS FIR system identification}

The previous discussion has concentrated on deterministic signal processing, but
a probabilistic approach is more suitable in scenarios involving random processes. One
specific formulation for channel identification is briefly presented in the sequel.

The correlation matrix of $\bmx$ can be defined as
\begin{equation}
\bR_{xx} = \ev \left[ \bX^H \, \bX \right]
\label{eq:correlationMatrixRx}
\end{equation}
and the cross-correlation as
\begin{equation}
\br_{yx} = \ev \left[ \bX^H \, \by \right].
\label{eq:crosscorrelationRxy}
\end{equation}
Using these definitions, \equl{ls_solution} can be written as
\begin{equation}
{\bh}^\dag = \bR_{xx}^{-1}  \, \br_{yx}.
\label{eq:ls_solution_correlationNotation}
\end{equation}

%\ignore{
\section{Channel Capacity}
%The channel under consideration is noise-free and has unlimited bandwidth. Therefore, the number $b$ of bits per symbol can be increased arbitrarily because the decoder will be always able to recover the transmitted symbol without error. Application~\ref{app:bytes_transmission} provides an example of error-free transmission of a file. Section~\ref{sec:pam-noise-free-bw} discusses that band-limited channels restrict the maximum $\rsym$ while Section~\ref{sec:pam-awgn} discusses that AWGN restricts the maximum $b$ for a given BER. Before that, a broad class of modulations is discussed in the sequel.
%
The \emph{channel capacity}\index{Channel capacity} is the maximum rate of communication for which an arbitrarily small error probability can be achieved. It is an intrinsic property of a channel and extensively studied in the Information Theory literature. 
%It does not depend on the communication system.
%Information theory. 
%$I=\log_2 (1/p)$
%

Each channel has its capacity and this concept gets confused because sometimes the word capacity is also used to describe the bit rate achieved by a given communication system. For example, under this alternative interpretation of ``capacity'', one can say that increasing the number $b$ of bits per symbol a PAM from $b=2$ to 4 doubled the ``capacity''. This terminology should be avoided and, instead, bit rate used in this case.

Because each channel has its capacity, a given expression for capacity must be used only for the specific channel that it was derived for. 

\subsection{Capacity of AWGN channels}

In the case of the continuous-time AWGN channel of \figl{awgnChannels}, the capacity $C$ in bits per second (bps) is\footnote{The proof for \equl{awgnChannelCapacity} and other capacity results can be found in \cite{Tse05}, which is available online at \akurl{http://www.eecs.berkeley.edu/~dtse/book.html}{7tse}.}
%
\begin{equation}
C_{[\textrm{bps}]} = \BW \log_2 (1 + \snr),
\label{eq:awgnChannelCapacity}
\end{equation}
where $\BW$ is given in Hz and $\snr$ is the SNR at the receiver, in linear scale (not dB).

Recall from Section~\ref{sec:continuousTimeAWGN}, that the transmit signal power of an AWGN channel is power-limited. This avoids the situation of having an infinite capacity by increasing the SNR in \equl{awgnChannelCapacity}. In practice, $C$ is finite because the transmit signal power is always bounded and the noise power is not zero.

From \equl{awgnChannelCapacity}, it can also be seen that doubling $\BW$ leads to twice the capacity, but the dependence of $C$ on $\snr$ is only logarithmic. 

The discrete-time AWGN channel does not have any time explicitly associated to the transmission of a vector $\bm$ of dimension $D$ and one cannot specify the symbol duration in seconds, for example. Hence, it is more sensible to account capacity as bits ``per channel use'' (bpcu)\index{Bits per channel use (bpcu)} or ``per transmission'' in this case, where each ``use'' is the transmission of a vector with $D$ symbols. For example, $D=1$ and 2 when using PAM and QAM, respectively. Assuming that the unit is ``bits per channel use per dimension'', the normalized capacity of the discrete-time AWGN channel is
\begin{equation}
\overline C_{[\textrm{bpcu}]} = \frac 1 2 \log_2 (1 + \snr)
\label{eq:capacityDiscreteAWGN}
\end{equation}
and its total capacity $C = D \overline C$ bits per channel use. 

\equl{capacityDiscreteAWGN} is also called the capacity of the discrete-time \emph{real} AWGN, while the capacity of the \emph{complex} version (that corresponds to using QAM instead of PAM) is
\begin{equation}
C_{[\textrm{bpcu}]} = \log_2 (1 + \snr).
\label{eq:capacityDiscreteAWGNComplex}
\end{equation}
%For $D=1$, one has $C = \overline C$.

Note that \equl{awgnChannelCapacity} is valid for both real and complex AWGN channels, and it can be obtained from \equl{capacityDiscreteAWGN} or \equl{capacityDiscreteAWGNComplex} as follows. When the symbols are real-valued (e.\,g., when PAM is used in baseband channels), \tabl{fsAndRsym} indicates that the maximum number of symbols that can be sent over a channel with bandwidth BW is $R_{\textrm{sym}}^{\textrm{max}} = 2 \BW$. Therefore, using \equl{capacityDiscreteAWGN}, the capacity is 
\[
C_{[\textrm{bps}]} = R_{\textrm{sym}}^{\textrm{max}} C_{[\textrm{bpcu}]} = 2 \BW \left( \frac 1 2 \log_2 (1 + \snr) \right) = \BW \log_2 (1 + \snr),
\]
as indicated by \equl{awgnChannelCapacity}.
Similarly, when the symbols are complex-valued (e.\,g., when QAM is used in bandpass channels), \tabl{fsAndRsymComplex}, indicates that the maximum number of symbols that can be sent over a channel with bandwidth BW is $R_{\textrm{sym}}^{\textrm{max}} = \BW$ and from \equl{capacityDiscreteAWGNComplex}:
\[
C_{[\textrm{bps}]} = R_{\textrm{sym}}^{\textrm{max}} C_{[\textrm{bpcu}]} = \BW \log_2 (1 + \snr).
\]

\subsection{Water filling}
%Capacity of the frequency-selective LTI Gaussian channel}

This section discusses the water filling (or water pouring) algorithm that provides the optimum allocation of power in order to achieve the capacity of the frequency-selective channel depicted in \figl{frequency_selective_channel}. A by-product of using water filling for power allocation is an estimate of the channel capacity itself. It is assumed that the transmit signal is constrained to have a maximum power value (otherwise the capacity would be infinite).
% and the channel bandwidth can be divided into several subchannels, as done in multicarrier transmission.

The strategy to achieve the capacity in this case is to segment the available BW into $K$ ``subchannels'' with small BW denoted as $\Delta_f$, such that the gain can be assumed constant within $\Delta_f$. With this strategy, the frequency-selective channel is transformed into a set of parallel ``subchannels'', with each one modeled as AWGN with a frequency response magnitude
that is flat over frequency, as depicted in \figl{waterfilling_channelgain}.
More specifically, the $k$-th ``subchannel'' has a center frequency $f_k = k\Delta_f$ corresponding to a \emph{subcarrier}\index{Subcarrier} or \emph{tone}\index{Tone}, and frequency response $H(f_k)$ that is assumed flat, as depicted in \figl{flat_channel}.
For convenience, the squared magnitude $|H(f_k)|^2$ is denoted as $g_k$. 

\begin{figure}[htbp]
\centering
  \subfigure[Channel gains.]{\label{fig:waterfilling_channelgain}\includegraphics[width=6.5cm]{FiguresTex/waterfilling_channelgain}}
    \subfigure[Water filling solution.]{\label{fig:waterfilling_discrete}\includegraphics[width=6.5cm]{FiguresTex/waterfilling_discrete}}
  \caption{Frequency-selective channel modeled as a set of parallel ``subchannels'' and corresponding water filling solution.\label{fig:waterfillingFigs}}
  %\label{fig:awgnChannels}
\end{figure}

Assuming the WGN $\nu(t)$ at the receiver has a unilateral PSD value of $\no$, the noise power on each ``subchannel'' is
\begin{equation}
\calP_k = \no \Delta_f.
\label{eq:subchannelNoisePower}
\end{equation}

To achieve capacity, the total transmit power $\calP$ needs to be distributed among the $K$ subcarriers such that $\sum_{k=1}^K \calP_k = \calP$. This power allocation problem does not appear in the AWGN of \figl{awgnChannels}.
%, where there is only one channel and all the available power is used .
\figl{waterfilling_discrete} indicates the solution, which is known as ``water filling'', and provides the optimum power value $\calP_k^*$ as
\begin{equation}
\calP_k^* = \max \left(\calC - \frac{\no}{g_k}, 0 \right),
\label{eq:waterfillingDiscreteSolution}
\end{equation}
where $\calC$ is known as the ``water level''. If the first argument of the \ci{max} function in \equl{waterfillingDiscreteSolution} is negative, $\calP_k^* = 0$ and no power is allocated to transmission using the $k$-th subcarrier. 

The water filling solution will be further discussed in the sequel, but note that after the optimum power allocation is obtained, \equl{capacityDiscreteAWGN} can be used to calculate the total capacity in bits per channel use as
\begin{equation}
C_{[\textrm{bpcu}]} = \sum_{k=1}^K \log_2 (1 + \snr_k)
\label{eq:totalCapacity}
\end{equation}
where $\snr_k$ is the SNR at subcarrier $k$. It is assumed in \equl{totalCapacity} that each subcarrier uses $D=2$ dimensions.

\equl{totalCapacity} can be converted to bits per second, by taking in account that each subchannel has a bandwidth $\Delta_f = \BW/K$. Using $R_{\textrm{sym}}^{\textrm{max}} = \Delta_f$ and \equl{totalCapacity}, the capacity of the parallel channels is
\begin{equation}
C_{[\textrm{bps}]} = R_{\textrm{sym}}^{\textrm{max}} C_{[\textrm{bpcu}]} = \Delta_f \sum_{k=1}^K \log_2 (1 + \snr_k).
\label{eq:totalCapacityInbps}
\end{equation}


Assuming that $r_k$ is the PSD value of the received signal (after passing the channel), which can be assumed\footnote{The value of $\Delta_f$ is supposed to be small enough for this assumption to be valid.} constant over $\Delta_f$, $\snr_k$ is the ratio between the received power $r_k \Delta_f$ and the noise power $\no \Delta_f$, which can be simplified to
\begin{equation}
\snr_k = \frac{r_k \Delta_f}{\no \Delta_f} = \frac{r_k}{\no}.
\label{eq:snr_multicarrier}
\end{equation}
\equl{snr_multicarrier} indicates that, instead of using power values, $\snr_k$ can be obtained from the associated PSD values given that $\Delta_f$ is canceled out.

%In case that, for tone $k$, the channel frequency response $H_k = H(f_k)$ and the transmit PSD $s_k^2$ are known, 
The PSD at the transmitter can be taken in account via \equl{wss_continuous_lti_output_psd}, which allows to write
%\footnote{Related to the Wiener-Khinchin theorem. See \url{http://dsp.stackexchange.com/questions/270/what-is-the-relation-between-the-psds-of-filter-input-and-output-called-r-y}.}
$r_k = s_k |H_k|^2 = s_k g_k$. Hence, the SNR in terms of the transmit PSD, channel response and noise at receiver is
\begin{equation}
\snr_k = \frac{r_k}{\no} = \frac{s_k g_k}{\no}.
\label{eq:snr_multicarrierFinal}
\end{equation}
Substituting \equl{snr_multicarrierFinal} into \equl{totalCapacityInbps} leads to
\begin{equation}
C_{[\textrm{bps}]} = \Delta_f \sum_{k=1}^K \log_2 (1 + \frac{s_k^* |H(f_k)|^2}{\no}),
\label{eq:totalCapacityInbpsComplete}
\end{equation}
where $s_k^*$ denotes the optimum transmit PSD.

%With this notation, the waterfilling solution can be 

\subsubsection{Water filling for a finite set of parallel channels}

As mentioned, the optimum solution to the \emph{power loading} problem is the water filling. More specifically, the power loading problem is the following: maximize \equl{totalCapacity} subject to a maximum transmit power $\Delta_f \sum_{k=1}^K s_k = \calP$. The problem is convex and can be solved using a Lagrange multiplier. The optimality relies on the fact that 
%the goal is to maximize the rate $XXX$ and 
\equl{totalCapacity} is a convex (in fact, concave) function of the PSD, which influences the SNR according to \equl{snr_multicarrierFinal}. 
Before delving in the the associated mathematics, a very simple example of water filling will be described.
% in the next section for illustrating waterfilling.

%\subsection{Example: Power loading with only two tones}
\bExample \textbf{Power loading with only two tones}.
\label{ex:waterfilling_two_tones}
The problem here is to allocate power considering that there are only $K=2$ parallel channels. 
%This could be obtained with a FFT size of $N \ge 4$ for example, but the actual modulation is not important here. 
At the tones $k=0$ and 1, the channel has squared magnitude $g_0=|H(f_0)|^2=0.1$ and $g_1=|H(f_1)|^2=10^{-10}$, respectively. The number of channel uses per second is $\rsym=4$~kbauds and $\Delta_f=4.3125$~kHz is adopted. The background noise is modeled as having a white (unilateral) PSD with $\no = -140$~dBm/Hz, which corresponds to $10^{-17}$~W/Hz and a noise power of $\no \Delta_f = 4.3125 \times 10^{-14}$ W per tone. It is assumed that both tones can use $D=2$ and transmit QAMs. The maximum transmit power is 1~mW, such that $\calP_0 + \calP_1 = 1$~mW.
\figl{convexity2Tones} shows the rate for each pair $(\calP_0, \calP_1)$.
\figl{convexity2Tones} indicates that the rate obtained with \equl{totalCapacity} is a concave function of the PSD.

\ignore{
that total rate for was obtained by using a channel with magnitudes $[1,10^{-10}]$ for the two tones. of user $n=1$ in $\grandmatrix$ and $\pmax=2 \times 10^{-7}$. The maximum rate $r_1 \approx 242.26$ kbps is achieved with $\bs_1 = 10^{-7}[1, 1]$ and is indicated by a star. A similar curve would be obtained when using the direct channel $[1,10^{-2}]$ of user $n=2$, with a maximum rate $r_2 \approx 335.28$ kbps is achieved with $\bs_2 = \bs_1 = 10^{-7}[1, 1]$.
}

\begin{figure}
\centering
\includegraphics[width=8cm]{./Figures/convexity2Tones}
\caption{Total rate for different allocations of power among $K=2$ tones with channel squared magnitudes $g_0=0.1$ and $g_1=10^{-10}$.}\label{fig:convexity2Tones}
\end{figure}

When all the power is allocated to the worst tone $k=1$, the total rate is only $R \approx 7$~kbps. When the total power is allocated to $k=0$, the rate is 124.7~kbps. The best allocation is $\calP_0 = 0.7072$~mW and $\calP_1 = 0.2928$~mW, which achieves $R=125.8$~kbps, as indicated in \figl{convexity2Tones} by the marker. The main point here is to observe that convexity can be assumed and this enables easier procedures to find the optimum solution.
\eExample

%\figl{oneuser} was generated using the code in \ci{MatlabOctaveFunctions/oneuser.m}.
%\lstinputlisting{./Code/MatlabOctaveFunctions/oneuser.m}

\subsubsection{Watefilling solution via Lagrange multiplier}

Given that \equl{totalCapacity} is convex, it is possible to maximize it using a Lagrange multiplier to constrain the total power. Hence, the Lagrangian can be formulated as
$$
\calL = \sum_{k=1}^K \log_2 \left(1 + \frac{g_{k} s_k}{\no}\right) + \lambda (\sum_{k=1}^K s_k \Delta_f - \calP).
$$
For a specific subchannel $j$, calculating $d \calL/ds_j = 0$ decouples $j$ from the tones $k \ne j$ as follows:
\[
\frac{d \calL}{ds_j} = \frac{d}{ds_j} \left[ \log_2 \left(1 + \frac{g_{j} s_j}{\no}\right) + \lambda ( s_j \Delta_f - \calP) \right] = \frac{g_j / \no}{\left( 1 + (g_j s_j)/\no \right) \ln 2} + \lambda \Delta_f =0,
\]
which can be simplified to
\begin{equation}
s_j +  \frac {\no} {g_j} = \frac{-1}{\lambda \Delta_f \ln 2} = \calC, \textrm{the \emph{water level} constant}.	
	\label{eq:waterfill}
\end{equation}
Hence, the solution, which was anticipated in \equl{waterfillingDiscreteSolution} is to pour power according to the transmit PSD $s_k$ (``water'') on a curve specified by $\no / g_k$, which can be seen as an ``inverse SNR'' such that the summation of these two parcels corresponds to the water level $\calC$, which is the same for all tones and needs to be found by the algorithm.
\codl{simplewaterfill} lists the first lines of a simple water filling algorithm.\footnote{For better understanding the code, the reader can use Chapter 4 of \cite{Ciofficn}.} The fourth parameter (\ci{gap}) of \ci{ak\_simplewaterfill} will be discussed in Section~\ref{sec:gap_approximation} and its default value can be assumed here.

\lstinputlisting[caption={MatlabOctaveFunctions/ak\_simplewaterfill.m},label=code:simplewaterfill,linerange={1-2,5-16}]{./Code/MatlabOctaveFunctions/ak_simplewaterfill.m}

%\includecode{MatlabOctaveFunctions}{simplewaterfill}

%The following code shows how to use the previous simple waterfilling algorithm.
%\lstinputlisting{./Code/MatlabOctaveFunctions/main_simplewaterfill.m}

With the help of \ci{ak\_simplewaterfill.m} (\codl{simplewaterfill}), the solution that maximizes rate in \figl{convexity2Tones} can be easily obtained with \codl{snip_channel_waterfilling}.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_channel\_waterfilling}{snip_channel_waterfilling}

Note that water filling in \codl{snip_channel_waterfilling} provided 0.706562 and 0.29344~mW as the optimum power allocation, which is slightly different (and better) than the values in \exal{waterfilling_two_tones}. The reason is that \exal{waterfilling_two_tones} used a discrete grid to search for the optimum and the grid resolution did not allow to find it exactly.

\bExample \textbf{Another example of water filling}.
For another water filling example, consider the conversion of a channel into a set of $K=7$ parallel channels with squared magnitudes $[1,0.1, 1.1, 0.04, 0.05, 1.2, 0.1]$. The total power is $\calP=5 \times 10^{-13}$~W.
%As indicated by \equl{snr_multicarrierFinal}, the SNR at each subchannel can be obtained via PSD or power values. While \codl{snip_channel_waterfilling} used power values, this example will use PSD values. 
Assuming the unilateral white noise PSD has value $\no = 10^{-17}$~W/Hz, and the subchannel BW is $\Delta_f = 1$~kHz, the noise power at each tone is $\no \Delta_f = 10^{-14}$~W. 
%But instead of using this value, $\no$ itself will be used. For that, instead of passing $\calP$ to \codl{simplewaterfill}, the normalized value $\calP/\Delta_f$ will be used, as
The script \ci{MatlabOctaveBookExamples/ex\_channel\_waterfill.m} was used with these values to obtain \figl{waterfill}. 
%Therefore, with these normalized inputs, the water filling solution will not provide the power per tone but the PSD per tone itself.

\begin{figure}
\centering
\includegraphics[width=8cm]{./Figures/waterfill}
\caption{Watterfilling example with $K=7$ tones, in which there was no power allocated to the fourth and fifth tones.}\label{fig:waterfill}
\end{figure}

As indicated in \figl{waterfill}, the water level in this case is $\calC=1.4548 \times 10^{-16}$. 
The optimum PSD is \ci{S}$=10^{-15}\times[0.1355    , 0.0455    , 0.1364         , 0        , 0 ,0.1372    ,0.0455]$.

Power was not allocated to tones 4 and 5 due to their $\no/g_k$ being higher than $\calC$. Most of the power is allocated to tones 1, 3 and 6, due to their relatively high gains.
In this case, the SNR per tone is $[13.5485    , 0.4548   , 15.0033         , 0         , 0, 16.4582    , 0.4548]$ in linear scale and, assuming $D=2$ dimensions can be used, the capacity is
13.07~bits per channel use.
\eExample

After a channel is partitioned in parallel channels and
the available power is distributed among these channels, \equl{capacityDiscreteAWGN} or \equl{capacityDiscreteAWGNComplex} can be used to determine their capacities. The capacity
can then be used to estimate the (maximum) bit rate.

The power loading problem is related to the bit loading problem, as will be discussed in Chapter~\ref{ch:multicarrier}. 
Often the number of bits (constellation cardinality) must be integer valued.\footnote{A method to obtain a non-integer number of bits per symbol are the multidimensional constellations (lattice codes)~\cite{Forney89}.} In this case, simply using a \emph{continuous} water filling algorithm for power loading, and then truncating (or rounding) the number of bits provided by capacity equations leads a simple but suboptimal solution.\footnote{Algorithms such as the \emph{Levin-Campello's} provide optimal solutions to the power and bit loading problems when the number of bits per parallel channel must be an integer.}

%But for simplicity this work assumes $b^k \in \Re$. In this case, \equl{waterfill} is an optimal solution.

%Assuming each user can transmit at most 1 mW of power, the normalized values are $\pmax = \pmaxn{1} = \pmaxn{2} = 10^{-3}/\Delta_f \approx 2 \times 10^{-7}$.  


\ignore{
Another view of the water filling solution is shown in \figl{waterfill_v2} (but the notation in this copied figure is not consistent with the text).
%
\begin{figure}
\centering
\includegraphics[width=15cm]{./FiguresNonScript/waterfill_v2}
\caption{Alternative view of water filling.}\label{fig:waterfill_v2}
\end{figure}
}

\subsubsection{Water filling for continuous frequency}

The capacity of a frequency selective LTI Gaussian channel with frequency response $H(f)$ and bandwidth BW is also achieved with a water filling solution, but now in a continuous frequency $f$. This water filling can be obtained by letting the number $K$ of subchannels go to infinite, which makes the bandwidth $\Delta_f = \BW/K$ of each subchannel go to zero. \equl{waterfillingDiscreteSolution} then converges to
\begin{equation}
S^*(f) = \max \left(\calC - \frac{\no}{|H(f)|^2}, 0 \right),
\label{eq:waterfillingContinuosSolution}
\end{equation}
where the water level $\calC$ is chosen such that
\begin{equation}
\int_{0}^{\BW} S^*(f) \textrm{d}f = \calP
\label{eq:waterfillingContinuosSolutionConstrain}
\end{equation}
and $S^*(f)$ is the optimum transmit PSD.

\subsection{Capacity of the frequency-selective LTI Gaussian channel}

Using the optimum PSD provided by \equl{waterfillingContinuosSolution}, the capacity of the frequency-selective LTI Gaussian channel in bits per second is then
\begin{equation}
C_{[\textrm{bps}]} = \int_{0}^{\BW} \log_2 \left( 1 + \frac{S^*(f) |H(f)|^2}{\no} \right) \textrm{d}f.
\label{eq:frequencySelectiveCapacity}
\end{equation}

\equl{frequencySelectiveCapacity} can be obtained from the capacity expression from a set of $K$ parallel channels and making $K \rightarrow \infty$. More specifically, substituting $\Delta_f = \BW/K$ into \equl{totalCapacityInbpsComplete}, leads to
\begin{equation}
C_{[\textrm{bps}]} = \frac{\BW}{K} \sum_{k=1}^K \log_2 \left( 1 + \frac{s_k^* |H(f_k)|^2}{\no} \right),
%\label{eq:}
\end{equation}
which converges to \equl{frequencySelectiveCapacity} when $\Delta_f \rightarrow 0 $ and the discrete frequencies $f_k$ become $f$ to describe a continuous frequency response $H(f)$.

\subsection{Revisiting the capacity of flat fading continuous-time AWGN}

Note that the channels in \figl{awgnChannels}, with a flat frequency response, can be seen as special cases of the frequency-selective LTI Gaussian channel. Applying water filling as in \equl{waterfillingContinuosSolution} to the channels in \figl{awgnChannels} would lead to flat transmit PSDs $S_x(f) = \calP / \BW$ with BW, where $\calP$ is the maximum transmit power (unilateral PSDs are adopted). Using \equl{wss_continuous_lti_output_psd}, the PSD at the receiver is $S_y(f) = \kappa^2 S_x(f)$, where $\kappa = |H(f)|$ is the frequency response magnitude over the channel passband.
Hence, the SNR at the receiver is
\begin{equation}
\snr = \frac{S_y(f) \BW}{ \no \BW} = \frac{\kappa^2 S_x(f)}{\no} = \frac{\kappa^2 \calP}{\no \BW }.
\label{eq:awgnSNRFromTx}
\end{equation}

Substituting \equl{awgnSNRFromTx} into \equl{awgnChannelCapacity} leads to
\begin{equation}
C_{[\textrm{bps}]} = \BW \log_2 (1 + \snr) = \BW \log_2 \left(1 + \frac{\kappa^2 \calP}{\no \BW } \right)
\label{eq:awgnChannelCapacityTransmitPower}
\end{equation}

Hence, only when the channel is frequency-selective the water filling solution is required for obtaining the optimum transmit PSD and, from that, the channel capacity. Alternatively, \equl{awgnChannelCapacityTransmitPower} suffices to calculate the capacity for channels with flat frequency responses.


%Example~\ref{exa:flatChannels}. 

\bExample \textbf{Example of capacity calculation for a continuous-time AWGN channel}.
\label{exa:flatChannels}
Assume that the bandpass channel depicted in \figl{awgnChannelsPassband} has $\BW=20$~Hz and magnitude $\kappa=|H(f)| = 1/2$ over the passband. The WGN has a unilateral PSD level $\no=1$~mW/Hz and the maximum transmit power is $\calP=240$~mW.

Using \equl{awgnChannelCapacityTransmitPower}, the capacity of this channel is
\[
C_{[\textrm{bps}]} = \BW \log_2 \left(1 + \frac{\kappa^2 \calP}{\no \BW } \right) = 20 \log_2 \left(1 + \frac{(1/4) 240}{20} \right) = 40.
\]

For practicing the interpretation of \equl{totalCapacityInbpsComplete}, consider that the current channel is modeled as a set of $K=4$ subchannels, each with $\Delta_f = \BW/K = 5$~Hz.
Because the channel is flat, the water filling solution would equally split $\calP$ among the four subchannels, with $\calP_k = \calP/K = 60$~mW being allocated to each, corresponding to a PSD level of $s_k^* = \calP_k/\Delta_f = 12$~mW/Hz, $\forall k$. In this case, the capacity per channel use of a subchannel is
\[
C_{[\textrm{bpcu}]} = \log_2 (1 + \frac{s_k^* |H(f_k)|^2}{\no}) = \log_2 (1 + \frac{12 (1/4)}{1}) = 2,
\]
which leads to
\[
C_{[\textrm{bps}]} = \Delta_f \sum_{k=1}^K C_{[\textrm{bpcu}]} = 5 \sum_{k=1}^4 2 = 40,
\]
which coincides with the result from \equl{awgnChannelCapacityTransmitPower}.
\eExample

%AK-TODO: PAM and QAM spectral efficiency - I followed Barry's book below but it is not a nice description.
%I will comment it out until organizing the idea better. Could start with a
%PAM of BW and show that a QAM can carry 2 PAMs within 2BW.
%\section{Comparing PAM and QAM Spectral Efficiencies}
%
%As defined in \equl{spectral_efficiency}, the spectral efficiency is 
%$\eta = \frac {R} {\BW}$, which can be written as $\eta=\frac {\rsym \, b} {\BW}$. 
%As presented in \tabl{fsAndRsym}, considering real-valued signals, the maximum symbol rate is $\rsym = 2\,\BW$ for a channel with bandwidth BW. Hence, the maximum baseband PAM spectral efficiency is
%\[
%\eta_{\textrm{max}}^{\textrm{PAM}} = 2\,b.
%\]
%
%In the case of complex QAM signals, 
%\tabl{fsAndRsymComplex} suggests that $\rsym = \BW$ is the maximum symbol rate and
%\begin{equation}
%\eta_{\textrm{max}}^{\textrm{QAM}} = \frac {\rsym \, b} {\BW} =  b.
%\label{eq:qam_max_efficiency}
%\end{equation}
%
%Comparing $\eta_{\textrm{max}}^{\textrm{PAM}}$ and $\eta_{\textrm{max}}^{\textrm{QAM}}$, one could erroneously conclude that PAM has a spectral efficiency higher than QAM. It should be noted that a QAM constellation can easily use $2b$ by adopting the Cartesian product of two PAM constellations of $b$ bits. This is typically the case and, effectively, the spectral efficiencies of baseband PAM and QAM are identical.


%AK-IMPROVE: mimic \ci{bersync}.  in %\section{Timing}
\section{Synchronization}

In IQ sampling, phase errors introduce ``crosstalk'' between the I and Q components and degrade performance.

Timing (or clock) synchronizer

Carrier synchronizer

Phase is not always needed. In DPSK for example, only carrier frequency and symbol timing are required


\subsection{Carrier recovery}

Synchronous demodulation relies on the proper recover of the carrier at the receiver. 
For example, assume for simplicity that the baseband signal $x(t)$ was upconverted at the transmitter via multiplication by a complex exponential $e^{j \aw_c t}$, such that the transmitted signal is $s(t) = x(t) e^{j \aw_c t}$. Without loss of generality, the phase of the transmitter carrier is assumed to be zero, given that it serves as a reference to the other phases. In practice, the signal arrives at the receiver delayed by the channel propagation delay $\tau$, but this is not important for this discussion and $\tau=0$ is assumed. 

Ideally, the receiver can use $e^{-j \aw_c t}$ to downconvert the signal and recover the signal of interest as indicated by:
\[
\hat x(t) = s(t) e^{-j \aw_c t} = x(t) e^{j \aw_c t} e^{-j \aw_c t} = x(t).
\]
However, the circuits that generate the sinusoids have imperfections, depend on the temperature, etc. Therefore, in practice, even expensive electronic components lead to some discrepancy between the carriers at the transmitter and receiver. Assuming the carrier generated by the receiver is $e^{j[(- \aw_c + \aw_{\Delta}) t + \phi]}$, where $\phi$ is a phase offset\index{Phase offset} that does not depend on $t$ and $\aw_{\Delta}$ is the frequency offset\index{Frequency offset} (in rad/s). If this mismatch is not compensated, the signal at the receiver is
\begin{equation}
\hat x(t) = x(t) e^{j \aw_c t} e^{j[(- \aw_c + \aw_{\Delta}) t + \phi]} = x(t) e^{j(\aw_{\Delta} t + \phi)}.
\label{eq:offsets_freq_phase}
\end{equation}
In this case, the effect is that $\hat x(t)$ is still shifted in frequency by a ``carrier'' composed by the frequency and phase offsets. The goal of carrier recovery is to estimate and then compensate these offsets, which in the current example would ideally consist of multiplying $\hat x(t)$ by $e^{-j(\aw_{\Delta} t + \phi)}$.

Multiplication by a discrete-time complex exponential $e^{-j(\dw_{\Delta} n + \phi)}$ in digital domain is relatively easy. But when continuous-time sinusoids are used to implement the multiplication by $e^{-j(\aw_{\Delta} t + \phi)}$, the frequency-conversion must be followed by a filtering stage to eliminate the extra spectrum replicas (or images). In this case, the filter(s) must be designed to avoid distorting the signal.

Another practical aspect is that receiver circuits, such as the one used in USRP's daughter boards, can have an intermediate downconversion step, where an oscillator converts the RF signal (for example centered in 900~MHz) to an intermediate frequency (IF, for example, 70~MHz). This oscillator has its frequency $\aw_{\textrm{mix}}$ chosen and then runs in ``open loop'', i.\,e., its phase $\Theta$ is not synchronized with the phase of the incoming signal $s(t)$. Again assuming for simplicity that a complex exponential $e^{j(-\aw_{\textrm{mix}}t + \Theta)}$ is used, the mixer stage outputs the signal
\[
x(t) e^{j \aw_c t} e^{j(-\aw_{\textrm{mix}}t + \Theta)} = x(t) e^{j ((\aw_c -\aw_{\textrm{mix}}) t + \Theta)} = x(t) e^{j (\aw_{\textrm{IF}} t + \Theta)}.
\]
This signal could then be digitized at the IF frequency $\aw_{\textrm{IF}} = \aw_c - \aw_{\textrm{mix}}$ or additional stages of downconversion (and filtering) used. Hence, it could be
convenient to consider that $\aw_{\textrm{IF}}$ is part of the frequency offset.
In any case, the offsets $\aw_{\Delta}$ and $\phi$ in \equl{offsets_freq_phase} represent the (eventually accumulated) effects of the channel and receiver processing and compose the ``carrier'' to be estimated. The following paragraphs discuss the estimation of $\aw_{\Delta}$ and $\phi$  using an FFT.

%Hence, at a given instant $t_c$, the value of interest $x(t_c)$ was multiplied by $e^{j \aw_c t_c}$. 

\subsection{Carrier recovery using FFT}

Some modulation schemes such as the commercial AM radio allocate power to the carrier, i.\,e., the PSD of the transmit signal shows a relatively large amount of power at the carrier frequency, as a \emph{spectrum line}, and modeled with an impulse (see \figl{amModulation}). Alternatively, some  communication systems such as the double-sideband suppressed carrier (DSB-SC) amplitude modulation, use a carrier to implement frequency upconversion but do not have spectral lines (impulses) at their
carrier frequencies (see right-most PSD in \figl{mixerOutputs}). 

When the transmitter generates a signal with suppressed carrier, a trick to recover the carrier, which is still embedded in the modulated signal $s(t)$, is to process the signal via a non-linearity, such as squaring the signal. The squared signal $s^2(t)$ can then be used as the input to a phase-locked loop (PLL), for example. A simpler example of carrier recovery, which uses FFT, is discussed in the sequel. A more advanced approach, using cyclostationary analysis was briefly discussed in Application~\ref{app:cyclostationary}.

\subsubsection{PAM carrier recovery using FFT}

Assume $s(t) = x_\textrm{PAM}(t) \cos(\aw_c t)$ is the transmitted signal and, for the sake of studying synchronization, the channel is ideal and $s(t)$ is available at the receiver. The
receiver then generates $r(t) = s(t) \cos((\aw_c +\aw_{\Delta}) t + \phi)$ using a carrier that
may have frequency $\aw_{\Delta}$ and phase $\phi$ offsets with respect to the carrier used by the transmitter. Using trigonometry (Appendix~\ref{sec:trigonometry}), $r(t)$ can be written as:
\[
r(t) = \frac{x_\textrm{PAM}(t)}{2} [ \cos((2\aw_c +\aw_{\Delta}) t + \phi)  + \cos(\aw_{\Delta} t + \phi) ]
\]
and the lower frequency replica is centered at $\aw_{\Delta}$, not at DC.
The task here is to compensate both offsets and recover the signal $x_\textrm{PAM}(t)$. 
The following paragraphs illustrate a procedure that is based on squaring $r(t)$. 

It is more effective to recover the carrier from $r^2(t)$ than $r(t)$ because, while $r(t)$ may have a DC value $\ev[r(t)] = \ev[x_\textrm{PAM}(t)] \approx 0$ close to zero, the new signal $r^2(t)$ has discrete spectrum components\footnote{Note that this is valid for PAM, but not necessarily for QAM. As discussed in Appendix~\ref{sec:balancedConstellation}, some QAM constellations lead to $\ev[x^2(t)]=0$ and a discrete spectrum component would not be created.} imposed by $\ev[x^2_\textrm{PAM}(t)] \ne 0$.
Using trigonometry again:
\begin{align*} %\numberthis 	is handy, or \nonumber
r^2(t) &= x^2_\textrm{PAM}(t) \cos^2(\aw_c t) \cos^2((\aw_c +\aw_{\Delta}) t + \phi) \\
			 &= \frac{x^2_\textrm{PAM}(t)}{4} [(1 + \cos(2 \aw_c t))(1 + \cos(2(\aw_c +\aw_{\Delta}) t + 2\phi))] \\
       &= \frac{x^2_\textrm{PAM}(t)}{4} \left[ 1 + \cos(2 \aw_c t) + \cos(2(\aw_c +\aw_{\Delta}) t + 2\phi) + \right.\\
			&\mathrel{\phantom{=}} \left. 0.5 \left( \cos((4 \aw_c + \aw_{\Delta}) t + 2\phi)+  \cos(2\aw_{\Delta} t + 2\phi) \right) \right] \numberthis \label{eq:squaredPAM} \\
			&= \textrm{term at DC} + \textrm{terms at high frequencies} + \frac{x^2_\textrm{PAM}(t)}{8} \cos(2\aw_{\Delta} t + 2\phi).  
\end{align*} %\numberthis 	is handy, or \nonumber

\equl{squaredPAM} indicates that, depending on the values of $\aw_c$ and $\aw_{\Delta}$, it may be possible to filter out all terms other than $\frac{x^2_\textrm{PAM}(t)}{8} \cos(2\aw_{\Delta} t + 2\phi)$ using a bandpass filter. A numerical example is provided in the sequel.

\bExample \textbf{Frequencies originated by squaring a PAM signal}.
A baseband signal with bandwidth BW when raised to the power of $n$ has its bandwidth expanded to $n \BW$. This can be seen by considering that multiplication in time-domain corresponds to convolution in frequency-domain, which expands the support of the original spectrum.
\codl{snip_channel_pam_squared_freqs} provides an example of the frequencies obtained via \equl{squaredPAM} assuming a carrier frequency $f_c=900$~MHz (\ci{fc} in the code) and a frequency offset of 27~kHz (\ci{fdelta}), where these frequencies correspond to $\aw_c=2 \pi f_c$ and $\aw_{\Delta} = 2 \pi$ \ci{fo}, respectively. The PAM signal is assumed to have $\BW=200$~kHz.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_channel\_pam\_squared\_freqs}{snip_channel_pam_squared_freqs}

\codl{snip_channel_pam_squared_freqs} outputs:
\begin{verbatim}
f_DC = [-0.4000    0.4000]
f_2w0 = 1.0e+03 * [1.7996    1.8004]
f_2w0woffset =  1.0e+03 * [1.7997    1.8005]
f_4w0woffset =  1.0e+03 * [3.5996    3.6004]
f_2woffset = [-0.3460    0.4540]
\end{verbatim}
This indicates that the DC term
$\frac{x^2_\textrm{PAM}(t)}{4}$ in \equl{squaredPAM} occupies a band from $-400$ to 400~kHz, overlapping with the term of interest $\frac{x^2_\textrm{PAM}(t)}{8} \cos(2\aw_{\Delta} t + 2\phi)$, which is located in the band from $-346$ to $454$~kHz. 

The overlap can be a problem for a routine that estimates offsets. Hence, the receiver may impose a value distinct from $\aw_c$ to purposely have
$\aw_{\Delta} > \BW$, where $\BW$ is the original PAM bandwidth, as discussed in the sequel.
\eExample

To avoid overlap, $\aw_{\Delta}$ will be assumed to be
\begin{equation}
\aw_{\Delta} = \aw_o + \aw_{\textrm{IF}},
\label{eq:syncViaIF}
\end{equation}
where $\aw_o$ is the actual (and unknown) offset to be estimated and $\aw_{\textrm{IF}}$ is
an intermediate frequency used by the receiver that can be chosen by the designer to avoid the mentioned overlap.

The FFT-based estimation of frequency and phase offsets is simplified
when three values are known (or can be estimated):
\begin{enumerate}
	\item carrier frequency $\aw_c$;
	\item PAM bandwidth BW;
	\item maximum of the absolute offset value $\aw_{\textrm{max}} = \max |\aw_o|$, which may be determined by a standard (see Application~\ref{app:gsmSync}), obtained from an oscillator datasheet, etc.
\end{enumerate}

In order to choose $\aw_{\textrm{IF}}$, it is often possible to assume that
$\aw_c \gg \aw_{\textrm{max}}$.
But choosing a large value for $\aw_{\textrm{IF}}$ may be inconvenient, for example,
if an ADC conversion should be used afterward. Fortunately, knowing $\aw_c$, BW (assumed to be in Hz) and $\aw_{\textrm{max}}$, allows to conveniently choose $\aw_{\textrm{IF}}$ as follows.

The angular frequency that will be used for the offsets estimation is within the range $2\aw_{\textrm{IF}} \pm 2 \aw_{\textrm{max}}$, i.\,e., the minimum search range is $4 \aw_{\textrm{max}}$. Its minimum value $2\aw_{\textrm{IF}} - 2 \aw_{\textrm{max}}$ should be higher than $2\pi(2 \BW)$, to avoid overlapping with the ``DC term'' in \equl{squaredPAM}.
Besides, its highest frequency $2\aw_{\textrm{IF}} + 2 \aw_{\textrm{max}}$ should be lower than
$2 (\aw_c - \aw_{\textrm{IF}} - \aw_{\textrm{max}} - 2 \pi \BW)$.
Hence, $\aw_{\textrm{IF}}$ (in rad/s) can be chosen within the range 
\begin{equation}
\aw_{\textrm{IF}} \in \textrm{~~} ]\aw_{\textrm{max}} + 2 \pi \BW, 0.5\aw_c - \pi \BW - \aw_{\textrm{max}}[.
\label{eq:ifRange}
\end{equation}

The following example illustrates the usage of \equl{ifRange}.

\bExample \textbf{Choosing the intermediate frequency for IF-based estimation of frequency and phase offsets}.
\codl{snip_channel_pam_sync_if} implements \equl{ifRange} to exemplify a possible range for $\aw_{\textrm{IF}}$.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_channel\_pam\_sync\_if}{snip_channel_pam_sync_if}

In this case the values of \ci{fIFmin} and \ci{fIFmax} were 236~kHz and 
449.9~MHz, respectively. Choosing such extreme values is avoided because they could impose severe restrictions to filtering operations.
\eExample

The following example illustrates a complete FFT-based estimation procedure.

\bExample \textbf{Complete example of frequency and phase offsets correction for PAM}.
\label{ex:offsetsPAM}
\codl{ex_fftBasedPAMCarrierRecovery} illustrates the complete procedure for 
estimating the frequency and phase offsets corresponding to a PAM transmission
In this example, the carrier frequency is $f_c=1$~GHz, the frequency offset is
$-500$~kHz ($\aw_o = -2 \pi 500$~rad/s) and the phase offset is $\phi=-0.3$~rad.
The intermediate frequency (\ci{fIF} in the code) is chosen as 130~MHz.

\lstinputlisting[firstline=6,firstnumber=6,lastline=89,caption=./Code/MatlabOctaveBookExamples/ex\_fftBasedPAMCarrierRecovery,label=code:ex_fftBasedPAMCarrierRecovery]{./Code/MatlabOctaveBookExamples/ex_fftBasedPAMCarrierRecovery.m}

\figl{fftBasedCarrierRecovery} and \figl{pamOffsetsUpconverted} illustrate the 
baseband PAM signal and its frequency-upconverted version, respectively. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/fftBasedCarrierRecovery}
\caption{PSD of baseband PAM signal with bandwidth of approximately 8~MHz.\label{fig:fftBasedCarrierRecovery}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/pamOffsetsUpconverted}
\caption{PSD of frequency-upconverted PAM signal to carrier frequency of 1~GHz.\label{fig:pamOffsetsUpconverted}}
\end{figure}

\figl{pamOffsetsAtIF} depicts the PAM signal after frequency-downconversion to an IF
of 130~MHz for the estimation of the offsets.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/pamOffsetsAtIF}
\caption[PSDs of PAM frequency-downconverted to IF.]{Top: PSD of PAM frequency-downconverted to IF of 130~MHz. Bottom: Zomming the range centered at IF. A careful observation of its nulls indicate the spectrum is not perfectly centered at IF, but shifted by the offset of $-500$~kHz.\label{fig:pamOffsetsAtIF}}
\end{figure}

\figl{pamOffsetsSquaredRx} depicts the PSD of the signal used to estimate the offsets, which is obtained by squaring the received signal \ci{r}. Note that the specific frequency in this case is
\ci{2*(fIF+freqOffset)/1e6=259}~MHz (corresponding to $2(\aw_{\textrm{IF}} + \aw_o)$~rad/s), which is
indicated in \figl{pamOffsetsSquaredRx} by a datatip. Note that a relatively high sampling frequency
of \ci{Fs=12.44}~GHz was adopted in this simulation in order to properly represent the squared
signal. Care should be exercised when using smaller values of \ci{Fs} in order to avoid 
aliasing.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/pamOffsetsSquaredRx}
\caption[PSD of the squared received signal \ci{r} used for the estimation of the offsets]{PSD of the squared received signal \ci{r} used for the estimation of the offsets. The frequency of interest (259~MHz) is indicated.\label{fig:pamOffsetsSquaredRx}}
\end{figure}

The squared received signal (whose PSD is in \figl{pamOffsetsSquaredRx}) is converted by an FFT
to the frequency domain.
The values of \ci{range=minFreqOffset:maxFreqOffset} indicate a search range of 258.4 to 261.6~MHz.
As discussed, this range is centered at twice \ci{fIF} (in this case 260~MHz) and has a band of \ci{4*worstFreqOffset} (3.2~MHz). Because in this example the FFT resolution is 1~kHz, the search range
 corresponds to the FFT indices \ci{minIndexOfInterestInFFT=258,400} to \ci{minIndexOfInterestInFFT=261,600}. Within this range, the FFT index \ci{indexMaxPeak=259001} corresponds to the maximum absolute value. \figl{pamOffsetsZoomOfR} depicts \ci{20*log(abs(R(range))))}, which clearly shows
the peak. The FFT value at this frequency (i.\,e., \ci{R(indexMaxPeak)}) has a magnitude of
approximately 6,200 and an angle of -0.6~rad. The magnitude value is not used but this angle allows to estimate the phase offset as $\phi=-0.6/2=-0.3$~rad. The frequency value corresponding
to $\aw_{\Delta} = 2 \pi 259 / 2$~Mrad/s is obtained from \ci{indexMaxPeak} as indicated
in \codl{ex_fftBasedPAMCarrierRecovery}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/pamOffsetsZoomOfR}
\caption{Values of the squared received signal FFT within the 
range of interest.\label{fig:pamOffsetsZoomOfR}}
\end{figure}

Having estimates of $\aw_{\Delta}$ and $\phi$, a complex exponential $e^{-j(\aw_{\Delta} n + \phi)}$ can be created and used to correct the offsets. The squared signal can be eventually discarded,
because the next steps will depend on \ci{r}.
\figl{pamOffsetsShiftedPSDs} shows the PSD of \ci{rc}, which is obtained by multiplying \ci{r}
by the mentioned complex exponential.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/pamOffsetsShiftedPSDs}
\caption{PSDs of the received signal \ci{r} and its version after
multiplication by the complex exponential $e^{-j(\aw_{\Delta} n + \phi)}$.\label{fig:pamOffsetsShiftedPSDs}}
\end{figure}

\figl{pamOffsetsShiftedPSDs} indicates that \ci{rc} is complex-valued because its PSD does not
have Hermitian symmetry. Discarding its imaginary part leads to the signal with PSD depicted
in the top plot of \figl{pamOffsetsFilteredRx}. This signal is then matched-filtered using the same square-root raised cosine (SRC) adopted by the transmitter, and the final signal is \ci{rf}, with
PSD showed in the bottom plot of \figl{pamOffsetsFilteredRx}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/pamOffsetsFilteredRx}
\caption[Received signal after carrier recovery.]{Top: Real-valued version of the received signal after carrier recovery (\ci{rc2}). Bottom: Zoom of \ci{rf}, obtained by filtering \ci{rc2}.\label{fig:pamOffsetsFilteredRx}}
\end{figure}

The adopted SRC has a group delay of \ci{gdelayRC=3,732} samples. Taking in account the two 
filtering stages (at transmitter and receiver), the first symbol is extracted at sample \ci{startSample=2*gdelayRC+1}.

The final received symbols constellation is depicted in \figl{pamOffsetsConstellations}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/pamOffsetsConstellations}
\caption{Transmit and receive symbol constellations.\label{fig:pamOffsetsConstellations}}
\end{figure}

In this simulation, the SER was 0\% (no symbol errors) and the EVM = 1.94\%.
\eExample

\exal{offsetsPAM} illustrated a ``perfect'' estimation, but there are several difficulties
related to this FFT-based procedure. For example:
\begin{itemize}
	\item \emph{Phase ambiguity}: Because the phase offset $\phi$ shows off as a phase of $2\phi$ at the frequency of interest, there is an ambiguity\footnote{See, e.\,g.~\cite{Thaiupathump00}.} 
	in this estimation, unless $\phi$  is restricted to be within a range of $\pi$~rad (e.\,g., $[0,\pi[$ or $[-\pi/2, \pi/2[$). For example, using \ci{phaseOffset=2} in \codl{ex_fftBasedPAMCarrierRecovery} leads to an erroneous estimated phase offset of $2-\pi \approx -1.1416$~rad.
	\item \emph{FFT leakage}: The FFT leakage can decrease the accuracy in case the frequency of interest does not coincide with a FFT bin.
	For example, using \ci{fc=pi/4*1e9} in \codl{ex_fftBasedPAMCarrierRecovery} leads to errors in frequency and phase estimation of approximately 110.6~Hz and -0.22~rad, respectively. In this specific case, the SER remains zero and the EVM increases to 2.1\%.
\end{itemize}
The latter issue and the high-computational cost of the FFT-based method motivate the adoption of adaptive algorithms, such as the ones based on the PLL. Before discussing them, the FFT-based method
is applied to QAM in the sequel.

\subsubsection{QAM carrier recovery using FFT}

The offsets estimation procedure used in \codl{ex_fftBasedPAMCarrierRecovery} is based on the discrete spectrum component $\frac{x^2_\textrm{PAM}(t)}{8} \cos(2\aw_{\Delta} t + 2\phi)$
that appears in \equl{squaredPAM}. The implicit condition for having this component available for the
estimation is that $\ev[x^2_\textrm{PAM}(t)] \ne 0$, which is a consequence of having 
$\ev[m^2] \ne 0$, where $m$ is the PAM constellation symbol.

As discussed in Appendix~\ref{sec:balancedConstellation}, some QAM constellations (called here ``balanced'') lead to $\ev[x^2(t)]=0$, where $x(t)$ is the transmit signal, and a discrete spectrum component would not be created.
In such cases, a power of $P \ne 2$ is used to
create $x^P(t)$ as the input to the FFT. The reader is invited to vary the
modulation order $M$ and $P$ to evaluate, for example:
\begin{lstlisting}
M=16; m=ak_qamSquareConstellation(M); mean(m.^2) %returns 0
M=16; m=ak_qamSquareConstellation(M); mean(m.^3) %returns 0
M=16; m=ak_qamSquareConstellation(M); mean(m.^4) %it's not zero
\end{lstlisting}
With a similar test, it can also be seen that, for a modulation order $M=8$ (which corresponds to a ``rectangular'', not
``square'' constellation), $\ev[m^2] \ne 0$ and, in this specific case, $P=2$ would work.
But in other cases $P=4$ is required. This choice makes a little more complicated
the FFT-based offsets estimation, as discussed in the sequel.

A QAM receiver uses both a cosine and sine to recover the in-phase $x_i(t)$ and quadrature $x_q(t)$ components, but the offsets estimation procedure is illustrated here with just the cosine.
The transmit QAM signal is 
$s(t) = x_i(t) \cos(\aw_c t) - x_q(t) \sin(\aw_c t)$ and (again considering an ideal channel) the
receiver then generates $r(t) = s(t) \cos((\aw_c +\aw_{\Delta}) t + \phi)$ (and it is assumed here that $\aw_c > \aw_{\Delta}$).
It is the analysis of $r^4(t)$ that will lead to the estimation of $\aw_{\Delta}$ and $\phi$.
The Matlab Symbolic Math Toolbox can be used as follows:
\begin{lstlisting}
syms xi xq wt Dt p %define symbolic variables and calculate:
simplify(expand(((xi*cos(wt)-xq*sin(wt))*cos(wt+Dt+p))^4))
\end{lstlisting}
where \ci{Dt} corresponds to $\aw_{\Delta} t$, \ci{p} to $\phi$, etc. This allows to decompose $r^4(t)$ in 54 parcels that after careful inspection can be written as:
\begin{align*}
r^4(t) =& x_0(t) + x_{1,i}(t) \cos(\Theta_1(t)) + x_{1,q}(t) \sin(\Theta_1(t)) + \ldots + x_{11,i}(t) \cos(\Theta_{11}(t)) +  \\
& x_{11,q}(t) \sin(\Theta_{11}(t)) + x_{12,i}(t) \cos(\Theta_{12}(t)) + x_{12,q}(t) \sin(\Theta_{12}(t)) \\
=& x_0(t) + \sum_{k=1}^{12} x_{k,i}(t) \cos(\Theta_{k}(t)) + x_{k,q}(t) \sin(\Theta_{k}(t)). \numberthis
\label{eq:qamToTheFourth}
\end{align*}
The (real-valued) signal centered at DC is given by $x_0(t) = 9/64[2x_i^2(t)x_q^2(t) + x_i^4(t) + x_q^4(t)]$, and has a bandwidth expanded by approximately $P=4$ times the one of the original QAM signal.
The other signals $x_{k,i}(t)$ and $x_{k,q}(t)$ are described in \tabl{thetaSignals}. 
%More specifically the ones corresponding to $\Theta_k(t), k=1,2,3$ and 12.

\begin{table}
\centering
\caption{Signals associated to $\Theta_k(t), k=1,\ldots,12$.\label{tab:thetaSignals}}
\begin{tabular}{|l|c|c|c|}
\hline
$k$ & $x_{k,i}(t)$ & $x_{k,q}(t)$ & Spec. line? \\ \hline
1 & $(x_i^4(t)-x_q^4(t))/8$ & $(x_i(t)x_q^3(t)+x_i^3(t)x_q(t))/4$ & no\\ \hline
2 & $(x_i^4(t) + x_q^4(t)-6x_i^2(t)x_q^2(t))/128$ & $(x_i^3(t)x_q(t)-x_i(t)x_q^3(t))/32$ & yes\\ \hline
3 & $4 x_{2,i}(t)$ & $4x_{2,q}(t)$ & yes\\ \hline
4 & $(3/2) x_{1,i}(t)$ & $(-3/2)x_{1,q}(t)$ & no\\ \hline
5 & $3(x_i^4(t) + x_q^4(t) + 2 x_i^2(t)x_q^2(t))/16$ & 0 & yes\\ \hline
6 & $x_{1,i}(t)/4$ & $x_{1,q}(t)/4$ & no\\ \hline
7 & $6 x_{2,i}(t)$ & $-6x_{2,q}(t)$& yes\\ \hline
8 & $x_{1,i}(t)$ & $-x_{1,q}(t)$ & no\\ \hline
9 & $x_{5,i}(t)/4$ & 0 & yes\\ \hline
10 & $x_{3,i}(t) = 4x_{2,i}(t)$ & $-x_{3,q}(t) = -x_{2,q}(t)$ & yes\\ \hline
11 & $x_{1,i}(t)/4$ & $-x_{1,q}(t)/4$ & no\\ \hline
12 & $x_{3,i}(t)/4$ & $x_{3,q}(t)/4$& yes\\ \hline
\end{tabular}
\end{table}

Some frequency values of interest in \equl{qamToTheFourth} are:
%x_{2,i}(t) \cos(\Theta_2(t)) + x_{2,q}(t) \sin(\Theta_2(t)) 
%x_{3i}(t) \cos(\aw_3) + x_{3q} \sin(\aw_3) +
%x_{4i}(t) \cos(\aw_4) + x_{4q} \sin(\aw_4) +
%x_{5i}(t) \cos(\aw_5) + x_{5q} \sin(\aw_5) +
%x_{6i}(t) \cos(\aw_6) + x_{6q} \sin(\aw_6) +
%x_{7i}(t) \cos(\aw_7) + x_{7q} \sin(\aw_7) +
%x_{8i}(t) \cos(\aw_8) + x_{8q} \sin(\aw_8) +
%x_{9i}(t) \cos(\aw_9) + x_{9q} \sin(\aw_9) +
$\Theta_{1}(t) = 2 \aw_{\Delta} t + 2 \phi$,
$\Theta_{2}(t) = 4 \aw_{\Delta} t + 4 \phi$ and
$\Theta_{3}(t) = 2 \aw_c t - 2 \aw_{\Delta} t - 2\phi$, which correspond
to the three lowest frequencies above DC. The term of interest to be used
to estimate the offsets is $\Theta_{2}(t)$, which centers the signals $x_{2,i}(t)$ and
$x_{2,q}(t)$
at frequency $4 \aw_{\Delta}$~rad/s.
The highest center frequency $8\aw_c + 4\aw_{\Delta}$ is associated to the term $\Theta_{12}(t) = 8\aw_c + 4\aw_{\Delta} + 4\phi$. The other frequencies can be found in \ci{snip\_channel\_qam\_squared\_freqs.m}.

As expected, the signals in \tabl{thetaSignals} depend on the PAM signals $x_i(t)$ and $x_q(t)$.
To better understand why some of these signals do not present a discrete spectral line (as indicated
in the last column of \tabl{thetaSignals}), it is useful to simplify the discussion assuming that the continuous-time
signals are perfectly sampled at the symbol rate $\rsym$ such that, for example,
instead of statistical moments of continuous-time signals such as $\ev[x_i^4(t)]$, one can deal directly with the associated constellation symbols and their moments.

Because we are interested in balanced QAM constellations, assume that the two underlying PAMs
have the same order $\sqrt{M}$. In this case, the signal 
$x_{1,i}(t)=(x_i^4(t)-x_q^4(t))/8$ sampled at $\rsym$ has an average value of
$(\ev[m_i^4]-\ev[m_q^4])/8=0$ because $\ev[m_i^4]=\ev[m_q^4]$, where $m_i$ and $m_q$ are the PAM transmit symbols.
Similarly, the average of $x_{1,q}(t)$ at the baud rate is
$(\ev[m_i m_q^3]+\ev[m_i^3 m_q])/4=0$ because $m_i$ and $m_q$ are independent, such that
e.\,g. $\ev[m_i m_q^3] = \ev[m_i] \ev[m_q^3] = 0$ because $\ev[m_i]=0$.
In contrast, $x_{r,1}(t)$ has a non-zero average dictated by
$(3/16)(\ev[m_i^4]+\ev[m_q^4] +2\ev[m_i^2] \ev[m_q^2])$.
Along this reasoning one can explain the presence of a spectrum line for $k=5$ and
the absence for $k=1$ as indicated in \tabl{thetaSignals}, for example.

%$c_{\textrm{PAM}}$ as $\ev[c_{\textrm{PAM}}^4]$.

A numerical example that further discusses these relations is provided in the sequel.
%$x_{1,i}(t) = (x_i^4(t)-x_q^4(t))/8$ and
%$x_{1,q}(t) = (x_i(t)x_q^3(t)+x_i^3(t)x_q(t))/4$, while the ones corresponding to $\Theta_1(t)$ are
%$x_{2,i}(t) = (x_i^4(t) + x_q^4(t)-6x_i^2(t)x_q^2(t))/128$ and
%$x_{2,q}(t) = (x_i^3(t)x_q(t)-x_i(t)x_q^3(t))/32$. Similarly, the signals corresponding to $\Theta_3(t)$ are 
%$x_{3,i}(t) = (x_i^4(t) + x_q^4(t)-6x_i^2(t)x_q^2(t))/32$ and
%$x_{3,q}(t) = (x_i^3(t)x_q(t)-x_i(t)x_q^3(t))/8$.
%
%The highest frequency corresponds to $\Theta_{10}(t) = 8\aw_c + 4|\aw_{\Delta}| + 4\phi$, which
%is related to
%$x_{10,i}(t) = x_{3,i}(t)/4$
%%(x_i^4(t) + x_q^4(t)-6x_i^2(t)x_q^2(t))/128$ 
%and
%$x_{10,q}(t) = x_{3,q}(t)/4$.
%(x_i^3(t)x_q(t)-x_i(t)x_q^3(t))/32$.


%Terms with 2*Dt
%$(x_i^4(t)\cos(\aw_1))/8 + 
% (x_q^4(t)\cos(\aw_1))/8 + 
%(xi*x_q^3*\sin(\aw_1))/4 +
 %(x_i^3*xq*\sin(\aw_1))/4 $

%Terms with 4*Dt
 %$(x_i^4(t)\cos(\aw_2))/128 
 %(x_q^4(t)\cos(\aw_2))/128 +
% (3x_i^2(t)x_q^2(t)\cos(\aw_2))/64 
%
% (x_i(t)x_q^3(t)\sin(\aw_2))/32 + 
%(x_i^3(t)x_q(t)\sin(\aw_2))/32
%$

%That can be rewritten as: w0 = 4*Dt + 4*p
%1/64 ( (x_i^4+xq+4)/2 -3* x_i^2*x_q^2) \cos(w0) + 1/32 (x_i^3*xq -xi*x_q^3) \sin(w0)


%High frequencies 2wt with Negative shift
%$(x_i^4(t)\cos(\aw_3))/32 +
%(x_q^4(t)\cos(\aw_3))/32 + 
% (3x_i^2(t)x_q^2(t)\cos(\aw_3))/16 + 
% (x_i(t)x_q^3(t)\sin(\aw_3))/8 + 
%(x_i^3(t)x_q(t)\sin(\aw_3))/8$

%\begin{verbatim}
%High frequencies 2wt with Positive shift
%(3*x_i^4*cos(2*Dt + 2*p + 2*wt))/16 +
%(x_i^4*cos(4*Dt + 4*p + 2*wt))/32 + 
%(3*x_q^4*cos(2*Dt + 2*p + 2*wt))/16 
%-(x_q^4*cos(4*Dt + 4*p + 2*wt))/32 + 
%(3*x_i^4*cos(2*wt))/16 + 
% (3*x_q^4*cos(2*wt))/16 + 
%(3*x_i^2*x_q^2*cos(2*Dt + 2*p + 2*wt))/8 
% (3*xi*x_q^3*sin(2*wt))/8 
% (3*x_i^3*xq*sin(2*wt))/8 + 
 %(xi*x_q^3*sin(4*Dt + 4*p + 2*wt))/16 
 %(x_i^3*xq*sin(4*Dt + 4*p + 2*wt))/16 
%
%High frequencies 4wt
%No shift
%(3*x_i^4*cos(4*wt))/64 
%(3*x_q^4*cos(4*wt))/64 +
% (9*x_i^2*x_q^2*cos(4*wt))/32 
 %(3*xi*x_q^3*sin(4*wt))/16 
% (3*x_i^3*xq*sin(4*wt))/16
%Shift by 2*Dt
%(x_i^4*cos(2*Dt + 2*p + 4*wt))/8 +
%- (x_q^4*cos(2*Dt + 2*p + 4*wt))/8 + 
% (xi*x_q^3*sin(2*Dt + 2*p + 4*wt))/4 
% (x_i^3*xq*sin(2*Dt + 2*p + 4*wt))/4 + 
%Shift by 4*Dt
%(3*x_i^4*cos(4*Dt + 4*p + 4*wt))/64 + 
 %(3*x_q^4*cos(4*Dt + 4*p + 4*wt))/64 
 %(3*x_i^2*x_q^2*cos(4*Dt + 4*p + 4*wt))/32 
%
%Positive High frequencies 6wt
%Shifted by +2*Dt
%(x_i^4*cos(2*Dt + 2*p + 6*wt))/32 + 
 %(x_q^4*cos(2*Dt + 2*p + 6*wt))/32 
% (3*x_i^2*x_q^2*cos(2*Dt + 2*p + 6*wt))/16 +
%(xi*x_q^3*sin(2*Dt + 2*p + 6*wt))/8 +
% (x_i^3*xq*sin(2*Dt + 2*p + 6*wt))/8 + 
%Shifted by +4*Dt
 %(x_i^4*cos(4*Dt + 4*p + 6*wt))/32 + 
%-(x_q^4*cos(4*Dt + 4*p + 6*wt))/32 + 
% (xi*x_q^3*sin(4*Dt + 4*p + 6*wt))/16
 % (x_i^3*xq*sin(4*Dt + 4*p + 6*wt))/16 +
%
%Positive High frequencies 8wt
%(x_i^4*cos(4*Dt + 4*p + 8*wt))/128 + 
%(x_q^4*cos(4*Dt + 4*p + 8*wt))/128 +
% (3*x_i^2*x_q^2*cos(4*Dt + 4*p + 8*wt))/64 
%(xi*x_q^3*sin(4*Dt + 4*p + 8*wt))/32 
% (x_i^3*xq*sin(4*Dt + 4*p + 8*wt))/32 + 
%\end{verbatim}

\bExample \textbf{Complete example of frequency and phase offsets correction for QAM}.
\label{ex:offsetsQAM}
The companion script \ci{MatlabOctaveBookExamples/ex\_fftBasedQAMCarrierRecovery.m} is similar to \codl{ex_fftBasedPAMCarrierRecovery} but incorporates some modifications to support
offsets estimation for balanced QAM constellations, restricted to a phase offset $\phi \in [-\pi/4, \pi/4[$~rad due to the inherent phase ambiguity created by the process of raising the signal to the fourth-power.

\figl{qamOffsetsTo4Rx} depicts the PSD of $r^4(t)$, obtained via \equl{qamToTheFourth}.
In this case, \ci{snip\_channel\_qam\_squared\_freqs.m} informs that
the 12 frequencies related to $\Theta_i(t), i=1,\ldots,12$ are:
0.259, 0.518,    1.741,    2.0, 2.259,    2.518,
    4.0, 4.259,    4.518,    6.259,    6.518 and   8.518.
respectively, all in GHz, as can be identified in \figl{qamOffsetsTo4Rx}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthLarge]{./Figures/qamOffsetsTo4Rx}
\caption{PSD of $r^4(t)$, obtained via \equl{qamToTheFourth}.\label{fig:qamOffsetsTo4Rx}}
\end{figure}

As indicated in \tabl{thetaSignals}, there is a factor of 4 between the signals
associated to $\Theta_3(t)$ and $\Theta_{12}(t)$ (located in \figl{qamOffsetsTo4Rx}
at frequencies 1.741 and 8.518~GHz, respectively), which corresponds to
$20 \log_{10} 4 \approx 12$~dB. This is confirmed by the difference of the
respective PSD values (see the datatips in \figl{qamOffsetsTo4Rx}): $143.4-131.2 \approx 12.2$~dB.

Datatips for the DC and eight of the center frequencies are shown in \figl{qamOffsetsTo4Rx}. With the exception of the datatip at 259~MHz (from $\Theta_1(t)$), all other eight parcels have a
discrete line. In other words, among the 13 parcels in \figl{qamOffsetsTo4Rx}, there are 5
that do not have a discrete line, and the one corresponding to 259~MHz is the only one with
a datatip. This is better visualized by zooming \figl{qamOffsetsTo4Rx} as done in 
\figl{qamOffsetsTo4RxAllZooms}.

\begin{figure}[!htb]
  \begin{center}
    \subfigure[Lowest band]{\label{fig:qamOffsetsTo4RxZoom1}\includegraphics[width=5cm]{Figures/qamOffsetsTo4RxZoom1}}
    \subfigure[Second band]{\label{fig:qamOffsetsTo4RxZoom2}\includegraphics[width=5cm]{Figures/qamOffsetsTo4RxZoom2}}
    \\
    \subfigure[Third band]{\label{fig:qamOffsetsTo4RxZoom3}\includegraphics[width=5cm]{Figures/qamOffsetsTo4RxZoom3}}
    \subfigure[Highest band]{\label{fig:qamOffsetsTo4RxZoom4}\includegraphics[width=5cm]{Figures/qamOffsetsTo4RxZoom4}}
  \end{center}
  \caption[Zooms of the PSD in \figl{qamOffsetsTo4Rx} in four regions.]{Zooms of the PSD in \figl{qamOffsetsTo4Rx} in four regions. The signal of interest to
estimate the frequency and phase offsets is at 518~MHz in the first band.\label{fig:qamOffsetsTo4RxAllZooms}}  
\end{figure}

The results generated by the script \ci{ex\_fftBasedQAMCarrierRecovery.m} 
are SER equals to zero and EVM=1.9~\%.
\figl{qamOffsetsConstellations} shows the transmit and received 
constellations after carrier recovery.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/qamOffsetsConstellations}
\caption{Transmit and received 
QAM constellations after correction of the offsets.\label{fig:qamOffsetsConstellations}}
\end{figure}

Different from the PAM case of \codl{ex_fftBasedPAMCarrierRecovery}, where the estimated
offset was obtained via a normalization by $P=2$,
the relation between the actual phase offset $\phi$ and the phase $\Phi$ at the chosen
FFT (of the $r^4(t)$ signal) peak is more involved.
One indication of this fact is that 
the 
signal of interest, related to $\Theta_2(t)$ in \tabl{thetaSignals}, is composed by
both in-phase $x_{2,i}(t)$ and quadrature $x_{2,q}(t)$ components. To illustrate
the issue, the script 
\ci{MatlabBookFigures/figs\_synchronism\_carrier\_recovery.m}
generated \figl{fftBasedQAMCarrierRecovery}, which shows in its top plot
the relation between $\Phi$ (called \ci{fftPeakPhase} in the script) versus $\phi$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./Figures/fftBasedQAMCarrierRecovery}
\caption{Top: Relation between $\Phi$ (\ci{fftPeakPhase}) versus the phase offset $\phi$. Bottom: Estimation error.\label{fig:fftBasedQAMCarrierRecovery}}
\end{figure}

\codl{ex_fftBasedQAMCarrierRecovery} shows the lines of the mentioned script that specifically
take care of obtaining the estimated offset $\hat \phi$ (\ci{estPhaseOffset} in the code) from
$\Phi$, according to the top plot in \figl{fftBasedQAMCarrierRecovery}.

\lstinputlisting[firstline=76,firstnumber=76,lastline=83,caption=./Code/MatlabOctaveBookExamples/ex\_fftBasedQAMCarrierRecovery,label=code:ex_fftBasedQAMCarrierRecovery]{./Code/MatlabOctaveBookExamples/ex_fftBasedQAMCarrierRecovery.m}

The bottom plot of \figl{fftBasedQAMCarrierRecovery} shows the estimation error $\phi - \hat \phi$.
Note that the abscissa range is $[-\pi/4, \pi/4[$.
\eExample

The FFT-based method used in \codl{ex_fftBasedPAMCarrierRecovery} (and \ci{ex\_fftBasedQAMCarrierRecovery.m}) is pedagogical but it is not used in practice due to its relatively high computational cost and the discussed limitations. The next section details the PLL, which is widely used in practice for synchronization purposes.

\subsection{Phase-locked loop (PLL)}
\label{sec:pll}

As the name suggests, a phase-locked loop (PLL) is a closed-loop control system
that generates an output signal with a phase
$\theta_o$ that depends on the input signal phase $\theta_i$. In other words, $\theta_o$ is said to be ``locked'' to $\theta_i$ because it is a function of the input phase. This relation typically corresponds to a time-delay such that $\theta_o = \theta_i - \Delta \theta$ or, for modeling purposes and without concerns with causality, a simple equality $\theta_o = \theta_i$. Notice that keeping the phases in lock, implies that the frequencies are also locked.

Some of the PLL applications are:
\begin{itemize}
	\item deriving a stable signal with frequency that is a multiple or submultiple of signal with a reference frequency;
	\item regenerating the carrier frequency for coherent demodulation;
	\item generate (synthesize) periodic signals with fundamental frequencies that are multiples of the PLL input frequency, such as a clock tree synchronized with a master clock;
	\item demodulate signals such as a frequency-modulated signal;
	\item symbol-timing recovery from a line code such as AMI.
\end{itemize}

A typical PLL is composed of three main blocks: phase detector, filter and voltage-controlled oscillator (VCO). 
The PLL can be implemented in the analog or digital domain. A substantial fraction of the literature is devoted to the analog PLL. The digital PLL is, of course, closely related to its analog counterpart, but there are differences on their operations and even terminology. For example, the VCO in the digital PLL is often called \emph{numerically controlled oscillator}\index{Numerically controlled oscillator (NCO)} (NCO). This discussion starts by the analog PLL and then concentrates on the digital version.

%The PLL is a control system with feedback and its main goal is to generate a signal that tracks the frequency and phase of its input signal $r(t)$, which is often contaminated by
%noise. 

As pointed out, the PLL is a versatile block and, therefore, used in many applications. 
There is a wide range of distinct PLL circuits and the name PLL denotes the whole family not
a specific circuit. Besides, the PLL output signal depends on the application. For example, the PLL can output a sinusoid $y(t)$ to be used on frequency downconversion in synchronous reception or can eventually already output the signal of interest (e.\,g., a demodulated FM signal). 

AK-TODO: finish this brief introduction on different types of PLL.

In many cases the PLL generates a pulse train (square waveform) or sinusoid.

The PLL can work with sinusoids at its input or digitally modulated signals at the baud rate.

The digital PLL (DPLL) can use NDA (non-data aided).

The digital PLL (DPLL) can use decision-directed schemes.

The strategy is to discuss continuous-time for sinusoids. Then...

\subsection{Continuous-time PLL for tracking sinusoids}

The behavior of a continuous-time PLL is briefly described in the sequel with help from \figl{continuousTimePLL}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/continuousTimePLL}		
	\caption{Block diagram of a basic continuous-time phase-locked loop (PLL).\label{fig:continuousTimePLL}}
\end{figure}

%Roughly speaking, 
When the PLL is in ``lock'', the VCO outputs a signal $y(t)$ that is compared, by the phase detector with $r(t)$. The error signal is then used to drive the VCO to adjust $y(t)$ to track $r(t)$. 

More specifically, the \emph{phase detector} estimates the phase difference $\epsilon(t) = \phi_i(t)-\phi_o(t)$, where $\phi_o(t)$ is the phase of the sinusoid generated by the PLL. This difference $\epsilon(t)$ passes through the \emph{loop filter}, which outputs a \emph{control signal} $c(t)$ that drives the \emph{voltage-controlled oscillator} (VCO).

To keep the PLL in lock, the error must be within a range, such as $|\epsilon(t)| < \pi$, which is called the \emph{lock} range. Hence, the PLL operates in one of the modes: \emph{acquisition} or \emph{tracking} mode. There are special techniques to bring the PLL to its lock range, i.\,e., from the acquisition to the tracking mode.

%\subsubsection{PLL with sinusoidal phase detector}

To make the discussion concrete and simple, it is assumed hereafter that the PLL outputs a sinusoid and that its
input is $r(t)=\cos(2\pi f_i t+\phi_i(t)) + \nu(t)$, where $\nu(t)$ is the noise.
The PLL goal it to deliver a ``clean'' version 
\begin{equation}
y(t)=\cos(2 \pi f_o t + \hat \phi_o(t))
\label{eq:vcoOutput}
\end{equation}
of $r(t)$.

As mentioned, keeping the phases in lock, implies that the frequencies are also locked. Assume that $f_i \ne f_o$, which can be modeled as $f_o = f_i + \Delta_f$. It is convenient
to rewrite 
\begin{equation}
y(t)=\cos(2 \pi (f_i + \Delta_f) t + \hat \phi_o(t)) = \cos(2 \pi f_i t +  \phi_o(t)), 
\label{eq:vcoTrick}
\end{equation}
where $\phi_o(t) = 2 \pi \Delta_f t + \hat \phi_o(t)$.
 %When the \emph{quiescent VCO frequency} $It is Eq (2.9) of Tranter
This way, without loss of generality, the PLL treatment can assume that if a frequency
offset exists, it is then incorporated in $\phi_o(t)$ as a linear function of $t$ with slope 
$2 \pi \Delta_f$.

The next sections provide more details about the PLL components, assuming 
a sinusoidal phase detector.

\subsubsection{VCO}

Before discussing the VCO, it is useful to recall that the \emph{instantaneous angular frequency} $\omega(t)$ of a sinusoid $\sin(\theta(t))$ is simply the derivative
\begin{equation}
\omega(t) = \frac{\textrm{d} \theta(t)}{\textrm{d} t}.
\label{eq:instantaneousFrequency}
\end{equation}
Inspired in \equl{vcoOutput} or \equl{vcoTrick}, $\theta(t)=2 \pi f_n t + \phi(t)$ is assumed to be composed
by a nominal frequency $f_n$ (in Hz) and time-varying phase $\phi(t)$ and, from \equl{instantaneousFrequency}:
\begin{equation}
\omega(t) = \frac{\textrm{d} (2 \pi f_n t + \phi(t)) }{\textrm{d} t} = 2 \pi f_n + \frac{\textrm{d} \phi(t) }{\textrm{d} t}.
\label{eq:vcoOmega}
\end{equation}

Now, it should be considered that the VCO has its oscillation frequency controlled by the input voltage $c(t)$.
The actual relation between input and output can be nonlinear but it is often modeled
as the linear relation
\begin{equation}
\omega(t) = 2 \pi f_n + g c(t),
\label{eq:vco1}
\end{equation}
where $f_n$ is the nominal (or quiescent) linear frequency of the oscillator and $g$ is a gain (also called oscillator sensitivity). For simplicity, assume that $g=1$, 
%and $f_n = f_i$ (any mismatch between $f_n$ and the input frequency $f_i$ could be incorporated into the other parcel of the sinusoid argument). 
and compare \equl{vcoOmega} and \equl{vco1} to obtain the control signal as
\begin{equation}
c(t) = \frac{\textrm{d} \phi(t) }{\textrm{d} t}.
\label{eq:vcoControl}
\end{equation}

Hence, when used in a PLL, the VCO output phase is
\begin{equation}
\phi_o(t) = \int_{-\infty}^t c(t) \textrm{d}t
\label{eq:vcoIntegral}
\end{equation}
such that the VCO output $y(t)=\cos(2 \pi f_i t + \phi_o(t))$, as in \equl{vcoTrick}, is controlled by $c(t)$. 

\subsubsection{Phase detector}

Among many options, a phase detector can be implemented in continuous-time domain with a mixer and lowpass filter. One alternative is to use the trigonometric identity
\[
\sin(a) \cos(b) = \frac{1}{2}[\sin(a-b) + \sin(a+b)]
\]
and, assuming $f_o = f_i$, obtain at the mixer output
\begin{equation}
r(t) y(t) = \frac{1}{2}[\sin(\phi_i(t) - \phi_o(t)) +  \sin(4 \pi f_i t + \phi_i(t) + \phi_o(t))].
\label{eq:mixerOutput}
\end{equation}


A lowpass filter is used to get rid of the term with angular frequency $\aw = 4 \pi f_i$ such that, 
ignoring the scaling factor $1/2$, the output of the phase detector composed by the mixer and filter is
\[
\sin(\phi_i(t) - \phi_o(t)) \approx \phi_i(t) - \phi_o(t) = \epsilon(t)
\]
when $\epsilon(t)$ is sufficiently small such that $\sin(x) \approx x$.

Similar to the variety of PLL circuits, there are many other phase detectors.
%In some cases, an estimate of $\epsilon(t)$ without the need of a lowpass filter.
%The design of the subsequent loop filter $L(s)$ is obviously done in accordance
%to the signal that the phase detector generates.

\subsubsection{Loop filter}

From a simplistic point of view, $L(s)$ can be seen as a linear filter.
%that rejects the unwanted components in \equl{mixerOutput}. 
However, because $L(s)$ controls
several important PLL features such as stability, settling time, etc., its study and design is based on control systems theory as discussed in the sequel. 

\subsubsection{A linear model for the continuous-time PLL in the phase domain}

It is convenient to perform a PLL analysis that does not depend on the signals amplitudes but only on their phases. This can be done in the so-called ``phase domain'', where $\phi_i(t)$ and $\phi_o(t)$ are
the input and output phases, respectively.
\figl{pllLinearModel} represents a linear model that is suitable
to design and analyze a PLL when operating in its linear range. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresTex/pllLinearModel}
\caption{A linear model for the continuous-time PLL in phase-domain using Laplace transforms.\label{fig:pllLinearModel}}
\end{figure}

All signals in \figl{pllLinearModel} are represented using Laplace, with 
$\Phi_i(s), \Phi_o(s), E(s)$ and $C(s)$ being the transforms of 
$\phi_i(t), \phi_o(t), \epsilon(t)$ and $c(t)$, respectively.
$N_{\phi}(s)$ is the Laplace transform of the signal $\nu_{\phi}(t)$, which is the PLL \emph{phase noise}\index{Phase noise}. In \figl{pllLinearModel}, the integral of \equl{vcoIntegral} is represented as $1/s$.


The PLL \emph{phase transfer function}\index{PLL phase transfer function} is defined as
$H_{\phi}(s) = \Phi_o(s)/\Phi_i(s)$, with the assumption that $N_{\phi}(s)=0$. From  \figl{pllLinearModel}, $E(s)=\Phi_i(s)-\Phi_o(s)$ and $\Phi_o(s) = E(s) L(s) (1/s)$, such that 
\begin{equation}
H_{\phi}(s) = \frac{\Phi_o(s)}{\Phi_i(s)} = \frac{L(s)}{L(s)+s}.
\label{eq:pllTransferFunction}
\end{equation}
Similarly, it is possible to obtain the \emph{phase error transfer function} as 
\begin{equation}
\frac{E(s)}{\Phi_i(s)} = \frac{1}{1+L(s)/s} = \frac{s}{L(s)+s}.
\label{eq:pllErrorTrFun}
\end{equation}
%AK-TODO: check eq above

It is useful to recall from control theory that a closed-loop with unitary feedback as
in \figl{pllLinearModel} has a transfer function 
\begin{equation}
H(s)=\frac{G(s)}{G(s)+1}, 
\label{eq:feedbackControl}
\end{equation}
where $G(s)$ is the \emph{open-loop transfer function}\index{Open-loop transfer function}. \equl{pllTransferFunction} can be obtained from \equl{feedbackControl} because in this case $G(s)=L(s)/s$.

One alternative for the loop filter is
\begin{equation}
L(s) = \alpha + \frac{\alpha \beta}{s} = \alpha \frac{(s + \beta)}{s},
\label{eq:pllSecondOrder}
\end{equation}
which corresponds to a PI (proportional-integral) controller. This option is called a second-order PLL given that \equl{pllTransferFunction} becomes a second-order system when $L(s)$ is given by \equl{pllSecondOrder}.

Alternatively, using $L(s) = \alpha$ would correspond to a first-order PLL. The first-order PLL has a non-zero steady-state phase error when there is a frequency offset (difference between $f_i$ and $f_o$). Hence, due to its improved robustness, the second-order PLL is discussed in the sequel.

Substituting \equl{pllSecondOrder} in \equl{pllTransferFunction} leads to
\begin{equation}
H_{\phi}(s) = \frac{s \alpha + \alpha \beta}{s^2 + \alpha s + \alpha \beta},
\label{eq:pllSecondOrder2}
\end{equation}
which corresponds to \equl{secondOrder4} when $\aw_n = \sqrt{\alpha \beta}$ and $\zeta = 0.5 \sqrt{\alpha/\beta}$. As indicated by \figl{secondOrderSys}b, the parameters $\aw_n$ and $\zeta$ (or, equivalently, $\alpha$ and $\beta$) can be tuned to obtain the desired behavior of the PLL. For example, a larger bandwidth (primarily controlled by $\aw_n$) implies in more phase noise at the output, given that $\nu_{\phi}(t)$ has a wide bandwidth, but allows
a faster tracking interval.

The transient behavior of a PLL can be observed by assuming that the input $\phi_i(t)=u(t)$ is a step function and obtaining the step response associated to \equl{pllSecondOrder2} (or, equivalently, \equl{secondOrder4}). \figl{pllStepResponse} illustrates the step responses for $\aw_n = 20$~rad/s and four values of $\zeta$. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/pllStepResponse}
\caption{PLL step responses for several values of $\zeta$.\label{fig:pllStepResponse}}
\end{figure}

From \figl{pllStepResponse} it can be seen that a larger $\zeta$ implies an improved
 tracking performance (faster and without overshoot) but a larger bandwidth, and consequently, output phase noise, as illustrated in \figl{secondOrderSys}b. Due to this
tradeoff, $\zeta = 0.707$ is often used in practice.

\bExample \textbf{PLL output when there is a frequency offset.}
Complementing the step response, the ramp response allows to evaluate the PLL when
there is a frequency offset $\Delta_f$ such that $f_o = f_i + \Delta_f$ and $\phi_i(t) = 2 \pi \Delta_f t u(t)$. Because the assumed model is linear, the output will scale with any
 input scaling factor, and it suffices to adopt $2 \pi \Delta_f=1$ and use the ramp $\phi_i(t) = t u(t)$ as input.

\figl{pllRampResponse} depicts ramp responses for $\aw_n = 20$~rad/s and two values of $\zeta$. It can be seen that the output presents a transient but then follows the input, which 
can be seen especially for $\zeta=0.5$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/pllRampResponse}
\caption{PLL step responses for several values of $\zeta$.\label{fig:pllRampResponse}}
\end{figure}

In complement to graphs of transfer functions as \equl{pllTransferFunction}, it is
illustrative to observe the phase error transfer function as in
\equl{pllErrorTrFun}, which for the second-order PLL with $L(s)$ given by \equl{pllSecondOrder}, corresponds to \equl{secondOrder3}.

\figl{pllErrorResponse} illustrates the PLL error for $\aw_n = 20$~rad/s and three values of $\zeta$ when the input is the same ramp as in \figl{pllRampResponse}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/pllErrorResponse}
\caption{PLL phase error when the input is a ramp.\label{fig:pllErrorResponse}}
\end{figure}

Recall that, when obtaining \figl{pllErrorResponse}, it was assumed $2 \pi \Delta_f=1$.
The values in \figl{pllErrorResponse} should be scaled by $2 \pi \Delta_f$ 
when the goal is to determine
the lock range, for example. To illustrate, consider the maximum error for $\zeta=0.5$ in \figl{pllErrorResponse} is 0.027 and that the error is restricted to $|\epsilon(t)|<\pi$.
In this case, $|2 \pi \Delta_f \times 0.027| < \pi$, which leads to a lock range given
by $|\Delta_f| < 18.5$~Hz.
\eExample 

\subsection{Discrete-time PLL (DPLL) for tracking sinusoids}

AK-TODO: include phase detector from Miao, page 397, before or later here.

The block diagram of a discrete-time PLL (also called digital PLL or DPLL) is depicted in 
\figl{discreteTimePLL}, where the VCO is sometimes called NCO, as previously mentioned. 


\begin{figure}[htbp]
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/discreteTimePLL}		
	\caption{Block diagram of a basic discrete-time phase-locked loop (DPLL).\label{fig:discreteTimePLL}}
\end{figure}

The PLL input and output in discrete-time are $r[n] = \cos(\dw_i n + \phi_i[n]) + \nu[n]$ and $y[n] = \cos(\dw_i n + \phi_o[n])$, respectively.
Similarly, the discrete-time version of \equl{vcoControl} is

where the sampling period $\ts$ can be substituted by another value, denoted here as $\gamma$, such that \equl{vcoControlDiscreteTime} can be written as 
\begin{equation}
\phi[n] = \phi[n-1] + \gamma c[n]
\label{eq:vcoControlDiscreteTime}
\end{equation}
and $\gamma$ is a gain (or ``learning rate'') that can be 1 or another value.

AK-TODO: Livro \cite{Farhang10} (e parece que \cite{Miao07}) usa(m):
\begin{equation}
\phi[n] = \phi[n-1] + \gamma c[n-1]
\label{eq:vcoControlDiscreteTime2}
\end{equation}


The Z-transform of \equl{vcoControlDiscreteTime} gives the transfer function
\begin{equation}
\frac{\Phi(z)}{C(z)} = \frac{\gamma}{1-z^{-1}},
\label{eq:vcoControlZtransform}
\end{equation}
which plays the role of the integrator in continuous-time.

\subsubsection{A linear model for the discrete-time PLL in the phase domain}

\figl{pllLinearModelDiscreteTime} describes a linear model that is
a discrete-time version of the model in \figl{pllLinearModel}.


\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresTex/pllLinearModelDiscreteTime}
\caption{A linear model for the discrete-time PLL in phase-domain using Z transforms.\label{fig:pllLinearModelDiscreteTime}}
\end{figure}

Assuming there is no phase noise (i.\,e. $N_{\phi}(z)=0$) and from \equl{feedbackControl} with the open loop transfer function as $G(z)=(\gamma L(z))/(1-z^{-1})$, the transfer function is
\begin{equation}
H_{\phi}(z) = \frac{\Phi_o(z)}{\Phi_i(z)} = \frac{G(z)}{G(z)+1} = \frac{\gamma L(z)}{\gamma L(z)+1-z^{-1}}.
\label{eq:pllTransferFunctionDiscreteTime}
\end{equation}

A second-order DPLL is obtained with the adoption of
\begin{equation}
L(z) = \frac{C(z)}{E(z)} = \alpha + \frac{\beta}{1-z^{-1}},
\label{eq:pllLzSecondOrder}
\end{equation}
where $\alpha$ and $\beta$ are the proportional and integral gains, respectively.
\equl{pllLzSecondOrder} can be implemented according to the signal flow graph
depicted in \figl{pllLz}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/pllLz}		
	\caption{Loop filter $L(z)$ of \equl{pllLzSecondOrder} for a second-order PLL.\label{fig:pllLz}}
\end{figure}

Using $L(z)$ given by \equl{pllLzSecondOrder} and the integrator of \equl{vcoControlZtransform} leads
to the open loop transfer function\footnote{When a DPLL has a $G(Z)$ with all poles at $z=1$ as in \equl{pllOpenLoop2order}, it is called a \emph{perfect PLL}.}
\begin{equation}
G(z) = \frac{\gamma (\alpha + \beta - \alpha z^{-1})}{(1-z^{-1})^2}.
\label{eq:pllOpenLoop2order}
\end{equation}

\figl{pllLz} can be obtained from \equl{pllLzSecondOrder} by adopting a parallel 
realization of $L(z)=H_1(z) + H_2(z)$, where $H_1(z)=\alpha$ and $H_2(z)=\beta/(1-z^{-1})$.
The corresponding difference equation is $c[n] = c_1[n] + c_2[n]$, where
$c_1[n]=\alpha \epsilon[n]$ and $c_2[n]=\beta \epsilon[n] + c_2[n-1]$, which leads to
\begin{equation}
c[n] = \alpha \epsilon[n] + \beta \epsilon[n] + c_2[n-1].
\label{eq:pllDifferenceEq}
\end{equation}

Other realizations
can be developed by rewriting \equl{pllLzSecondOrder} as
\begin{equation}
L(z) = \frac{\alpha + \beta - \alpha z^{-1}}{1-z^{-1}},
\label{eq:pllLzSecondOrder2}
\end{equation}
which naturally leads to 
\begin{equation}
c[n] = (\alpha+\beta)\epsilon[n]  - \alpha \epsilon[n-1] + c[n-1].
\label{eq:pllDifferenceEq2}
\end{equation}
The value $c[n-1]$ in \equl{pllDifferenceEq2} is the previous output value\footnote{In contrast, note that $c_2[n-1]$ in \equl{pllLzSecondOrder} is not an output value, but a parcel of the output $c[n]$.} and allows \equl{pllDifferenceEq2} to be implemented, e.\,g.,  as the transposed direct form II illustrated in \figl{iir_transposed} or any other standard realization.

Incorporating \figl{pllLz} into \figl{pllLinearModelDiscreteTime} leads to 
\figl{dpll}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/dpll}		
	\caption{Second-order PLL model when $L(z)$ is given by \equl{pllLzSecondOrder}.\label{fig:dpll}}
\end{figure}

Substituting \equl{pllLzSecondOrder2} into \equl{pllTransferFunctionDiscreteTime} leads to
\begin{equation}
H_{\phi}(z) = \frac{\Phi_o(z)}{\Phi_i(z)} = \frac{\gamma(\alpha+\beta) - \alpha \gamma z^{-1}}{\gamma(\alpha+\beta)+1 - (2+\alpha \gamma) z^{-1} + z^{-2}}.
\label{eq:pllTransferFunctionDiscreteTimeComplete}
\end{equation}

%where $\kappa$ is a scaling gain, $\rho$ imposes a zero in $z=-\rho$ and the 
%pole at $z=1$ leads to the operation of an integrator.
%\subsection{Carrier recovery using Costas loop}

\subsection{Design of PLL and DPLL}

There are several distinct options for implementing the blocks that compose a PLL. Hence,
PLLs differ in many details. The reader is warned that {AQUI} and {AQUI}, for example,
are only options among many alternatives.\footnote{
For example, \cite{Nguyen94} thoroughly evaluates the model of a PLL used by NASA that
slightly differs from the models discussed here.}

\subsubsection{Design of continuous-time PLL}

A second-order PLL is assumed here.
The loop filter is given by \equl{pllSecondOrder}
and the VCO is modeled as $1/s$. Hence, the PLL
phase transfer function is assumed to be given by
\equl{pllSecondOrder2}, which corresponds to \equl{secondOrder4}.
AK-TODO TO-DO - Hence, tabl {sosParameters4} provides the relations among parameters of interest.

\subsubsection{Design of discrete-time PLL}

\subsection{Discrete-time PLL (DPLL) for tracking digitally modulated... decision directed...}

\section{Channel Model with Frequency Offset}

AK-TODO TO-DO: get Channel Model with Frequency Offset from PDF
\akurl{2011_Xu_Likelihood-Ratio Approaches to Automatic Modulation Classification.pdf}{oi}
and code at
\akurl{C:/svns/laps/telecom/modulationClassification/sampleBasedSimulation}{oi}

\section{Applications}

%AK-IMPROVE: dar um exemplo de uso de BSC
\bApplication \textbf{The binary symmetric channel (BSC)}.
In some applications it is convenient to represent signals as vectors and suppress waveforms. For example, in digital communications, the vector channel model assumes that $\bx$ and $\by$ are the input and output vectors of a channel as indicated:
\[
\bx \arrowedbox{vector channel} \by
\]
and $\bx$ is a random vector with $M$ possible values.

The conditional probability $p(\by|\bx)$ completely describes the channel and $\by$ can have a continuous probability density function or a discrete probability mass function depending on $p(\by|\bx)$. If both cases are denoted as $p(\by)$, it is possible to write
\[
p(\by) = \sum_{i=0}^{M-1} p(\by|\bx_i) p(\bx_i).
\]
Study the BSC channel and describe its distributions $p(\by|\bx)$ and $p(\by)$ when the input is uniformly distributed. Inspect the code
\ci{MatlabOctaveBookExamples/ex\_BSC\_channel\_capacity.m} that explores the capacity of a BSC.
\eApplication


\bApplication \textbf{Alternatives for generating raised cosine filters: {\matlab} and GNU Radio}.
\label{app:raisedCosinesGeneration}
The raised cosine filter is often used in the context of digital communications. Because of that, most of the software to generate it has input parameters that are not used for designing regular filters. For example, the design of a raised cosine typically involves the concept of symbol rate $\rsym$, which is of course never used in functions for general purpose filters such as \ci{fir1}, \ci{firls}, etc.
Another fact is that most raised cosines are FIR filters, but it is possible to use IIR approximations too. Check the support to IIR filters in Matlab's \ci{rcosine} if interested. There are two popular FIR raised cosine filters: the ``normal'' and the square-root or RRC. The former is a Nyquist pulse that obeys \equl{nyquistPulses} and is given by \equl{raisedCosineDiscreteTime} while the latter is the discrete-time version of \equl{sqrtRC}.

\codl{snip_digi_comm_raised_cosine_pulse} illustrates how to generate a normal FIR raised-cosine pulse using three alternatives and compares the first and third.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_raised\_cosine\_pulse}{snip_digi_comm_raised_cosine_pulse}

GNU Radio has the function \ci{gr.firdes.root\_raised\_cosine} to design a RRC. \codl{raisedCosine} provides an example.

\lstinputlisting[language=Python,caption=Python\_Language/raisedCosine.py,label=code:raisedCosine]{./Code/Python_Language/raisedCosine.py}

After successfully running \codl{raisedCosine}, the saved filter can be compared via {\matlab} using \codl{snip_digi_comm_raised_cosine_GR}.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_raised\_cosine\_GR}{snip_digi_comm_raised_cosine_GR}

Note that the gain at DC had to be imposed outside the function \ci{rcosine}.
\eApplication

\bApplication \textbf{Zero ISI pulses}.
\label{app:zeroISIPulses}
There is an infinite number of pulses that lead to zero ISI. To illustrate their formation law, \codl{generateZeroISIPulses} exemplifies two cases, using random numbers.

\includecode{MatlabOctaveFunctions}{generateZeroISIPulses}

For example, the commands \ci{randn('seed',0),p=ak\_generateZeroISIPulses(2,2,1)} led to \co{p=[0    0.0751         0    1.1650    1.0000    1.1650         0 0.0751         0]}. It is interesting to observe the spectra of pulses obtained with the command \ci{p=ak\_generateZeroISIPulses(2,3,0,1)} and \ci{p=ak\_generateZeroISIPulses(2,3,0,2)}. In both cases, invoking \ci{ak\_plotNyquistPulse(p,2)} show $P_s(e^{j\dw})$ constant, as required for zero ISI. However, one should notice that \ci{ak\_plotNyquistPulse} uses the DFT to approximate the DTFT and this imposes some restrictions. For example, the pulse \ci{p=[2 1 3 0 3]} leads to zero ISI, which can be confirmed with the commands:
\includecodelong{MatlabOctaveCodeSnippets}{snip\_digi\_comm\_ak\_calculateISI}{snip_digi_comm_ak_calculateISI}
%\begin{lstlisting}
%p=[2 1 3 0 3] %shaping pulse
%symbols=[3 -1 3 1]; %symbols used for all plots
%L=2; %oversampling factor
%N0=2; %sample to start obtaining the symbols
%[isi,signalParcels] = ak_calculateISI(symbols, p, L, N0);
%\end{lstlisting}
However, in this case, \ci{ak\_plotNyquistPulse} does not show the correct $P_s(e^{j\dw})$.
\eApplication

\bApplication
\textbf{Time evolution of a WGN orthogonal expansion}.
\label{app:xcorrWGN}
%The next paragraphs complement the discussion in 
%Section~\ref{sec:timeEvolutionWGN} and provide a concrete example where the noise expansion generates sequences $r_i[n]$ that may have correlation over time.
%
%Because $r[n]$ is zero mean ($\mu=0$), the variance of the discrete-time AWGN obtained from the continuous time is $\sigma^2 = \ev[r^2[n]] - \mu^2 = \no/2$.
%
\equl{uncorrelated} was obtained for a given time instant. 
Assuming the correlative decoding scheme of \figl{correlative_decoding}, the $n$-th noise symbol $\overline \rvn_n$ is obtained at $t=n \tsym$. Each of the $D$ elements of $\overline \rvn_n$ creates a random sequence $r_i[n]$ over time, which captures the evolution of the $i$-th element of $\overline \rvn_n$. Depending on the duration of $\varphi_i(t)$ and $\tsym$, the samples of $r_i[n]$ are correlated over $n$ or not. 
To study this correlation considering more than one instant, it is useful to recall the discussion about filtered AWGN in Section~\ref{sec:conversion_awgn} and also \exal{discretizingWhiteNoiseViaUnitaryEnergy}.
%Section \ref{sec:discreteAndContinuousAWGN} 
%with the task of obtaining orthogonal expansions of $\nu(t)$ over time. 
The first thing to note is that the correlation of $r_i[n]$ over time depends on the support of $\varphi(t)$ and the adopted time interval (e.\,g. $\tsym$) between expansions.

Here, the simplest case is adopted, in which $\tsym > \varphi_i(t)$ and the inner products $\int \nu(t)\varphi_i(t) dt$ are calculated over non-overlapping segments of duration $\tsym$ (e.\,g., $[n\tsym, (n+1)\tsym]$). In this case, there is no correlation over time as indicated by considering two time instants $k$ and $l$:
\begin{align*}
\ev[r_i[k] r_i[l]] &= \ev \left[ \int_{k\tsym}^{(k+1)\tsym}{\nu(t)\varphi_i(t) dt} \int_{l\tsym}^{(l+1)\tsym}{\nu(t)\varphi_i(t) dt} \right] \\
 &= \ev \left[ \int_{k\tsym}^{(k+1)\tsym}\int_{l\tsym}^{(l+1)\tsym}{\nu(t)\varphi_i(t) \nu(s)\varphi_i(s) dt ds} \right] \\
 &= \int_{k\tsym}^{(k+1)\tsym}\int_{l\tsym}^{(l+1)\tsym} { \ev \left[ \nu(t) \nu(s) \right] \varphi_i(t) \varphi_i(s) dt ds} \\
 &= \int_{k\tsym}^{(k+1)\tsym}\int_{l\tsym}^{(l+1)\tsym} { R(t-s)  \varphi_i(t) \varphi_i(s) dt ds} \\
 &=  \int_{k\tsym}^{(k+1)\tsym}\int_{l\tsym}^{(l+1)\tsym} { \frac{\no}{2} \delta(t-s) \varphi_i(t) \varphi_i(s) dt ds} \\
 &= \frac{\no}{2} \delta[k-l] \int_{l\tsym}^{(l+1)\tsym}{\varphi_i^2(t) } dt = \frac{\no}{2} \delta[k-l].
\stepcounter{equation}\tag{\theequation} \label{eq:uncorrelatedOverTime}
\end{align*}	
%\eTheorem
In this case, $\ev[r_i[n]^2]=\ev[\rvn_i^2]=\no/2$~W, as expected from \equl{filteredAWGNNotUnitaryEnergy}, and there is no correlation over time for a given element of $\overline \rvn_n$.

In spite of the similarity of \equl{uncorrelatedOverTime} and \equl{uncorrelated}, they are distinct in the sense that \equl{uncorrelated} indicates the elements of an orthogonal expansion of WGN are always uncorrelated among them, while \equl{uncorrelatedOverTime} takes time into account and depends on $\tsym$ and the durations of $\{\varphi_i(t)\}$. 
The next paragraphs further discusses this issue.

First, consider the example of \codl{snip_digi_comm_correlative_decoding} where correlation and convolution are made equivalent. This equivalence suggests interpreting the correlative decoding scheme in \figl{correlative_decoding} as 
simultaneously filtering with a set of $N$ orthonormal functions $\{ \varphi_i \}, i=1,\ldots,N$ and periodically sampling the output with interval $\tsym$. Note that $\{ \varphi_k \}$ plays the role of a set of impulse responses composing a filter bank with $N$ filters. This interpretation is used in the sequel.

As mentioned, if the interval $\tsym$ is longer than the impulse responses, the filters do not correlate the noise at their inputs. 
To illustrate a case where this is not the case, a {\matlab} code was created using the same
$D=2$ basis functions of \figl{correlativeDecoderFunctions}. The received signal $r[n]=\nu[n]$ is simply WGN, stored in array \ci{noise} and representing a discrete-time version of $\nu(t)$. Some relevant lines of this code are shown in \codl{figs_digi_comm_awgn_decomposition}.

\lstinputlisting[caption={MatlabBookFigures/figs\_digi\_comm\_awgn\_decomposition},label=code:figs_digi_comm_awgn_decomposition,linerange={18-32},firstnumber=18]{./Code/MatlabBookFigures/figs_digi_comm_awgn_decomposition.m}

The two main signals in \codl{figs_digi_comm_awgn_decomposition} are 
\ci{innerProductViaConv} and \ci{rxSymbols}. The array \ci{innerProductViaConv} stores the samples obtained at the output of the two correlators, at a rate corresponding to $\fs$. In contrast, \ci{rxSymbols} is a decimated version of \ci{innerProductViaConv}, by a factor $L=16$ to 1. Given the assumed $\tsym=16$, this decimation corresponds to having \ci{rxSymbols} at the symbol rate $\rsym$. \figl{wgnExpansion} shows the autocorrelations for the output signals considering these two options and the $D=2$ basis functions.

As depicted in \figl{wgnExpansion}, if the symbols are extracted with a long enough time interval ($\tsym$, in this example), \equl{uncorrelatedOverTime} holds and the noise sequences $r_i[n]$ do not present strong correlation over time as shown in the bottom plot of \figl{wgnExpansion} (which plots \ci{R1d} and \ci{R2d} in \codl{figs_digi_comm_awgn_decomposition}). In contrast, the top plot (plots \ci{R1} and \ci{R2} in \codl{figs_digi_comm_awgn_decomposition}) shows the influence of the filtering operation that correlates the noise.% such that the spectrum of the output sequences $r_i[n]$ is not white.
%, as discussed in Section \ref{sec:discreteAndContinuousAWGN}.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/wgnExpansion}
\caption{Comparison of autocorrelations for the signals at the outputs of the $D=2$ correlators at $\fs$ (top) and $\rsym$ (bottom) rates.\label{fig:wgnExpansion}}
\end{figure}

Agreeing with \equl{uncorrelated}, The correlation matrices generated by \codl{figs_digi_comm_awgn_decomposition}, corresponding to \ci{innerProductViaConv} and \ci{rxSymbols}, were:
\begin{verbatim}
C =[18.5811   -0.0050   and   Cd =[19.9123   -0.3484
   -0.0050   20.2618]              -0.3484   20.0750]
\end{verbatim}
Hence, 
another view of the correlator action can be depicted as in \figl{wgnExpansionHistogram}, 
which shows that the two-dimensional histogram of $[\rvn_1; \rvn_2]$ (columns of \ci{rxSymbols}) resembles a Gaussian. 

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/wgnExpansionHistogram}
\caption{Histogram of vectors $[\rvn_1; \rvn_2]$ from \ci{rxSymbols} in \codl{figs_digi_comm_awgn_decomposition}.\label{fig:wgnExpansionHistogram}}
\end{figure}

The diagonal elements for both matrices \ci{C} and \ci{Cd} are approximately the specified noise power of 20~W.
But note that the out-of-diagonal elements of \ci{Cd} (with decimation) are almost $0.3484/0.005 \approx 70$ times the one of \ci{C} (without decimation). According to \equl{uncorrelated}, both should be $\ev[\rvn_1 \, \rvn_2] = 0$. 
In practice, results such as $\ev \left[ \nu(t) \nu(s) \right] = \frac{\no}{2} \delta(t-s)$ used in \equl{uncorrelated}, are impacted by the finite number of samples to estimate statistics. The example used only
\ci{P=6250} vectors and better estimates can be achieved by increasing this number.

\figl{wgnExpansionFrequency} provides a frequency-domain interpretation of \figl{wgnExpansion} using the first basis function.
%and was obtained with \codl{frequency_awgn_decomposition}. 

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/wgnExpansionFrequency}
\caption{Frequency domain interpretation of \figl{wgnExpansion} using the first basis function: the filter gain (top), PSDs of output signal at rate $\fs$ and $\rsym$ (bottom).\label{fig:wgnExpansionFrequency}}
\end{figure}

The PSD of \ci{rxSymbols(1,:)} (bottom plot in \figl{wgnExpansionFrequency}) is approximately white, while the PSD of \ci{innerProductViaConv(1,:)} is imposed by the corresponding basis function.
%\lstinputlisting[caption={MatlabBookFigures/figs\_digi\_comm\_awgn\_decomposition (2nd snippet)},label=code:frequency_awgn_decomposition,linerange={56-62},firstnumber=56]{./Code/MatlabBookFigures/figs_digi_comm_awgn_decomposition.m}
\eApplication

%In summary, filtering and sampling a continuous-time AWGN creates an \emph{equivalent} discrete-time WGN that can be used as the noise in a discrete-time scalar AWGN model\index{Equivalent AWGN}. Similarly, in case a continuous-time AWGN with bilateral PSD level $\no / 2$ is  simultaneously filtered with a set of $D$ orthonormal functions $\{ \varphi_k \}, k=1,\ldots,N$ playing the role of the impulse responses of a filter bank with $D$ filters, the sampled output of each filter is a random variable independent from the other $D-1$ random variables and with variance $\no/2$. In other words, using a bank of $D$ filters followed by samplers to process a continuous-time AWGN can be modeled as an equivalent discrete-time vector AWGN. 

\ignore{ %already have this result when getting the LTI output power
%\subsubsection{Filtering discrete-time white Gaussian noise}
\label{sec:filteringGaussianNoise}
%
While the previous discussion focused on filtering a continuous-time AWGN, it is also useful to consider the effect of filtering and sampling a discrete-time AWGN. The results are similar. In the following discussion, the signals will be assumed to have finite duration and vectors will represent them. In this case, it is convenient to use inner products to represent the output of a LTI filter at a given time instant. With this notation in mind, the following result is useful.
%
%\bTheorem \textbf{Filtering white Gaussian noise}.
Assuming $\bn=[\rvn_1, \ldots, \rvn_P]$ is a random vector with $P$ elements $\rvn_i$ that are i.\,i.\,d. Gaussian variables with zero mean and variance $\sigma^2$, i.\,e., $\rvn_i \sim \calN(0,\sigma^2), \forall i$, the inner product $\rvy = \langle\bn,\ba \rangle $ of $\bn$ with any (non-random) vector $\ba = [a_1,\ldots,a_P]$ (which represents the filter) is a Gaussian random variable $\rvy \sim \calN(0,\sigma^2 E_a)$, where $E_a=\langle\ba,\ba \rangle ^2$.
A proof sketch follows.
%
Because $\bn$ is zero mean, the inner product $\rvy = \langle\bn,\ba \rangle $ is a linear combination of zero-mean random variables and, consequently, a zero-mean random variable itself, i.\,e., $\ev[\rvy]=0$. The general expression for the corresponding power is suggested by first assuming the special case of only $P=3$ samples. 
%and a shaping pulse $\bp=[p_1, p_2, p_3]$ with energy $E_p=\langle\bp,\bp \rangle ^2 = p_1^2+p_3^2+p_3^2$. 
The noise samples are then $\bn=[\rvn_1, \rvn_2, \rvn_3]$
%where the notation reminds that $\bn$ has elements that are random variables, while the elements of $\bp$ are complex or real numbers. O
and one can write:
\begin{align*}
\ev[\rvy^2] &= \ev[\langle\bn,\ba \rangle ^2]= \ev[ (\rvn_1 a_1 + \rvn_2 a_2 + \rvn_3 a_3)^2 ] \\
&= \ev[ (\rvn_1 a_1)^2 + (\rvn_2 a_2)^2 + (\rvn_3 a_3)^2 + 2 \rvn_1 a_1 \rvn_2 a_2 + 2 \rvn_1 a_1 \rvn_3 a_3\\
&\mathrel{\phantom{=}} + 2 \rvn_2 a_2 \rvn_3 a_3]. \end{align*}
Using the linearity of $\ev[\cdot]$ and noting that $\ev[\rvn_i \rvn_j]=0, i \ne j$ because the noise is assumed independent (therefore uncorrelated), leads to
\begin{align*}
\ev[\rvy^2] &= a_1^2 \ev[\rvn_1^2] + a_2^2 \ev[\rvn_2^2] + a_3^2 \ev[\rvn_3^2] \\
& = a_1^2 \sigma^2 + a_2^2 \sigma^2 + a_3^2 \sigma^2 \\
& = \sigma^2 E_a,
\end{align*}
where $E_a=\langle\ba,\ba \rangle ^2$ is the energy of $\ba$.
%
In general, when linearly filtering WGN and sampling the output, the power $\ev[\rvy^2]$ of this random variable is the original noise power $\sigma^2$ multiplied by the energy $E_a$. When $\ba$ is the filter impulse response $h[n]$ (or its time-reversed version $h[-n]$), $E_a$ is the energy of $h[n]$.
%\eTheorem
}

%Applications related to GSM (they were planned to be a chapter)
\input{../ak_telecombook/gsm} 

\section{Comments and Further Reading}

A very good description about wireless channels and associated material can be found in~\cite{Cho10}.
For an introduction to Information Theory see, e.\,g.,~\cite{Cover91}. Classical procedures for channel estimation can be found in textbooks such as \cite{Kay93}. 
There are books dedicated to simulating communication systems. For example, in \cite{Tranter04}, the semi-analytical approach, which is a hybrid between an analytical evaluation and a Monte Carlo simulation, is discussed because it brings advantages with respect to computational cost over the Monte Carlo approach. Monte Carlo is emphasized in this text due to its simplicity but it may be unfeasible in some applications.

Before studying linear equalization, it is useful to read about linear regression and linear least squares. For example, understanding the operator \ci{\textbackslash} or \ci{mldivide} in {\matlab} helps 
interpreting the linear equalizers.

There are many books about PLL. Two of them, which discuss the discrete-time PLL in
the context of software radios are \cite{Farhang10} and \cite{Miao07}. 
PLL routines for Matlab and Simulink can be found in \cite{Tranter10}, which is available
online at
\akurl{http://www.morganclaypool.com/doi/abs/10.2200/S00245ED1V01Y201009COM005}{3pll}.
Additional source code for synchronization algorithms such as the Costas loop can be found at
\cite{Farhang10,Johnson11}.

The GSM system is described in \cite{Yacoub02} and other textbooks. Here, the interest is only the PHY layer.
Frequency offset estimation methods for GSM under different SNR conditions can be found in~\cite{Hars99,Varma04}. 

\section{Exercises}

\begin{exercises}

\item Assume a binary digital communication system with equiprobable polar symbols ($-1$ for bit 0 and +1 for bit 1). The shaping pulse is 
$g(t) = 4 \sin (\pi t / T)$ from $t \in [0,T]$ or zero otherwise, with $T=10 \mu$s. Draw the transmitted signal $s(t)$ for $t \in [0,200]$~$\mu$s if the bit rate is 25~kbps and the bitstream repeats the pattern 1, 0, 1, 0, 1, 0, etc., with the first symbol 1 starting at time instant $t=0$. Note that the symbol period $\tsym$ may be different than $T$.

\item To study ISI, a discrete-time simulation with oversampling $L=3$ is conceived. The symbols to be transmitted are $-1$ and 1. However the shaping pulse is $p[n]=1/5\{\delta[n+4]+2\delta[n+3]+3\delta[n+2]+4\delta[n+1]+5\delta[n]+4\delta[n-1]+3\delta[n-2]+2\delta[n-3]+\delta[n-4]\}$. The symbols to be transmitted are (after upsampling by $L=3$) $m[n]=\delta[n]-\delta[n-3]-\delta[n-6]+\delta[n-9]$. a) Obtain the convolution $m[n] \conv p[n]$, b) the ISI parcel at $n=3,6$, c) design a new pulse $p'[n]$ with 7 non-zero samples that leads to ISI free, d) assuming a channel with bandwidth 60 kHz, even not knowing about the spectrum of $p'[n]$, what is the maximum signaling rate for zero ISI? e) Why?

\item Draw the eye diagram of a PAM system when the pulse $p(t)$ has amplitude 1 V in the interval $[0, \tsym[$, where $\tsym=1$ s is the symbol period. The symbols are $-10$ and 10. The channel impulse response is $h(t)=\delta(t)-0.2 \delta(t-2\tsym)$, where $x(t)$ is the PAM signal. Indicate the maximum horizontal opening in s and the maximum vertical opening in V.

\item A discrete-time scalar AWGN channel model assumes $\br = \bs + \mathbf{\nu}$ where $\bs$ is a discrete-time sinusoid that corresponds to 3~Hz and power 10~dBm, while $\mathbf{\nu}$ was obtained by filtering a WGN via an ideal lowpass filter of unit gain and cutoff frequency $\fs/2$, where the sampling frequency is $\fs=20$ Hz. Given that the SNR of $\br$ is 20 dB, what is the bilateral PSD constant value $\no/2$ of the continuous-time WGN that creates $\mathbf{\nu}$?

\item A flat-fading channel has an impulse response given by $h(t) = 3 e^{j 0.2 \pi} \delta(t-t_0)$, with $t_0=4$~s. a) Find the impulse and frequency responses for its zero-forcing (ZF) equalizer. b) How does the ZF equalizer should be changed if $t_0$ is changed to 6~s?

\item The task is to estimate the receiver SNR of a flat-fading channel with impulse response $h(t) = \kappa e^{j \phi} \delta(t-t_0)$, when these parameters are unknown. As depicted in \figl{flat_channel}, discrete-time scalar WGN $\nu(t)$ is added at the receiver and it is uncorrelated with the signal $g(t)$. Known pilot symbols are used and after being properly decoded, allow to estimate the EVM at the receiver as 2~\%. The transmitted $s(t$) and received $r(t)=g(t) + \nu(t)$ signals have power values of 6~dBm and 4~dBm, respectively. a) What is the SNR defined as $\SNR = \ev[|g(t)|^2/|\nu(t)|^2]$? b) What is the estimated channel gain $\kappa$?

\item Assuming a complex-valued shaping pulse $p(t) = u(t) - 2u(t-3) + u(t-6) + j [2u(t) - u(t-1) - u(t-2)]$, draw the graph of its corresponding matched filter impulse response.

\item Consider the correlative decoding block of \figl{correlative_decodingNoise}. Assuming the input signal is a white noise with bilateral PSD of constant value $\no / 2$ = 2 mW/Hz, and the basis functions compose an orthonormal set with $D=4$ basis, a) what is the average total power of a vector $\bx$ with elements corresponding to the four outputs? b) what is the mean and variance of each element of $\bx$? c) when a cosine of amplitude $A=4$~V is added to the noise at the input, what is the power of the first element of $\bx$ in case the inner product between the cosine and the corresponding basis function $\varphi_1(t)$ over $\tsym$ is 2?

\item a) Using the matched-filter (MF) bound, estimate the SNR at the output of a MF with a transmit shaping pulse
$p(t)$ consisting of a NRZ pulse with duration of 1~ms and amplitude 2~V. The modulation is PAM with $M=4$ symbols from the constellation $[-3, -1, 1, 3]$~V. The channel is AWGN with $\no/2=-60$~dBm/Hz being the noise PSD value. b) If instead of AWGN, the channel is now a frequency-selective Gaussian LTI, would you consider that the new SNR can eventually be larger than the previously calculated SNR? Why?

\item A digital communication system was designed to target a baseband channel with $\BW=2$~MHz and have zero ISI. The adopted symbol rate was $\rsym = 3$~Mbauds. An ADC operating at sampling frequency $\fs$ is used to digitize the received signal, such that the symbols are recovered using digital signal processing. Given that you know both the sampling theorem and the Nyquist criterion for zero ISI, what is the minimum value for $\fs$?

\item a) A complex-valued analytic signal has support in frequency domain ranging from 100~kHz to 800~kHz. Using IQ sampling (two ADCs), what is the minimum sampling frequency $\fs$ to represent this signal without doing any frequency downconversion? b) In case one can use frequency up or downconversion with $e^{\pm j \aw_0 t}$, what is the new minimum $\fs$ and the value of $\aw_0$?

\item A digital baseband communication system achieves 16 Mbps using QAM with $M=256$ symbols. It uses a raised cosine as the shaping pulse with roll-off $r=1$. a) What is the required bandwidth? b) What is the symbol rate?

\item Before DSL technologies were adopted using $\BW > 1$~MHz over copper cables, the Internet access over POTS (``plain old telephony system'') was based on the ITU V.90 and V.34 standards in downstream (operator's central office to user premises) and upstream, respectively. The V.90 downstream connection used a PAM signal with $\rsym=8$~kbauds and $b=7$ bits per symbol to achieve 56~kbps. The V.34 upstream achieved 33.6~kbps adopting QAM. Some people thought the V.34 maximum bit rate was close to the channel capacity $C$, when estimated using the AWGN channel with $C=\BW \log_2 (1+\SNR)$, with a passband from 300 to 3400~Hz and $\SNR \le 40$~dB. But the download speed of 56~kbps modems were a breakthrough. Read 
\url{https://perswww.kuleuven.be/~u0068190/Onderwijs/Extra_info/Rockwell%2056.PDF}
and explain why the techniques used for downloading could not be used for uploading.

\item The overall impulse response of a communication channel is $h(t)=\delta(t) + \alpha \delta(t - \tsym)$, where $\tsym$ is the symbol duration and $\alpha < 1$. Find the impulse response $h_e(t)$ of the corresponding zero-forcing equalizer filter. Show that the concatenation $h(t) \conv h_e(t)$ mitigates the ISI.

%\item After sampling at the baud rate, the received symbols are 

%AK-TODO: finish the exercises about channels \item In \ref{sec:orthogonalExpansionWGN} is the dependence on the support of $\varphi(t)$ and the time interval between expansions

%\item Describe your understanding of \equl{uncorrelated} using plots.

%\item Why and how frequency-selective is related dispersion 

%\item AK-IMPROVE: mimic \ci{bersync}.  in %\section{Timing}

%\item Why increasing the number of tones lead to C not infinite capacity? This has to do with what Ck converges to.

%\item AK-TODO: exercise for equalization of flat-fading channel. Heath's book has a nice example at page 153.

%\item Assuming the AWGN channel and a matched-filter receiver, how would you try to design line codes (see Section~\ref{sec:linecodes}) that minimize the BER? How does the pulse shape impact BER?

\end{exercises}

