\section{To Learn in This Chapter}

The skills we aim to develop in this chapter are:
\begin{itemize}
        \item Apply basic concepts of linear algebra such as inner products and projections, and then make analogies
				between vectors and signals to better understand transforms
        \item Use inner products to efficiently obtain the transform coefficients when the basis functions are orthogonal
				\item Get familiar with the most used block transforms: DCT and DFT
				\item Observe how wavelets localize information in both frequency and time by using basis functions with 
				limited time support				
        \item Interpret transforms (Fourier, Z, Laplace) as obtaining coefficients given by the inner product between the signal to be transformed and the corresponding basis function
				\item Know what \emph{Hermitian symmetry} is and when it happens in the signal spectrum
        \item Understand advantages of linear transformations such as DCT for coding applications
				\item Understand the relations among the four pairs of equations for Fourier analysis with eternal (infinite duration) sinusoids as basis functions: Fourier series (FS), Fourier transform (FT) and their discrete-time (DT) counterparts: DTFS and DTFT
				\item Understand how to use an efficient (fast) FFT algorithm to compute the block transform DFT
				\item Learn how the $N$-points FFT with basis functions that last $N$ samples can be used to estimate
				the FS, FT, DTFS and DTFT
				\item Note the three most used alternatives of normalization factors for the FFT and learn which one to use according to the application: unitary, DFT as samples of the DTFT and DFT coinciding with the DTFS
				\item Properly interpret the results of DFT and DTFS in spite of their equations differing simply by a constant factor $N$
				\item Observe that the Fourier transform can be expressed in terms of linear (Hz) or angular (rad/s) frequencies, and
				how this impacts the spectra $X(f)$ and $X(\aw)$ when impulses are involved
				\item Learn the relation $X(e^{\dw}) = \fs X(\aw)$ between the discrete-time spectrum $X(e^{\dw})$ and its continuous-time version $X(\aw)$ by using $\aw = \dw \fs$
				\item Relate the continuous-time Fourier transform $X(\aw)$ and Laplace transform $X(s)$ based on their respective basis functions, noting that $e^{j \aw t}$ is the special case of $e^{st}$ when the complex-valued independent variable $s$ is $s = j \aw$
				\item Relate the discrete-time Fourier transform (DTFT) $X(e^{j \dw})$ and Z transform $X(z)$ based on their respective basis functions, noting that $e^{j \dw n}$ is the special case of $z^{n}$ when the complex-valued independent variable $z$ is $z = e^{j \dw}$
%        \item Relate constellation symbols in digital communication to transform coefficients
				\item Calculate and properly interpret the region of convergence (ROC) for the Laplace and Z transforms
				\item Obtain the inverse Laplace and Z transforms using \emph{partial fraction expansion}, even when there are poles with
				multiplicity larger than one
				\item Observe the notation $X(e^{j \dw})$ aims at making explicit that $\dw$ is an angle (while in continuous-time, the unit of $\aw$ is rad/s) and, consequently, $X(e^{j \dw})$ is periodic with fundamental period $\dw = 2 \pi$
				\item Obtain by inspection the Fourier series coefficients of periodic signals composed by harmonic sinusoids
\end{itemize}

%\section{Motivation}

Transforms are a very important tool in several applications. The continuous-time Fourier transform, for example, provides an alternative ``view'' $X(f)$ of a signal $x(t)$. Sometimes this extra view is essential for efficiently solving a problem. For instance, designing a filter to eliminate a frequency band is easier in the transform domain. Few examples of transforms and applications can be found in \tabl{transforms_examples}.

\begin{table}
 \centering
 \caption{Examples of transforms and applications.\label{tab:transforms_examples}}
 \begin{tabularx}{\textwidth}{p{3cm}X}
 \toprule
 \textbf{Transform} & \textbf{Example of application} \\ \midrule
 Fourier & visualize a signal in frequency ``domain'' as a sum or integral of sinusoidal components \\
        Z &  analyze discrete-time systems by transforming difference equations into polynomials \\
        Discrete cosine  transform (DCT) & image coding, where the image details are represented
         by high-frequency DCT coefficients, which can be discarded without 
         significant loss of perceptual quality \\ 
				Wavelet & Analyze a signal in different frequency resolutions \\ \bottomrule
\end{tabularx}
\end{table}

% AK_GUIDELINE: digamos que eu tenha um vetor \bx, um elemento desse vetor seria?... posso usar ( ) para ficar meio Matlab. Mas daih confunde com x no tempo continuo. Uso \bx[1] ou x[1]? Acho que vou usar o ultimo: x[1]. Seria interessante ajudar o aluno a distinguir x[n] o sinal de uma amostra em \sum_{n=0}^{N-1} x[n] usando \sum_{n=0}^{N-1} x(n). Mas daih no caso continuo no tempo, nao se poderia fazer isso com x(t).

%EU INDICO AQUI O QUE QUERO FAZER COM TRANSFORMADA DE FOURIER E COEFICIENTES.

We start our study focusing on linear transforms and making a convenient connection with linear algebra.

\section{Linear Transform}

The goal in this section is to analyze a signal ($x(t)$ or $x[n]$) using a linear transform. The following steps will be discussed:
\begin{enumerate}
	\item We will choose the linear transform.
	\item The linear transform will represent the signal using its \emph{basis functions}, which are often \emph{orthogonal} among themselves.
	\item The transform operation corresponds to finding the values called \emph{transform coefficients}, which multiplied by the corresponding
	basis functions (one coefficient per basis functions) reconstruct the original signal.
\end{enumerate}
To fully understand linear transforms, we will discuss these concepts using vectors and linear algebra. Later we will generalize from vectors to signals.
We start this study of linear transform, associating it to a simple matrix multiplication. 

\subsection{Matrix multiplication corresponds to a linear transform}
\label{sec:transform_as_matrix_mpy}
In linear algebra, any linear transformation\footnote{See \akurl{http://en.wikipedia.org/wiki/Transformation_matrix}{2tra}.} (or transform)
%corresponds to a mapping between vector spaces and
can be represented by a matrix $\bA$. The linear transform operation is given by
\begin{equation}
\by = \bA \bx,
\label{eq:linear_transform}
\end{equation}
where $\bx$ and $\by$ are the input and output column vectors, respectively. 

\bExample \textbf{Example of linear transform}.
\label{ex:lin_transform}
The matrix
\begin{equation}
\bA =
\left[ \begin{array}{cc}
\cos(\theta) & \sin(\theta)\\
-\sin(\theta) & \cos(\theta)
 \\ \end{array} \right]
\label{eq:mat_rotation}
\end{equation}
implements a transform that corresponds to a clockwise rotation of the input vector by an angle $\theta$.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall]{Figures/orthogonal_2d_vectors}
        \caption{Rotation of a vector $\bx$ by an angle $\theta=\pi/2$ radians using $\by=\bA\bx$ with $\bA$ given by \equl{mat_rotation}.\label{fig:orthogonal_2d_vectors}}				
\end{figure}
\figl{orthogonal_2d_vectors} illustrates the rotation for a vector $\bx=[4, 8]^T$ by an angle $\theta=\pi/2$ radians,  i.\,e., $\bA=[0,1;-1,0]$, resulting in $\by=\bA \bx = [8, -4]^T$.
%AK talvez mencionar isso depois, por didatica:
%Note that this transformation does not change the norm of the input vector, i.\,e., $||\by||=||\bx||$. The conditions that must be imposed on $\bA$ such that it preserves energy will be discussed later on.
\eExample

\subsection{Basis: standard, orthogonal and orthonormal}

Another important concept in linear transforms, which has origin in linear algebra, is the concept of \emph{basis}. The linear combination of basis vectors allow to create any possible vector in the corresponding \emph{vector space}. Many basis are \emph{orthogonal} or orthonormal.
\figl{orthogonal_2d_vectors} indicates a pair of \emph{orthonormal}\footnote{Vectors that are orthogonal and have norm equal to one.} vectors $\overline i=[1, 0]$ and $\overline j=[0, 1]$ that span $\Re^2$. The vectors $\overline i$ and $\overline j$ form a \emph{standard basis} and allow to easily represent any vector $\by \in \Re^2$, such as $\by=8 \overline i - 4 \overline j$.
It is useful to get a geometric interpretation by studying basis vectors, and later generalize the concepts to basis functions that represent discrete or continuous-time signals.

When the basis vectors are organized as the columns of a matrix $\bA$, the
%coefficients
elements of the input vector $\bx$ indicate the coefficients of a linear combination of the basis functions that leads to $\by$. For example, in the case of the standard basis:
\[
\by = \left[ \begin{array}{c}
8 \\
-4
 \\ \end{array} \right] =
\left[ \begin{array}{cc}
1 & 0\\
0 & 1
 \\ \end{array} \right] \left[ \begin{array}{c}
8 \\
-4
 \\ \end{array} \right] = \bA \bx,
\]
which is a trivial relation because $\bA$ is the identity matrix. More interesting transforms are used in practice. 


\bExample \textbf{Interpreting the given example as a linear transform}.
Assume one is dealing with computer graphics and wants to rotate vectors. \equl{mat_rotation} of \exal{lin_transform} with $\theta=\pi/2$ radians leads to (see \figl{orthogonal_2d_vectors}):
\[
\by = \left[ \begin{array}{c}
8 \\
-4
 \\ \end{array} \right] =
\left[ \begin{array}{cc}
\cos(\pi/2) & \sin(\pi/2)\\
-\sin(\pi/2) & \cos(\pi/2)
 \\ \end{array} \right] \left[ \begin{array}{c}
4 \\
8
 \\ \end{array} \right] = 
\left[ \begin{array}{cc}
0 & 1\\
-1 & 0
 \\ \end{array} \right] \left[ \begin{array}{c}
4 \\
8
 \\ \end{array} \right] =
\bA \bx.
\]
This can be interpreted as a transform as follows:
\begin{enumerate}
	\item We chose $\bA$ with $\theta=\pi/2$ as the linear transform.
	\item In this case, the elements of $\by$ are interpreted as the \emph{transform coefficients},
	while $\bx$ is interpreted as the original vector.
\end{enumerate}
The forward transform operation corresponds to finding the coefficients to compose $\by$. In the inverse operation,
the coefficients $\by$ would be the input values, and $\bx$ could be found using $\bx = \bA^{-1} \by$. In practice, we avoid
the task of inverting a matrix and often choose $\bA$ with special properties.
For instance, because the columns of $\bA$ are orthonormal, its inverse $\bA^{-1} = \bA^H$ is equal to its Hermitian
(and the basis vectors are the rows of $\bA$).
Tricks to avoid inverting matrices will be further explored, alongside with the adoption of inner products.
\eExample

%\begin{array}{|l|l|l|} \hline
%\textbf{Vector space} & \textbf{Generalization} & \textbf{+ Completeness}\\\hline
%\text{metric}& \text{metric space} & \text{complete space}\\
%\text{norm} & \text{normed} & \text{Banach space}\\
%\text{scalar product} & \text{inner product space}  & \text{Hilbert space}\\\hline
%\end{array}


\section{{\akadvanced} Inner Products to Obtain the Transform Coefficients}
\label{sec:innerProductsForTransforms}

%Linear algebra plays an important role in transforms for signal processing (and, of course, many more applications). 
%One crucial concept is that 
In the general case, calculating the forward or inverse transforms corresponds to performing a matrix multiplication. However, most linear transforms are designed such that ``transforming'' the original signal corresponds to the calculation of \emph{inner products}.
Hence, this section will elaborate on inner products and their use in transforms.

The main motivation is that we
can calculate an inner product between a pair of vectors, but also between a pair of functions, and a pair of signals. 
We start by discussing the inner product between vectors, to make useful analogies with the inner product between signals in the context of linear transforms.

An inner product\index{Inner product} is a generalization of the \emph{dot product}\index{Dot product} $\bx \cdot\by$ between two vectors $\bx$ and $\by$ of $N$ elements or, equivalently, equal-length sequences (with $N$ samples). The dot product is defined for vectors with real-valued elements as (the notation of an inner product $\langle\bx,\by\rangle$ will be used hereafter instead of $\bx \cdot\by$):
\begin{equation}
\langle\bx,\by\rangle \triangleq \lVert\bx\rVert~\lVert\by\rVert \cos(\theta),
\label{eq:dot_product_geometric}
\end{equation}
where $\theta$ is the angle between $\by$ and $\bx$, which is restricted to the range $0 \le \theta \le 180$ degrees. Alternatively, this inner product can also be calculated as
\begin{equation}
\langle\bx,\by\rangle = \sum_{i=1}^N x_i \textrm{~} y_i = x_1 y_1 + x_2 y_2 + \cdots + x_N y_N,
\label{eq:dot_product}
\end{equation}
where $x_i$ and $y_i$ are the $i$-th elements of $\bx$ and $\by$, respectively.

Another aspect of the inner product $\langle\bx,\by\rangle$ is that it is proportional to the norm of the projection of $\bx$ on $\by$ and vice versa. 
Repeating \equl{projection_vector} below, for convenience
\begin{equation}
\lVert\bp_{xy}\rVert= \frac{| \langle\bx,\by\rangle |}{\lVert\by\rVert},
%\label{eq:projection_vector}
\end{equation}
and interpreting $\by$ as a basis vector of norm equals to 1 ($\lVert\by\rVert=1$, one has $\lVert\bp_{xy}\rVert= | \langle\bx,\by\rangle |$.
A large magnitude of $\langle\bx,\by\rangle$ corresponds to a basis vector $\by$ that represents well (a reasonable part of the energy) of the vector $\bx$
(later interpreted as signal $x[n]$). On the other hand, $\langle\bx,\by\rangle = 0$ means that the vectors are orthogonal.

\bExample \textbf{Calculating the angle between vectors using their inner product}.
For example, 
consider $\bx=[1,1,1]$ and $\by=[1,3,2]$. Their inner product is calculated from \equl{dot_product} as $\langle\bx,\by\rangle=6$, such that $\cos(\theta)=\langle\bx,\by\rangle / (\lVert\bx\rVert~\lVert\by\rVert) \approx 6/(1.732 \times 3.742) = 0.926$ and consequently, $\theta=0.387$~radians.

As another example, if $\bx=[3,-1,0]$ and $\by=[1,3,2]$, their inner product is $\langle\bx,\by\rangle=0$. In this case, from \equl{dot_product_geometric} and observing that the norms of both vectors are non-zero, the inner product is zero necessarily because $\cos(\theta)=0$ and consequently, $\theta=\pi/2$. Hence, when vectors are \emph{perpendicular}\index{Perpendicular} ($\theta=\pi/2$), their inner product is zero.
\eExample 

Orthogonal is a generalization of perpendicular.
In general, when two vectors are \emph{orthogonal} their inner product is zero.

There are many distinct inner products. But an operation must obey specific properties such as \emph{linearity} to be ``valid'' as an inner product and, consequently, define an \emph{inner product space}, where concepts such as \emph{norm} and \emph{orthogonality} are natural extensions of the ones with geometric interpretations provided by \equl{dot_product_geometric}. This geometric interpretation is highly beneficial when interpreting the inner products used in transforms.
\tabl{inner_products} illustrates alternative definitions of inner products that are discussed in the sequel. 

\begin{table}
 \centering
 \caption{Examples of inner product definitions.\label{tab:inner_products}}
 \begin{tabularx}{\textwidth}{p{3cm}Xc}
 \toprule
 \textbf{Equation} & \textbf{Used for} & \textbf{Number} \\ \midrule
$\sum_{i=1}^N x_i \textrm{~} y_i$ & finite-length real-valued vectors or sequences & (\ref{eq:dot_product}) \\
$\sum_{n=-\infty }^\infty x[n] \textrm{~} y^*[n]$ &  infinite-length complex-valued vectors or sequences & (\ref{eq:inner_product_infinite_vectors}) \\
$\int_{-\infty}^\infty x(t)y^*(t) \textrm{d}t$ & continuous-time complex-valued signals & (\ref{eq:innerprod_signals}) \\
$\int_{\langle T\rangle} x(t)y^*(t) \textrm{d}t$ & continuous-time complex-valued signals with duration $T$ & (\ref{eq:innerprod_signals_finite_duration}) \\ \bottomrule
\end{tabularx}
\end{table}

As indicated in \tabl{inner_products}, it is also possible to define an inner product for infinite duration signals. For example, consider the complex-valued signal $y[n]=e^{j \dw n}$, where $n=-\infty,\ldots,-1,0,1,\ldots,\infty$. It is a common operation in Fourier transform to calculate inner products among such signals using the definition
\begin{equation}
\langle x[n],y[n]\rangle \triangleq  \sum_{n=-\infty}^\infty x[n]y^*[n].
\label{eq:inner_product_infinite_vectors}
\end{equation}
Note that when applied to complex-valued signals, the inner product is defined using a complex conjugation.
%This operation is useful to make the transition from block to continuous-time transforms.

%After adopting a definition of inner product $\langle x(t),y(t)\rangle$ between two continuous-time signals, one can benefit from making analogies with the traditional inner products operations in the Euclidean vector space. For example, if $y(t)$ represents a linear combination of orthogonal basis functions, the coefficients for representing $x(t)$ using these basis functions can be obtained through inner products as in orthogonal block transforms.

When dealing with continuous-time signals, a convenient definition is obtained by changing the summation by an integral:
\begin{equation}
\langle x(t),y(t)\rangle = \int_{-\infty}^\infty x(t)y^*(t) \textrm{d}t.
\label{eq:innerprod_signals}
\end{equation}

The inner product of finite-duration signals with support $T$ is simplified to
\begin{equation}
\langle x(t),y(t)\rangle = \int_{\langle T\rangle} x(t)y^*(t) \textrm{d}t,
\label{eq:innerprod_signals_finite_duration}
\end{equation}
where $\langle T\rangle$ denotes a range of $T$ such as $[0,T]$ or $[-T/2,T/2]$.

%\equl{innerprod_signals} allows to geometrically interpret signals.
%In order to be able to represent signals $x(t)$ of interest, typically the number of basis functions is infinite.
When using inner products between continuous-time signals, it is possible to make useful analogies to vectors in the Euclidean space and benefit from geometrical interpretations.
% (e.\,g., used in block transforms). 
For example, similar to vectors, the squared norm $\langle x(t),x(t)\rangle = \lVert x(t)\rVert^2$ is the signal energy.\footnote{Unless otherwise stated, $\lVert \cdot \rVert^2$ denotes the Euclidean or $L^2$ norm.}
But the most important lesson in this context is that orthogonal signals share similar properties with orthogonal vectors.\footnote{As mentioned, orthogonal is a generalization of perpendicular.}

\bExample \textbf{Example of a pair of orthogonal signals}.
The signals $x(t)=1$ and $y(t)=2u(t)-1$ can be interpreted as orthogonal because $\langle x(t),y(t)\rangle = 0$. This inner product can be obtained by noting that $x(t)y(t)$ is $-1$ for $t<0$ and $1$ for $t\ge0$, leading to a zero integral when using \equl{innerprod_signals}.
\eExample

%Because this pair of signals has a zero inner product, one can interpret that they are not 

At this point, the reader may eventually benefit from 
Appendices~\ref{sec:projection} and \ref{sec:orthogonalBasis}, which provide a review of linear algebra applied to transforms. The goal of these appendices is to interpret, for example, the Fourier transform
\[
X(\aw) = \int_{-\infty}^\infty x(t) e^{- j \aw t} \textrm{d}t = \langle x(t),e^{j \aw t}\rangle
\]
as the inner product $\langle x(t),e^{j \aw t}\rangle$ between the signal $x(t)$ and the basis function $e^{j \aw t}$. %For example, $X(\aw_0)=0$ when a specific basis $e^{j \aw_0 t}$ is orthogonal to $x(t)$, indicating that $e^{j \aw_0 t}$ is not useful for representing $x(t)$.

As discussed in Appendix~\ref{sec:orthogonalBasis}, if a signal $x(t)$ can be represented as a linear combination $x(t)=\sum_{d=1}^D m_d \varphi_d(t)$, where the $D$ functions $\varphi_d(t)$ compose a set $\{\varphi_j(t)\}$ of orthonormal basis functions, then the values (or \emph{coefficients}) $m_d$ can be recovered using inner products:
\begin{equation}
m_d = \langle x(t),\varphi_d(t) \rangle.
\label{eq:coefficientViaInnerProduct}
\end{equation}
This result is discussed in Appendix~\ref{sec:orthogonalBasis} via examples with vectors.

Before dealing with infinite duration signals, the next section discusses transforms that operate on blocks of samples.

\section{Block Transforms}

% AK_GUIDELINE: Vou usar indice 0 pois as formulas de FFT ficam ruins com 1. ERA COISA PRA DECIDIR: INDICE 0 OU 1 DE VETORES

% AK_GUIDELINE: ORTOGHONAL VERSUS ORTONORMAL EM MATRIZES E VETORES. - basis and base functions?


%, and show some applications.

Generic block processing was presented in Section~\ref{sec:block_proc}.
This section discusses block transforms as a specific block-oriented signal processing,
%There are many block transforms such as the KLT. 
including DCT, DFT, and Haar transforms, which are among the most used transforms, together with the already presented KLT (or PCA).

%AK_GUIDELINE: distinguish transformation from transform

In many signal processing tasks, it is useful to have two related transformations called a \emph{transform pair}. One transform is the inverse of the other, with the \emph{inverse} undoing the \emph{forward} transform. When dealing with \emph{block} transforms,\footnote{There are other transforms, such as Laplace, which is not a block transform and will be discussed later in this chapter.} one simply uses linear algebra (as detailed in Appendix~\ref{app:linearAlgebra}), and both transforms are
simple matrix multiplications. The pair of transforms is defined by a pair of matrices.
The matrices can be rectangular, as in \emph{lapped} transforms,\footnote{See~\cite{Malvar92} for a nice description of transforms, with focus on lapped transforms.} but most block transforms are defined by a pair of $N \times N$ \emph{square} matrices $\bA$ and $\bA^{-1}$ and the jargon \emph{$N$-point transform} indicates their dimension. The inverse matrix $\bA^{-1}$ is assumed to exist and can ``undo'' the transformation $\bA$ (and vice-versa). Here, 
the \emph{forward} (or direct) transformation is denoted as
%commonly adopted notation (not so intuitive) is to call \emph{inverse} transformation\index{Inverse transform} the matrix multiplication
\[
\bX = \bA \bx,
\]
while the \emph{inverse} transformation\index{Inverse transform} is denoted here as
\begin{equation}
\label{eq:inverse_transform}
\bx = \bA^{-1} \bX.
\end{equation}
The vector $\bX$ is called the transform of $\bx$ and the elements of $\bX$ are called \emph{coefficients}\index{Coefficients, transform}. 
In this text, vectors are represented by bold lower case letters, but the vector with coefficients will be denoted by capital letters to be consistent with the jargon (unfortunately, vectors of coefficients such as $\bX$ can be confused with matrices, but the context will distinguish them).

An important concept is:
\begin{itemize}
\item The \textbf{columns} of the inverse transform matrix $\bA^{-1}$ are called the \emph{basis functions} (or basis vectors). 
\end{itemize}

\subsection{{\akadvanced} Unitary or orthonormal transforms}
\label{sec:unitary_transforms}

A matrix $\bB$ with orthonormal columns is called \emph{unitary}\index{Unitary matrix}. The rows of a unitary matrix are also orthonormal. If one takes any pair of distinct rows or columns of the unitary matrix $\bB$, their inner product is zero (they are orthogonal) and their norms are equal to one.

Unitary matrices are widely used in transforms because their inverse is 
simply the conjugate transpose as indicated in:
\[
\bB^{-1} = \bB^H = \left(\bB^{*}\right)^T,
\]
where $H$ denotes the Hermitian (conjugate transposition).

Considering \equl{inverse_transform}, if the basis functions (columns of $\bA^{-1}$) are orthonormal, then $\bA^{-1} = \bA^H$. Consequently, the rows of the direct transform $\bA$ are the complex-conjugate of the basis functions.
Two important facts are:
\begin{itemize}
	\item \emph{Forward}: the $k$-th coefficient $X[k]$ of the forward transform $\bX = \bA \bx$ is obtained by performing the inner product between $\bx$ and the $k$-th basis function. Based on the definition of inner product for complex-valued vectors in \equl{innerProductComplexVectors}, it is adopted the complex conjugate of the $k$-th basis function. The larger is this coefficient $X[k]$  magnitude, the better the $k$-th basis function represents signal $\bx$.
	\item \emph{Inverse}: in the inverse transform $\bx = \bA^{-1} \bX$, the column vector $\bx$ is obtained by the linear combination of the basis functions: the $k$-th element (coefficient) in $\bX$ multiplies the $k$-th column (basis function) of $\bA^{-1}$ in a linear combination that generates $\bx$.
\end{itemize}

\subsubsection{Observing that the inverse of a unitary real matrix is its transpose}
%\bExample \textbf{Observing that the inverse of a unitary real matrix is its transpose}.
To get insight on why $\bB^{-1} = \bB^H$ for a unitary $\bB$, consider the elements of $\bB$ are real numbers, such that $\bB^H = \bB^T$. The result of the product $\bB^T~\bB = \bI$ is the identity matrix (that is $\bB^{-1} = \bB^T$), because the inner product between the rows of $\bB^T$ (columns of $\bB$) with the columns of $\bB$ is one when they coincide (main diagonal of $\bI$) and zero otherwise (due to their orthogonality). In other words, the inner products of columns of $\bB$ with themselves is the identity, given they are orthonormal. In case $\bA$ is complex-valued, one has $\bA^{-1} = \bA^H$ via a similar reasoning.
%\eExample

%AK NAO ACHO QUE PRECISE SE RECORRER A LINEARIDADE:
%\footnote{This analysis by inspection is facilitated by linearity, a property of the Fourier tools that will be better detailed later on.}
The next paragraphs present the DCT transform, which can be used for both frequency analysis and coding.

\subsection{DCT transform}

An example of a unitary matrix transform very useful for coding is the discrete cosine transform (DCT). When $N=4$, the corresponding matrices are
\[
\bA = \frac{1}{2}
\left[ \begin{array}{cccc}
 1 &  1 & 1 & 1 \\
 1.307 &  0.541 & -0.541 & -1.307 \\
 1 &  -1 & -1 & 1 \\
 0.541 &  -1.307 & 1.307 & -0.541 \\ \end{array} \right]
\]
and
\[
\bA^{-1} = \bA^H = \bA^T = \frac{1}{2}
\left[ \begin{array}{cccc}
  1 &  1.307 & 1 & 0.541 \\
 1 &  0.541 & -1 & -1.307 \\
 1 &  -0.541 & -1 & 1.307 \\
 1 &  -1.307 & 1 & -0.541 \\ \end{array} \right]
\]
for the direct and inverse transforms, respectively.

Because they have finite duration, one can represent the input signals of block transforms as vectors
or sequences. For instance, the DCT basis function corresponding to the second column of $\bA^{-1}$
could be written as 
\[
b[n] = \frac{1}{2} \left( 1.307 \delta[n] + 0.541 \delta[n-1] - 0.541 \delta[n-2] -1.307 \delta[n-3] \right).
\]

Considering that the first element is $a_{0,0}$ (first index in equations is zero, not one as in \matlab), an element $a_{n,k}$ of the $N$-point inverse DCT matrix $\bA^{-1} = \{ a_{n,k} \}$ can be obtained by
\[
a_{n,k} = w_k \cos \left( \frac{\pi (2 n + 1) k}{2N}  \right),
\]
where $w_k$ is a scaling factor that enforces the basis vectors to have unit norm, i.\,e.,  $w_k = \frac{1}{\sqrt{N}}$ for $k=0$ and $w_k = \sqrt{\frac{2}{N}}$ for $k=1,2,\ldots,N-1$.


\bExample \textbf{DCT calculation in {\matlab}}.
\codl{dctmtx} illustrates how the DCT matrices can be obtained in {\matlab}.

\lstinputlisting[caption={MatlabOctaveFunctions/ak\_dctmtx.m},label=code:dctmtx]{./Code/MatlabOctaveFunctions/ak_dctmtx.m}

The matrices obtained with \ci{ak\_dctmtx.m} can be used to perform the transformations but
this has only pedagogical value. There are algorithms for computing the DCT that are faster
than a plain matrix multiplication. Check the functions \ci{dct} and \ci{idct} in {\matlab}
and \ci{scipy.fftpack.dct} in Python.
\eExample
%AK_GUIDELINE: BECAUSE IT IS CONFUSING THAT ORTHOGONAL MATRICES ARE IN FACT ORTHONORMAL, CALL A UNITARY UNTIL THE ISSUE IS DISCUSSED IN DETAILS

\bExample \textbf{The DCT basis functions are cosines of distinct frequencies}.
\figl{dctbasis} shows four basis functions of a 32-points DCT transform.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/dctbasis}            
        \caption[{The first three ($k=0,1,2$) and the last ($k=31$) basis functions for a 32-points DCT.}]{The first three ($k=0,1,2$) and the last ($k=31$) basis functions for a 32-points DCT. Note that the frequency increases with $k$.\label{fig:dctbasis}}
\end{figure}

\figl{dctbasis} indicates that, in order to represent signals composed by ``low frequencies'', DCT coefficients of low order (small values of $k$ can be used), while higher order coefficients are more useful for signals composed by ``high frequencies''.
For example, the commands:
\begin{lstlisting}
N=32;k=3;n=0:N-1;x=7*cos(k*(pi*(2*n+1)/(2*N)));stem(x); X=dct(x)
\end{lstlisting}
return a vector \ci{X} with all elements equal to zero but \ci{X(4)=28}, which corresponds to $k=3$ (recall the first index in {\matlab} is 1, not 0). Using a larger $k$ will increase
the frequency and the order of the corresponding DCT coefficient.
\eExample 

\bExample \textbf{Example of a DCT transformation}.
For example, assuming a 4-points DCT and $\bx=[1,2,3,4]^T$, the \emph{forward} transform can be obtained in this case with
\begin{align*}
\bX = &
\left[ \begin{array}{c}
 X(0)\\
 X(1)\\
 X(2)\\
 X(3)\\ \end{array} \right] =
 \bA \bx \\
 \approx & \frac{1}{2}
\left[ \begin{array}{cccc}
 1 &  1 & 1 & 1 \\
 1.307 &  0.541 & -0.541 & 1.307 \\
 1 &  -1 & -1 & 1 \\
 0.541 &  -1.307 & 1.307 & -0.541 \\ \end{array} \right]
\left[ \begin{array}{c}
 1\\
 2\\
 3\\
 4\\ \end{array} \right] \\
   \approx &
\left[ \begin{array}{c}
 5\\
 -2.230\\
 0\\
 -0.158\\ \end{array} \right].
\end{align*}

In this case,
%antiga notacao
%X(0) \textrm{~}= \textrm{~}<\bx, (a_{*,0})^H)> \textrm{~}=\textrm{~} <[1,2,3,4]^T,0.5[1,1,1,1]^T> \textrm{~}= 5,
\[
X[0] \textrm{~}= \textrm{~}\langle\bx, \bA(0,:)\rangle \textrm{~}=\textrm{~} \langle [1,2,3,4]^T,0.5[1,1,1,1]^T\rangle \textrm{~}= 5,
\]
where $\bA(0,:)$ represents the first (0-th) row of matrix $\bA$. Similarly, $X(2)$ is given by
\[
X[2] \textrm{~}= \textrm{~}\langle\bx, \bA(2,:)\rangle \textrm{~}=\textrm{~} \langle [1,2,3,4]^T,0.5[1,-1,-1,1]^T\rangle\textrm{~} = 0,
\]
and so on.

The previous expressions provide intuition on the \emph{direct} transform. In the \emph{inverse} transform, when reconstructing $\bx$, the coefficient $X[k]$ is the scaling factor that multiplies the $k$-th basis function in the linear combination $\bx = \bA^{-1} \bX$. Still considering the 4-points DCT, the inverse corresponds to
\begin{align*}
\left[
\begin{array}{l}
        x[0] \\ x[1] \\ x[2] \\ x[3]
\\ \end{array}
\right]
&=
\frac{1}{2}
\left[ \begin{array}{cccc}
 1 &  1.307 & 1 & 0.541 \\
 1 &  0.541 & -1 & -1.307 \\
 1 &  -0.541 & -1 & 1.307 \\
 1 &  -1.307 & 1 & -0.541 \\ \end{array} \right]
\left[
\begin{array}{c}
        5 \\ -2.230 \\ 0 \\ -0.158
\\ \end{array}
\right]
\\
&=
\frac{1}{2} \left\{
5
\left[
\begin{array}{c}
        1 \\ 1 \\ 1 \\ 1
\\ \end{array}
\right]
- 2.230
\left[
\begin{array}{c}
        1.307 \\ 0.541 \\ -0.541  \\ -1.307
\\ \end{array}
\right] \right.\\
&\mathrel{\ifpdf \phantom{=} \else \fbox{\phantom{=}} \fi} \left. +0
\left[
\begin{array}{c}
        1 \\ -1 \\ -1 \\ 1
\\ \end{array}
\right]
-0.158
\left[
\begin{array}{c}
        0.541 \\ -1.307 \\ 1.307 \\ -0.541
\\ \end{array}
\right]
\right\}.
\end{align*}
Note that $X[2]=0$ and, consequently, the basis function $0.5[1,-1,-1,1]^T$ is not used to reconstruct $\bx$. The reason is that this specific basis function is orthogonal to $\bx$ and does not contribute to its reconstruction.
As another example, the DCT coefficient $X[0]=5$ indicates that a scaling factor equals to 5 should multiply the corresponding $0$-th basis function $0.5[1,1,1,1]^T$ in order to reconstruct the vector $\bx$ in time-domain.
\eExample

Alternatively, the matrix multiplication can be described by a transform equation. For example, the DCT coefficients can be calculated by
\[
X[k] = \sqrt{\frac{2}{N}} \sum_{n=0}^{N-1} x[n] \cos \left( \frac{\pi (2 n + 1) k}{2N}  \right), k=1,\ldots,N-1
\]
and
\[
X[0] = \frac{1}{\sqrt{N}} \sum_{n=0}^{N-1} x[n].
\]
%It is important to interpret these summations as calculating the inner product between the input vector $\bx$ and the basis functions with 

As mentioned, the $k$-th element (or coefficient) $X[k]$ of $\bX$ can be calculated as the inner product of $\bx$ with the complex conjugate of the $k$-th basis function. This can be done because the DCT basis functions are orthogonal among themselves, as discussed in Section~\ref{sec:orthogonalBasis}.
The factors $\sqrt{2/N}$ and $1/\sqrt{N}$ are used to have basis vectors with norms equal to 1.
The $k$-th basis vector is a cosine with frequency $(k\pi)/N$ and phase $(k \pi)/ (2N)$.

Section~\ref{sec:transf_applic} discusses examples of DCT applications, including coding (signal compression).
One advantage of adopting block transforms in coding applications is the distinct importance of coefficients. In the original domain, all samples (or pixels in image processing) have the same importance, but in the transform domain, coefficients typically have distinct importance. Hence, the coding scheme can concentrate on representing the most important coefficients and even discard the non-important ones.
Another application of DCTs is in frequency analysis (finding the most relevant frequencies that compose a signal). But, in this application, the DFT is more 
widely adopted than the DCT.

\subsection{DFT transform}

As the DCT, the discrete Fourier transform (DFT) is a very useful tool to accomplish frequency analysis, where the goal is to estimate the coefficients for basis functions that are distinguished by their frequencies. The DFT is related to the discrete-time Fourier series,
%~\cite{Oppenheim96},
which also uses cosines $\cos \left(\frac {2\pi nk} N \right)$ and sines $\sin \left(\frac {2\pi nk} N \right)$,  $k=0,1,\ldots,N-1$, as basis functions, and will be discussed in this chapter. While the DCT uses cosines and its matrices are real, the DFT uses complex exponentials as basis functions.

Using Euler's formula, \equl{euler}, complex numbers provide a more concise representation of sines and cosines and
the $k$-th \emph{DFT basis function} is given by
\begin{equation}
\frac{1}{N} e^{\frac {j2\pi n k} N} = \frac{1}{N} \left[ \cos \left(\frac {2\pi nk} N \right) + j \sin \left(\frac {2\pi nk} N \right) \right],
\label{eq:dft_basis_function}
\end{equation}
where $n=0,1,\ldots,N-1$ expresses time evolution, as for the DCT. The value $k$ determines the angular frequency $\dw_k = \frac {2\pi k} N$ (in radians) of the basis function $b[n]$. \equl{dft_basis_function} can be written as
\begin{equation}
b[n] = \frac{1}{N} e^{j \dw_k n} = \frac{1}{N} \left[ \cos \left(\dw_k n \right) + j \sin \left(\dw_k n \right) \right],
\label{eq:dft_basis_rewritten}
\end{equation}
which gives the $n$-th value of the time-domain basis function $b[n]$.

Hence, one can notice that the DFT operates with an \emph{angular frequency resolution} $\Delta \dw = 2 \pi / N$,
such that $\dw_k = k \Delta \dw$.
For instance, considering $N=4$, the resolution is $\Delta \dw = 2 \pi / 4 = \pi /2$~rad and the DFT uses basis functions with angular
frequencies $\dw_k =0, \pi/2, \pi$ and $3\pi/2$~rad. 
This is depicted in \figl{dftcirclecounter4}.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall]{Figures/dftcirclecounter4}
        \caption{Angles $\dw_k=0, \pi/2, \pi$ and $3\pi/2$ rad (or, equivalently, discrete-time angular frequencies) used by a DFT of $N=4$ points when $k$ varies from $k=0$ to 3, respectively.\label{fig:dftcirclecounter4}}				
\end{figure}

When $k=2$ in \figl{dftcirclecounter4}, the angular frequency is $\dw_k=\pi$~rad, and the basis function
is $b[n]=\frac{1}{N} e^{j \pi n} = \frac{1}{N} \left(e^{j \pi} \right)^n = \frac{1}{N} (-1)^n$. The signal $(-1)^n$ corresponds to the
highest angular frequency $\dw=\pi$~rad among discrete-time signals, and this frequency is
called Nyquist frequency, as discussed in Section~\ref{sec:nyquist_frequency}.

To complement \figl{dftcirclecounter4}, which uses an even value $N$, \figl{dftcirclecounter5} describes the DFT angular frequencies
when $N=5$.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall]{Figures/dftcirclecounter5}
        \caption{Angles $\dw_k=0, 2\pi/5, 4\pi/5, 6\pi/5$ and $8\pi/5$ rad (that in degrees corresponds to $0$, $\degree{72}$, $\degree{144}$, $\degree{-144}$, $\degree{-72}$) used by a DFT of $N=5$ points when $k$ varies from $k=0$ to 4, respectively.\label{fig:dftcirclecounter5}}				
\end{figure}

As depicted in \figl{dftcirclecounter5}, when $N$ is odd, the Nyquist frequency $\dw = \pi$~rad is not
used by the DFT.

The $N$-point forward and inverse DFT pair can be written as a pair of equations. 
The forward (direct) transform is
\begin{equation}
        X[k] = \sum_{n=0}^{N-1} x[n] e^{-\frac{j 2 \pi n k}{N}},\textrm{~~~for~} k=0, \ldots, N-1,
        \label{eq:dft_long}
\end{equation}
which corresponds to calculating the $k$-th DFT coefficient $X[k]=\langle x[n],  e^{\frac{j 2 \pi n k}{N}} \rangle$ as the inner product between
the time-domain signal $x[n]$ and the $k$-th basis vector of \equl{dft_basis_function} multiplied by $N$. In this
calculation, the adopted inner product for complex-valued
vectors of \equl{innerProductComplexVectors} is responsible for \equl{dft} using $e^{-\frac{j 2 \pi n k}{N}}$ instead of $e^{\frac{j 2 \pi n k}{N}}$.

The inverse DFT transform equation is
\begin{equation}
        x[n] = \frac{1}{N} \sum_{k=0}^{N-1} X[k] e^{\frac{j 2 \pi n k}{N}},\textrm{~~~for~} n=0, \ldots, N-1.
        \label{eq:idft_long}
\end{equation}

\equl{dft_long} and \equl{idft_long} are the most adopted definitions of a DFT pair. But the DFT scaling factor $1/N$ in \equl{idft} can be changed to $1/\sqrt{N}$, and the same factor $1/\sqrt{N}$ be also used in \equl{dft_long}, if one wants to have a DFT pair with basis functions with unit norm. In this case the transform is called \emph{unitary DFT}\index{Unitary DFT} and has the
properties already discussed in Section~\ref{sec:unitary_transforms}.\footnote{Other concepts related to different choices for scaling the DFT basis functions are further discussed later, in Sections~\ref{sec:ortho_not_unit} and \ref{sec:dft_normalization_options}.}

Instead of using transform equations, sometimes it is convenient to use a matrix notation for transforms. 
With a matrix notation, it is easier to interpret \equl{idft} as composing the time-domain
signal $x[n]$ as a linear combination of basis functions $(1/N)e^{\frac{j 2 \pi n k}{N}}$ scaled by
the respective coefficient $X[k]$.

To develop the matrix notation, one can observe that an element $a_{n,k}$ of the $N$-point inverse DFT matrix $\bA^{-1} = \{ a_{n,k} \}$ is given by
\begin{equation}
a_{n,k} = \frac{1}{N} e^{\frac {j2\pi n k} N}.
\label{eq:dft_matrix_element}
\end{equation}
Because $n$ and $k$ appear in \equl{dft_matrix_element} as multiplication parcels, $a_{n,k}=a_{k,n}$  
and the inverse DFT matrix $\bA^{-1}$ is symmetric, that is $\bA^{-1} = \left(\bA^{-1}\right)^T$.

For the sake of comparison, when $N=4$, the inverse DCT and DFT matrices are, respectively:
\[
\bA^{-1}_{\textrm{DCT}} = 
\frac{1}{2}
    \begin{bmatrix}
  1 &  1.307 & 1 & 0.541 \\
 1 &  0.541 & -1 & -1.307 \\
 1 &  -0.541 & -1 & 1.307 \\
 1 &  -1.307 & 1 & -0.541 \\
    \end{bmatrix}
\textrm{~~~and~~~~}
\bA^{-1}_{\textrm{DFT}} = 
\frac{1}{4}
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & j & -1 & -j \\
    1 & -1 & 1 & -1 \\
    1 & -j & -1 & j \\
    \end{bmatrix}
.\]
The example shows that the inverse DFT matrix $\bA^{-1}_{\textrm{DFT}}$ is symmetric, while 
the inverse DCT matrix $\bA^{-1}_{\textrm{DCT}}$ is not.
It can also be noticed that the inverse DFT matrix $\bA^{-1}_{\textrm{DFT}}$ is complex-valued, having elements that are real and imaginary. In contrast,
the inverse DCT matrix $\bA^{-1}_{\textrm{DCT}}$ is real-valued.
However, the first and third columns of $\bA^{-1}_{\textrm{DFT}}$ are real-valued (all elements are $+1$ or $-1$). For $N=4$, these rows correspond to $k=0$ and $k=N/2$, which are called DC and Nyquist frequencies,\index{Nyquist frequency} respectively.
All $N$ values corresponding to the basis vector of frequency $k=0$ (DC) are given by $a_{n,0} = 1/4 = 0.25, n=0,\ldots,N-1$, as indicated
in the first column of $\bA^{-1}_{\textrm{DFT}}$. And for $N$ even, all $N$ values corresponding to the basis vector of the Nyquist frequency $k=N/2$ (third column of $\bA^{-1}_{\textrm{DFT}}$) are given by $a_{n,N/2} = (-1)^{n}, n=0,\ldots,N-1$.

From \equl{dft_matrix_element} and adopting $N=4$, the DFT inverse matrix is given by
\[ 
\bA^{-1} = \frac{1}{N}
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & e^{j \pi / 2}   & e^{j \pi} &  e^{j 3\pi/2} \\
    1 & e^{j \pi}       & e^{j 2 \pi} & e^{j 3 \pi} \\
    1 & e^{j 3 \pi / 2} & e^{j 3 \pi} & e^{j 9 \pi/2} \\
    \end{bmatrix} =
					\frac{1}{4}
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & j & -1 & -j \\
    1 & -1 & 1 & -1 \\
    1 & -j & -1 & j \\
    \end{bmatrix}. 		
\]
Note that the complex numbers $e^{j \dw}$ with angles $\dw=0, \pi/2, \pi$ and $3\pi/2$ (which is equivalent to $-\pi/2$)~rad and magnitudes equal to 1, can be 
written as $1, j, -1$ and $-j$, respectively. This simplifies writing the elements of $\bA^{-1}$ for $N=4$ and other values of $N$.

\bExample \textbf{Examples of 4-points DFT calculation}.
\label{ex:four_points_dft}
Let us calculate the 4-points DFTs of three signals represented by the column vectors $\bx = [2, 2, 2, 2]^T$, $\by = [0, 5, -5, 0]^T$ and $\bz = [1,2,3,4]^T$.
In this case, the forward and inverse matrices can be written as
\[ \bA = 
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & -j & -1 & j \\
    1 & -1 & 1 & -1 \\
    1 & j & -1 & -j \\
    \end{bmatrix}
		\textrm{~~~and~~~}
\bA^{-1} = \frac{\bA^H}{N} = \frac{1}{4}
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & j & -1 & -j \\
    1 & -1 & 1 & -1 \\
    1 & -j & -1 & j \\
    \end{bmatrix},
\]
respectively.
The coefficient vectors are $\bX = \bA \bx = [8, 0, 0, 0]^T$, $\bY = \bA \by = [0, 5-j5, -10, 5+j5]^T$ and $\bZ = \bA \bz = [10, -2+j2, -2, -2-j2]^T$.


Because the basis functions (columns of $\bA^{-1}$) are orthogonal, but not orthonormal, the $k$-th coefficient of the forward transform is obtained by performing the inner product between $\bx$ and the complex conjugate of the $k$-th basis function multiplied by $N=4$. 

The first basis function (corresponding to $k=0$) is always the first column of $\bA^{-1}$, which in this case is $[0.25, 0.25, 0.25, 0.25]^T$. 
Because all elements of this basis function are a constant ($0.25$ when $N=4$), the job of the respective coefficient is to represent the average value (DC level) of the input signal.
After multiplication by $N=4$, the scaled basis function for $k=0$ is $[1, 1, 1, 1]$ (first row of $\bA$) and the corresponding coefficients in $\bX$, $\bY$ and $\bZ$ are 8, 0 and 10, respectively. This indicates that the DC level of $\bx$, $\by$ and $\bz$ are 2, 0 and 2.5, respectively. 

This example also illustrates the symmetry of DFT coefficients when the input signal is real-valued. One can note that the coefficients corresponding to $k=1$ and $k=3$ are complex conjugates, while the coefficients corresponding to $k=0$ and $k=2$ (more generally, $k=N/2$, for $N$ even) is always real-valued.
In fact, because the basis functions for frequencies $k=0$ (DC) and $k=N/2$ (Nyquist) are real-valued, if the input
vector is also real-valued, these two DFT coefficients will always be real-valued (in the case $N$ is even).
\eExample 

The following example addresses the situation when $N$ is an odd number. In this case, the Nyquist frequency is not
represented in the DFT values.

\bExample \textbf{Examples of 5-points DFT calculation}.
\label{ex:five_points_dft}
Let us calculate the 5-points DFTs of three signals that have five elements but are similar to the ones in the previous example: $\bx = [2, 2, 2, 2, 2]^T$, $\by = [0, 5, -5, 0, 0]^T$ and $\bz = [1,2,3,4,5]^T$.
In this case, the inverse matrix can be written as
\[
\bA^{-1} = \frac{1}{5}
    \begin{bmatrix}
    1 & 1 & 1 & 1 & 1\\
    1 & e^{j 2\pi / 5}  & e^{j 4\pi / 5} & e^{-j 4\pi / 5} & e^{-j 2\pi / 5} \\
    1 & e^{j 4\pi / 5}  & e^{-j 2\pi / 5} & e^{j 2\pi / 5} & e^{-j 4\pi / 5} \\
    1 & e^{-j 4\pi / 5} & e^{j 2\pi / 5} & e^{-j 2\pi / 5} & e^{j 4\pi / 5} \\
    1 & e^{-j 2\pi / 5} & e^{-j 4\pi / 5} & e^{j 4\pi / 5} & e^{j 2\pi / 5} \\
    \end{bmatrix},
\]
which can be written in Cartesian form using two decimal places as
\[
\bA^{-1} \approx \frac{1}{5}
    \begin{bmatrix}
    1 & 1 & 1 & 1 & 1\\
    1 & 0.31-0.95j & -0.81-0.59j & -0.81+0.59j & 0.31 + 0.95j \\
    1 & -0.81-0.59j & 0.31 + 0.95j & 0.31-0.95j & -0.81 + 0.59j \\
    1 & -0.81+0.59j & 0.31 - 0.95j & 0.31+0.95j & -0.81 -0.59j \\
    1 & 0.31+0.95j & -0.81+0.59j & -0.81-0.59j & 0.31 -0.95j \\
    \end{bmatrix}.
\]
		
When $N=5$, the DFT angular resolution is $\Delta \dw = 2\pi/5$~rad, which corresponds to $72$~degrees. To better visualize
the involved angles, as a third altenative to denote $\bA^{-1}$, one can
use the notation $m \angledeg{\theta}$ for complex values and use angles in degrees to write
\[
\bA^{-1} = \frac{1}{5}
    \begin{bmatrix}
    1 & 1 & 1 & 1 & 1\\
    1 & 1\angledeg{72} & 1\angledeg{144} & 1\angledeg{-144} & 1\angledeg{-72} \\
    1 & 1\angledeg{144} & 1\angledeg{-72} & 1\angledeg{72} & 1\angledeg{-144} \\
    1 & 1\angledeg{-144} & 1\angledeg{72} & 1\angledeg{-72} & 1\angledeg{144} \\
    1 & 1\angledeg{-72} & 1\angledeg{-144} & 1\angledeg{144} & 1\angledeg{72} \\
    \end{bmatrix}.
\]

Performing the direct transform, one finds that the coefficient vectors in this case are $\bX = \bA \bx = [10, 0, 0, 0, 0]^T$, $\bY = \bA \by = [0, 5.59-j1.82, -5.59-7.69j, -5.59+7.69j, 5.59+j1.82,]^T$ and $\bZ = \bA \bz = [15, -2.5+3.44j, -2.5+0.81j,  -2.5-0.81j, -2.5-3.44j]^T$.

The first (DC) coefficients in $\bX$, $\bY$ and $\bZ$ are 10, 0 and 15, respectively. After division by $N=5$, one can find that the DC level (or average value) of the time-domain vectors $\bx$, $\by$ and $\bz$ are 2, 0 and 3, respectively. 

This example also illustrates the symmetry of DFT coefficients when the input signal is real-valued. One can note that the coefficients corresponding to $k=1$ and $k=3$ are complex conjugates, while the coefficients corresponding to $k=0$ and $k=2$ (more generally, $k=N/2$, for $N$ even) is always real-valued.
In fact, because the basis functions for frequencies $k=0$ (DC) and $k=N/2$ (Nyquist) are real-valued, if the input
vector is also real-valued, these two DFT coefficients will always be real-valued (in the case $N$ is even).
\eExample 

\subsubsection{On the symmetry of DFT coefficients when the input signal is real-valued}
\label{sec:dft_symmetry}

Assuming a DFT with $N=6$ points, \figl{dft_N_6_mapping} indicates the angular frequency $\dw_k$ corresponding to each DFT coefficient $X[k]$. For instance, the DC coefficient corresponding to $k=0$ is the first element of the vector with
DFT coefficients. For $N$ even, the Nyquist frequency is used and its respective coefficient is the element corresponding to
$k=N/2$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{./FiguresNonScript/dft_N_6_mapping}
\caption{Mapping between angular frequencies $\dw_k$ and the corresponding DFT coefficient $X[k]$ for $N=6$.\label{fig:dft_N_6_mapping}}
\end{figure}

\figl{dft_N_5_mapping} shows a figure similar to \figl{dft_N_6_mapping} but for a DFT with $N=5$ points. In the case of an odd value of $N$, the Nyquist frequency is not used in the DFT calculation.


Another aspect depicted by both \figl{dft_N_5_mapping} and \figl{dft_N_6_mapping} is that the negative frequencies are located after the positive ones in the DFT coefficients
vector. The {\matlab} function \ci{fftshift} can be used to move the negative frequencies to the first elements of this vector.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{./FiguresNonScript/dft_N_5_mapping}
\caption{Mapping between angular frequencies $\dw_k$ and the corresponding DFT coefficient $X[k]$ for $N=5$.\label{fig:dft_N_5_mapping}}
\end{figure}

\figl{dft_N_6_mapping} and \figl{dft_N_5_mapping} are also useful to observe symmetries related to the DFT transform.
Starting from \equl{dft_basis_rewritten}, if we substitute the original angular frequency $\dw_k$ by $-\dw_k$, the new
basis $d[n]$ is given by
\begin{equation}
d[n] = \frac{1}{N} e^{-j \dw_k n} = \frac{1}{N} \left[ \cos \left(-\dw_k n \right) + j \sin \left(-\dw_k n \right) \right] =
\frac{1}{N} \left[ \cos \left(\dw_k n \right) - j \sin \left(\dw_k n \right) \right].
\end{equation}
Comparing $d[n]$ with \equl{dft_basis_rewritten}, one can note that $d[n]=b^*[n]$ is the complex conjugate\footnote{Because, as discussed in Section~\ref{sec:even_odd}, the cosine is an even function while the sine is an odd function.} of the function that has frequency $\dw_k$. 
Therefore, if $X[k]=\langle x[n], b[n] \rangle$, in case one changes the sign of the angular frequency of $b[n]$ from
$\dw_k$ to $-\dw_k$ to obtain a new basis function $d[n]=b^*[n]$, the new coefficient $Z[k]=\langle x[n], d[n] \rangle$
has the property $Z[k] = X^*[k]$ of being the complex conjugate of $X[k]$ if $x[n]$ is real-valued.

Observing \figl{dft_N_6_mapping}, the angles corresponding to $k=1$ and $k=5$ are opposites. Also, $k=2$ is the opposite of $k=4$.
Therefore, the respective pairs of DFT coefficients are complex conjugates. More specifically, $X[1] = X^*[5]$ and $X[2] = X^*[4]$, when a 6-points DFT is adopted.
Similarly, when assuming $N=5$ and following the mentioned reasoning, one can conclude from \figl{dft_N_5_mapping}  that 
$X[1] = X^*[4]$ and $X[2] = X^*[3]$.

In general, if the input signal is real-valuded one has DFT coefficients with the complex conjugate property
\[
X[k'] = X^*[N-k'].
\]
If $N$ is even, this is valid for $k'=1, \ldots, (N/2-1)$. And if $N$ is an odd number, 
the range is $k'=1, \ldots, (N-1)/2$. The following example illustrates this fact.

\bExample \textbf{Coefficients that are complex conjugates when the signal is real-valued}.
\label{ex:symmetry_dft_real}
All the time-domain input vectors of \exal{four_points_dft} and \exal{five_points_dft} are real-valued and
can be used to help studying the symmetry of the respective DFT coefficients.

In \exal{four_points_dft}, the coefficient vectors are $\bX = [8, 0, 0, 0]^T$, $\bY = [0, 5-j5, -10, 5+j5]^T$ and $\bZ = [10, -2+j2, -2, -2-j2]^T$. Recalling that the first element has index $k=0$ (not $k=1$), it  can be seen that 
the coefficients corresponding to $k=1$ and $k=3$ are always complex conjugates.

Similarly, in \exal{four_points_dft}, the coefficient vectors are $\bX = [10, 0, 0, 0, 0]^T$, $\bY = [0, 5.59-j1.82, -5.59-7.69j, -5.59+7.69j, 5.59+j1.82,]^T$ and $\bZ = [15, -2.5+3.44j, -2.5+0.81j,  -2.5-0.81j, -2.5-3.44j]^T$, 
and one can observe that 
the coefficients corresponding to the pairs $(k=1, k=4)$ and $(k=2, k=3)$ are always complex conjugates.
\eExample 

The basis functions $b[n]$ for the DC and Nyquist coefficients are real-valued\footnote{The existence of a Nyquist coefficient
requires $N$ to be an even number.}
Hence, another property of the DFT for real-valued inputs is that 
$X[k]=\langle x[n], b[n] \rangle$ are real-valued coefficients for $k=0$ and, in case $N$ is even, $k=N/2$.
This can be observed in the DFT coefficients discussed in \exal{symmetry_dft_real}.

\subsubsection{{\akadvanced} DFT and FFT denoted with the twiddle factor $W_N$}

For convenience, the \emph{twiddle factor}\index{Twiddle factor} $W_N$ is defined as
\begin{equation}
W_N = e^{-\frac {j2\pi} N}
\label{eq:twiddle_factor}
\end{equation}
such that the $k$-th basis is $(1/N) \left(W_N \right)^{-nk}$ for the conventional DFT and 
$(1/\sqrt{N})\left(W_N \right)^{-nk}$ for the unitary DFT. Twiddle means to lightly turn over or around and is used because the complex number $W_N$ has magnitude equals to 1 and changes only the angle of a complex number that is multiplied by it.
Each element of the inverse DFT matrix $\bA^{-1} = \{ a_{n,k} \}$ is
\[
a_{n,k} = \frac{1}{N} \left(W_N \right)^{-nk}.
\]
Hence, the twiddle factor $W_N = e^{-\frac {j2\pi} N}$ denotes the 
DFT angular frequency resolution $\Delta \dw$.

\figl{circledivided} illustrates the complex numbers $W_N$ as vectors for different values of $N$. Because $|W_N|=1$, the twiddle factor is located on the unit circle of the complex plane and effectively informs an angle. For example, the three angles used by a DFT of $N=3$ points are 0, 120 and 240 degrees, while a 4-points DFT uses 0, 90, 180 and 270.


%\footnote{ They are primitive $N$-th roots of unity (also called Moivre numbers). The $N$-th roots of unity are all the complex numbers that yield 1 when raised to a given power $N$. The factors $W_N$ are located on the unit circle of the complex plane.}

\begin{figure}[!htb]
  \begin{center}
    \subfigure[$N=3$]{\label{fig:circledivided3}\includegraphics[width=5cm,keepaspectratio]{FiguresNonScript/circledivided3}}
    \subfigure[$N=4$]{\label{fig:circledivided4}\includegraphics[width=5cm,keepaspectratio]{FiguresNonScript/circledivided4}}
    \\
    \subfigure[$N=5$]{\label{fig:circledivided5}\includegraphics[width=5cm,keepaspectratio]{FiguresNonScript/circledivided5}}
    \subfigure[$N=6$]{\label{fig:circledivided6}\includegraphics[width=5cm,keepaspectratio]{FiguresNonScript/circledivided6}}
  \end{center}
  \caption{The angles corresponding to $W_N$ on the unit circle, for $N=3,4,5,6$.\label{fig:circledivided}}  
\end{figure}

For $N=4$, the DFT pair is given by
\begin{equation}
\bA = 
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & W_4^{1} & W_4^{2} & W_4^{3} \\
    1 & W_4^{2} & W_4^{4} & W_4^{6} \\
    1 & W_4^{3} & W_4^{6} & W_4^{9} \\
    \end{bmatrix}
\label{eq:4point_dft}
\end{equation}
and
\[\bA^{-1} = \frac{1}{N}
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & W_4^{-1} & W_4^{-2} & W_4^{-3} \\
    1 & W_4^{-2} & W_4^{-4} & W_4^{-6} \\
    1 & W_4^{-3} & W_4^{-6} & W_4^{-9} \\
    \end{bmatrix}
.\]
Note that $\bA^{-1} \ne \bA^H$ because the basis functions have norms equal to $1/\sqrt{N}$ (the DFT matrix $\bA$ was not defined as unitary). In this case, 
\begin{equation}
\bA^{-1} = N \bA^H.
\label{eq:dfts_relation_nonorthogonal}
\end{equation}

Also note that the reason to have $W_N$ defined as a negative exponent in \equl{twiddle_factor} is that the direct transform $\bA$ uses positive powers of $W_N$.
Another important fact is that $(W_N)^{aN+b}=(W_N)^{b}$ for any $a \in \integers$. This can be seen by noting that $W_N^n$ has a period of $N$ samples and $(W_N)^{aN} = 1$. Hence, the 4-points direct DFT matrix of \equl{4point_dft} can be written as
\[\bA = 
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & W_4^{1} & W_4^{2} & W_4^{3} \\
    1 & W_4^{2} & 1 & W_4^{2} \\
    1 & W_4^{3} & W_4^{2} & W_4^{1} \\
    \end{bmatrix}
=
    \begin{bmatrix}
    1 & 1 & 1 & 1\\
    1 & -j & -1 & j \\
    1 & -1 & 1 & -1 \\
    1 & j & -1 & -j \\
    \end{bmatrix}
.\]
It can be seen that the matrix $\bA$ (and also $\bA^{-1}$) has several symmetries that can be explored to save computations. There is a large set of algorithms for efficiently computing the DFT. These are collectively called \emph{fast Fourier transform} (FFT) algorithms\index{FFT algorithms}.
%As mentioned in Section~\ref{subsec:dft}, 
%An FFT is a fast algorithm for calculating the DFT, i.\,e., its 
Apart from numerical errors, both DFT (the plain matrix multiplication) and FFT algorithms lead to the same result.  Therefore, the DFT is the name of the transform and FFT is a fast algorithm (from a large collection) used to speed up calculating the DFT. Implementing an FFT routine is not trivial but they are typically available in softwares such as {\matlab}. 

There are FFT algorithms specialized in the case where the FFT size $N$ is a power of two (i.\,e., $N=2^a,~a \in \naturals$). Other FFT algorithms do not present this restriction but typically are less efficient. The computational costs of DFT and FFT are quite different when $N$ grows large. The DFT matrix multiplication requires computing $N$ inner products, so the order of this computation is $\calO(N^2)$ while FFT algorithms achieve $\calO(N \log_2N)$. \figl{dftfftcost} illustrates this comparison.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/dftfftcost}          
        \caption{Computational cost of the DFT calculated via matrix multiplication versus an FFT algorithm. Note that $N=4096$ are used in standards such as VDSL2, for example, and it is clearly unreasonable to use matrix multiplication.\label{fig:dftfftcost}}
\end{figure}


%In general, the element in row $k$ and column $n$ of the forward DFT matrix $\bA$ is $W_N^{nk}$. The element in row $n$ and column $k$ of an inverse DFT $\bA^{-1}$ corresponds to the complex conjugate $W_N^{-nk}$ normalized by $N$.

%AKGUIDELINE:
\ignore{To develop insight let us first consider a $N=10$-points orthogonal transform that has $\cos(2 \pi/5 n)$, $\sin(2 \pi/5 n)$ and $\cos(2 \pi/10 n)$ as part of its basis set (note the basis functions have energy $N/2=5$ in this case).
%FAZER A TRANSICAO DE COS E SIN PARA FOURIER
%DEFINIR MINHA TRANSFORMADA COM SENO E COSENO E SENDO REAL
}

Instead of using matrix notation, one can use transform equations. 
The $N$-point DFT pair (forward and inverse, respectively) is:
\begin{equation}
        X[k] = \sum_{n=0}^{N-1} x[n] \left(W_N \right)^{nk}
        \label{eq:dft}
\end{equation}
and
\begin{equation}
        x[n] = \frac{1}{N} \sum_{k=0}^{N-1} X[k] \left(W_N \right)^{-nk}.
        \label{eq:idft}
\end{equation}
For the unitary DFT pair, the normalization factors should be changed to:
\begin{equation}
        X[k] = \frac{1}{\sqrt N} \sum_{n=0}^{N-1} x[n] \left(W_N \right)^{nk}
        \label{eq:unitary_dft}
\end{equation}
and
\begin{equation}
        x[n] = \frac{1}{\sqrt N} \sum_{k=0}^{N-1} X[k] \left(W_N \right)^{-nk}.
        \label{eq:unitary_idft}
\end{equation}


\ignore{
\ubsection{DFT as a matrix multiplication}
This section will explain why the DFT calculation corresponds to multiplying by a matrix
$\bF$, that is $\bX=\bF \bx$, where $\bX$ and $\bx$ are $N \times 1$ column-vectors. In fact, one can write $\bF = \alpha \bQ$, where both $\bF$ and $\bQ$ are $N \times N$ matrices. Similarly, the inverse DFT calculation corresponds to multiplying by a matrix $\bF^{-1} = \beta Q^*$.
We first discuss the matrix $\bQ$ and later the choice of the scalar values $\alpha$ and $\beta$.
%
The element $a_{kn}$ in row $k$ and column $n$ of a Vandermonde matrix $\bQ$ used in the forward DFT is $a_{kn} = W_N^{nk}$. The element $a_{nk}$ in row $n$ and column $k$ of an inverse DFT corresponds to the complex conjugate of $a_{nk}=W_N^{-nk}$. That is, the inverse Fourier uses $\bQ^*$.
%
Matrix $\bQ$ is symmetric (equal to its transpose $\bQ = \bQ^T$). The same is valid for the inverse DFT matrix. Because of their symmetry, the Hermitian operation (consists in transposing and complex conjugating) is equivalent to just the conjugate, i.\,e., $\bQ^H = \bQ^*$.
%
Note that $\bQ \times \bQ^* = \bQ^* \times \bQ = N \bI$, where $\bI$ is the identity matrix. That is, there is a factor of $N$ that must be taken into account to have $\bQ^*$ as the inverse of $\bQ$ (and vice-versa). The value $N$ appears because all basis functions of $\bQ$ have energy $N$. This is easy to see because every element of $\bQ$ has magnitude 1, hence summing their magnitude squared over a row or column is equal to $N$.
%
The DFT pair is defined as
\[
\bX = \alpha \bQ \bx
\]
for the forward transform (where $\bx$ and $\bX$ are the time and frequency-domain column vectors, respectively) and
\[
\bx = \beta \bQ^* \bX
\]
for the inverse transform. One wants to convert a vector $\bx$ into the frequency domain and use the inverse to get back a vector $\hat \bx = \bx$, hence:
\begin{align*} 
        \label{eq:dftfactors}
\hat \bx &= \beta \bQ^* \bX \\
 &= \beta \bQ^* \alpha \bQ \bx \\
 &= \alpha \beta \bQ^* \bQ \bx \\
 &= \alpha \beta N \bI \bx \\
 &= \alpha \beta N \bx
\end{align*}
and one just needs to make
\[
\alpha \beta = 1/N
\]
to have a transform pair (with $\hat \bx = \bx$). There is flexibility in choosing one of the constants $\alpha$ or $\beta$. For example, arbitrarily one could pick $\alpha=0.5$ and then chose $\beta = 2/N$. However, there are three options that are the most common ones and are discussed in the next section.
}


\subsection{Haar transform}

In signal processing, the Haar transform is intimately related to \emph{wavelets}\index{Wavelets}. This section will not explore this relation nor detail the generation law of Haar matrices. The goal here is to motivate the study of wavelets by indicating how basis functions with support shorter than $N$ can be useful.

The Haar transform is unitary, such that $\bA^{-1}=\bA^H$.
%, where $\bA$ is the direct transform matrix. %the inverse transform is given by .
The reader can use the script \ci{MatlabOctaveThirdPartyFunctions/haarmtx.m} with \ci{A=haarmtx(N)} to obtain the forward matrix $\bA$. For $N=2$, the Haar, DCT and unitary DFT forward matrices are all the same:
\[
\bA = \frac{1}{\sqrt{2}}
\left[ \begin{array}{cc}
1 & 1 \\
1 & -1
\\ \end{array} \right].
\]
For $N=4$, the Haar forward matrix is
\[
\bA = \frac{1}{2}
\left[ \begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & -1 & -1 \\
\sqrt{2} & \sqrt{2} & 0 & 0 \\
0 & 0 & \sqrt{2} & -\sqrt{2}
\\ \end{array} \right].
\]

\begin{figure}[htpb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/firsthaarbasis}              
        \caption[{The first four ($k=0,1,2,3$) and the last ($k=31$) basis functions for a 32-points Haar.}]{The first four ($k=0,1,2,3$) and the last ($k=31$) basis functions for a 32-points Haar. Note that the frequency increases with $k$.\label{fig:firsthaarbasis}}
\end{figure}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/lasthaarbasis}               
        \caption[{Basis functions $k=0,17$ and the last four ($k=28,29,30,31$) for a 32-points Haar.}]{Basis functions $k=0,17$ and the last four ($k=28,29,30,31$) for a 32-points Haar. The support (range of non-zero samples) of the last 15 functions ($k \ge 17$) is just two samples.\label{fig:lasthaarbasis}}
\end{figure}

\figl{firsthaarbasis} and \figl{lasthaarbasis} depict some Haar basis functions. The most interesting aspect is that, in contrast to DCT and DFT, some Haar functions are non-zero only at specific intervals. This characteristic allows the Haar coefficients to provide information not only about ``frequency'', but also about ``time'' (or localization). For example, it is well-known in signal processing that Fourier transforms are better suited for stationary signals while wavelets can better represent transient signals in a time-frequency plan. This kind of behavior can already be visualized by comparing DFT (and DCT) basis with Haar's and is explored in Application~\ref{app:haar_transform}.

\subsection{{\akadvanced} Properties of orthogonal and unitary transforms}

As commented, for transforms with unitary matrices, $X[k]$ is simply the inner product between $\bx$ and the $k$-th basis function.\footnote{With the complex conjugate of the basis incorporated to the definition of this inner product.} In the inverse transform, the coefficient $X[k]$ is the scaling factor that multiplies the $k$-th basis function. This interpretation is not valid for a general non-unitary transform pair $\bA$ and $\bA^{-1}$.

The next paragraphs discuss the fact that unitary matrices do not change the norm of the input vectors.
After that, the orthogonal transforms are compared to the unitary, to show that even when the
norm of the basis vectors is not equal to one, the orthogonality still enables inverting a
matrix using inner products.

\subsubsection{{\akadvanced} Unitary matrices lead to energy conservation}

%This text will assume hereafter the transform matrices are unitary unless otherwise stated.
%Transforms based on unitary matrices have several advantages and properties. The most evident is that obtaining the inverse $\bx = \bA^H \bX$ avoids the matrix inversion of the general case \equl{inverse_transform}.

Another interesting property of a unitary matrix (or transform) $\bA$ is that it does not change the norm of the input vector, that is
\[
||\bX|| = ||\bA \bx|| = ||\bx||.
\]
As indicated in \equl{vector_energy}, the squared norm of a vector corresponds to its energy and, consequently, a unitary matrix \emph{conserves energy} when $\bx$ is transformed into $\bX$.
%they conserve energy or, equivalently, the norms of $\bx$ and $\bX$ are the same. In other words, as indicated by \equl{vector_energy}, the squared norm of a vector is its energy and for $\bA$ unitary, one has
%where $|| \cdot ||$ is the Euclidean norm of a vector.
The following example summarizes this result.

\bTheorem
\label{th:parseval} \textbf{Parseval theorem for unitary block transforms.} If $\bA$ is unitary, the transform $\bX=\bA \bx$ conserves energy ($||\bX||^2=||\bx||^2$) or, equivalently, $||\bX||=||\bx||$.

Proof sketch: First, recall that
\begin{align*} 
        \label{eq:norm_sum}
        \lVert\ba + \bb\rVert^2 &= \langle\ba+\bb, \ba+\bb\rangle
                                                                                = \langle\ba,\ba\rangle + \langle\ba,\bb\rangle + \langle\bb,\ba\rangle + \langle\bb,\bb\rangle \\
                                                                                &= \lVert\ba\rVert^2 + \lVert\bb\rVert^2 + 2 \langle\ba,\bb\rangle.
\end{align*}
To visualize that $\lVert\bX\rVert = \lVert \bx\rVert$, for simplicity, assume that $N=2$ and $\bx=[\alpha,~\beta]^T$. Hence, $\bX = \alpha \bA(:,0) + \beta \bA(:,1)$ and, because $\bA$ is unitary,  $||\bA(:,0)||=||\bA(:,1)||=1$ and $\langle\bA(:,0),\bA(:,1)\rangle=0$. Therefore, the squared norm is $\lVert\bX\rVert^2 = \alpha^2 \lVert\bA(:,0)\rVert^2 + \beta^2 \lVert\bA(:,1)\rVert^2 + 2\langle\bA(:,0),\bA(:,1)\rangle = \alpha^2 + \beta^2 = \lVert\bx\rVert^2$. Same reasoning applies for $N>2$ as indicated by \equl{parsevalViaInnerProduct}.
\eTheorem

\bExample \textbf{If the transform is unitary in a coding application, the total error energies in time and frequency domains are equal}.
In a coding application that uses a unitary transform, the error energy in time and ``frequency'' domains are the same. In other words, assume that $\bA$ is a unitary matrix and the original vector $\bx = \bA^H \bX$ is obtained from the coefficients $\bX$. If the original coefficients $\bX$ are encoded and represent an approximation $\hat \bX$, the error vector $\be_f=\bX - \hat \bX$ has the same norm $||\be_f||=||\be_t||$ of the error vector in time domain $\be_t = \bx - \hat \bx$.

To better understand this fact, one can write:
\begin{equation}
\lVert \be_f \rVert = \lVert\bX-\hat \bX \rVert = \lVert \bA (\bx- \hat \bx) \rVert = \lVert \bA \be_t \rVert = \sqrt{(\bA\be_t)^H \bA\be_t} = \lVert \be_t \rVert.
\label{eq:sameErrorWhenTransformIsUnitary}
\end{equation}
The proof above uses the reasoning adopted in \equl{parsevalViaInnerProduct}.
\eExample

\subsubsection{{\akadvanced} Orthogonal but not unitary also allows easy inversion}
\label{sec:ortho_not_unit}

Orthogonal but not unitary matrices also lead to useful transforms. They do not lead to energy conservation but the coefficients are conveniently obtained by inner products, as for unitary matrices. An important detail is that when used in transforms, the inner products with the basis vectors of a orthogonal matrix must be normalized by the norms, as discussed for the DFT. This aspect is further discussed in the sequel. The term ``energy'' is used here instead of vector ``norm'' because the results are valid not only for vectors, but also for continuous-time signals, for example.

An orthogonal matrix $\bB$ can be written as $\bB = \bA \bD$, where $\bA$ is unitary and $\bD=\textrm{diag}[\sqrt{E_1},\ldots,\sqrt{E_N}]$ is a diagonal matrix with the norm of the $i$-th column of $\bB$, or square root of its energy $E_i$, composing the main diagonal. The inverse of a matrix with orthogonal columns is
\[
\bB^{-1}=\textrm{diag}[1/\sqrt{E_1},\ldots,1/\sqrt{E_N}] \bA^H = \textrm{diag}[1/E_1,\ldots,1/E_N] \bB^H.
\]
\ignore{
as the following steps prove
\begin{align*}
\bB \bB^{-1} &= \bI \\
 \bA ~ \textrm{diag}[e_1,\ldots,e_N] \bB^{-1} &= \bI \\
 \textrm{diag}[e_1,\ldots,e_N] \bB^{-1} &= \bA^H \\
 \bB^{-1} &= \textrm{diag}[1/e_1,\ldots,1/e_N] \bA^H.
\end{align*}
The last two steps simply pre-multiply both sides by the inverses of $\bD$ and $\bA$.
}

\bExample \textbf{Inversion of orthogonal but not unitary matrix}.
For example, $[3,3]^T$ and $[-1,1]^T$ form a basis for $\Re^2$ with energies $E_1=18$ and $E_2=2$, respectively. The matrix $\bB=[3, -1; 3, 1]$ with orthogonal columns can be written as $\bB=\bA [\sqrt{18}, 0; 0, \sqrt{2}]$, where $\bA=[3/\sqrt{E_1}, -1/\sqrt{E_2}; 3/\sqrt{E_1}, 1/\sqrt{E_2}]$ is orthonormal. The inverse of $\bB$ is $\bB^{-1}=[1/\sqrt{E_1}, 0; 0, 1/\sqrt{E_2}] A^H$ or, equivalently, $\bB^{-1}=[1/E_1, 0; 0, 1/E_2] B^H$.
\eExample

If all columns of an orthogonal matrix $\bB$ have the same energy $E_i=E, \forall i$, its inverse is
\begin{equation}
\bB^{-1}  = \frac {1} {E} \bB^H.
\label{eq:nonorthogonaltransform}
\end{equation}

%This result is used in the next sections. 
\bExample \textbf{About the orthogonal (non-unitary) DFT transform}.
For example, in \equl{dft_basis_function} or, equivalently, \equl{dft}, the DFT transform was defined by an orthogonal matrix with the same energy $E=1/N$ for all basis functions. Therefore, \equl{nonorthogonaltransform} leads to $\bB^{-1}=N \bB$, which is equivalent to \equl{dfts_relation_nonorthogonal}.
%(see \equl{forms_of_fft}) and, the DFT is defined such that the factor $1/N$ is present in the inverse transform such .
\eExample

The next section focuses in Fourier transforms and series. The connections with block transforms are plenty. For example, extending \equl{nonorthogonaltransform} for continuous-time signals, the basis functions for Fourier series, other than the one for $k=0$, have energy $E=T_0$ over the duration of a fundamental period $T_0$. Hence, the inverse transform in \equl{fourier_series} has the factor $1/T_0$.

\section{Fourier Transforms and Series}

The transforms in this section adopt eternal (infinite duration) sinusoids as basis functions and complement the DFT, which uses finite-length basis functions. For mathematical convenience, complex exponentials are also used given that they conveniently represent sinusoids.

Using sinusoids as basis functions is very useful in many applications. Depending on the type of signal to be analyzed, \emph{there are four pairs of analysis and synthesis transform equations that adopt eternal sinusoids as basis functions}. These four pairs can be collectively called \emph{Fourier analysis tools} and are discussed in the sequel.

%\subsection{Fourier pairs with infinite duration basis functions}

The reason for not having only one transform pair when dealing with Fourier analysis is that two properties of the signal to be analyzed must be taken in account: whether the signal is continuous or discrete in time, and periodic or non-periodic. Covering all possible combinations, \tabl{fourier_equations} lists the four pairs of equations to conduct Fourier analysis with eternal sinusoids (later we will discuss the DFT, which is used for finite-duration signals).

\begin{table}
 \centering
 \caption{The four pair of equations for Fourier analysis with eternal sinusoids and the description of their spectra: $c_k$, $X(f)$ (or $X(\aw)$), $X[k]$ and $X(e^{j\dw})$. For periodic continuous and discrete-time signals the periods are $T_0$ and $\Nperiod$, respectively, with fundamental (angular) frequencies $\aw_0=2\pi/T_0$ rad/s and $\dw_0=2\pi/\Nperiod$ rad. For continuous-time signals, one can alternatively use the linear frequency $f$ instead of $\aw = 2\pi f$, such that $f_0 = 1/T_0$ is the fundamental frequency in Hz.\label{tab:fourier_equations}}
 \begin{tabularx}{\textwidth}{cCC}
 \toprule
 & \textbf{Continuous-time} & \textbf{Discrete-time} \\ \midrule
 & Fourier \textbf{series} (FS) & Discrete-time Fourier series (\textbf{DTFS}) \\
\multirow{4}{*}{\parbox{1.4cm}{\textbf{Periodic}}} & $c_k = \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) e^{- j 2 \pi k f_0 t} \textrm{d}t$ &  \\ 
& $x(t) = \sum_{k=-\infty}^\infty c_k e^{j 2 \pi k f_0 t}$ & $X[k] =  \frac{1}{\Nperiod} \sum_{n=\langle \Nperiod \rangle} x[n] e^{-j k \dw_0 n} $ \\ \cline{2-2}
& or $c_k = \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) e^{- j k \aw_0 t} \textrm{d}t$ & $x[n] = \sum_{k=\langle \Nperiod \rangle} X[k] e^{j k \dw_0 n} $ \\ 
& $x(t) = \sum_{k=-\infty}^\infty c_k e^{j k \aw_0 t}$ &  \\ 
  \midrule
  &  Fourier \textbf{transform} (FT) & Discrete-time Fourier transform (\textbf{DTFT}) \\
	\multirow{4}{*}{\parbox{1.4cm}{\textbf{Non-periodic}}} &
$X(f) = \int_{-\infty}^\infty x(t) e^{- j 2 \pi f t} \textrm{d}t$ &  \\
& $x(t) = \int_{-\infty}^\infty X(f) e^{j 2 \pi f t} \textrm{d}f$ & $X(e^{j\dw}) = \sum_{n=-\infty}^\infty x[n] e^{-j \dw n}$ \\ \cline{2-2}
& or $X(\aw) = \int_{-\infty}^\infty x(t) e^{- j \aw t} \textrm{d}t$ & $x[n] = \frac{1}{2 \pi} \int_{\langle 2 \pi\rangle} X(e^{j \dw}) e^{j \dw n} \textrm{d} \dw$ \\
& $x(t) = \frac{1}{2 \pi} \int_{-\infty}^\infty X(\aw) e^{j \aw t} \textrm{d} \aw$ & \\ \bottomrule
\end{tabularx}
\end{table}


%\begin{table}
 %\centering
 %\caption{The four pair os equations for Fourier analysis with eternal sinusoids and the description of their spectra: $c_k$, $X(f)$ (or $X(\aw)$), $X[k]$ and $X(e^{j\dw})$.\label{tab:fourier_equations}}
 %\begin{tabularx}{\textwidth}{cCC}
 %\toprule
 %& \textbf{Continuous-time} & \textbf{Discrete-time} \\ \midrule
 %\textbf{Periodic} & Fourier \textbf{series} & Discrete-time Fourier series (\textbf{DTFS}) \\
 %& $c_k = \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) e^{- j 2 \pi k f_0 t} \textrm{d}t$ & $X[k] =  \frac{1}{N} \sum_{n=\langle N\rangle} x[n] W_N^{nk}$ \\
 %& $x(t) = \sum_{k=-\infty}^\infty c_k e^{j 2 \pi k f_0 t}$ & $x[n] = \sum_{k=\langle N\rangle} X[k] W_N^{-nk}$ \\ \midrule
  %&  Fourier \textbf{transform} & Discrete-time Fourier transform (\textbf{DTFT}) \\
%\textbf{Non-} & $X(f) = \int_{-\infty}^\infty x(t) e^{- j 2 \pi f t} \textrm{d}t$ & $X(e^{j\dw}) = \sum_{n=-\infty}^\infty x[n] e^{-j \dw n}$ \\
%\textbf{periodic} & $x(t) = \int_{-\infty}^\infty X(f) e^{j 2 \pi f t} \textrm{d}f$ & $x[n] = \frac{1}{2 \pi} \int_{\langle 2 \pi\rangle} X(e^{j \dw}) e^{j \dw n} d\dw$ \\ \bottomrule
%\end{tabularx}
%\end{table}

It can be seen from \tabl{fourier_equations} that the terminology \emph{series} is used when the signal to be analyzed is periodic. In this case the spectrum is discrete in frequency and represented by \emph{coefficients} $c_k$ or $X[k]$. Obtaining a Fourier series is a special case of the general procedure of representing a function (in this case, a periodic signal) via a series expansion such as Taylor's, Laurent's, etc. In contrast, the spectrum of non-periodic signals is continuous in frequency and the tools to analyze non-periodic signals are called \emph{transforms}. Note this nomenclature is not 100\% consistent with block transforms in the sense that the DFT, which has a discrete spectrum, is called ``transform''.

As highlighted in \tabl{fourier_tools}, in Fourier analysis, there is an interesting and maybe not evident duality between the time and frequency domains: periodicity in one domain leads to a discrete function in the other domain, while  non-periodicity leads to a continuous function. For example, \tabl{fourier_tools} shows that the spectrum of a discrete-time signal $x[n]$ is always periodic.

\begin{table}
 \centering
 \caption{Duality of periodicity and discreteness in Fourier analysis. \label{tab:fourier_tools}}
\begin{tabularx}{\textwidth}{|l|C|C|}
 \toprule
 & \textbf{Continuous-time} $x(t)$ & \textbf{Discrete-time} $x[n]$ \\ \midrule
 \multirow{3}{*}{\textbf{Periodic}}  & Fourier \textbf{series} & Discrete-time Fourier series (\textbf{DTFS}) \\
  & Basis: $e^{j \aw_0 k t}$ & Basis: $e^{j \frac{2 \pi}{\Nperiod}k n}$ \\
  & $c_k$ non-periodic, $k$ discrete & $X[k]$ periodic, $k$ discrete \\
 \hline
 \multirow{3}{*}{\textbf{Non-periodic}}         &  Fourier \textbf{transform} & Discrete-time Fourier transform (\textbf{DTFT}) \\
 & Basis: $e^{j \aw t}$ & Basis: $e^{j \dw n}$ \\
         & $X(\aw)$ non-periodic, $\aw$ continuous  & $X(e^{j\dw})$ periodic, $\dw$ continuous     \\
          \bottomrule
\end{tabularx}
\end{table}

\tabl{fourier_tools} and \tabl{fourier_equations} indicate that the DTFS is the only pair that has discrete signals in both domains, time and frequency. This indicates that the DTFS is related to the DFT. In fact, the DFT and DTFS are mathematically equivalent in case $\Nperiod=N$, with the exception of the normalization factor $1/N$ in \equl{dft_basis_function}. But their interpretation has a remarkable distinction: 
The basis functions of the block transform DFT in \equl{dft_basis_function} are finite-duration sequences or, equivalently, vectors, with dimension $N$, while the basis functions of the DTFS are infinite-duration complex exponentials $e^{j \frac{2 \pi}{\Nperiod}k n}$ of period $\Nperiod$. Hence, the DTFS and DFT are more naturally interpreted and used for (infinite-duration) periodic and finite-duration signals, respectively. 

%These discrete sequences can be organized as vectors $\bx$ (time domain) and $\bX$ (frequency domain) and the DTFS analysis and synthesis operations can be formulated as matrix multiplications. In fact, the DTFS is equivalent to the DFT block transform. 
Another reason for keeping their names different is that the DFT (via FFT algorithms) is often used in computers, digital oscilloscopes, spectrum analyzers, etc., to analyze non-periodic and even continuous-time signals that were digitized. Hence, there are aspects that must be studied to apply DFT in these cases, such as the relation between an original spectrum $X(\aw)$ of a continuous-time signal and $X[k]$, the one obtained by the DFT of its discrete-time version. Therefore, in spite of the operations in DTFS and DFT being mathematically equivalent apart from a normalization constant, it is convenient to restrict the discussion of DTFS to the analysis of periodic and discrete-time signals and call DFT the tool for finite-duration signals, which in practice is used for analyzing any digitized signal.

Besides their relation to the DFT, there are many other similarities and relations within the four Fourier pairs in \tabl{fourier_tools} themselves. For example, transforms are meant for non-periodic signals but, as discussed in Appendix~\ref{app:impulseMotivation}, impulses can be used to also represent periodic signals via a transform (instead of a series). The next sections will discuss some of these relations.

One point that will not be explored in this text is the important aspect of convergence. Similar to the fact that a vector outside the span of a given basis set cannot be perfectly represented by the given basis vectors, there are signals that cannot be represented by Fourier transforms or series. In other words, the transform/series may not converge to a perfect representation even when using an infinite number of basis functions. A related aspect is the well-known \emph{Gibbs phenomenon}\index{Gibbs phenomenon}: when the signal $x(t)$ has discontinuities (such as $u(t)$ at $t=0$), the Fourier representation has to use an infinite number of basis functions. Any truncation of this number (i.\,e., using a finite number of basis functions) leads to ripples in the reconstructed signal.
Given the adopted emphasis in the engineering application of transforms, 
%in detriment to mathematical issues, 
this text assumes the signals are well-behaved and the transforms and series properly converge.

Complementing \tabl{fourier_equations}, \tabl{fourier_units} indicates the assumed units when the signals in time domain are given in volts and is useful to observe the difference for continuous and discrete spectra. 

\begin{table}
 \centering
 \caption{Units for each pair of Fourier equations in \tabl{fourier_equations}.\label{tab:fourier_units}}
 \begin{tabularx}{\textwidth}{cCC}
\toprule
 & \textbf{Continuous-time} & \textbf{Discrete-time} \\
 & x(t) in volts & x[n] in volts \\
\midrule
 \textbf{Periodic} & Fourier series & Discrete-time Fourier series (DTFS) \\
 & $c_k$ (volts) & $X[k]$ (volts) \\
 \midrule
 &  Fourier transform & Discrete-time Fourier transform (DTFT) \\
        \textbf{Non-periodic} & $X(f)$ (volts/Hz) & $X(e^{j\dw})/(2 \pi)$ (volts/radians) \\
  \bottomrule
\end{tabularx}
\end{table}

\subsection{Fourier series for continuous-time signals}

The Fourier series for a continuous-time signal uses an infinite number of complex \emph{harmonic} sinusoids $e^{j k \aw_0 t}, k \in \ZZ,$ as basis functions. These functions allow to represent any periodic signal $x(t)$ with period $T_0$ (i.\,e., $x(t)=x(t+T_0), \forall t$), where $\aw_0 = \frac{2 \pi}{T_0} = 2 \pi f_0$ (rad/s). The frequency $f_0$ in Hz (or $\aw_0$ in rad/s) is called the \emph{fundamental frequency}.

\subsubsection{{\akadvanced} On the orthogonality of sinusoids}
\label{sec:orthogonality_sinusoids}

The results in this subsection are useful for proving Fourier pairs. They can be eventually skipped 
in case the mathematical proofs based on the orthogonality of sinusoids are not of interest at this moment.

%theorem proves that any pair of infinite-duration (\emph{eternal}) sinusoids are orthogonal unless they have the same frequencies. This fact will be used in the section about Fourier transforms.

%\bExample 
\bResult
\label{ex:eternal_sinusoids} \textbf{Eternal (infinite-duration) sinusoids at different frequencies are orthogonal}. If $\aw_1 \ne \aw_2$, then 
\[
\int_{-\infty}^\infty A_1 \sin (\aw_1 t + \phi_1) A_2 \sin (\aw_2 t + \phi_2) \textrm{d}t = 0.
\] %that is $A_1 \sin (\aw_1 t + \phi_1) \perp A_2 \sin (\aw_2 t + \phi_2)$.

\textbf{Proof}: 
%Any sinusoid or cosine can be expressed as $A \sin (\aw t + \phi)$. 
To simplify notation, let $\aw_1 t + \phi_1 = \alpha$ and $\aw_2 t + \phi_2 = \beta$. From \equl{sinatimessinb}:
\[
A_1 \sin (\alpha) A_2 \sin (\beta) = \frac{1}{2}[\cos (\alpha-\beta) - \cos(\alpha+\beta)].
\]
First let us assume that the interval from $-\infty$ to $\infty$ corresponds to an integer number of periods
for both $\cos (\alpha-\beta)$ and  $\cos(\alpha+\beta)$. In this case, it is easy to see that the following result is zero:
\[
\int_{-\infty}^\infty [A_1 \sin (\alpha) A_2 \sin (\beta)] \textrm{d}t = \frac{1}{2}[\int_{-\infty}^\infty \cos (\alpha-\beta) \textrm{d}t -  \\ \int_{-\infty}^\infty \cos(\alpha+\beta) \textrm{d}t] = 0,
\]
unless $\alpha=\beta$ (in this case $\cos (\alpha-\beta)=1$).
But the astute reader may be concerned that the range$]-\infty,\infty[$ does not necessarily represent an integer number of periods. In fact, using the 
reasoning that we adopt for finite-duration signals, we should have the integral interval (recall the concept of
commensurate frequencies in \exal{commensurate}) corresponding to a multiple
of the periods of $\alpha-\beta$ and $\alpha+\beta$. But things are different when the integral interval has infinite duration.
In this case, a sinusoid alternates between positive and negative values, and over an infinite interval, the areas under these positive and negative sections cancel each other out. As a result, the integral converges to zero.
In spite of not being rigorously stated, this result is important to understand the proof of the expression for the Fourier transform (see, e.\,g., Section~\ref{sec:ctft_hz}).
\eResult
%\eExample

\bResult \label{ex:cos_sin_orthogonality} \textbf{Cosine and sine at the same frequency are orthogonal}. Similar to \exal{eternal_sinusoids}, it can be shown that:
\[
\int_{-\infty}^\infty A_1 \cos (\aw_0 t ) A_2 \sin (\aw_0 t) \textrm{d}t = A_1 A_2 / 2 \int_{-\infty}^\infty  \sin (2 \aw_0 t) \textrm{d}t = 0,
\]
which used \equl{sin2a}.
\eResult 

The following result is useful when the integration interval is the fundamental period $T_0$ or a multiple of $T_0$.

\bResult \label{ex:harmonic_sinusoids} \textbf{Harmonic sinusoids are orthogonal when the inner product (integral) is over a time interval multiple of the fundamental period}. If $\aw_0 = 2 \pi/ T_0$ is the fundamental frequency in rad/s and $k \ne m$, then 
\[
\int_{<T_0>} A_1 \sin (k \aw_0 t + \phi_1) A_2 \sin (m \aw_0 t + \phi_2) \textrm{d}t = 0.
\] %that is $A_1 \sin (\aw_1 t + \phi_1) \perp A_2 \sin (\aw_2 t + \phi_2)$.

\textbf{Proof}: 
%Any sinusoid or cosine can be expressed as $A \sin (\aw t + \phi)$. 
%To simplify notation, let $\aw_1 t + \phi_1 = \alpha$ and $\aw_2 t + \phi_2 = \beta$. 
Note that the sinusoids are assumed here to be eternal, but the result is also valid in case they have a finite-duration coinciding with the time duration of the integral.
From \equl{sinatimessinb}:
\begin{flalign*}
& \int_{<T_0>} A_1 \sin (k \aw_0 t + \phi_1) A_2 \sin (m \aw_0 t + \phi_2) \textrm{d}t  =  & \\
& \frac{1}{2} \left[ \int_{<T_0>} \cos ((k-m) \aw_0 t + \phi_1 - \phi_2) \textrm{d}t - \int_{<T_0>} \cos ( (k+m) \aw_0 t + \phi_1 + \phi_2) \textrm{d}t \right]  = 0. & 
\end{flalign*}
The cosine with angular frequency $(k+m) \aw_0$ of the second parcel has a period $T=T_0/(k+m)$ and its integral over $T_0$ is zero, because $T_0$ is an interval corresponding to an integer number of periods $T$. The same reasoning can be applied to the cosine with angular frequency $(k-m) \aw_0$ with period $T_0/|k-m|$, which is especially easy to observe when $k>m$. If $k<m$, because $\cos(x)=\cos(-x)$, the cosine argument of the first parcel can be changed to $(m-k) \aw_0 t + \phi_2 - \phi_1$ to help concluding that this integral over $T_0$ is also zero.
\eResult

\bResult \textbf{On the energy of Fourier basis functions}.
\exal{harmonic_sinusoids} proves that the Fourier series basis functions are orthogonal. But they are not orthonormal and the energy over a duration $T_0$ is
\[
E = \int_{\langle T_0\rangle} { \left| e^{j k \aw_0 t} \right|^2} \textrm{d}t = \int_0^{T_0} \textrm{d}t = T_0,
\]
which is the normalization factor that will appear in the Fourier series equations, similar to the result for block transforms in \equl{nonorthogonaltransform}.
\eResult

\bResult \textbf{Proof of the Fourier series pair}.
One can use the reasoning in Theorem~\ref{th:analysis_inner_prod} (page \pageref{th:analysis_inner_prod}) to prove the Fourier series equations. Writing the synthesis equation
\[x(t)  =  \sum_{k=-\infty}^\infty c_k e^{j 2 \pi k f_0 t}\]
is similar to
$\bx=\sum_{i=1}^N \alpha_i \bb_i$. Calculating $\langle\bx,\bb_i\rangle$ is therefore equivalent to the inner product
between $x(t)$ and the $m$-th basis function $e^{j 2 \pi m f_0 t}$, which is written as:
\[
\langle x(t),e^{j 2 \pi m f_0 t}\rangle = \int_{\langle T_0\rangle} x(t) e^{- j 2 \pi m f_0 t} \textrm{d}t = \int_{\langle T_0\rangle} \left[\sum_{k=-\infty}^\infty c_k e^{j 2 \pi k f_0 t} \right] e^{- j 2 \pi m f_0 t} \textrm{d}t.
\]
(recall from \tabl{inner_products} that $\langle x(t),y(t)\rangle=\int x(t)y^*(t)\textrm{d}t$ the inner product is defined with
the complex conjugate of the second argument when the signals are complex). Due to the properties of the inner product one can write
\begin{equation}
\langle x(t),e^{j 2 \pi m f_0 t}\rangle = \sum_{k=-\infty}^\infty \left[\int_{\langle T_0\rangle} c_k e^{j 2 \pi (k-m) f_0 t} \textrm{d}t\right] = T_0 c_m.
\label{eq:fourier_series_proof}
\end{equation}
The last step is due to the orthogonality of the basis functions:
\[\langle e^{j 2 \pi k f_0 t}, e^{j 2 \pi m f_0 t}\rangle = \int_{\langle T_0\rangle} e^{j 2 \pi k f_0 t} e^{- j 2 \pi m f_0 t} \textrm{d}t = \left\{
\begin{array}{ccc}
T_0, & ~~k=m \\
0, & ~~k \ne m.\\
\end{array}
\right.\]
In summary, the previous steps used the basis functions orthogonality to prove that the analysis equation
\[
c_k = \frac{1}{T_0} \langle x(t),e^{j 2 \pi k f_0 t}\rangle = \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) e^{- j 2 \pi k f_0 t} \textrm{d}t
\]
corresponds to using the inner product as in Theorem~\ref{th:analysis_inner_prod} (page \pageref{th:analysis_inner_prod}),
and taking in account that here the basis functions have an energy $T_0$.
\eResult

\subsubsection{Fourier series using complex exponentials}

Looking at \tabl{fourier_equations}, the basis $b(t)=1, \forall t$, corresponding to $k=0$ in a Fourier series, is responsible for representing the DC level of $x(t)$. All other basis functions have a frequency
\[f_k = k f_0.\]
When $k>1$, frequencies higher than $f_0$ are generated and, consequently, a period $T_k = T_0 / k$ that is smaller than $T_0$. Therefore, all basis functions but the one for $k=0$ are periodic in $T_0$.
The frequencies $f_k$ that obey a relation $f_k = k f_0$ with respect to a fundamental frequency $f_0$ are called \emph{harmonics}\index{Harmonics}. The frequency $f_2 = 2 f_0$ is called the second harmonic, $3 f_0$ is the third harmonic and so on.

It seems intuitive that Fourier series should not be used to represent non-periodic signals. In fact, the basis functions even depend on the period $T_0$ of the signal to be analyzed. On the other hand, because the signal is periodic, it suffices to find coefficients that represent the signal $x(t)$ during a single period. The trick is then to use these coefficients (obtained with inner products of duration $T_0$) to multiply eternal complex exponentials and properly represent the periodic (and consequently infinite-duration) $x(t)$. Hence, the Fourier series pair is:
%\begin{array}{ccc}
%\end{array}
%left-align from tex.stackexchange.com/questions/28650/how-can-i-use-an-align-environment-flush-left
\begin{equation}
\left\{
\begin{array}{llll}
%\begin{aligned}
c_k  &=&  \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) e^{- j 2 \pi k f_0 t} \textrm{d}t  & ,  k=-\infty,\ldots,-1,0,1,\ldots,\infty   \\
& & &  \\
x(t) &=& \sum_{k=-\infty}^\infty c_k e^{j 2 \pi k f_0 t}   & ,  \forall t.
\label{eq:fourier_series}
%\end{aligned}
\\ \end{array}
\right.  %\raisetag{-1ex}
\end{equation}


The negative frequencies $f_k, k=-\infty,\ldots,-2,-1$ exist for mathematical convenience.
%However, the reader should keep in mind that negative frequencies are used for mathematical convenience,
They allow, for example, to represent a cosine as a sum of complex exponentials with ``positive'' and ``negative'' frequencies as in \equl{cos_as_complex_exponentials}. %, and do not exist ``in practice''. => How about in DSP, it is ``practice"!

\begin{figure}[!htb]
  \begin{center}
    \subfigure[$k=0$ (DC)]{\label{fig:complexexponential0}\includegraphics[width=5cm]{Figures/complexexponential0}}
    \subfigure[$k=1$ (fundamental)]{\label{fig:complexexponential1}\includegraphics[width=5cm]{Figures/complexexponential1}}
    \\
    \subfigure[$k=2$]{\label{fig:complexexponential2}\includegraphics[width=5cm]{Figures/complexexponential2}}
    \subfigure[$k=-2$]{\label{fig:complexexponential3}\includegraphics[width=5cm]{Figures/complexexponential3}}
  \end{center}
  \caption{Fourier series basis functions for analyzing signals with period $T_0 = 1/50$ seconds. Because the basis functions are complex-valued signals, the plots show their real (top) and imaginary (bottom) parts.}
  \label{fig:complexexponentials}
\end{figure}

\bExample \textbf{Basis functions of Fourier series}.
\figl{complexexponentials} shows four basis functions for analyzing signals with period $T_0 = 1/50=0.02$ seconds. Note that the basis for $k=0$ is a real signal while the others are complex. The basis for $k=2$ has the same real part of the one for $k=-2$ and the negative of the imaginary part. This is due to the fact that cosines are even functions $\cos(x)=\cos(-x)$ while sines are odd functions $\sin(x)=-\sin(-x)$ (see Section~\ref{sec:even_odd}).
\eExample

Similar to the DFT case discussed in Section~\ref{sec:dft_symmetry},
when the Fourier series is used to analyze a real signal $x(t)$, the symmetry of the basis functions for $k$ and $-k$ leads to interesting properties for the coefficients: the real part of $c_k$ and their magnitude compose even sequences while the imaginary part and their phase compose odd sequences.
In summary, for real signals $x(t)$: $|c_k| = |c_{-k}|$, $\real{c_k} = \real{c_{-k}}$, $\angle {c_k}=-\angle {c_{-k}}$ and $\imag{c_k} = -\imag{c_{-k}}$. These properties can be written compactly as 
\begin{equation}
c_{-k} = c_k^*,
\label{eq:hermitianSymmetry}
\end{equation}
which corresponds to \emph{Hermitian symmetry}\index{Hermitian symmetry} for the Fourier series, as for the DFT in Section~\ref{sec:dft_symmetry}.
In fact, the properties based on the symmetry of the basis functions and, eventually, also of the signal to be analyzed ($x(t)$ in this case), are valid not only for \equl{fourier_series} but for other Fourier pairs.

Another aspect due to the symmetry is often invoked: because any signal $x(t)$ can be decomposed  as the sum $x(t)=x_e(t) + x_o(t)$ of an even part $x_e(t)$ and an odd part $x_o(t)$ (see Section~\ref{sec:even_odd}), the cosines are in charge of representing $x_e(t)$, while the sines represent $x_o(t)$. This leads to conclusions such as: if $x(t)$ is real and even,
%i.\,e., $x(t)\in \Re$ and $x(t)=x_e(t)$,
then $\angle{c_k}=\imag{c_k}=0, \forall k$.

\figl{simplespectrum} shows the Fourier coefficients for the periodic and real-valued signal $x(t)=4 + 10\cos(2 \pi 50 t + 2) + 4\sin(2 \pi 150 t - 1)$, with fundamental period $T_0 = 1/f_0 = 1/50=0.02$~s.
The graphs in \figl{simplespectrum} are called the \emph{spectrum}\index{Spectrum} of the signal. Because the coefficients $c_k$ are in general complex numbers, a spectrum is represented using the polar or rectangular form of complex numbers.  \figl{simplespectrum} uses the polar format. One can notice that, because $x(t)$ is real, $c_{-k} = c_k^*$, i.\,e., the spectrum presents the Hermitian symmetry of \equl{hermitianSymmetry}.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/simplespectrum}              
        \caption{Spectrum of $x(t)=4 + 10\cos(2 \pi 50 t + 2) + 4\sin(2 \pi 150 t - 1)$.\label{fig:simplespectrum}}
\end{figure}

\bExample \textbf{Continuous-time Fourier series coefficients by inspection.}
\label{ex:coefficients_by_inspection}
When the periodic signal $x(t)$ is composed by a sum of sines and cosines, it is convenient to obtain the Fourier series coefficients by inspection. For example, assume the signal $x(t)=4 + 10\cos(2 \pi 50 t + 2) + 4\sin(2 \pi 150 t - 1)$ of \figl{simplespectrum}. Using Euler's, one has
\[
10\cos(\textcolor{red}{2 \pi 50 t} + 2)=5 \textcolor{red}{e^{j 2 \pi 50 t}} e^{j 2} + 5 \textcolor{red}{e^{-j 2 \pi 50 t}}  e^{-j 2}
\]
and
\begin{eqnarray*}
4\sin( \textcolor{red}{2 \pi 150 t} - 1) & = & \frac{4}{2j} [\textcolor{red}{ e^{j 2 \pi 150 t}} e^{-j 1} -  \textcolor{red}{e^{-j 2 \pi 150 t}}  e^{j 1}] \\
 & = & \frac{2}{e^{j \pi/2}} [\textcolor{red}{e^{j 2 \pi 150 t}} e^{-j 1} + e^{j \pi}  \textcolor{red}{e^{-j 2 \pi 150 t}}  e^{j 1}] \\
 & = & 2\textcolor{red}{e^{j 2 \pi 150 t}} e^{-j (1+\pi/2)} + 2 \textcolor{red}{e^{-j 2 \pi 150 t}}  e^{j (1+\pi/2)}.
\end{eqnarray*}

Hence, the only non-zero coefficients are $c_0 = 4$, $c_1=5e^{j2}$, $c_3=2e^{-j(1+\pi/2)}$, $c_{-1}=5e^{-j2}$, $c_{-3}=2e^{j(1+\pi/2)}$, with $1+\pi/2 \approx 2.57$ as indicated in \figl{simplespectrum}, which 
% \figl{simplespectrum} 
shows the magnitude and phase graphs of the Fourier series coefficients for representing $x(t)$. 
%magnitude and phase are even and odd functions, respectively.
\eExample

\subsubsection{Trigonometric Fourier series}

%Most undergraduate textbooks discuss the trigonometric Fourier series, which are briefly described in the sequel.
Instead of complex exponentials $e^{j k \aw_0 t}$ with negative frequencies as in \equl{fourier_series}, the Fourier series can be written in terms of cosines and sines with positive frequencies only. In this case, one has a set of coefficients for the cosines and another set for the sines that are usually called $a_k$ and $b_k$, respectively, and related as follows:
\begin{equation}
\left\{
\begin{array}{ccc}
a_k &= \frac{2}{T_0} \int_{\langle T_0\rangle} x(t) \cos(2 \pi k f_0 t) \textrm{d}t\quad,k=1,2,\ldots,\infty \\
b_k &= \frac{2}{T_0} \int_{\langle T_0\rangle} x(t) \sin(2 \pi k f_0 t) \textrm{d}t\quad,k=1,2,\ldots,\infty \\
a_0 &= \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) \textrm{d}t \\
&\\
x(t) &= a_0 + \sum_{k=1}^\infty \left( a_k \cos (2 \pi k f_0 t) + b_k \sin (2 \pi k f_0 t) \right).
\label{eq:cos_sin_fourier_series}
\\ \end{array}
\right.
\end{equation}

Comparing the expression for $x(t)$ in \equl{fourier_series} and \equl{cos_sin_fourier_series}, they are related via Euler's formula (see \equl{euler})~
\[e^{j k \aw_0 t} = \cos(k \aw_0 t) + j \sin(k \aw_0 t).\]
This allows to write $c_0 = a_0$ and, for $k > 0$:
\[
c_k = \frac{1}{2} (a_k - j b_k)~~\textrm{and}~~c_{-k} = \frac{1}{2} (a_k + j b_k).
\]

\ignore{
and related to $c_k$ in \equl{fourier_series} as $c_k = a_k + j b_k$.
From \equl{fourier_series} and using Euler's formula (see \equl{euler})
\[e^{j k \aw_0 t} = \cos(k \aw_0 t) + j \sin(k \aw_0 t),\]
one can write
\begin{align*}
c_k & = & \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) e^{- j 2 \pi k f_0 t} \textrm{d}t \\
a_k + j b_k & = & \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) [\cos(- 2 \pi k f_0 t) + j \sin(- 2 \pi k f_0 t)] \textrm{d}t \\
a_k + j b_k & = & \frac{1}{T_0} \int_{\langle T_0\rangle} x(t) \cos(2 \pi k f_0 t) \textrm{d}t - j \frac{1}{T_0} \int_{\langle T_0\rangle} x(t)  \sin(2 \pi k f_0 t) \textrm{d}t
\end{align*}
where the last step is due to the fact that $\cos$ and $\sin$ are even and odd functions, respectively.
}

\equl{cos_sin_fourier_series} is called the trigonometric Fourier series while \equl{fourier_series} is the exponential version. %Having only one set of coefficients $c_k$ by allowing negative frequencies is more convenient and this text adopts it.

\bExample \textbf{Bilateral and unilateral spectrum representations.}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/unilateralsimplespectrum}            
        \caption{Unilateral spectrum of (real) signal $x(t)=4 + 10\cos(2 \pi 50 t + 2) + 4\sin(2 \pi 150 t - 1)$.\label{fig:unilateralsimplespectrum}}
\end{figure}

The spectrum of \figl{simplespectrum} is called \emph{bilateral} because the negative frequencies are explicitly represented. In this case, a sinusoid of amplitude $A$ is represented by a pair of coefficients with magnitudes $A/2$ each. An alternative representation, valid only for real signals, is the \emph{unilateral} spectrum\index{Unilateral spectrum}, where only $c_k', k \ge 0$ are shown. In this case, $c_0' = c_0$ and $c_k' = 2 c_k, k>0$ as illustrated in \figl{unilateralsimplespectrum}. This text emphasizes the bilateral representation because it is more general and capable of representing the spectrum of a complex-valued signal $x(t)$.
\eExample

\subsection{Discrete-time Fourier series (DTFS)}

The Fourier series for discrete-time signals is used to analyze a periodic signal $x[n]$ with fundamental period $\Nperiod$. Similar to the continuous-time case, the basis functions consist of a set of complex exponentials formed by a fundamental frequency and its harmonics. The basis corresponding to the fundamental frequency is $e^{j \dw_0 n}$, where $\dw_0 = \frac{2 \pi}{\Nperiod}$ radians, and the harmonics are $\dw_k = k \dw_0$. A major distinction between the DTFS and the Fourier series is that there are only $\Nperiod$ distinct angular frequencies for the DTFS because
\[
(k+\Nperiod) \dw_0 = (k+\Nperiod) \frac{2 \pi}{\Nperiod} = k \frac{2 \pi}{\Nperiod} + 2 \pi = k \dw_0.
\]
This is a consequence of the fact that discrete-time angular frequencies are angles. Hence, the basis function
corresponding to a frequency $\dw=\pi$ is
equal to the one corresponding to $\dw=\pi + 2\pi = 3\pi$.

Hence, the DTFS pair is:
\begin{equation}
\left\{
\begin{array}{llll}
X[k] & = & \frac{1}{\Nperiod} \sum_{n=\langle \Nperiod \rangle} x[n] e^{- j k \dw_0 n} & ,k=- \infty,\ldots,-1,0,1,\ldots,\infty \\
& & & \\
x[n] & = & \sum_{k=\langle \Nperiod \rangle} X[k] e^{j k \dw_0 n} & , n=- \infty,\ldots,-1,0,1,\ldots,\infty.
\label{eq:dtfs}
\\ \end{array}
\right.
\end{equation}
If convenient,
\equl{dtfs} can be represented using the twiddle factor $W_{\Nperiod} = e^{-\frac {j2\pi} \Nperiod}$ as in \tabl{fourier_equations}.
The notation $n=\langle \Nperiod\rangle$ and $k=\langle \Nperiod \rangle$ represent any interval of $\Nperiod$ consecutive integers, such as $0,1,\ldots,\Nperiod-1$ or $-\Nperiod,-(\Nperiod-1),\ldots,-1$. Note the basis functions depend on the period $\Nperiod$ of the signal to be analyzed.

%As mentioned, apart from a scaling factor $1/N$, the DTFS is mathematically equivalent to the DFT. 

Because $x[n]$ is periodic, it suffices to specify its samples in an interval $\langle \Nperiod \rangle$. However, 
using $k=- \infty,\ldots,-1,0,1,\ldots,\infty$ and $n=- \infty,\ldots,-1,0,1,\ldots,\infty$ in
\equl{dtfs} allows to indicate that $X[k]$ is periodic and $x[n]$ has an infinite duration (in contrast to the finite-duration sequence obtained by an inverse DFT), respectively. As indicated in \equl{dtfs}, only $k=\langle \Nperiod \rangle$ values of $X[k]$ are necessary to represent the periodic $x[n]$, and along $k$ the values $X[k]$ are periodic, i.\,e., $X[k]=X[k+\Nperiod]$. The reason is that, as mentioned, the angles repeat
$(k+\Nperiod) \dw_0 = k \dw_0$ after $\Nperiod$ samples (as also illustrated by the divided unit circle in \figl{circledivided}) and, consequently, their corresponding basis functions and coefficients $X[k]$.
% also repeat in $N$ and the values of $X[k]$ repeat themselves.

%\equl{dft} and \equl{idft}, or represented in a matrix form. The normalization factor (e.\,g. $1/N$ or $1/\sqrt{N}$) is discussed in the sequel.
Similar to \exal{coefficients_by_inspection}, which obtained the Fourier series of a continuous-time signal, when $x[n]$ is composed by a sum of sinusoids, it is relatively easy to find the DTFS coefficients $X[k]$ by inspection, as discussed next. 


\bExample \textbf{DTFS coefficients by inspection.}
Assume $x[n] = A \cos (\textcolor{red}{ \frac{\pi}{6} n } + \pi/3)$, which has a period of $N=12$ samples. Instead of calculating via \equl{dtfs}, one can rewrite the signal as a sum of complex exponentials:
\[
x[n] = \frac{A}{2} \left[ e^{j(\frac{\pi}{6} n + \pi/3)} + e^{-j(\frac{\pi}{6} n + \pi/3)} \right] =
 \frac{A}{2} \textcolor{red}{ e^{j\frac{\pi}{6} n}}  e^{j\pi/3} + \frac{A}{2} \textcolor{red}{ e^{-j\frac{\pi}{6} n}}  e^{-j\pi/3}.
\]
With $N=12$, the DTFS synthesis equation is:
\[
x[n] = \sum_{k=\langle 12\rangle} X[k] \textcolor{red}{ e^{j k \frac{2 \pi}{12}  n}},
\]
where the range $k=\langle 12\rangle$ is chosen to be: $k=-6,-5,\ldots,0,1,\ldots,5$, which leads to
\[
x[n] = \sum_{k=-6}^5 X[k] \textcolor{red}{ e^{j k \frac{\pi}{6}n}}.
\]
By inspection, $x[n]$ can be represented by two coefficients: $X[-1] = \frac{A}{2} e^{-j\pi/3}$ and $X[1] = \frac{A}{2} e^{j\pi/3}$, while all other coefficients in the range $k \in [-6,5], k \ne \pm 1$ are zero.

If the range $k=\langle 12\rangle$ were $k=0,1,\ldots,11$, it would be necessary to recall that $X[k]=X[k+N]$. Hence, $X[11]=X[1]$ and the spectrum of $x[n]$ could be represented by $X[1]=\frac{A}{2} e^{j\pi/3}$ and $X[11]=\frac{A}{2} e^{-j\pi/3}$ with $X[k]=0$ for $k=0$ and $1<k<11$.
An alternative view of this periodicity is to sum $2 \pi n$ to the angle $e^{-j\frac{\pi}{6} n}$, which leads to $e^{j 2 \pi n}e^{-j\frac{\pi}{6} n} = e^{j\frac{11\pi}{6} n}$ and allows to write
\[
x[n] = \frac{A}{2} \textcolor{red}{ e^{j\frac{\pi}{6} n}}  e^{j\pi/3} + \frac{A}{2} \textcolor{red}{ e^{j\frac{11\pi}{6} n}}  e^{-j\pi/3}.
\]
Sometimes the graphs do not emphasize, but DTFS is always periodic and the Fourier coefficients repeat over $k$ with a period $N$.
\eExample


\subsubsection{Calculating the DTFS via the DFT}

As mentioned, mathematically the DFT and DTFS differ only by the scaling factor $1/N$, as indicated in \equl{dft} and 
\equl{dtfs}, respectively. Using the proper scaling factor will not alter the spectrum shape but it is important in case the numerical values should be interpreted with the correct units. For example, if a cosine is generated and its FFT calculated in {\matlab}, the numerical values will be influenced by both the cosine amplitude and value of $N$. Only after the normalization by $1/N$ one can interpret the spectrum of a periodic signal in volts.
The following example illustrates this procedure.

\bExample \textbf{DTFS using an FFT routine}.
Having a DFT routine, typically implemented as an FFT, one can obtain the forward DTFS by dividing the DFT spectrum by $N$ and, consequently, interpreting as the spectrum of a periodic signal.

\codl{snip_transforms_DTFS} illustrates how to calculate the DTFS in {\matlab} using both their built-in \ci{fft} function and the companion \ci{ak\_fftmtx.m}, with \ci{X} and \ci{X2} being the same apart from numerical errors around $~10^{-13}$. \figl{dtfs_example} shows the resulting spectrum.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/dtfs_example}        
        \caption[{DTFS / DFT of $x[n] = 10 \cos (\frac{\pi}{6} n + \pi/3)$ calculated with $N=12$.}]{DTFS / DFT of $x[n] = 10 \cos (\frac{\pi}{6} n + \pi/3)$ calculated with $N=12$. The plot on top is the magnitude obtained with \ci{abs(X)} and the bottom is the phase obtained with \ci{angle(X)} (note some random phases when the magnitude is close to zero).\label{fig:dtfs_example}}
\end{figure}


\includecodepython{MatlabOctaveCodeSnippets}{snip\_transforms\_DTFS}{snip_transforms_DTFS} 
%\begin{lstlisting}
%N=12; %period in samples
%n=transpose(0:N-1); %column vector representing abscissa
%A=10; x=A*cos(pi/6*n + pi/3); %cosine with amplitude 10 V
%X=(1/N)*fft(x);%calculate DFT and normalize to obtain DTFS
%Ah=ak_fftmtx(N,2); %DFT matrix with the DTFS normalization
%X2=Ah*x; %calculate the DTFS via the normalized DFT matrix
%\end{lstlisting}


While DFT graphs typically show only the chosen range of $N$ coefficients as in \figl{dtfs_example} and \figl{fftshift_example}, sometimes the periodicity of DTFS / DFT must be represented. Therefore, the strict representation of the spectrum of $x[n]$ is given in
\figl{complete_dtfs}, which does not limit the abscissa to $N$ values of $k$.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/complete_dtfs}               
        \caption{Complete representation of the DTFS / DFT of $x[n] = 10 \cos (\frac{\pi}{6} n + \pi/3)$ indicating the periodicity $X[k]=X[k+N]$.\label{fig:complete_dtfs}}
\end{figure}

Note in the example of \figl{complete_dtfs} the value $N=12$ was chosen to coincide with the period of the cosine and the non-zero coefficients were $k=\pm 1$. Choosing $N=24$ would move the non-zero coefficients to $k=\pm 2$. If the number of points of the DFT / DTFS was not a multiple of 12, $x[n]$ would still be interpreted as a periodic signal but not a single cosine. In this case the spectrum would have many non-zero coefficients. 
\eExample


As mentioned, all four pairs of Fourier representations are related. The two pairs of series have been discussed. One can obtain the expressions for transforms from expressions for series by using the limit when the period ($N$ or $T_0$) goes to infinite. This allows to create an aperiodic signal from a periodic one, and also illustrates that the discrete components of the spectrum ($c_k$ or $X[k]$) get so close to each other that create a continuum of frequencies ($X(f)$ or $X(e^{j\dw})$).
The next sections discuss Fourier transforms.

\subsection{Continuous-time Fourier transform using frequency in Hertz}
\label{sec:ctft_hz}

The continuous-time Fourier transform\footnote{Check the URLs at \akurl{http://www.educypedia.be/electronics/javafourier.htm}{2jav}.} uses an infinite number of exponentials $e^{j \aw t}$ as basis functions, where $\aw$ is the angular frequency in radians per second and varies from $-\infty$ to $\infty$. Recall from Result~\ref{ex:eternal_sinusoids} that two exponentials $e^{j \aw_0 t}$ and $e^{j \aw_1 t}$, $\aw_0 \ne \aw_1$, are orthogonal,
%By analogy to the Hermitian in block transforms, the direct Fourier transform uses the complex conjugate $e^{-j \aw t}$ of the basis functions.
and $\aw=2 \pi f$, where $f$ is the frequency in Hertz.
Hence, the continuous-time Fourier equations are given by
\begin{equation}
\left\{
\begin{array}{ccc}
X(f) & = & \int_{-\infty}^\infty x(t) e^{- j 2 \pi f t} \textrm{d}t \\
& & \\
x(t) & = & \int_{-\infty}^\infty X(f) e^{j 2 \pi f t} \textrm{d}f.
\label{eq:transforms_fourier}
\\ \end{array}
\right.
\end{equation}
One should interpret the coefficient $X(f_0)$ for a given frequency $f_0$ as the inner product $X(f_0) = \langle x(t),e^{j 2 \pi f_0 t}\rangle$.
% (in the definition of inner product, the second term is conjugated).
%In other words, $X(f_0)$ is the norm of the projection of $x(t)$ on signal $e^{- j 2 \pi f_0 t}$ (scaled by its norm).

As anticipated in \tabl{fourier_units}, if $x(t)$ is given in volts, the unit of $X(f)$ is volts $\times$ seconds or, equivalently, volts/Hz. Similarly, for $x(t)$ in ampere, $X(f)$ is in ampere/Hz, and so on.
%For example, $X(f) = 10\textrm{rect}(f-5)$, indicates that the corresponding signal $x(t)$ has 0.1 V from 

\subsection{Continuous-time Fourier transform using frequency in rad/s}
\label{sec:ctft_in_rads}

In some applications of Fourier transform, it is more convenient to use $X(\aw)$ with $\aw$ in rad/s, instead of $X(f)$ with $f$ in Hz.

Having $X(\aw)$ as a convenient alternative to $X(f)$, where $\aw=2 \pi f$, is very different from deriving a \emph{new} function $Y(f)=X(2 \pi f)$ by contracting the frequency axis by the factor $2 \pi$ (see, e.\,g., \figl{time_scaling}).
In case $Y(f)=X(2 \pi f)$, the two functions $Y(f)$ and $X(f)$ are different. In contrast, 
$X(f)$ and $X(\aw)=X(2 \pi f)$ represent the very same function, but with distinct independent variables. In summary, a plot of $X(f)$ without impulses $\delta(f)$ can be converted to $X(\aw)$ by simply multiplying the abscissa by $2 \pi$. 
But due to the scaling property of the impulse, the area of each eventual impulse in $X(f)$ needs to be scaled by
$2 \pi$ when representing it in $X(\aw)$. Some examples are provided in the sequel.

\bExample \textbf{Examples contrasting Fourier transforms in linear and angular frequencies}
As depicted in \figl{from_f_to_rads}, when there are no impulses in $X(f)$, the Fourier transform $X(\aw)$
is obtained by simply multiplying the abscissa by $2 \pi$. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{./FiguresNonScript/from_f_to_rads}
\caption{Example of a Fourier transform $X(f)$ and its equivalent representation
$X(\aw)$ with $\aw$ in rad/s.\label{fig:from_f_to_rads}}
\end{figure}

However, when $X(f)$ contains impulses, the scaling property of an impulse described in Appendix~\ref{impulse_scaling_property} has to be taken into account. This property states that each area $A$ of an impulse $A\delta(f)$ in $X(f)$ is multiplied by $2\pi$ to become $2\pi A \delta(\aw)$ in $X(\aw)$. For example, the transform of $x(t)=\cos(2\pi f_0 t)$ is $X(f)=0.5[\delta(f+f_0)+\delta(f-f_0)]$ or, equivalently, $X(\aw)=\pi[\delta(\aw+2 \pi f_0)+\delta(\aw-2 \pi f_0)]$. \figl{cos_hz_rads} provides an example for $x(t)=6 \cos(10 \pi  t)$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{./FiguresNonScript/cos_hz_rads}
\caption{Fourier transform of $x(t)=6 \cos(10 \pi  t)$
represented in Hertz and radians per second, indicating the scaling of the impulses areas by the factor $2 \pi$.\label{fig:cos_hz_rads}}
\end{figure}

It is instructive to compare \figl{from_f_to_rads} and \figl{cos_hz_rads}, observing that $X(f)$ and $X(\aw)$ are two
distinct representations of the
\emph{same} Fourier transform of the respective signal $x(t)$.

As a final example, consider that $X(f)= 5 \rect(f/4) + 3 \delta(f-2)$ is the Fourier transform of a complex-valued signal $x(t)$. The spectrum $X(f)$ does not have Hermitian symmetry because $x(t)$ is not real-valued. When $\aw$ in rad/s is adopted, this spectrum is represented as $X(\aw)= 5 \rect \left(\aw/(8\pi)\right) + 6 \pi \delta(\aw-4\pi)$. The amplitude of the $\rect ( \cdot )$  function is 5 in both $X(f)$ and $X(\aw)$, but the area of the impulse is scaled by $2 \pi$ in $X(\aw)$.
\eExample 

Because they \emph{represent the same spectrum}, $X(f)$ and $X(\aw)$ share the same properties with few subtle differences imposed by the factor $2 \pi$. If a given property is defined for one of these two functions (the ``mother'' definition for this property), an equivalent property can be derived for the other (``child'' definition).

For instance, assume the Fourier transform given in Hertz is the ``mother'' definition described by \equl{transforms_fourier}. 
The ``child'' definition using rad/s instead of Hz, requires a change of variable $f= \frac \aw {2\pi}$ to obtain the alternative Fourier transform definition:
\begin{equation}
\left\{																																						
\begin{array}{ccc}
X(\aw) & = & \int_{-\infty}^\infty x(t) e^{- j \aw t} \textrm{d}t \\
& & \\
x(t) & = & \frac{1}{2 \pi} \int_{-\infty}^\infty X(\aw) e^{j \aw t} \textrm{d}\aw
\\ \end{array}
\right.
\label{eq:continuousFourierRadS}
\end{equation}
given that $\textrm{d}f = \frac {\textrm{d}\aw} {2\pi}$.

\subsubsection{Units of the Fourier transform in linear and angular frequencies}

\tabl{fourier_units} indicated that, when $x(t)$ is given in volts, the unit of $X(f)$ is volts/Hz.
The unit of $X(\aw)/(2 \pi)$ is volts/(rad/s), and the need to incorporate the $2\pi$ factor is
better understood via an example.

For instance, when $X(f)= 5 \rect(f/4)$, the value is 5~V/Hz from $[-2, 2]$~Hz, and zero otherwise.
The integral of $X(f)$ is 20~volts, because:
\[
\int_{-\infty}^\infty X(f) \textrm{d}f = 5 \times 4 = 20. 
\]
Equivalently, this spectrum can be denoted as $X(\aw)= 5 \rect(f/(8\pi))$. The unit of
$X(\aw)$ is not directly in volts/(rad/s) because taking the value of 5 along the frequency band $[-4\pi, 4\pi]$ leads to
\[
\int_{-\infty}^\infty X(\aw) \textrm{d}\aw = 5 \times 8 \pi = 40 \pi \ne 20. 
\]
But using a normalization factor of $2\pi$, the function $X(\aw)/(2 \pi)$ can be interpreted 
in units of rad/s. For the given example, one has a density of $5/(2 \pi)$~V/(rad/s), which leads to
\[
\int_{-\infty}^\infty \frac{X(\aw)}{2 \pi}  \textrm{d}\aw = \frac{5}{2 \pi} \times 8 \pi = 20. 
\]

Hence, the factor $\frac{1}{2 \pi}$ in \equl{continuousFourierRadS} is essential for having the
proper results and interpreting $X(\aw)/(2\pi)$ in units of volts/(rad/s). For example, for $t=0$
\begin{equation}
x(t)|_{t=0} = \int_{-\infty}^\infty X(f) \textrm{d}f = \frac{1}{2 \pi} \int_{-\infty}^\infty X(\aw) \textrm{d}\aw.
%\label{eq:}
\end{equation}

\subsection{Discrete-time Fourier transform (DTFT)}
\label{subsec:dft}

The discrete-time Fourier transform uses an infinite number of exponentials $e^{j \dw n}$ as basis functions, where $\dw$ is the angular frequency in radians.
%and varies in a range of $2 \pi$, such as $[0,2\pi[$ or $[-\pi/2,\pi/2[$.
%The direct transform uses the complex conjugate $e^{-j \dw n}$ of the basis functions.
The equations are given by
\begin{equation}
\left\{
\begin{array}{ccc}
X(e^{j\dw}) & = & \sum_{-\infty}^\infty x[n] e^{-j \dw n} \\
& & \\
x[n] & = & \frac{1}{2 \pi} \int_{\langle 2 \pi\rangle} X(e^{j \dw}) e^{j \dw n} d\dw.
\\ \end{array}
\right.
\label{eq:fourier_transform}
\end{equation}

If $x[n]$ is given in volts, the unit of $X(e^{j \dw})/(2 \pi)$ is volts per radians.
%$\dw / (2\pi)$. The unit of $X(e^{j \dw})$ is \emph{not} volts/rad and the division by $2 \pi$ in the inverse transform of \equl{fourier_transform} reminds that.
For instance, assuming $X(e^{j \dw})= 10 \pi \rect(\dw)$, the density is $X(e^{j \dw})/(2\pi)=5$~volts/rads over the frequency band $\dw \in [-0.5, 0.5]$~rads, and one has
\[
x[n]|_{n=0} = \int_{-\pi}^\pi \frac{X(e^{j \dw})}{2 \pi} \textrm{d}\dw = \int_{-0.5}^{0.5} \frac{10 \pi}{2 \pi} \textrm{d}\dw = 5 \times 1 = 5~\textrm{volts}. 
\]

One should interpret the coefficient $X(\dw_0)$ for a given frequency $\dw_0$ as the inner product $X(\dw_0) = \langle x[n],e^{j \dw_0 n}\rangle$.
% (remember that the second term in the inner product is conjugated when dealing with complex signals).

The notation is $X(e^{j\dw})$ because $\dw$ only appears in the imaginary part of an exponent, i.\,e., $\dw$ is always the angle of a complex exponential, unless $\dw$ is the argument of an impulse $\delta(\dw)$. For example, $X(e^{j\dw})=\frac{1+\dw}{3+\dw}$ will never be a ``valid'' discrete-time Fourier transform for the signals of interest. In contrast, a ``valid'' expression is
$X(e^{j\dw})=\frac{1+e^{j2\dw}}{3+e^{-j4\dw}} + \delta(\dw)$.
One can also note that \[X(e^{j\dw}) = X(e^{j(\dw+m 2 \pi)}), m \in \ZZ,\]
which indicates that $X(e^{j\dw})$ is always periodic in $2 \pi$ (for any $x[n]$). This discreteness-periodicity duality had been already summarized in \tabl{fourier_tools}.
% is a consequence of $n$ being dimensionless and $\dw$ an angle.

\subsubsection{Calculating the DTFT via the DFT}
\label{sec:dtft_via_dft}

The DFT corresponds to sampling the DTFT in the frequency domain such that
\begin{equation}
X[k] = \left. X(e^{j\dw}) \right|_{\dw = \frac{2 \pi }{N} k}
\label{eq:dtft_via_dft}
\end{equation}
with $N$ being the DFT dimension. In other words, the DFT uses a frequency increment of $\Delta \dw = 2 \pi / N$ and calculates the value of the DTFT $X(e^{j\dw})$ at the angles $0, \Delta \dw, 2\Delta \dw, \ldots, (N-1)\Delta \dw$ rad.

The discrete-values of the angles used in an $N$-point DFT are called DFT (or FFT) \emph{bins}.\index{DFT bin}\index{FFT bin}\index{Bin (DFT/FFT)} Hence, in Fourier analysis, the number of bins 
coincides with the number $N$ of DFT / FFT points.
\figl{fft_bin_definition} depicts an example with $N=4$. In this case, a cosine $x[n]=\cos((\pi/2) n)$ with angular frequency $\dw_c=\pi/2$ is called \emph{bin-centered}\index{Bin-centered signal}
because its frequency coincides with a DFT bin (also called DFT frequency line).\footnote{Section~\ref{sec:ExampleLeakagePicketFence} provides an example of the impact of having a signal that is bin-centered or not in DFT-based spectral analysis.}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall]{./FiguresNonScript/fft_bin_definition}
\caption{Example of bins when using an $N$-points DFT with $N=4$. The bin centers are $0, \pi/2, \pi$ and $3\pi/2$, all marked with '$\times$'.\label{fig:fft_bin_definition}}
\end{figure}

\figl{fft_bin_definition} has similarities to the histogram in \figl{histogram_example}.
For instance, for the DFT one can assume that the minimum and maximum angular frequencies
are always $\dw=0$ and $2 \pi$, respectively. Hence, the bin width can be calculated 
using a strategy similar to the one adopted in \equl{hist_bin_width}, 
as $\Delta \dw = (2\pi - 0)/N = 2\pi/N$. However, the bin centers in histograms
are calculated differently than DFT bin centers.
A histogram with $B$ bins uses the minimum signal value
as an edge of the first bin. In contrast, as indicated in \figl{fft_bin_definition}, the minimum angular frequency
value $\dw = 0$~rad is considered to be the first bin center (not the edge). Because of
this, the first and last bins of a DFT/FFT have a width of $\Delta \dw / 2$, which differs
from the other bins.

Another issue when using a DFT is to improve its resolution given by $\Delta \dw = 2\pi / N$.
When one wants to sample the DTFT using a high-resolution grid, the value $N$ is chosen to be larger than the duration of $x[n]$
and \emph{zero-padding}\index{Zero-padding} is adopted.
Zero-padding corresponds to simply increasing the size of a sequence by appending extra values to it, all with zero amplitude.


%AK-IMPROVE %\subsection{Using DFT to approximate the other three Fourier tools}

%\subsection{Interpreting the DFT result}
%As in the continuous-time case, the DTFS is capable of perfectly representing periodic signals. When using the DFT (or equivalently, the DTFS or FFT) for non-periodic signals, some artifacts occur and these will be investigated later on.


%\section{Using FFT in Practice}

%AK-IMPROVE explain radix 2 and radix 4

%\subsection{Use FFT algorithms, never the DFT matrix multiplication}


\section{Relating spectra of digital and analog frequencies}
%\subsection{%
%\ifpdf%
%       \texorpdfstring{Relating discrete $\dw$ (rad) and analog $\aw$ (rad/s) frequencies.}{Relating discrete omega (rad) and analog omega (rad/s) frequencies.}
%\else%
%       Relating discrete $\dw$ (rad) and analog $\aw$ (rad/s) frequencies.
%\fi%
%}

%Basically show the regra de 3.

If $x[n]$ is obtained by sampling $x(t)$, it is convenient to relate their spectra. This is
a followup of Section~\ref{sec:relatingContinuousDiscrete}.
Recall from \equl{freqdiscrete2continuous} that $\aw = \dw \fs$. Hence, $\fs$ can be used to relate the abscissas of graphs of $X(e^{j\dw})$ and $X(\aw)$. But the periodicity of $X(e^{j\dw})$ should be taken into account as follows.

%The sampling theorem indicates the value that should be adopted for the sampling frequency $\fs$.
After a sampling frequency $\fs$ is specified (e.\,g., $\fs= 8$~kHz), the value $\fs/2$  represents the frequency $f$ that will be mapped to the angle $\pi$ rad and its multiples in $2\pi$. This can be seen by
\begin{equation}
\dw = \aw / \fs = 2 \pi f / \fs = 2 \pi 4000/8000 = \pi.
\label{eq:foldingFrequencyExample}
\end{equation}
In discrete-time (or ``digital'') signal processing, the value of $\fs/2$ is called \emph{Nyquist frequency}\index{Nyquist frequency} or \emph{folding frequency}\index{Folding frequency}. If the spectrum  $X(f)$ is zero\footnote{More strictly, only the magnitude matters: the condition is $|X(f)|=0, f \ge \fs/2$ because the phase can be discarded when the magnitude is zero.} for $f \in [\fs/2, \infty[$, then the values of $X(\aw)$ in the range $2 \pi \times [-\fs/2, \fs/2]$ coincides  with the values of $X(e^{j\dw})$ normalized by $\fs$ in the corresponding range of $[-\pi, \pi]$. In other words:
\[
X(e^{j\dw}) = \fs X(\aw),
\]
%is a consequence of the relation $\aw = \dw  \fs$ and
which will be proved later on. Note that $X(e^{j\dw}) = \fs X(\aw)$ only holds if the sampling theorem is obeyed, otherwise the components of signal $x(t)$ with frequencies in the range $]-\infty,\fs/2[$ and $]\fs/2,\infty[$ will be mapped (or folded, which is the reason for calling $\fs/2$ the folding frequency) to components of $x[n]$ with angular frequencies in the range $]-\pi,\pi[$, eventually distorting the discrete-time representation of the original $x(t)$. This phenomenon is called \emph{aliasing} because a folded component from $X(\aw)$ appears in $X(e^{j\dw})$ as an ``alias'' or impostor.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/continuousdiscretespectra}           
        \caption[{Spectrum $X(f)$ (top) and $X(e^{j\dw})$ when $\fs=60$~Hz.}]{Spectrum $X(f)$ (top) and $X(e^{j\dw})$ when $\fs=60$~Hz. The two indicated points are related by $\aw=\dw \fs$ and the frequency $f=6.238$ Hz is mapped into $\dw = 0.6532$ rad. Notice three of the infinite number of replicas of $X(f)$ centered at $\dw \in [-2\pi, 0, 2\pi]$. In this case there was no aliasing because the sampling theorem was obeyed.
\label{fig:continuousdiscretespectra}}
\end{figure}

For the sake of illustration, \figl{continuousdiscretespectra} depicts the spectrum $X(f)$ of a continuous-time signal $x(t)$. It also shows the spectrum $X(e^{j\dw})$ of $x[n]$, obtained by sampling $x(t)$ with $\fs=60$~Hz.
In this case, $\aw =  2 \pi \times 6.238$ rad/s is mapped to $0.6532$ rad, with the magnitude being scaled by 60, according to $X(e^{j\dw}) = \fs X(\aw)$. This discussion aims at illustrating how the Fourier transform $X(f)$ of $x(t)$ and the DTFT of the corresponding $x[n]$ (obtained from $x(t)$ via a C/D conversion) are related. The next paragraphs  complement \equl{dtft_via_dft} and discuss how to interpret the DFT in Hz.

%The Nyquist frequency is important because signal processing in the discrete-time domain can represent frequencies only in the range $[-\fs/2, \fs/2]$~Hz (or, alternatively, $[0, \fs/2]$, in case one prefers to exclude the negative frequencies from the discussion).

\section{{\akadvanced} Summary of equations for DFT / FFT Usage}

The DFT resolution in radians is $\Delta \dw = 2 \pi / N$.
Using $\aw = \dw \fs$, one can note that the continuous-time angular frequency $\aw = 2 \pi \fs$ rad/s corresponds to the angle $\dw = 2 \pi$ rad. Hence, the DFT frequency spacing is
\begin{equation}
\Delta f = \frac{\fs} {N}.
\label{eq:fft_freq_resolution}
\end{equation}
For example, assuming $\fs=100$~Hz, a DFT of $N=256$ points has a resolution of $\Delta f = 0.3906$~Hz. In radians, this resolution corresponds to 
$\Delta \dw = 2 \pi / 256 \approx 0.0245$ rad.

Note that $\Delta f$ can also be written as $\Delta f = \fs/N = 1/(N \ts)$, where $N \ts$ is the total duration $T_{\textrm{total}}$ of the signal being analyzed, such that
\begin{equation}
\Delta f = \frac{1}{T_{\textrm{total}}}.
\label{eq:fft_total_duration}
\end{equation}
Hence, a spectral analysis with resolution of at least $\Delta f$~Hz requires a data record of length 
\begin{equation}
T_{\textrm{total}} \ge \frac{1}{\Delta f}.
\label{eq:fft_total_duration2}
\end{equation}

The following example illustrates the interpretation in Hz of a DFT result.
The DFT performs a sampling operation on the frequency domain $\dw$ according \equl{dtft_via_dft}. As illustrated in \figl{circledivided}, a DFT with $N=3$ points (or 3-DFT) uses three basis functions $e^{j\dw}$ with the angles $\dw=0, 2\pi/3, 4\pi/3$ rad, i.\,e., a counter-clockwise rotation using a step of $\Delta w = 2 \pi/3$. These angles correspond to $k=0,1,2$, respectively, as indicated by \equl{dft_basis_function}. Note that in \figl{circledivided}, incrementing $k$ corresponds to a clockwise rotation, given that \equl{twiddle_factor} defines the twiddle factor with a negative exponent.

The three angles of a 3-DFT correspond to $0, \fs/3, -\fs/3$ Hz, respectively, as indicated by $\aw = \dw \fs$ or directly observing that the DFT uses a grid of $\Delta f = \fs/N$. For $N=4$, the four angles correspond to the frequencies $0, \fs/4, \fs/2, -\fs/4$ Hz. The angle corresponding to $k=0$ is always 0 rad, which is called DC because corresponds to 0~Hz. Note that for $k>N/2$ the DFT values correspond to negative frequencies.
The angular frequency corresponding to the $k$-th angle is
\begin{equation}
\dw_k = k \Delta \dw = k \frac{2 \pi }{N},
\label{eq:fft_freq_tone_rad}
\end{equation}
which corresponds to the regular (linear) frequency
\begin{equation}
f_k = k \Delta f = k \frac{\fs}{N}.
\label{eq:fft_freq_tone_hz}
\end{equation}
The values representing the largest frequency when $N$ is even is
\begin{equation}
k=N/2
\label{eq:fft_max_even},
\end{equation}
which corresponds to $\pi$ rad and $\fs/2$ Hz. When $N$ is odd, the value at
\begin{equation}
k=(N-1)/2
\label{eq:fft_max_odd}
\end{equation}
is the one representing the largest frequency $\fs (N-1)/(2 N)$ Hz. \tabl{fft_equations} summarizes some equations typically used with FFT algorithms.

\begin{table}
\centering
\caption{Summary of equations useful for signal processing with FFT.\label{tab:fft_equations}}
\begin{tabular}{|l|c|l|}
\hline
Expression & Reference & Comment \\ \hline
$\Delta f = \fs/N$ & \equl{fft_freq_resolution} & Frequency spacing (Hz) \\ \hline
$\Delta f = 1 / T_{\textrm{total}}$ & \equl{fft_total_duration} & Dependence on signal duration (Hz) \\ \hline
$\Delta \dw = (2 \pi)/N$ & \equl{fft_delta_dw} & Frequency spacing (rad)  \\ \hline
$f_k = k \textrm{~} \Delta f $ & \equl{fft_freq_tone_hz} & Frequency of $k$-th tone (Hz) \\ \hline
$\dw_k = k \textrm{~} \Delta \dw $ & \equl{fft_freq_tone_rad}& Frequency of $k$-th tone (rad)  \\ \hline
$\fs / 2 $ & \equl{fft_max_even} & Highest frequency (Hz) for even $N$ \\ \hline
$\fs (N-1)/(2 N)$ & \equl{fft_max_odd} & Highest frequency (Hz) for odd $N$ \\ \hline
\end{tabular}
\end{table}


\subsection{{\akadvanced} Three normalization options for DFT / FFT pairs}
\label{sec:dft_normalization_options}

%\subsubsection{Orthonormal and non-orthonormal definitions of DFT / FFT pairs}
%\subsection{Scaling Factors for DFT Pairs}

As discussed, the following three transforms differ only on the normalization factor: DFT, unitary DFT and DTFS.
%A DFT pair can use orthogonal basis functions that do not have unitary norms. 
%Hence, g
Generalizing \equl{dft} and \equl{idft}, the ``core'' DFT transform equations can be written as follows:
\begin{equation}
X[k] = \alpha \sum_{n=0}^{N-1} x[n] \left(W_N \right)^{nk}
\label{eq:forms_of_fft}
\end{equation}
and
\begin{equation}
x[n] = \beta \sum_{k=0}^{N-1} X[k] \left(W_N \right)^{-nk}.
\end{equation}
As indicated in \equl{nonorthogonaltransform}, the only requirement to have a valid pair is that 
\begin{equation}
\alpha \beta = \frac{1}{N}.
\label{eq:fftAlphaBeta}
\end{equation}

In summary, the three popular options for choosing $\alpha$ and $\beta$:
\begin{itemize}
        \item \textbf{DFT as samples of the DTFT (in frequency domain)}: 
\begin{equation}
\alpha=1 \textrm{~~and~~} \beta=\frac{1}{N}. 
\label{eq:dft_as_sampled_dtft}
\end{equation}								
				{\matlab} implements this transform in its \ci{fft} and \ci{ifft} routines. It allows to estimate the DTFT at specific values of $\dw$ using \equl{dtft_via_dft}. Also, it is the fastest in the direct transform because there is no scaling factor to be accounted for.
        \item \textbf{Unitary DFT}: 
\begin{equation}
\alpha = \beta=\frac{1}{\sqrt{N}}. 
\label{eq:dft_as_unitary}
\end{equation}								
Because this transform is unitary, Theorem \ref{th:parseval} applies and the energy \ci{sum(abs(X).\^{}2)} in frequency and time \ci{sum(abs(x).\^{}2)} domains coincide. To implement the unitary DFT when using {\matlab} one can call \ci{X=fft(x)/sqrt(N)} and \ci{x=ifft(X)*sqrt(N)} for the direct and inverse transforms, respectively. 
        \item \textbf{DFT as DTFS in case the number of DFT points coincides with the signal period}:
\begin{equation}
\alpha=\frac{1}{N} \textrm{~~and~~} \beta=1. 
\label{eq:dft_as_dtfs}
\end{equation}				
In this case, the DFT coefficients can be interpreted in volts when the time-domain signal is given in volts. If the signal is periodic with fundamental period $\Nperiod = N$ coinciding with the number $N$ of DFT points, choosing \equl{dft_as_dtfs} allows to obtain the DTFS coefficients via the DFT. To implement this version when using {\matlab}, one can call \ci{X=fft(x)/N} and \ci{x=ifft(X)*N} for the direct and inverse transforms, respectively. 
% coefficient $X(e^{j\dw'})$ for a finite-duration signal $x[n]$ corresponding to a given angle $\dw'$.
\end{itemize}

%\codl{fftmtx} illustrates a code that provides the pair of matrices for the direct \ci{Ah} and inverse \ci{A} matrices for the three main FFT normalization options.
%\includecode{MatlabOctaveFunctions}{fftmtx}


The next two sections present the Laplace and Z transforms.\footnote{There are lots of materials on the Web. For example, check:
\akurl{http://web.mit.edu/2.14/www/Handouts/PoleZero.pdf}{2pol}, \akurl{http://web.mit.edu/2.14/www/Handouts/Handouts.html}{2lap} and \akurl{http://www.youtube.com/watch?v=bMYgv8pfPEY}{2vid}.}
 Both have two parameters that identifies each basis functions, in contrast to Fourier equations that have only the frequency. This extra degree of freedom, represented by sets of basis functions that are more powerful than the ones used in Fourier analysis, has the advantage of allowing the representation of a larger class of signals (and systems). A disadvantage is that the inverse transform is defined as a \emph{contour integration} in the complex plane, which is an area of \emph{complex analysis} that is out of the scope of this text. But it is rarely necessary to calculate the inverse transform using complex analysis. The Laplace and Z inverse transforms will be calculated here only by an alternative method called partial fraction expansion, which is discussed in the Appendix, Section~\ref{sec:partial_fraction}.

The Laplace and Z transforms are widely used to represent the action of linear and time-invariant systems, as will be detailed in Chapter~\ref{ch:systems}.
The Laplace transform has a number of properties that make it useful for analyzing linear systems. The most prominent advantage is that differentiation and integration become multiplication and division, respectively, by $s$. For example, this converts differential equations into polynomial equations, which are much easier to solve and, once solved, the inverse Laplace transform can provide the time domain formula. Similarly, Z transforms are useful to analyze discrete-time systems.

%AK_NOTE: While we have been talking about transforms that represent signals. Less emphasis on orthogonality, and more in properties to analyze systems.

%AK_NOTE: Books emphasize convergence. Need to show that it is the representational property of Fourier that is limited.

\section{Laplace Transform}

We start our study of Laplace transform by motivating it as an extension of the Fourier transform.

\subsection{Motivation to the Laplace transform}
The Fourier analysis is not able to provide a representation for some specific signals. For example,
the signal $x(t)= e^{3t} u(t)$, does not have a Fourier transform because the integral
\begin{align*}
X(\aw) &= \int_{-\infty}^\infty x(t) e^{- j \aw t} \textrm{d}t = \int_{0}^\infty e^{(3 - j \aw) t} \textrm{d}t =
\left. \frac 1 {3-j \aw} {e^{(3-j \aw) t}} \right|_{0}^{\infty}\\
& = \frac 1 {3-j \aw} \lim_{t \rightarrow \infty} \frac{e^{3t}}{e^{j \aw t}} = \infty
\end{align*}
does not converge. Note that one should evaluate
\[
\lim_{t \rightarrow \infty} \frac{e^{3t}}{e^{j \aw t}}
\]
considering that $|e^{j \aw t}| = 1$ (this limit is not $\infty / \infty$ and L'Hospital's rule is not appropriate).

This limitation in the representational power of Fourier analysis can be circumvented with the trick of pre-multiplying the input signal by an exponential. Assuming the continuous-time domain, this corresponds to multiplying $x(t)$ by an exponential $e^{-\sigma t}$, $\sigma \in \Re$, and then taking the Fourier transform of $x(t) e^{-\sigma t}$. For example, if $x(t)= e^{3t} u(t)$, then pre-multiplying by $e^{-4t}$ leads to the following Fourier transform for $z(t) = x(t) e^{-4t} = e^{-t} u(t)$:
\[
Z(\sigma,\aw) = \int_{-\infty}^\infty z(t) e^{- j \aw t} \textrm{d}t = \int_{0}^\infty e^{(-1 - j \aw) t} \textrm{d}t = \frac {1} {1+j \aw}.
\]
The signal $x(t)$ could be recovered from the inverse Fourier transform of $Z(\sigma,\aw)$ and a post-multiplication by $e^{4t}$.
This trick can be adopted with several different values of $\sigma$. For example, $z(t) = x(t) e^{-5t}$ would also work in the previous example, as well as all values of $\sigma$ such that $\sigma > 3$ (note that the exponent of $e^{-\sigma t}$ uses $- \sigma$, not $\sigma$).

Based on this motivation, the Laplace transform $X(s)$, where $s=\sigma+j\aw$ is a complex number, can be interpreted as the Fourier transform of the signal multiplied by the exponential $e^{-\sigma t}$, as follows:
\begin{equation}
X(s) = \calL\{x(t)\} = \calF\{x(t) e^{-\sigma t}\} = \int_{-\infty}^\infty x(t) e^{-s t} \textrm{d}t.
\label{eq:laplace_forward}
\end{equation}

The pair of Laplace equations is defined as:
\begin{equation}
\left\{
\begin{array}{ccc}
X(s) & = & \int_{-\infty}^\infty x(t) e^{-s t} \textrm{d}t \label{eq:laplace}\\
& & \\
x(t) & = & \frac{1}{2 \pi j} \oint_{\gamma -j \infty}^{\gamma +j \infty} X(s) e^{st} ds,
\\ \end{array}
\right. %to avoid having to ``close'' the \left\{
\end{equation}
where $\gamma$ is a real number so that the contour path of integration is in the region of convergence of $X(s)$.
Because there are two parameters, $\sigma$ and $\aw$, the values that $s=\sigma+j \aw$ can assume as the independent variable of a Laplace transform $X(s)$ compose a plane, while in the Fourier transform $X(\aw)$ the independent variable the angular frequency $\aw$ (the abscissa line, not a plane).

As will be discussed in Chapter~\ref{ch:systems}, the choice of $e^{st}$ and the corresponding popularity of the Laplace transform is due to their use as a tool for analyzing \emph{systems} not \emph{signals}. 

\subsection{{\akadvanced} Laplace transform basis functions}

One can interpret the Laplace transform as having basis functions $e^{(-\sigma + j \aw)t}$ with two parameters $\sigma$ and $\aw$.
The Laplace transform can then be seen as obtaining coefficients from inner products $\langle x(t), e^{(-\sigma + j \aw)t}\rangle$, given that the complex conjugate of the basis function is $e^{(-\sigma - j \aw)t} = e^{-st}$.

But an interesting distinction between the Fourier and Laplace transforms is the following. When using the Fourier transform, if the signal coincides with a given basis function (e.\,g., $x(t)=\cos(\aw_0 t)$), a signal impulse represents the signal in the transform domain ($X(\aw) = \pi \delta(\aw - \aw_0)$ for the given example). This impulse indicates that a single basis functions represents the input signal $x(t)$. Such situations do not occur when the Laplace transform is used. The transform domain $X(s)$ does not use impulses and there is no signal $x(t)$ that is represented by a single basis function.

It is intuitive that the Fourier basis functions, which are limited in amplitude, are not the most adequate to represent a signal such as the $x(t)= e^{3t} u(t)$, which has magnitude increasing with $t$ for $t>0$. Consider picking a very large value $X(f')$ for the Fourier ``coefficient'' corresponding to the basis function $e^{j 2 \pi f' t} = \cos(2 \pi f' t) + j \sin(2 \pi f' t)$, this value $X(f')$ would scale the basis along all the time axis, eventually failing to provide a sharp increase in magnitude because $\sin(\cdot)$ and $\cos(\cdot)$ have amplitudes limited to the range $[-1, 1]$.

As mentioned, the Laplace transform multiplies $x(t)$ by an exponential weighting function $e^{-\sigma t}$, where $\sigma \in \Re$  can control the amplitude of the exponential, which increases and decreases with $t$ for $\sigma < 0$ and $\sigma > 0$, respectively. By multiplying the exponential sinusoid $e^{j \aw t}$ by $e^{-\sigma t}$, the new basis function $e^{(-\sigma + j \aw) t}$ can represent
a broader variety of signals. 
%With $\sigma > 0$, the envelope amplitude of $e^{(\sigma + j \aw) t}$ increases with $t$. 

As an example, the left graph in \figl{laplace_basis} was obtained with \codl{snip_transforms_laplace_basis}
and the second graph used $\sigma =0.3$ ($\sigma$ is denoted in the code as \ci{sigma}), with both showing the real part of $e^{(-\sigma + j \aw) t}$, with $s=\pm 0.3 + j 10 \pi$. The envelope $e^{\pm 0.3t}$ imposes the peak amplitude of the sinusoid $e^{j 10 \pi t}$ of 5 Hz.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_transforms\_laplace\_basis}{snip_transforms_laplace_basis}
%\begin{lstlisting}
%sigma = 0.3; w = 2*pi*5; %frequency of 5 Hz
%s=sigma+j*w; %define the complex variable s
%t=linspace(0,3,1000); %interval from [0, 3] sec.
%x=exp(s*t); %the signal
%envelope = exp(sigma*t); %the signal envelope
%subplot(121);plot(t,real(x));hold on,plot(t,envelope,':r')
%\end{lstlisting}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/laplace_basis}               
        \caption[{Real part of $e^{(-\sigma + j 10 \pi)t}$. The values of $\sigma$ are $-0.3$ and 0.3 for the first (left) and second graphs, respectively.}]{Real part of $e^{(-\sigma + j 10 \pi)t}$. The values of $\sigma$ are $-0.3$ and 0.3 for the first (left) and second graphs, respectively. The complex exponential $e^{j 10 \pi t}$ corresponds to a frequency of 5~Hz and the amplitude envelope is imposed by $e^{-\sigma t}$.\label{fig:laplace_basis}}
\end{figure}

The basis functions $e^{(-\sigma + j \aw) t}$ have some peculiarities. If $\sigma < 0$ their amplitudes reach $\infty$ when $t \rightarrow \infty$. If $\sigma > 0$ their amplitudes reach $\infty$ when $t \rightarrow -\infty$. Therefore, the Laplace transform is more useful in the analysis of one-sided signals, such as the ones that incorporate $u(t)$ (right-sided, also called causal) or $u(-t)$ (left-sided or anti-causal). In these cases, the basis function do not create mathematical problems at infinite because the ROC is chosen in a way that the signal itself is zero at the problematic limiting values $t=\infty$ or $-\infty$.

\subsection{Laplace transform of one-sided exponentials}
Using \equl{laplace_forward} and assuming a complex-valued $a \in \CC$, it is useful to calculate and get familiar with the Laplace transform for the
causal $x(t)=e^{at} u(t)$ and anti-causal $x(t)=-e^{at} u(-t)$ signals. 
Before discussing ROCs in general, it is useful to study the ROCs and transform pairs of these two signals:
\begin{equation}
x(t)=e^{at} u(t) \Leftrightarrow X(s)=\frac{1}{s - a}, \textrm{~which converges when~} \real{s} > \real{a},
\label{eq:laplace_causal}
\end{equation}
and
\begin{equation}
x(t)=-e^{at} u(-t) \Leftrightarrow X(s)=\frac{1}{s - a}, \textrm{~which converges when~} \real{s} < \real{a}.
\label{eq:laplace_anticausal}
\end{equation}
These pairs are widely proved and discussed in the literature (see, e.\,g. \akurl{https://eng.libretexts.org/Bookshelves/Electrical_Engineering/Signal_Processing_and_Modeling/Signals_and_Systems_(Baraniuk_et_al.)/11\%3A_Laplace_Transform_and_Continuous_Time_System_Design/11.06\%3A_Region_of_Convergence_for_the_Laplace_Transform}{2roc}).
The next example proves \equl{laplace_causal}.

\bExample \textbf{A time-domain complex exponential leads to a rational function in $s$.}
%For example, an exponential in time-domain leads to a rational function in $s$:
Consider $a \in \CC$ and the following Laplace transform pair
\[
e^{at} u(t) \Leftrightarrow \frac{1}{{s - a}},\textrm{~~which converges for:~~} \real{s} > \real{a}.
\]
\ignore{
\s ubsection{Laplace Transform Properties}
\begin{enumerate}
\item Modulation
\[
e^{at} f(t) \Leftrightarrow F(s - a)
\]
\item Convolution
\[
f(t) \conv g(t) \Leftrightarrow F(s) G(s)
\]
\end{enumerate}  
}
\ignore{
\s ubsection{Laplace transform pairs}
This section lists the most important pairs.
\begin{enumerate}
\item exponential $\Leftrightarrow$ rational function
\[
e^{at} u(t) \Leftrightarrow \frac{1}{{s - a}},\textrm{~~converges for: \Re\{s\} > a}
\]
and
\[
-e^{at} u(-t) \Leftrightarrow \frac{1}{{s - a}},\textrm{~~converges for: \Re\{s\} < a}
\]
\end{enumerate}  
}
This result is proved as follows:
\begin{align*}
X(s) &= \int_{-\infty}^\infty x(t) e^{- s t} \textrm{d}t = \int_{0}^\infty e^{(a - s) t} \textrm{d}t =
\left. \frac 1 {a-s} {e^{(a-s) t}} \right|_{0}^{\infty}\\
& = \frac 1 {a-s} \left[ (\lim_{t \rightarrow \infty} e^{(a-s)t} )- 1 \right].
\end{align*}
Given that $s=\sigma+j\aw$, the limit can be obtained as
\[
\lim_{t \rightarrow \infty} e^{(a-s) t} = \lim_{t \rightarrow \infty} \frac{e^{(a-\sigma)t}} {e^{j\aw t}} = 0
\]
when $\real{a - \sigma} < 0$ (note $|e^{j\aw t} |=1$ and only the numerator defines the convergence).
\eExample

Knowing $X(s)$ does not suffice to uniquely identify $x(t)$, because the two different signals $x(t)$ have the same Laplace transform $X(s)=1/(s-a)$. Hence, the information about the values of $\real{s}=\sigma$ for which the Fourier transform in \equl{laplace_forward} converges, must be known too in order to properly identify the original $x(t)$ when recovering it from $X(s)$. In summary, distinct signals $x(t)$ can have the same $X(s)$, differing only in their regions of convergence. This fact will be further explored in the next subsections.

\subsection{Region of convergence for a Laplace transform}

%Instead of choosing a specific value of $\sigma$, when calculating a Laplace transform $X(s)$, one considers what can be achieved with virtually all possible complex values of $s$. 
The values of $s$ for which the associated Fourier transform $\calF\{x(t) e^{-\sigma t}\}$ converges compose the so-called \emph{region of convergence} (ROC) of the Laplace transform\index{Region of convergence (ROC), Laplace transform}. 
Observing \equl{laplace_causal} and \equl{laplace_anticausal}, one can conclude that for these signals, the ROC is a region $\sigma < a$ at the left of $a$ or at the right ($\sigma > a$). 

This can be 
generalized and, in fact, the ROC of Laplace transforms are always defined by ranges that depend solely on $\sigma$ (the angular frequency $\aw$ does not influence ROCs).
For any signal $x(t)$, the Laplace transform convergence depends on $\sigma$, and the ROCs are regions formed at the left or right of a given value $\sigma_0$. When $x(t)$ is right-sided, the ROC is the area at the right of some $\sigma_0$, while for left-sided $x(t)$ the ROC is the area at the left of a value $\sigma_0$. When $x(t)$ is composed by several parcel, the ROC is the intersection of the
ROCs of these parcels.

For instance, $y(t)=e^{-3t} u(t)$ has Laplace transform $Y(s)=1/(s+3)$ with ROC $\sigma > -3$.
The anti-causal signal $z(t)=-e^{2t} u(-t)$ has Laplace transform $Z(s)=1/(s-2)$ with ROC $\sigma < 2$.
Because the Laplace is a linear transform, the bilateral signal $x(t)=y(t)+z(t)$
has Laplace transform $X(s)=Y(s)+Z(s)$ with the ROC being $-3 < \sigma < 2$, the intersection between
the ROCs of $Y(s)$ and $Z(s)$.

Another example that $x(t)$ can be recovered from the knowledge of $X(s)$ and its associated ROC is the following.

\bExample \textbf{Distinct signals may have the same Laplace transform, and only the ROC can disambiguate.}
\label{ex:roc_disambiguates}
Given the Laplace transform $X(s)=1/(s+2)$, one should find the corresponding signal $x(t)$. In fact, there are two possible signals, depending on the ROC. The pole is at $s=-2$ and assuming the ROC is $\real{s}>-2$, then $x(t)=e^{-2t} u(t)$. If the ROC is $\real{s}<-2$, then $x(t)=-e^{-2t} u(-t)$.
\eExample

%For the example $x(t)= e^{3t} u(t)$, the ROC is $\sigma < -3$.

\subsection{Inverse Laplace of rational functions via partial fractions}
\label{sec:inverse_laplace}

In general, the inverse Laplace transform is calculated using \equl{laplace}.
But for the specific, and very important category of Laplace transforms $X(s)$ that are ratios $X(s)=B(s)/A(s)$ of two polynomials,
one can use partial fraction expansion (see Appendix~\ref{sec:partial_fraction}) to conveniently calculate the inverse $x(t)=\calL^{-1} \{ X(s) \}$.

The inverse transform of a rational function $X(s)=B(s)/A(s)$ can be obtained by the following steps:
\begin{enumerate}
        \item make sure the degree $N$ of the denominator is larger than the degree $M$ of the numerator, that is, $N>M$. If this is not the case, perform some steps of polynomial division until being able to rewrite $X(s)$ as $X(s)=D(s) + R(s)/A(s)$, such that the degree $Q$ of $R(s)$ is $Q < N$. Perform partial fraction expansion on $R(s)/A(s)$ instead of $B(s)/A(s)$.
        \item find the poles (roots of $A(s)$) and expand the rational function ($B(s)/A(s)$ or $R(s)/A(s)$) in partial fractions 
				as discussed in Appendix~\ref{sec:partial_fraction}. These parcels can be written as $r_i/(s-p_i)$, where $r_i$ is the
				\emph{residue} of the pole $p_i$. If a pole has multiplicity $\lambda_i$ larger than 1, partial fractions $r_{ij}/(s-p_i)^{m_j}, j=1, \ldots, \lambda_i$ also compose the expansion.
				\item Based on the region of convergence of $X(s)$, determine for each partial fraction whether it should generate a causal
				or non-causal time-domain signal. Then convert the parcel from the transform to the time domain accordingly.
				For instance, if the ROC is $\real{s} > \real{a}$ and the parcel is
				$\frac{1}{s - a}$, the corresponding time-domain signal is $e^{at} u(t)$. But if the ROC associated to
				$\frac{1}{s - a}$ is $\real{s} < \real{a}$, then $-e^{at} u(-t)$.
				At this step, if there are poles with multiplicity larger than 1, the following property of the Laplace transform is useful:
				$\mathcal{L}^{-1}\left\{\frac{1}{(s+a)^n}\right\} = \frac{t^{n-1} e^{-at}}{(n-1)!}$.
        \item Assuming the coefficients of $X(s)$ are real-valued, if there are complex conjugate poles, their
				respective residues will be also complex conjugates. Rearrange each pair of partial fractions corresponding
				to complex conjugate poles to obtain a simplified time-domain representation.
\end{enumerate}

The next example illustrates the inverse Laplace transform and other concepts.

\bExample \textbf{Zeros and poles of a Laplace transform}.
Assume that the Laplace transform $X(s)$ of a causal signal $x(t)$ is given by
\begin{equation}
X(s) =  \frac{s-1}{s^3+4s^2+9s+10} = \frac{s-1}{(s+2)(s+1-j2)(s+1+j2)},
\label{eq:laplace_example}
\end{equation}
with ROC $\sigma > -1$, which corresponds to the intersection between the ROCs of individual parcels of $x(t)$.
\figl{s_polezero} depicts the positions of the poles and zeros in the S-plane. And \figl{s_mag} and \figl{s_phase} show the magnitude (in dB) and phase of $X(s)$, respectively.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/s_polezero}
        \caption{Three poles (marked with `x') and zero (marked with `o') of \equl{laplace_example}.\label{fig:s_polezero}}
\end{figure}


\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/s_mag}               
        \caption{Magnitude (in dB) of $X(s) =  \frac{s-1}{s^3+4s^2+9s+10} = \frac{s-1}{(s+2)(s+1-j2)(s+1+j2)}$ (\equl{laplace_example}).\label{fig:s_mag}}
\end{figure}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/s_phase}             
        \caption{Phase (in rad) of \equl{laplace_example}.\label{fig:s_phase}}
\end{figure}

The values for which $X(s)=0$ are called \emph{zeros} and the ones that lead to $X(s)=\infty$ are called \emph{poles}\index{Zeros and poles}.
\figl{s_polezero} and \figl{s_mag} provide complementary views of the positions of the zero at $s=1$ and the tree poles, as well as their impacts on the magnitude of $X(s)$. The locations of zeros appear as ``valleys'' while the poles are located at ``volcanoes''. It is intuitive that the ROC cannot include poles because they are the positions for which $X(s)=\infty$, which violates ``convergence''. For the example in \figl{s_mag}, the ROC is $\sigma > -1$ because $x(t)$ is right-sided and the right-most poles (assuming the orientation of the $\sigma$-axis is from $-\infty$ to $+\infty$) are at $\sigma_0 = -1$.

\figl{s_phase} shows the phase of $X(s)$. In most cases the phase is less instructive than the magnitude. \figl{s_phase} emphasizes that the poles are conventionally signalized with ``x'' marks while zeros are represented by ``o'' marks.

The inverse Laplace transform can be obtained via partial faction decomposition (see Appendix~\ref{sec:partial_fraction}). Using the method \ci{residue} in Python or {\matlab} is very convenient. For instance, in the case of {\matlab}:
\begin{lstlisting}
[r,p,k]=residue([1, -1], [1 4 9 10])
\end{lstlisting}
gives the residues \ci{r} for each pole in \ci{p}. This allows to write:
\begin{equation}
X(s) =  \frac{s-1}{(s+2)(s+1-j2)(s+1+j2)} = \textcolor{red}{\frac{-0.6}{s+2}} + \textcolor{blue}{\frac{0.3-j0.1}{s+1-j2}} + \frac{0.3+j0.1}{s+1+j2}.
\label{eq:complex_residues}
\end{equation}
When all coefficients of $X(s)$ are real-valued, the pair of residues corresponding to complex conjugate poles are also complex conjugates.
This is the case of the residues $0.3 \pm j0.1$ in \equl{complex_residues}.
By inspection of each of the three parcels and knowing $x(t)$ is causal, one can obtain the inverse transform as
\[
x(t) = \left[ \textcolor{red}{-0.6 e^{-2t}} +  \textcolor{blue}{(0.3-j0.1)e^{(-1+j2)t}} +  (0.3+j0.1)e^{(-1-j2)t} \right] u(t).
\]
It is convenient to expand the second (in blue) and third (in black) parcels and rewrite this equation as:
\[
x(t) = \left[ \textcolor{red}{-0.6 e^{-2t}} + \textcolor{blue}{0.3e^{-t} e^{j2t} - j0.1e^{-t} e^{j2t}} + 0.3e^{-t} e^{-j2t} + j0.1e^{-t} e^{-j2t}
 \right] u(t).
\]
Grouping the parcels that can be written as a cosine and a sine, multiplying them by $2/2$ and $2j/(2j)$, and then using \equl{euler} (Euler's formula), one can write $x(t)$ as:
\begin{align*}
x(t) = & \left[ \textcolor{red}{-0.6 e^{-2t}} + 2 \times 0.3e^{-t} \cos(2t) + 2 \times 0.1e^{-t} \sin(2t) \right] u(t) \\
 = & \frac{1}{5} \left[e^{-t} (3 \cos(2 t) + \sin(2 t)) - 3 e^{-2 t} \right] u(t).
\end{align*}

This result can be obtained with symbolic math. For instance, using method \ci{ilaplace} for the inverse Laplace transform in {\matlab}:
\begin{lstlisting}
syms s % define s as a symbolic variable
Xs = (s-1)/(s^3 + 4*s^2 + 9*s + 10) 
xt = ilaplace(Xs)
pretty(simplify(xt))
\end{lstlisting}
These commands provide the same $x(t)$ calculated here.
\eExample 

An interesting fact is that the Laplace transform is a redundant representation and $x(t)$ could be eventually recovered from $X(s)|_{\sigma = \sigma_1}$ if $\sigma_1$ is in the ROC. The following subsection will explore the possibility of finding the Fourier transform (if it exists) from the Laplace transform, by imposing the restriction of $\sigma = 0$. From this Fourier transform, one can eventually recover $x(t)$, which illustrates the point that a Laplace transform has considerable redundancy in its representation of $x(t)$.

\subsection{Calculating the Fourier transform from a Laplace transform}
\label{sec:fourier_from_laplace}

The Fourier transform basis function is equivalent to the Laplace's when $\sigma=0$.
Hence, the Laplace transform properties and pairs are similar to the Fourier ones.

Knowing that $X(s) = \calF\{x(t) e^{-\sigma t}\}$ the Fourier transform of $x(t)$ can be derived from its Laplace transform, i.\,e., 
\begin{equation}
X(\aw)=X(s)|_{\sigma=0}=X(s)|_{s=j\aw}
\label{eq:fourierFromLaplace}
\end{equation}
in case $\sigma=0$ is in the ROC of $X(s)$ and, consequently, $x(t)$ has a Fourier transform. 

Repeating this important result: \equl{fourierFromLaplace} is valid only if the signal $x(t)$ has a valid Fourier transform. Using \exal{roc_disambiguates}, note that the axis $j \aw$ is part of the ROC of $x(t)=e^{-2t} u(t)$, which then has a Fourier transform
\[
X(\aw)=X(s)|_{s=j\aw}=\frac{1}{j\aw + 2}.
\]
In contrast, $x(t)=-e^{-2t} u(-t)$ does not have a valid Fourier transform: $X(\aw)$ does not converge and using \equl{fourierFromLaplace} would lead to a wrong result!

When the signal has both Fourier and Laplace transforms, a graph of $X(s)$ incorporates the corresponding graph of $X(\aw)$. \figl{s_mag_and_jw} shows the magnitude values for $s=j\aw$ (the $j\aw$ axis) as a curve in black, superimposed to \figl{s_mag}. Alternatively,  \figl{s_jw} shows only the values of $X(s)$ at the $s=j\aw$ axis.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/s_mag_and_jw}        
        \caption{Graph of the magnitude (in dB) of $X(s) =  \frac{s-1}{s^3+4s^2+9s+10} = \frac{s-1}{(s+2)(s+1-j2)(s+1+j2)}$ (\figl{s_mag}) with the identification of the corresponding values of the Fourier transform (magnitude).\label{fig:s_mag_and_jw}}
\end{figure}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/s_jw}        
        \caption{The values of the magnitude of the Fourier transform corresponding to \figl{s_mag_and_jw}.\label{fig:s_jw}}
\end{figure}

Both representations in \figl{s_mag_and_jw} and \figl{s_jw} are three-dimensional and difficult to work with.
\figl{s_freqresponse} depicts the frequency response in the conventional way, which is much easier to interpret than looking at \figl{s_jw}, for example. However, it is interesting to compare the figures and note that the peaks in \figl{s_freqresponse} are related to the position of the poles and draw conclusions such as that, the closer the pole is to the $\sigma=0$ axis, the more evident is the corresponding peak at the Fourier transform. In filter design, it is sometimes necessary to locate poles in the vicinity of the $\aw$ axis because this conducts to filters with high \emph{quality factor}.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/s_freqresponse}              
        \caption[{Two dimensional representation of \figl{s_jw} obtained with the command \ci{freqs} in {\matlab} showing the peak at $\aw=2$ rad/s due to the respective pole.}]{Two dimensional representation of \figl{s_jw} obtained with the command \ci{freqs} in {\matlab} showing the peak at $\aw=2$ rad/s due to the respective pole. Because the signal $x(t)$ is real and $X(\aw)$ exhibits Hermitian symmetry, only the graphs for $\aw \ge 0$ are shown.\label{fig:s_freqresponse}}
\end{figure}

The need to use the Laplace as an alternative to the Fourier transform would be even greater if impulses were not allowed in Fourier analysis.
The use of impulses in both the time and frequency domains allows a much larger class of signals to be represented with Fourier equations.
For example,
the \emph{ramp}\index{Ramp signal} signal $x(t)=t~u(t)$, strictly does not have a Fourier transform because the following integral does not converge:
\[
X(\aw) = \int_{-\infty}^\infty x(t) e^{- j \aw t} \textrm{d}t = \int_{0}^\infty t e^{- j \aw t} \textrm{d}t =
\left. \frac{1+j \aw t}{\aw^2  e^{j \aw t}} \right|_{0}^{\infty} =
\lim_{t \rightarrow \infty} \frac{1+j \aw t}{\aw^2  e^{j \aw t}}.
\]
However, using impulses, it is possible to use Fourier representations of signals such as $x(t)=t~u(t)$, $x(t)=u(t)$ and $x(t)=\cos(\aw_0 t)$. This fact influences the main applications of the Laplace transform to be the representation of systems, not signals. For example, in terms of signals, while $x(t) = \cos (\aw_0 t) u(t)$ has the transform $X(s) = s/(s^2 + \aw_0^2)$, an infinite-duration sinusoid is not represented in the Laplace transform domain.

Because most signals that are analyzed with the Laplace transform are right-sided, sometimes the adopted definition is
\begin{equation}
X(s) = \int_{0}^\infty x(t) e^{-st} \textrm{d}t,
\label{eq:unilateralLaplace}
\end{equation}
which is called the \emph{unilateral Laplace transform}\index{Unilateral Laplace transform} (in contrast to the bilateral definition of \equl{laplace}). Both coincide if the signal is right-sided (e.\,g., by the action of $u(t)$).

%Make a distinction: have the transform or the integral converge.
%Functions that do not asymptote to zero in both the $\infty$ and $-\infty$ directions generally do not have Fourier transforms.

\section{Z Transform}

The Z transform is the counterpart of the Laplace transform for discrete-time signals. The pair of equations is given by
\begin{equation}
\left\{
\begin{array}{ccc}
X(z) & = & \sum_{n=-\infty}^\infty x[n] z^{-n} \\
& & \\
x[n] & = & \frac{1}{2 \pi j} \oint_{\calC} X(z) z^{n-1} dz,
\\ \end{array}
\right.
\label{eq:z_transform_pair}
\end{equation}
where $\calC$  is a counterclockwise closed path encircling the origin and entirely in the region of convergence (ROC). The contour (or path) $\calC$ must encircle all of the poles of $X(z)$.

%AK-lATER - REVIEW The Laplace and Z transforms are related.

\subsection{Relation between Laplace and Z transforms}

The Laplace and Z transforms are related.
When the Laplace transform is performed on a sampled signal $x_s(t)$ and a C/D is used, the result is the Z transform\footnote{
Some textbooks (e.\,g.~\cite{Ciofficn})
%Cioffi, Barry and other authors
that deal with coding theory use the $D$ instead\index{D transform} of the $Z$ transform, where $D=z^{-1}$. For example, $H(z)=1-0.9z^{-1}$ corresponds to $H(D)=1-0.9D$ and $H(z)=1-0.5z$ corresponds to $H(D)=1-0.5D^{-1}$.}
of a discrete-time sequence $x[n]$ where
\begin{equation}
z=e^{s \ts}
\label{eq:relating_s_z}
\end{equation}
and $\ts$ is the sampling period.

\equl{relating_s_z} is used in the \emph{matched Z-transform}\index{Matched Z-transform} method for converting $H(s)$ in $s$ into $H(z)$ in $z$, and is further discussed in
Section~\ref{sec:HsIntoHz}.

\bExample \textbf{Converting $x(t)$ to discrete-time and finding the corresponding Z-transform}.
\label{ex:exampleZ} 
As in \exal{simplifiedCD}, assume a continuous-time signal $x(t) = 4 e^{-2t} u(t)$ should be transformed to a discrete-time $x[n]$ with a sampling period $\ts$, and then have its Z-transform
$X(z) = \calZ \{x[n] \}$ calculated. From \equl{cdConversionExample2}:
\begin{equation}
X(z) = \calZ \left\{ 4 e^{-2 n \ts} u[n] \right\} = 4 \calZ \left\{ \left(e^{-2 \ts}\right)^n u[n] \right\} = \frac{4}{1 - e^{-2 \ts}z^{ -1 }},
\label{eq:exampleZ}
\end{equation}
where the last step used \equl{zOfExponential}.
% with $a = e^{-2 \ts}$ and the fact that $|a|<1$ for $\ts>0$.
%Section~\ref{ex:simplifiedCD}
\eExample

\subsection{{\akadvanced} Z transform basis functions}

The basis functions of the Z transform are $z^n$, $z \in \complex$. 
The complex-value $z$ could be written in Cartesian coordinates but it is more convenient
to write it in polar coordinates, as $z= r e^{j \dw}$.
Taking in account the discrete-time $n$, a Z basis function can be written as
\[
x[n] = z^n = (r e^{j \dw})^n = r^n e^{j n \dw} = r^n [ \cos(n \dw) + j \sin(n \dw) ].
\]

Similar to the Laplace basis function $e^{(-\sigma + j \aw)t}$, which has $\sigma$ and $\aw$,
the Z basis function is also controlled by two parameters: $r$ and $\dw$. As $\sigma$ in the Laplace transform,
the value of $r$ imposes the envelope of the Z basis function while $\dw$ (similar to $\aw$) controls the
rate of its oscillation (see \figl{laplace_basis}).

The similarity of Laplace and Z basis functions can be made more explicit by
writing 
\[
x(t) = e^{(-\sigma + j \aw)t} = e^{-\sigma t} e^{j \aw t} = r^t [ \cos(\aw t) + j \sin(\aw t) ],
\]
where $r = e^{-\sigma}$.
Comparing the Laplace basis function $x(t)$ with the Z basis function $x[n]$, it is clear that
they describe the same category of signals, but in continuous-time and discrete-time, respectively.

But the region of convergence of a Laplace transform depends on $\sigma$, so it is convenient
to write the complex value $s = \sigma + j \aw$ in Cartesian coordinates, to make $\sigma$ explicit.
On the other hand, as will be discussed in this section, the region of convergence of a Z transform
is determined by the magnitude $r$ of a $z$ value, which makes more convenient to write the complex value $z = r e^{j \dw}$
in polar coordinates, making $r$ explicit.

\subsection{Some pairs and properties of the Z-transform}

A very useful Z transform pair is
\begin{itemize}
\item $\delta[n] \Leftrightarrow 1$ 
\end{itemize}
and an important Z transform property is
\begin{itemize}
\item $x[n-n_0] \Leftrightarrow X(z)z^{-n_0}$.
\end{itemize}
Putting them together leads to 
\begin{itemize}
\item $\delta[n-n_0] \Leftrightarrow z^{-n_0}$.
\end{itemize}
With this result, and using linearity one can write, e.\,g.,
\[
3\delta[n+4] + 2\delta[n] - 2.5\delta[n-3] \Leftrightarrow 3 z^{4} + 2 - 2.5 z^{-3}.
\]

In cases $x[n]$ is left-sided or right-sided, $X(z) = \sum_{n=-\infty}^\infty x[n] z^{-n}$ can be written as a geometric series and \equl{sum_inf_pg} used to obtain $X(z)$.
For example, the Z transform of $x[n] = a^n u[n]$ is obtained as follows:
\[
X(z) = \sum_{n=-\infty}^\infty x[n] z^{-n} = \sum_{n=-\infty}^\infty a^n u[n] z^{-n} =
\sum_{n=0}^\infty a^n z^{-n} = \sum_{n=0}^\infty (a/z)^n.
\]
Using \equl{sum_inf_pg} with a scale factor $\alpha=1$, ratio $r=a z^{-1}$ and $|a/z|<1$ leads to
\begin{equation}
a^n u[n] \Leftrightarrow \frac{1}{1 - az^{ -1 }} = \frac{z}{z - a}, \left| z \right| > \left| a \right|.
\label{eq:zOfExponential}
\end{equation}


\subsection{Region of convergence for a Z transform}

Similar to the Laplace transform, the values of $z$ for which the transform exists are called the region of convergence (ROC). The ROC of Z transforms are annular regions of the form $|z| > m$ (for right-sided sequences, also called \emph{causal}), $|z|<m$ (for left-sided sequences) or $m < |z| < p$ (for two-sided sequences), where $m,p \in \Re_+$. When the signal in time-domain has a finite support (duration), the ROC of the associated Z transform is the whole $z$-plane, eventually with the exceptions of $z=0$ and $z=\infty$.

To recover $x[n]$ from its transform $X(z)$, it is essential to know the ROC.
For example, the Z transform of $x[n] = -a^n u[-n-1]$ is:
\[
X(z) = \sum_{n=-\infty}^\infty x[n] z^{-n} = -\sum_{n=-\infty}^{-1} a^n z^{-n} =
-\sum_{n=1}^\infty a^{-n} z^{n} = -\sum_{n=1}^\infty (z/a)^n.
\]
In order to use \equl{sum_inf_pg} with factor $\alpha=1$ and ratio $r=z/a$ one can modify the summation interval
\[
X(z) = -\sum_{n=1}^\infty (z/a)^n = 1 - \sum_{n=0}^\infty (z/a)^n = 1 - \frac{1}{1-z/a} = \frac{z}{z-a},
\]
with the ROC $|z|<|a|$ (because \equl{sum_inf_pg} requires $|z/a|<1$). In summary, both $a^nu[n]$ (see \equl{zOfExponential}) and $-a^n u[-n-1]$ have $X(z)=z/(z-a)$ and only the ROC can disambiguate them when calculating the inverse Z transform.

\subsection{Inverse Z of rational functions via partial fractions}
\label{sec:inverse_z}

In general, the inverse Z transform is calculated using \equl{z_transform_pair}.
But similar to the methodology described for the Laplace transform in Section~\ref{sec:inverse_laplace}, 
for the important category of Z transforms $X(z)$ that are ratios $X(z)=B(z)/A(z)$ of two polynomials,
one can use partial fraction expansion (see Appendix~\ref{sec:partial_fraction}) to conveniently calculate the inverse $x[n]=\calZ^{-1} \{ X(z) \}$.

The inverse transform of rational functions $X(z)$ can be obtained by following two simple steps and then the 
procedure in Section~\ref{sec:inverse_laplace} adopted for inverse Laplace transforms. The procedure is the following:
\begin{enumerate}
        \item make the rational function $X(z)$ to have only non-negative\footnote{This is not mandatory, but allows to use the same procedure for partial fraction expansion as for the Laplace transform.} powers of $z$,
        \item find the poles and expand $X(z)$ in partial fractions as discussed in Section~\ref{sec:partial_fraction},
        \item eventually multiply by $z$ to create $Y(z)=z X(z)$ and force the appearance of terms term $z/(z-a)$,
        \item convert each parcel of $Y(z)$ to the time domain,
        \item rearrange the terms, especially the ones corresponding to complex conjugate poles and
        \item because $Y(z)=z X(z)$, then $x[n]=y[n-1]$. Find the final result substituting $n$ by $n-1$ in $y[n]$.
\end{enumerate}
For example, to obtain the inverse transform of
\[
X(z) = \frac{z^{-2} + 0.9 z^{-3}}{1 - 1.8z^{-1} + 1.41 z^{-2}- 0.488 z^{-3}},
\]
with ROC $|z|>0.8$, one can multiply numerator and denominator by $z^3$ and obtain their roots:
\[
X(z) = \frac{z + 0.9}{z^3 - 1.8z^2 + 1.41 z - 0.488}=\frac{z+0.9}{(z-0.8)(z-0.5 \pm j0.6)}.
\]
The partial fraction expansion is
\[
X(z) = \frac{3.78}{z-0.8} + \frac{-1.89+j0.11}{z-0.5+j0.6} + \frac{-1.89-j0.11}{z-0.5-j0.6}.
\]
%Note that a complex conjugate pair have residues that are complex conjugates.
Multiplying both sides by $z$ leads to
\[
Y(z) = z X(z) = \frac{3.78z}{z-0.8} + \frac{(-1.89+j0.11)z}{z-0.5+j0.6} + \frac{(-1.89-j0.11)z}{z-0.5-j0.6},
\]
which can be rewritten by converting the complex numbers from Cartesian to polar form
\[
Y(z) = \frac{3.78z}{z-0.8} + \frac{(1.89e^{j3.08})z}{z-0.78e^{j2.26}} + \frac{(1.89e^{-j3.08})z}{z-0.78e^{-j2.26}},
\]
Because in this case the ROC is for a right-sided sequence, each term $z/(z-a)$ corresponds to $a^n u[n]$ and the time domain signal is
\begin{eqnarray*}
y[n] & = &[3.78 (0.8)^n+1.89e^{j3.08} (0.78e^{j2.26})^n + 1.89e^{-j3.08} (0.78e^{-j2.26})^n] u[n] \\
           & = &[3.78 (0.8)^n + 1.89  (0.78)^n (e^{j(3.08+2.26n)}+e^{-j(3.08+2.26n)}) ] u[n]\\
           & = &[3.78 (0.8)^n + 2 \times 1.89 (0.78)^n \cos(2.26n+3.08)] u[n]
\end{eqnarray*}
which leads to
\[
x[n] = [3.78 (0.8)^{n-1} + 2 \times 1.89 (0.78)^{n-1} \cos(2.26(n-1)+3.08)] u[n-1].
\]

Note that a pair of complex conjugate poles have complex conjugate residues. Let
$r=b e^{j\alpha}$ and $r^*=b e^{-j\alpha}$ be the residues for poles $p=a e^{j \theta}$ and $p^*=a e^{-j \theta}$, respectively, both with multiplicity one. With a ROC corresponding to right-sided signals, the two terms $rz/(z-p)$ and $r^*z/(z-p^*)$ in the partial fraction expansion can be rearranged in time domain to compose the general expression
\[
2 b a^n \cos(\theta n + \alpha) u[n].
\]
If the ROC corresponds to left-sided signals, the same terms correspond to
\[
- 2 b a^n \cos(\theta n + \alpha) u[-n-1].
\]

Different approaches to obtain $x[n]$ can lead to distinct expressions, but these expressions must correspond to the same values of $x[n], \forall n$. For example, some people prefer to obtain the partial fraction expansion of $X(z)/z$
%, multiply the obtained fractions by $z$ and then compose $x[n]$ by inspection (
instead of using the suggested steps 3) and 6). Expanding $X(z)/z$ allows to multiply the obtained partial fractions by $z$ to get $z/(z-a)$ factors. An example better illustrates the equivalence of both procedures and the reason for suggesting ours.

Assume the task is to find the inverse $x[n]$ of $X(z)=(8z-19)/[(z-2)(z-3)]$ knowing that $x[n]$ is right-sided.
Using the alternative procedure of expanding $X(z)/z$, one has
\[
\frac{X(z)}{z} = (8z-19)/[z(z-2)(z-3)] = -\frac{19/6}{z} + \frac{3/2}{z-2} + \frac{5/3}{z-3},
\]
which can be conveniently multiplied by $z$ to obtain parcels in the form $z/(z-a)$:
\[
X(z) = -\frac{19}{6} + 3/2 \frac{z}{z-2} + 5/3 \frac{z}{z-3},
\]
that leads to the inverse
\begin{equation}
x[n]=-\frac{19}{6}\delta[n] + \left( \frac{3}{2} 2^n + \frac{5}{3} 3^n \right) u[n].
\label{eq:example_of_inverse}
\end{equation}

Using the steps of the suggested procedure, which expands $X(z)$ in partial fractions instead of $X(z)/z$, the result is
\[
x[n]=[5 (3^{n-1}) + 3 (2^{n-1})] u[n-1],
\]
which seems different than \equl{example_of_inverse}. However, a closer inspection indicates that, for both expressions, the sample values $x[0]=0, x[1]=8$, etc., are the same, i.\,e., the procedures led to the same signal, as expected.

\subsubsection{Simplifying the partial fractions corresponding to complex conjugates}


The Web has several tables of properties and pairs related to the Z transform.\footnote{Such as the ones at \akurl{http://en.wikipedia.org/wiki/Z-transform}{2ztr}.}
An interesting result is the \emph{initial value theorem}\index{Initial value theorem}, which is valid for signals for which $x[n]=0$ for $n<0$ (right-sided) and states that
\[
x[0] = \lim_{z \rightarrow \infty} X(z).
\]
Using this theorem, one can anticipate that $x[0]=0$ when $X(z)$ has a denominator with degree larger than the numerator, such as in $X(z)=(8z-19)/[(z-2)(z-3)]$.

%a=-0.9; b=0.8; c=0.5+j*0.6;
%poly([b c conj(c)])

\ignore{
\  subsection{Z Transform Pairs}
\begin{enumerate}  
\item exponential $\Leftrightarrow$ rational function
\[
a^n u[n] \Leftrightarrow \frac{1}{1 - az^{ -1 }} = \frac{z}{z - a}, \left| a \right| < 1
\]
\end{enumerate}  
\ subsection{Z Transform Properties}
\begin{enumerate}  
\item Convolution
\item Multiplication
\item Time-shift
\end{enumerate}
Moreover, according to the final-value theorem for z-transforms, the steady
provided that the region of convergence of E(z) includes the unit circle.
}


%Xz(x,y)=(z-a)/((z-b)*(z-c)*(z-conj(c)));
%a=-0.9; b=0.8; c=0.5+j*0.6;

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/z_mag}               
        \caption{Magnitude (in dB) of $X(z) = \frac{z+0.9}{(z-0.8)(z-0.5-j0.6)(z-0.5+j0.6)}$ (\equl{example_z_transform}).\label{fig:z_mag}}
\end{figure}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/z_phase}             
        \caption{Phase (in rad) of \equl{example_z_transform}.\label{fig:z_phase}}
\end{figure}

Similar to \figl{s_mag} and \figl{s_phase}, which are for Laplace, \figl{z_mag} and \figl{z_phase} depicts the magnitude and phase, respectively, for
\begin{equation}
X(z) = \frac{z+0.9}{(z-0.8)(z-0.5-j0.6)(z-0.5+j0.6)},
\label{eq:example_z_transform}
\end{equation}
which has one finite zero and three poles as indicated in \figl{z_polezero}.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/z_polezero}          
        \caption{Pole / zero diagram for \equl{example_z_transform}.\label{fig:z_polezero}}
\end{figure}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/z_mag_and_circle}            
        \caption{Graph of the magnitude (in dB) of $X(z) = \frac{z+0.9}{(z-0.8)(z-0.5-j0.6)(z-0.5+j0.6)}$ (\figl{z_mag}) with the identification of the corresponding values of the DTFT (unit circle $|z|=1$).\label{fig:z_mag_and_circle}}
\end{figure}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/z_circle}            
        \caption{The values of the magnitude of the DTFT corresponding to \figl{z_mag_and_circle}.\label{fig:z_circle}}
\end{figure}

\subsection{Calculating the DTFT from a Z transform}

The relation between the Z transform and the DTFT is similar to the one between Laplace and Fourier transforms discussed
in Section~\ref{sec:fourier_from_laplace}.
\figl{z_mag_and_circle} and
\figl{z_circle} provide an example using \equl{example_z_transform}.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/z_freqresponse}              
        \caption{Magnitude (top) and phase (bottom) of the DTFT corresponding to \equl{example_z_transform}. These plots can be obtained with the {\matlab} command \ci{freqz} and are a more convenient representation than, e.\,g., \figl{z_circle}.\label{fig:z_freqresponse}}
\end{figure}

Sometimes it is not convenient to deal with 3-d plots such as \figl{z_circle}. An alternative is to represent the DTFT using a figure similar to \figl{z_freqresponse}, which shows the magnitude and phase with the angle as independent variable. As discussed in Section~\ref{sec:frequency_normalization}, for convenience, the abscissa is normalized by $\pi$, such that ``1'' corresponds to $\pi$ rad. Due to the symmetry of $X(z)$ when $x[n]$ is real, it is also common to represent the abscissa in the range $[0, \pi]$ (instead of $[0,2\pi[$ as in \figl{z_freqresponse}).


\section{Applications}
\label{sec:transf_applic}

This section will briefly discuss some applications of transforms.
%will be discussed: electrocardiogram (ECG) and image compression.

\ifdefined\akAmazonBook
\else
\bApplication \textbf{Example of Gram-Schmidt transform.}
\label{app:gram_schmidt}
As an example of the Gram-Schmidt procedure, assume that \codl{ak_gram_schmidt} is invoked with the commands
\begin{lstlisting}
x=[0,-1,-1,0;  0,2,2,0;  0,1,0,1;  1,1,1,1;  -2,2,2,1] %row vectors
[Ah,A]=ak_gram_schmidt(x) %perform the orthonormalization procedure
X=Ah*transpose(x(1,:)) %coefficients corresponding to first vector
x1=A*X; %reconstruction of first vector x(1,:) (as a column vector)
\end{lstlisting}
where $M=5$ vectors and $D=4$ is the space dimension. In this case, the number of orthonormal basis functions is $N=4$, such that both matrices \ci{Ah} and \ci{A} have dimension $4 \times 4$. The basis functions correspond to the columns of matrix \ci{A}. The first element of \ci{x1}, corresponding to the first basis function, is the only non-zero element and has value $\sqrt{2}$ which is the coefficient that must multiply the first basis function $[0,-\sqrt{2}/2,\sqrt{2}/2,0]$ to reconstruct the first vector $[0,-1,-1,0]$. The reason is that \ci{ak\_gram\_schmidt} chooses the first basis as a normalized (unit-norm) version of the first vector in \ci{x}.

Debugging the execution of the code \ci{ak\_gram\_schmidt}, step-by-step, allows to observe more details as listed in \codl{snip_transforms_granschmidt_debug}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_transforms\_granschmidt\_debug}{snip_transforms_granschmidt_debug}
%\begin{lstlisting}
%tol = 1.1102e-015  %calculated tolerance
%%first basis in y is [0  -0.7071  -0.7071  0], numBasis=1
%k = 2, m = 1
%projectionOverBasis = [0 2.0 2.0 0] %2nd input vector
%errorVector = 1.0e-015 * [0    0.4441    0.4441  0]
%magErrorVector = 6.2804e-016%do not add error to basis set
%%2nd vector is already represented, go to next iteration
%k = 3, m = 1
%projectionOverBasis = [0 0.5 0.5 0] %3rd input vector
%%errorVector below is orthogonal to [0 -0.7071 -0.7071 0]
%errorVector = [0 0.5 -0.5 1]
%magErrorVector = 1.2247 %add normalized error to basis set
%%second basis is [0 0.4082 -0.4082 0.8165], numBasis=2
%k = 4, m = 1
%projectionOverBasis = [0 1.0 1.0  0] %4th input vector
%errorVector = [1.0 0.0 0.0 1]%using 1st basis
%k = 4, m = 2
%projectionOverBasis = [0.0  0.3333   -0.3333    0.6667]
%errorVector = [1.0 -0.3333 0.3333 0.3333]%using 2 basis vectors
%magErrorVector = 1.1547%add normalized error to basis set
%%3rd basis is [0.8660 -0.2887 0.2887 0.2887], numBasis=3
%k = 5, m = 1
%projectionOverBasis = [0 2.0 2.0 0] %5th input vector
%errorVector = [-2.0 0.0 0.0 1.0]%using only 1st basis
%k = 5, m = 2
%projectionOverBasis = [0 0.3333 -0.3333 0.6667]
%errorVector = [-2.0 -0.3333 0.3333 0.3333]% using 2 basis vectors
%k = 5, m = 3
%projectionOverBasis = [-1.250 0.4167 -0.4167 -0.4167]
%errorVector = [-0.75 -0.75 0.75 0.75] %using 3 basis vectors
%magErrorVector = 1.5000%add normalized error to basis set
%%4th basis is [-0.5 -0.5 0.5 0.5], numBasis=4
%%abort because (numBasis >= N)
%\end{lstlisting}
The reader can perform a similar analysis for another set of vectors and compare with the results obtained via calculating by hand.
\eApplication
\fi

\bApplication \textbf{Time-localization property of Haar coefficients.}
\label{app:haar_transform}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/dct_haar_impulse}            
        \caption[{Signal $x[n]=\delta[n-11]$ analyzed by 32-points DCT and Haar transforms.}]{Signal $x[n]=\delta[n-11]$ analyzed by 32-points DCT and Haar transforms. The right column shows the transform coefficients (abscissa is $k$). The left column shows time-domain plots. Below the signal itself, one can visualize the ``best'' (the one corresponding to the coefficient with largest absolute value) basis functions for each transform.\label{fig:dct_haar_impulse}}
\end{figure}


A very simple experiment will help illustrating the time-localization property of Haar functions. Consider the signal to be analyzed in the transform domain is a single impulse $x[n]=\delta[n-n_0],n_0=0,\ldots,31$, which will be represented by a vector $\bx$ will zeroed elements but the one corresponding to the $n_0$ position. The script \ci{MatlabOctaveBookExamples/ex\_transforms\_dcthaar\_example.m} can be used to observe the behavior of the transforms. \figl{dct_haar_impulse} shows the output of the script when the signal is $x[n]=\delta[n-11]$. In this case, many Haar coefficients are zero and the one with largest magnitude ($k=21$) has a corresponding basis function (bottom of the left column) that helps to localize the occurrence of the impulse in $x[n]$. In contrast, most DCT coefficients have relatively large values and the basis functions do not help in time-localization. The mentioned script allows to investigate this aspect more deeply.
Compare the results of DCT and Haar transforms when the signal is a cosine summed to an impulse.
\eApplication 

\bApplication \textbf{Inverse Laplace transforms with Matlab's Symbolic Math Toolbox.}
Keeping in mind that Matlab adopts the unilateral Laplace transform, its Symbolic Math Toolbox can be used to calculate Laplace transforms and their inverses. For example, the unilateral Laplace transform $X(s)=1/s$ of $u(t)$ can be obtained with the commands \ci{syms t; laplace(t / t)}. Similarly, the commands \ci{syms w0,t; laplace(sin(w0*t))} indicate that $X(s) = \aw_0/(s^2 + \aw_0^2)$ for $x(t) = \sin (\aw_0 t) u(t)$.
The following commands illustrate the inverse transform of $X(s)$ given by \equl{laplace_example}:
\includecodepython{MatlabOnly}{snip\_transforms\_ilaplace}{snip_transforms_ilaplace}
%\begin{lstlisting}
%syms s %defines s as a symbolic variable
%a=1; b=-2; c=-1+j*2; %choose poles and zeros
%X=(s-a)/((s-b)*(s-c)*(s-conj(c))); %define X(s)
%ilaplace(X) %inverse unilateral Laplace transform
%\end{lstlisting}
The command \ci{pretty} (e.\,g., \ci{pretty(ilaplace(X))}) can be used to beautify the output, which in this case is \co{- 3/5 exp(-2 t) + 1/5 exp(-t) (3 cos(2 t) + sin(2 t))}.
Find the inverse Laplace transform for other signals, such as $x(t) = \cos (\aw_0 t) u(t)$.
\eApplication

\bApplication \textbf{ECG transform coding.}
\label{app:ecgcoding}
Because a block transform is completely specified by an invertible matrix $\bA$, going from one domain to another ($\bx$ to $\bX$ or vice-versa) is \emph{loss-less}, which means there is no loss of information. In many applications, such as image coding, where the goal is to minimize the number of bits to represent an image, it is useful to compress the signal using a lossy algorithm. Transform coding can be a lossy algorithm when it discards or quantizes coefficients in the transform domain. This is effective in many applications because, while all samples in $\bx$ have the same ``importance'', in the transform domain $\bX$ the coefficients can be organized according to some hierarchy or rank. In transform coding the most ``important'' coefficients are quantized more carefully than the unimportant ones.

\figl{ecg_signal} depicts a segment (first 1,500 samples) of an ECG signal. The ECG data used here is from the MIT-BIH Arrhythmia Database.\footnote{Available from \akurl{www.physionet.org/physiobank/database/mitdb}{2ecg}.}, and \figl{ecg_signal} corresponds to the first channel of file 12531\_04.dat. The function \ci{ak\_rddata.m} indicates that the DC offset of the ADC chip was calibrated to be zero. It also indicates that the ECG signals were digitized with a sampling frequency $\fs=250$ Hz, 12 bits per sample, and a quantizer with step $\Delta_{\textrm{AD}} = 1/400$~mV. The (empirical) signal power is 3.07 mW and, following the conventional quantization model, \equl{variance_uniformrv} suggests the quantization noise power was $5.2 \times 10^{-10}$ mW. Hence, the SNR corresponding to the quantization stage alone is approximately 97.7 dB.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/ecg_signal}          
        \caption{A segment of one channel of the original ECG data.\label{fig:ecg_signal}}
\end{figure}

A very simple example of a transform coding system is provided as {\matlab} code in the companion software (in directory \ci{Applications/ECGTransformCoding}). The idea is to simply discard the higher-order coefficients in the transform domain. For example, assume that $N$ is the dimension of $\bx$ and $\bX$, which are related by a $N \times N$ transform matrix $\bA$. In the encoding stage, a discrete-time input signal $x[n]$ is segmented into blocks of dimension $N$, composing a set of $M$ input vectors $\{\bx_1,\ldots,\bx_M\}$, where $N \times M$ is the available number of samples of $x[n]$. The coding scheme converts each $\bx_i$ into $\bX_i = A^H \bx_i$ and keeps in a new vector $\hat \bX_i$ only the first $K \le N$ elements of $\bX_i$ (the remaining $N-K$ are discarded). The $M$ vectors $\hat \bX_i$ can be concatenated to create an encoded output signal $\hat X[n]$ with $M \times K$ samples.

In the decoding stage, in order to reconstruct a signal $x'[n]$ from $\hat X[n]$ (if there were no losses, $x'[n]=x[n]$), an inverse procedure is adopted. The signal $\hat X[n]$ is blocked into vectors of dimension $K$ and $N-K$ zero elements are inserted to create vectors $\bX'_i$ of dimension $N$ (this operation is called \emph{zero-padding}\index{Zero-padding}). Each of these $N$-dimensional vectors is converted to time-domain vectors $\bx'_i = \bA \bX'_i$, which are then concatenated to form $x'[n]$.

\figl{ecg_reconstructedsignal} was obtained using a DCT matrix $\bA$ of $N=32$ points and discarding 26 (high-frequency) coefficients. It can be seen that keeping only $K=6$ out of 32 coefficients is enough to provide a rough representation of $x[n]$ but the error is significant in high-frequency regions. Note that the DCT operation is performed in blocks of $N=32$ samples and \figl{ecg_reconstructedsignal} is the result of processing many of these blocks.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/ecg_reconstructedsignal}             
        \caption[{Original and reconstructed ECG signals with DCT of $N=32$ points and discarding 26 (high-frequency) coefficients.}]{Original and reconstructed ECG signals with DCT of $N=32$ points and discarding 26 (high-frequency) coefficients. Therefore, the error is predominant in high-frequency regions as indicated in the plot.\label{fig:ecg_reconstructedsignal}}
\end{figure}

In practice, a quantization scheme should be adopted to represent $\hat X[n]$ with a small number of bits per sample. The compression ratio is the number of bits to represent $x[n],n=0,\ldots,M \times N$ divided by the number of bits to represent $\hat X[n],n=0,\ldots,M\times K$. For simplicity, the proposed example does not involve quantization and the compression ratio is evaluated by $N/K$. This corresponds to assuming that each sample of both $x[n]$ and $\hat X[n]$ is represented with the same number of bits.

\figl{ecg_ratedistortion} shows the percentage of kept coefficients $K/N$ in the abscissa and $10 \log_{10} \MSE$ in the ordinate, where
\[\MSE = \frac{1}{M \times N} \sum_{n=0}^{M \times N-1}(x[n]-x'[n])^2\]
is the mean-squared error, which is equivalent to the power of the error signal. If $K=N$ (100\% in the abscissa) the system is lossless and $\MSE=0$ ($-\infty$ in log scale), so the graphs do not show these points. The larger $N$, the better the coding performance but the higher the computational cost.
%AK-PUTBACK This fact will be discussed in details in Chapter~\ref{ch:sourcecoding}.

Assuming $N=128$ and $K=10$ as indicated in \figl{ecg_ratedistortion}, the abscissa is $K/N=10/128 \approx 7.81$\% and the corresponding error is $\MSE = 1.4824 \times 10^{-8}$~W ($-78.29$ dBW) or, equivalently, $-48.29$~dBm.
Given that the signal power was estimated as 3.07 mW ($4.87$ dBm), the $\snr_\dB \approx 4.87 + 48.3 = 53.16$ dB when considering only the distortion incurred by this specific encoding procedure.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/ecg_ratedistortion}          
        \caption[{Performance of five DCT-based ECG coding schemes. The number of points is varied $N \in \{4,8,32,64,128\}$ and $K=1,2,\ldots,M-1$.}]{Performance of five DCT-based ECG coding schemes. The number of points is varied $N \in \{4,8,32,64,128\}$ and $K=1,2,\ldots,M-1$. The larger $N$, the better the coding performance but the higher the computational cost.\label{fig:ecg_ratedistortion}}
\end{figure}

It should be noted that the dataset used is formed by abnormal ECG signals and, eventually, better results could be obtained when using another dataset.

Write an encoder that uses DCT coding to generate files of a relatively small size and a decoder to ``decompress'' such files and recover an ECG signal (an approximation to the original one). To do that, it is necessary to quantize the DCT coefficients and pack them in a way that the resulting file has a small size. Consider using scalar or \emph{vector quantization}\index{Vector quantization}. The advantage of vector quantization is that you need to store only the index to the codebook entry (codeword) with a vector that will represent the DCT. In case you use a codebook with $2^8=256$ codewords, each entry can be conveniently written as an unsigned char of 8 bits. To design the codebook, besides the Mathwork's code in the DSP System Toolbox, there are many algorithms such as the \emph{K-means} with implementations available on the Web.

Another interesting exercise is to apply KLT and other transforms to this problem, trying to get better results than with the DCT.
\eApplication

\bApplication \textbf{DCT coding of image.}
\label{app:dctcoding}
%\subsection{DCT coding of images}

The usefulness of DCT in coding is illustrated by the JPEG image coding standard.\footnote{A good way to visualize DCT in image coding is by running the (deprecated) \ci{dctdemo.m} Matlab demonstration.}
Before describing an example, two dimensional transforms are briefly discussed.

Linearly-separable two dimensional (2-D) transforms of a block $\bx$ (e.\,g., a matrix with pixel values) is obtained by first using the transform along the rows (or columns) and then transforming this result along the columns (or rows). Alternatively, one can use matrix notation:
$\bX = \bA^H \bx \bA^*.$
Because, $\bA$ is real for a DCT, one has
\[
\bX = \bA^T \bx \bA.
\]
This is equivalent to first calculating $\bT=\bA^H \bx$: the 1-D DCT transform of each column of $\bx$ is placed in its corresponding column in a temporary matrix $\bT$. Then, each row of $\bT$ is transformed by another 1-D DCT, which could be accomplished by
$\bA^H \bT^T$. This result should be transposed to generate $\bX$. These operations are equivalent to:
\[
\bX = (\bA^H (\bA^H \bx)^T)^T = \bA^H \bx \bA^*.
\]

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/lenna_eye}           
        \caption{A zoom of the eye region of the Lenna image.\label{fig:lenna_eye}}
\end{figure}

\codl{dctimagecoding} illustrates the adoption of a 2-D DCT for coding an image block (represented by a matrix).
\lstinputlisting[caption={MatlabBookFigures/figs\_transforms\_dctimagecoding},label=code:dctimagecoding,linerange={2-12,15-23}]{./Code/MatlabBookFigures/figs_transforms_dctimagecoding.m}

Similar to the 1-D ECG coding example in Application~\ref{app:ecgcoding}, few DCT coefficients can be enough to represent an image.
Study the code \ci{MatlabOctaveFunctions/ak\_dctdemo.m} using DCT to represent different images,\footnote{A popular repository is \akurl{http://sipi.usc.edu/database}{2usc}.} including ones with text. Even better results can be obtained with the wavelet transform, which is used in the JPEG 2000 image coding standard. Write an encoder and decoder that use a transform to compress and decompress an image file.
\eApplication

\bApplication \textbf{Orthogonality of continuous-time sinusoids over a given interval.}
In applications such as frequency modulation, it is important to know the conditions
in which two cosines with frequencies $f_1$ and $f_2$ and phase $\theta$, are orthogonal
over a given interval $\tsym$ of interest. 
Mathematically, this requirement can be expressed as
\[
\int_{<\tsym>} \cos (2 \pi f_1 t + \theta) \cos (2 \pi f_2 t + \theta) = 0
\]
and, because the phase $\theta$ is the same for both cosines, this requirement is called ``coherent orthogonality''.
It can be shown\footnote{See, e.\,g., page 451, Eq. (6.68), in \cite{Dayan09}.} that this condition is satisfied with integers $m$ and $n$ such that
\[
f_1 = \frac{2n-m}{4 \tsym}, f_2 = \frac{2n+m}{4 \tsym} \textrm{~and~} f_2 - f_1 = \frac{m}{2 \tsym}.
\]
For example, assume that $\tsym=0.01$~seconds, $m=4$ and $n=35$. In this case, $f_1$ and $f_2$ are 1650 and 1850~Hz, respectively. Note that the minimum value of $f_2 - f_1$ is $1/(2\tsym)=50$~Hz in this case, which corresponds to choosing $m=1$.

As a side note, for ``noncoherent orthogonality'', the minimum frequency separation\footnote{See, e.\,g., page 534, Eq. (6.155), in \cite{Dayan09}.} is $1/\tsym$.
\eApplication

\bApplication \textbf{Orthogonality of finite duration discrete-time sinusoids.}
%When using discrete-time sinusoids in a digital communication system,
%that is based on correlative demodulation, the basis functions have a finite duration given by the symbol period. I
It is also important to evaluate in what conditions a pair of discrete-time sinusoids are orthogonal considering a finite duration interval. %This issue is discussed in the sequel.

Two discrete-time exponentials $e^{j \dw_0 n}$ and $e^{j \dw_1 n}$, $\dw_0 \ne \dw_1$, are always orthogonal when summed over a range $-\infty$ to $\infty$ (see \exal{eternal_sinusoids}), but this is not the case when the summation interval is finite. One way of inspecting what happens when the duration is finite is to reason as following: 1) keep in mind that the sum of the samples of a single sinusoid over a multiple of its fundamental period is zero. 2) Interpreting \exal{harmonic_sinusoids} 
%(page \pageref{th:eternal_sinusoids}) 
in discrete-time allows to decompose the inner product for testing orthogonality into a sum of two sinusoids. 3) the inner product is mapped to summations of sinusoids with frequencies equal to $\dw_0+\dw_1$ and $\dw_0-\dw_1$ and the goal is to find an integer $N$ that is a multiple of both fundamental periods.

One application of this orthogonality principle is the design of a binary transmission, where the two signals are distinguished by their frequencies $f_0$ and $f_1$. This scheme is called frequency shift keying (FSK).
%for space and mark, respectively.
We are interested on determining the minimum symbol period that guarantees orthogonality between the two sinusoids. In the context of telecommunications, this minimum period will correspond to the highest symbol rate.
The script \ci{MatlabOctaveBookExamples/ex\_transforms\_check\_orthogonality.m} can be used for studying this question and is repeated below. The sampling frequency is $\fs$ and one should check different pairs of $f_0$ and $f_1$.
\lstinputlisting[caption={[MatlabOctaveBookExamples/\protect\linebreak  ex\_transforms\_check\_orthogonality.m]MatlabOctaveBookExamples/ex\_transforms\_check\_orthogonality.m},label=code:ex_transforms_check_orthogonality]{./Code/MatlabOctaveBookExamples/ex_transforms_check_orthogonality.m}

Modify \codl{ex_transforms_check_orthogonality} to use discrete-time sinusoids in radians, not Hz. Assume $N=8$ and find a set of $M$ sinusoids that are mutually orthogonal over an interval of eight consecutive samples. What was the maximum value of $M$ that you could find? Now, consider the set of basis functions for a $N$-point DFT and evaluate their orthogonality. Going back to the question about $M$, can it be larger than $N$? Why?
\eApplication

\bApplication \textbf{Better visualizing the FFT spectrum: Using \ci{fftshift} to reorganize the spectrum and masking numerical errors.}
Two improvements of the procedure adopted to obtain \figl{dtfs_example} are typically used.
The first one is with respect to the range $k=\langle N\rangle$.
\figl{dtfs_example} reminds that the definition of an $N$-point DFT assumes $k=0,\ldots,N-1$. Sometimes it is more convenient to work with ranges that use ``negative'' frequencies. Typical ranges are $k=-N/2,-N/2+1,\ldots,0,1,N/2-1$ when $N$ is even and $-(N-1)/2,-(N-1)/2+1,\ldots,0,1,(N-1)/2$ when $N$ is odd. In these cases, to represent negative frequencies (or angles in discrete-time processing) the command \ci{fftshift} can be used. For example, \ci{fftshift([0 1 2 3])} returns \co{[2 3 0 1]}, while \ci{fftshift([0 1 2 3 4])} returns \co{[3 4 0 1 2]}.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/fftshift_example}            
        \caption[{Alternative representation of the DTFS / DFT of $x[n] = 10 \cos (\frac{\pi}{6} n + \pi/3)$ using \ci{fftshift}.}]{Alternative representation of the DTFS / DFT of $x[n] = 10 \cos (\frac{\pi}{6} n + \pi/3)$ using \ci{fftshift}. Compare with \figl{dtfs_example} and note that the current plots clearly indicates that when $x[n]$ is real, the magnitude and phase are even and odd functions, respectively.\label{fig:fftshift_example}}
\end{figure}

The second one aims at eliminating spurious values.
Note that, due to numerical errors, the elements of \ci{X} that should be zero, have small values such as \ci{-1.4e-15 - j 3.3e-16}, which leads to random angles for $k \ne 1,11$ in \figl{dtfs_example}.
It is a good practice to eliminate these random angles based on a threshold for the magnitude. This post-processing step can be done with a command such as \ci{X(abs(X) $<$ 1e-12)=0}, and is adopted in the {\matlab} \ci{fft} routine.

\figl{fftshift_example} benefits from these two improvements and was obtained using \codl{snip_transforms_fftshift}.
\lstinputlisting[firstline=6,caption={MatlabOctaveCodeSnippets/snip\_transforms\_fftshift.m},label=code:snip_transforms_fftshift]{./Code/MatlabOctaveCodeSnippets/snip_transforms_fftshift.m}

Note that \ci{fftshift} does not calculate the FFT but simply reorders the elements in a vector.
\eApplication


\bApplication \textbf{DTFS analysis of single cosines.}
\figl{dftexamples} depicts five cosines $x_i[n]=10 \cos(\dw_i n)$ with angular frequencies $\dw_i = i 2 \pi/32$ rad, where $i=0,1,2,16,31$, and their respective DTFS spectrum calculated with a DFT of $N=32$ points and normalized by $N$. 

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/dftexamples}         
        \caption{Five cosine signals $x_i[n]=10 \cos(\dw_i n)$ with frequencies $\dw_i = 0$, $2 \pi / 32$, $4 \pi /32$, $\pi$, $31 \pi /16$ for $i=0,1,2,16,32$, and the real part of their DTFS using $N=32$ points. \label{fig:dftexamples}}
\end{figure}

The first $x_0[n]=10$ is a DC signal ($\dw_0=0$), the second is $x_1[n]=10 \cos(2\pi/32 n)$, where $\dw_1 = \frac{2 \pi}{32}$ is the fundamental frequency in this DTFS analysis (given that $N=32$) and the other three signals are cosines with the following harmonic frequencies: $\dw_2=2 \dw_1$, $\dw_{16}=16 \dw_1$ and $\dw_{31}=31 \dw_1$ rad. Because the cosines are even functions, the imaginary part of their DTFS is zero.

Note in \figl{dftexamples} that the frequency increases up to the $N/2$-th harmonic $\dw_{16}=16 \dw_0=\pi$.
When $N$ is even, the coefficient of $k=N/2$ corresponds to $\dw = \frac{2 \pi}{N} k |_{k=N/2}= \pi$.
It is important to note that the highest ``frequency'' of a discrete-time signal is always $\dw = \pi$ rad. 

%Observe that an orthonormal DFT was adopted. Therefore, the values of $X[k]$ cannot be interpreted in volts. The range was $k=0,\ldots,N-1$. Another 
Another aspect of \figl{dftexamples} is that for $x_0[n]$ and $x_{16}[n]$, corresponding to the angles $\dw=0$ and $\dw=\pi$, the spectra have only one non-zero coefficient $X[k]$, while the other three signals are represented by two coefficients. \figl{circledivided}, for $N=4$ and $N=6$ can be invoked to help interpreting why the coefficients for angles different than $\dw=0$ and $\dw=\pi$ occur in pairs with Hermitian symmetry (i.\,e., $X[k]=X^*[-k]$ when $x[n]$ is real). The basis functions for $\dw=0$ and $\dw=\pi$ are real signals while the other angles lead to complex-valued signals. In the analysis of a real signal $x[n]$, using the $k$-th complex basis requires also using the $-k$-th basis, such that their imaginary parts can cancel in the synthesis of a real $x[n]$.
\eApplication

%AK: will not use another example. Too much repetition.
\ignore{
\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/dftexamples2}        
        \caption[{The same five cosine signals of \figl{dftexamples} but using the normalization adopted in the DTFS definition.}]{The same five cosine signals of \figl{dftexamples} but using the normalization adopted in the DTFS definition. The right plots were obtained with \ci{fftshift}. \label{fig:dftexamples2}}
\end{figure}
%
\figl{dftexamples2} depicts graphs of the same five cosines when using the normalization provided by \ci{Ah=ak\_fftmtx(N,2)}. The abscissa is shown for components representing angles from 0 to $2 \pi$ and from $\pi$ to $\pi$.
}

\bApplication \textbf{DTFS analysis of a sum of sines and cosines.}
This example aims at illustrating how the cosines and sines are mapped in the DTFS, especially when two of them have the same frequency. \figl{dftexamples3} shows the spectrum of a more complex example obtained with \codl{snip_transforms_DTFS_sinusoid}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_transforms\_DTFS\_sinusoid}{snip_transforms_DTFS_sinusoid} 
%\begin{lstlisting}
%clear all
%N=32; %number of DFT-points
%n=0:N-1; %abscissa to generate signal below
%x=2+3*cos(2*pi*6/32*n)+8*sin(2*pi*12/32*n)-...
%    + 4*cos(2*pi*7/32*n)+ 6*sin(2*pi*7/32*n);
%X=fft(x)/N; %calculate DTFS spectrum via DFT
%X(abs(X)<1e-12)=0; %mask numerical errors
%\end{lstlisting}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/dftexamples3}        
        \caption[{Analysis with DFT of 32 points of $x[n]$ composed by three sinusoids and a DC level.}]{Analysis with DFT of 32 points of $x[n]$ composed by three sinusoids and a DC level. From top to bottom: $x[n]$, real, imaginary, magnitude and phase of $X[k]$.\label{fig:dftexamples3}}
\end{figure}

Note in \figl{dftexamples3} that the coefficient $X[0]=2$ is due to the DC level, $X[6]=X[-6]=1.5$ are due to the cosine $3 \cos(6 (2 \pi/ 32) n)$, $X[12]=-4j$ and $X[-12]=4j$ are due to $8 \sin(12 (2 \pi/ 32) n)$, $X[7]=-2-3j$ and $X[-7]=-2+3j$ are due to the two parcels of frequency $7 (2\pi/32)$ with the $-4\cos(7 (2 \pi/ 32) n)$ being represented by the real part ($-2$) and $6 \sin(7 (2 \pi/ 32) n)$ represented by the imaginary part ($3j$).
\eApplication

\bApplication \textbf{Spurious frequency components: When the number $N$ of DFT points is not a multiple of the signal period.}
\label{app:spectral_leakage}
\figl{dftexamples4} illustrates the DTFS spectrum of a signal $x[n]=4 \cos((2\pi/6)n)$ with period of 6 samples obtained via a 16-points DFT. Note that because 16 is not a multiple of 6, the spectrum has many non-zero spurious components. The coefficients $X[3]=0.99 - 1.48 j$ and $X[-3]=0.99 + 1.48 j$, both with magnitude 1.78, are the closest to representing the angle $\pi/3 \approx 1.05$ rad. In fact, when using $N=16$ points to divide $2 \pi$ (see \figl{circledivided}), the angle increment is
\begin{equation}
\Delta \dw = \frac{2 \pi}{N},
\label{eq:fft_delta_dw}
\end{equation}
which in this case is $\Delta \dw = \pi/8 \approx 0.39$, and the DFT / DTFS can deal only with the angles $[0, 0.39,    0.78,    1.18,    1.57, \ldots, 5.89]$ rad. This explains why $X[3]$ and $X[-3]$ are concentrating most of the power $P=4^2/2$. The other coefficients absorb the remaining power that ``leaks'' due to the imperfect match of the cosine frequency $\pi/3$ with the discrete grid of 16 points imposed by the DFT / DFTS.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/dftexamples4}        
        \caption{Spectrum of a signal $x[n]=4 \cos((2\pi/6)n)$ with period of 6 samples obtained with a 16-points DFT, which created spurious components.\label{fig:dftexamples4}}
\end{figure}

The creation of spurious components is associated to \emph{leakage}\index{Leakage, spectrum} and the 
 \emph{picket-fence effect}, which are discussed in Section~\ref{sec:windows}. This
phenomenon is very common when using the FFT to analyze signals that are non-periodic, have infinite duration, etc. In the current case, the spurious components could be avoided by choosing an appropriate number of points for the DFT / DTFS (a multiple of 6). In practice, spurious components due to FFT usage occur most of the times. Even when the original signal is periodic, say $x(t)$ with period $T$, it is typically not guaranteed that $N \ts$ is a multiple of $T$, where $N$ is the number of DFT points and $\ts$ the sampling period.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/leakage}             
        \caption{Explicitly repeating the block of $N$ cosine samples from \figl{dftexamples4} to indicate that spurious components are a manifest of the lack of a perfect cosine in time-domain.\label{fig:leakage}}
\end{figure}

\figl{leakage} shows part of the signal obtained by repeating the segment of 16 samples depicted in \figl{dftexamples4}. This represents the signal ``assumed'' by the DFT / DTFS. Spurious components appear because, in spite of $x[n]$ having a period of $N=6$, an abrupt transition (from $-4$ to 4) occurs in the boundary of the segments of 16 samples. In general, the periodic extension (that is
assumed by the FFT when approximating a DTFS) of a signal $x[n]$ not commensurate\footnote{See \exal{commensurate} for a discussion of commensurate frequencies.} with its 
period, leads to discontinuities at the boundaries of the replicas of $x[n]$.
\eApplication

\bApplication \label{ex:periodic_pulse}\textbf{DTFS for periodic discrete-time pulses}. Assume a periodic train of pulses $x[n]$ with period $N$ and one period described as $x[n]=1$ from $n=0$ to $N_1-1$ and $x[n]=0$ from $N_1$ to $N-1$.
Its DTFS coefficients are given by
\[
X[k] = \frac{1}{N} \sum_{n=0}^{N-1} x[n] e^{-j \frac{2\pi}{N} n k}= \frac{1}{N} \sum_{n=0}^{N_1-1} e^{-j \frac{2\pi}{N} n k} = \frac{1}{N} \sum_{n=0}^{N_1-1} \left( e^{-j \frac{2\pi}{N} k } \right)^n,
\]
which is the sum of $N_1$ terms of a geometric series with ratio $r=e^{-j \frac{2\pi}{N} k}$. Applying \equl{sum_inf_pg} leads to
\[
X[k] = \frac{1}{N}  \left( \frac{1-e^{-j \frac{2\pi}{N} N_1 k}}{1 - e^{-j \frac{2\pi}{N} k}} \right).
\]
This expression can be simplified by applying \equl{complex_exp_trick_sin} to both numerator and denominator:
\begin{equation}
X[k] =  \frac{1}{N}  \frac{\sin( k N_1 \pi / N)}{\sin(k \pi / N)} e^{-j \frac{k \pi}{N} (N_1-1)},
\label{eq:dtfs_pulses}
\end{equation}
for $k=0,\ldots,N-1$ and then periodically repeated in $k$. For $k=0$ the expression is undetermined (because of the $0/0$ fraction) and L'Hospital's rule leads to $X[0]=N_1 / N$.

\codl{snip_transforms_DTFS_pulses} obtains the DTFS of the periodic pulses in two distinct ways.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_transforms\_DTFS\_pulses}{snip_transforms_DTFS_pulses} 
%\begin{lstlisting}
%N=10; N1=5; k=0:N-1; %define durations and k
%Xk=(1/N)*(sin(k*N1*pi/N)./sin(k*pi/N)) .* ...
%    exp(-j*k*pi/N*(N1-1)); %obtain DTFS directly
%Xk(1)=N1/N; %eliminate the NaN (not a number) in Xk(1)
%%second alternative, via DFT. Generate x[n]:
%xn=[ones(1,N1) zeros(1,N-N1)] %single period of x[n]
%Xk2=fft(xn)/N %DTFS via DFT, Xk2 is equal to Xk
%\end{lstlisting}

\begin{figure}[!htb]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/dtfs_pulse}
\caption{Three periods of each signal: pulse train $x[n]$ with $N=10$ and $N_1=5$ and amplitude assumed to be in volts (a), the magnitude (b) and phase (c) of its DTFS.\label{fig:dtfs_pulse}}
\end{figure}

\figl{dtfs_pulse} illustrates the result using $N=20$ and $N_1=10$. Note that $|X[k]|$ has the behavior of a sinc function, as expected. The DTFS phase in \figl{dtfs_pulse} c) is the combined effect of the linear phase $e^{-j \frac{k \pi}{N} (N_1-1)}$ and the sign of $\sin( k N_1 \pi / N)/\sin(k \pi / N)$ in \equl{dtfs_pulses}. This sign is responsible for \figl{dtfs_pulse} c) not being a linear function of frequency.

\begin{figure}[!htb]
  \begin{center}
    \subfigure[$N_1=4$]{\label{fig:dtfs_pulse_duty_4}\includegraphics[width=5cm]{Figures/dtfs_pulse_duty_4}}
    \subfigure[$N_1=3$]{\label{fig:dtfs_pulse_duty_3}\includegraphics[width=5cm]{Figures/dtfs_pulse_duty_3}}
    \\
    \subfigure[$N_1=2$]{\label{fig:dtfs_pulse_duty_2}\includegraphics[width=5cm]{Figures/dtfs_pulse_duty_2}}
    \subfigure[$N_1=1$]{\label{fig:dtfs_pulse_duty_1}\includegraphics[width=5cm]{Figures/dtfs_pulse_duty_1}}
  \end{center}
  \caption{Behavior when $N_1$ of \figl{dtfs_pulse} is decreased from $N_1=4$ to 1.}
  \label{fig:dtfs_pulse_duty}
\end{figure}

\figl{dtfs_pulse_duty} shows the behavior of the DTFS of $x[n]$ when $N_1$ of \figl{dtfs_pulse} is decreased from $N_1=4$ to 1
Note the duality between time and frequency: as the pulse duty cycle gets smaller (smaller $N_1$), the spectrum gets wider and with smaller maximum values. When $N_1=1$, then $|X[k]|=N_1/N=0.05$.
Also, in this case, $N_1$ dictates the number of nulls in the spectrum, while $N$ controls how the unit circle $e^{j \dw}$ is sampled by the DFT.
\eApplication

\bApplication \textbf{Calculating the DTFT of a pulse via the DFT.}
Following steps similar to the ones that led to \equl{dtfs_pulses}, the DTFT pair of a pulse is given by
\begin{equation}
x[n] = \left\{ {\begin{array}{*{20}c} {1,~~0 \le n \le N_1 - 1} \\ {0,{\textrm{otherwise}}} \\ \end{array} \lra } \right.\frac{{\sin (\dw N_1/2)}}{{\sin (\dw /2)}}e^{ - j\dw (N_1 - 1)/2}
\label{eq:dtft_pulse}
\end{equation}

\codl{snip_transforms_DTFT_pulse} illustrates how to obtain samples of the DTFT for a pulse with $N_1=5$ non-zero samples using a DFT of $N=20$ points.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_transforms\_DTFT\_pulse}{snip_transforms_DTFT_pulse}
%\begin{lstlisting}
%N=20; %DFT size
%N1=5; %num. non-zero samples in the (aperiodic) pulse
%x=[ones(1,N1) zeros(1,N-N1)]; %signal x[n]
%N2=512; %num. samples of the DTFT theoretical expression
%k=0:N2-1; %indices of freq. components
%wk = k*2*pi/N2; %angular frequencies
%Xk_fft = fft(x); %use N-point DFT to sample DTFT
%Xk_theory=(sin(wk*N1/2)./sin(wk/2)).*exp(-j*wk*(N1-1)/2);
%Xk_theory(1) = N1; %take care of 0/0 (NaN) at k=0
%subplot(211); stem(2*pi*(0:N-1)/N,abs(Xk_fft));
%hold on, plot(wk,abs(Xk_theory),'r');
%subplot(212); stem(2*pi*(0:N-1)/N,angle(Xk_fft));
%hold on, plot(wk,angle(Xk_theory),'r');
%\end{lstlisting}
The code plots the DTFT superimposed to the DFT result, as illustrated in \figl{dtft_via_dft}.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/dtft_via_dft}        
        \caption{DTFT of an aperiodic pulse with $N_1=5$ non-zero samples and DTFT estimates obtained via a DFT of $N=20$ points.\label{fig:dtft_via_dft}}
\end{figure}

As stated by \equl{dtft_via_dft}, 
\figl{dtft_via_dft} pictorially illustrates that the DFT values at the discrete frequencies $\dw_k = k (2 \pi / N)$ coincide with 
the DTFT of the aperiodic pulse $x[n]$ calculated at these frequencies using \equl{dtft_pulse}.
%i.\,e., $X(e^{j \dw})|_{\dw=\dw_k}$.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/N256dtft_via_dft}            
        \caption{A version of \figl{dtft_via_dft} using a DFT of $N=256$ points.\label{fig:N256dtft_via_dft}}
\end{figure}

The DTFT can be calculated in a grid of frequencies with arbitrary resolution by increasing the number $N$ of DFT points. For example, using $N=256$ leads to the spectrum in \figl{N256dtft_via_dft}.
Note that if \ci{x} has less than $N$ samples, the command \ci{fft(x,N)} conveniently extends its duration with zero-padding.
\eApplication

%AK-IMPROVE %Eu podia mostrar como uma analise DFT usando varios periodos de um sinal vai inserindo zeros entre as amostras do espectro. Daih eu aproveitaria a figura abaixo:
\ignore{
\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidth,keepaspectratio]{Figures/pulse_and_repetition}        
        \caption{The pulse $x[n]$ (top) and its periodic repetition (bottom), which is the signal assumed by the DFT / DTFS.\label{fig:pulse_and_repetition}}
\end{figure}
\figl{pulse_and_repetition} illustrates the pulse $x[n]$ and its periodic repetition, which is the signal assumed by the DFT / DTFS to obtain the representation $X[k]$ in \figl{dtft_via_dft}. 
}

\bApplication \textbf{Calculating the DTFT using {\matlab}'s \ci{freqz}.}
When $N$ is relatively large, it is more convenient to use \ci{plot} instead of \ci{stem} (as in \figl{zeromean_randomsignal}), in spite of $X[k]$ being discrete. This is also convenient when $X[k]$ is used to represent values of $X(e^{j\dw})$ as in this example. {\matlab} has the command \ci{freqz} that uses the DFT to calculate the DTFT at a grid of discrete frequencies. Assuming the same vector \ci{x} of the previous code snippet, the command \ci{freqz(x)} generates \figl{dtft_via_freqz}.

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/dtft_via_freqz}              
        \caption{A version of \figl{dtft_via_dft} using \ci{freqz} with 512 points representing only the positive part of the spectrum.\label{fig:dtft_via_freqz}}
\end{figure}

\begin{figure}[!htb]
        \centering
                \includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/replicate_freqz}             
        \caption{Reproducing the graphs generated by \ci{freqz} in \figl{dtft_via_freqz}.\label{fig:replicate_freqz}}
\end{figure}

To better understand how \ci{freqz} works, \codl{snip_transforms_freqz} mimics \ci{freqz} and was used to obtain \figl{replicate_freqz}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_transforms\_freqz}{snip_transforms_freqz}
%\begin{lstlisting}
%N1=5; %num. non-zero samples in the (aperiodic) pulse
%x=[ones(1,N1) zeros(1,2)]; %signal x[n]
%halfN=512; N=2*halfN; %half of the DFT dimension and N
%k=0:N-1; %indices of freq. components
%wk = k*2*pi/N; %angular frequencies
%Xk_fft=fft(x,N);%use 2N-DFT to get N positive freqs.
%Xk_fft=Xk_fft(1:halfN);%discard negative freqs.
%wk=(0:halfN-1)*(2*pi/N); %positive freq. grid
%subplot(211); plot(wk,20*log10(abs(Xk_fft))); %in dB
%subplot(212); plot(wk,angle(Xk_fft)*180/pi); %in degrees
%\end{lstlisting}

It can be seen that \figl{replicate_freqz} is very similar to \figl{dtft_via_freqz}. The only distinction is that, by default, {\matlab} represents the abscissa normalizing $\dw$ by $\pi$, such that the maximum frequency is not $\pi$ but 1. It is interesting to note that {\matlab} and GNU Radio, for example, indicate $\dw$ in rad/sample, while this and other texts assume $\dw$ is given in radians ($n$ is dimensionless).
\eApplication

\section{Comments and Further Reading}

Some authors recognize the advantages of presenting  concepts of digital signal processing before their analog counterpart. This chapter adopts this approach. For example, it presents block transforms before the continuous time Fourier transform.

A commonly adopted notation is to call \emph{inverse} transformation the matrix multiplication
$
\bx = \bA \bX,
$
while the \emph{forward} (or direct) transformation is denoted as
$
\bX = \bA^{-1} \bx.
$, such that the basis are the columns of $\bA$~\cite{Malvar92}. Experience proved that this notation confuses beginners and it was not adopted here.

Most textbooks introduce Z and Laplace transforms using the fact that their basis functions are eigenfunctions of linear and time-invariant systems. In this chapter, both transforms were presented as extensions of their Fourier counterparts.
%The discussion property will be

All the four Fourier representations are interrelated. There are many interesting properties among the representations, which are explored in, e.\,g.,~\cite{Oppenheim09}.
%The strategy adopted here was to deemphasize the Fourier series because periodic signals can be represented through Fourier transforms that use impulses.

With respect to block transforms, the attention is restricted to the ones represented by square matrices. Also, the emphasis is on their use and interpretation. Fast algorithms are out of the scope. A more advanced treatment of transforms, including lapped transforms (that correspond to non-square matrices) and fast algorithms can be found in~\cite{Malvar92}.

There are four DCT transforms \cite{Rao90}. The DCT-II was assumed in this text because it is the most popular for coding applications.

It is assumed by default the bilateral Laplace and Z transforms. The unilateral versions are useful for solving differential and difference equations, which is a task not emphasized in this text.

Unitary matrices are sometimes called orthogonal matrices (instead of orthonormal), which is confusing. For vectors, the jargon is more consistent.

A nice geometrical explanation about linearly independent, orthogonal, and uncorrelated variables is
provided in~\cite{Rodgers84}.

