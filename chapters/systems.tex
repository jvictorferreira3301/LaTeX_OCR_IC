\section{To Learn in This Chapter}
The skills we aim to develop in this chapter are:
\begin{itemize}
	\item Learn how system models $\calH$ are used to characterize the output signal $y$ corresponding to a given input signal $x$ in both continuous-time and discrete-time
	\item Understand what a linear and time-invariant (LTI) system is and the role of the convolution operation to determine the
	output signal
	\item Learn convolution properties and how to calculate the discrete-time convolution via correlation or matrix multiplication
	\item Calculate the circular (or periodic) convolution between two signals or two spectra
	\item Distinguish signals and systems, in spite of the \textbf{signal} called \emph{impulse response} $h(t)$ being used to represent a LTI \textbf{system}
	\item Characterize a continuous-time LTI system based on its impulse response $h(t)$ or linear constant-coefficient differential equation relating the input $x(t)$ to the output $y(t)$	
	\item Characterize a discrete-time LTI system based on its impulse response $h[n]$ or linear constant-coefficient difference equation (LCCDE) relating the input $x[n]$ to the output $y[n]$
	\item Learn the concept of \emph{system function} and \emph{frequency response}
	\item For LTI systems, use Laplace and Z transforms to calculate the system function $H(s)=\calL \{h(t) \}$ 
	and $H(z)=\calZ \{h[n] \}$ from impulse responses $h(t)$ and $h[n]$ for continuous and discrete-time, respectively. Similarly
	use these transforms to obtain $H(s)$ and $H(z)$ from the respective linear constant-coefficient differential or difference equations
	\item For LTI systems, use continuous-time Fourier transform and DTFT to calculate the frequency response $H(\aw)=\calF \{h(t) \}$ 
	and $H(e^{j \dw})=\calF \{h[n] \}$ from impulse responses $h(t)$ and $h[n]$ for continuous and discrete-time, respectively. Similarly
	use these transforms to obtain $H(\aw)$ and $H(e^{j \dw})$ from the respective linear constant-coefficient differential or difference equations	
	\item Understand that complex exponentials are \emph{eigenfunctions} of LTI systems
	\item Design and use analog and digital frequency-selective filters both via software and analytically
	\item Implement in software a digital filter using its LCDDE equation
	\item Get familiar with the definitions of key frequencies for dealing with frequency-selective filters: cuttof, natural, etc.
	\item Interpret the group delay and use it to evaluate the delay imposed by a system
	\item Learn to predict how a system modifies an input signal based on system properties
	\item Reinterpret sampling and signal reconstruction, now being able to use the convolution operation in the mathematical model
	\item Learn how to perfectly reconstruct a band-limited signal when the sampling theorem is obeyed
	\item Know details about first and second-order systems
	\item Learn the definitions of bandwidth and quality factor for filters or individual poles
	\item Know the group delay of a system is the derivative of its frequency response phase, and how useful is to have a system with linear phase (or equivalently, a constant group delay)
	\item Learn about commercial filters based on technologies such as SAW and ceramics
	\item Know the concepts of FIR, IIR, AR, MA and ARMA discrete-time systems
	\item Design analog filters using filter frequency scaling and bandform transformation
	\item Design IIR filters using matched Z-transform, impulse invariance, backward difference, forward difference and bilinear transformation (also called Tustin's method) 
	\item Learn how to use prewarping when designing IIR filters with the bilinear transformation
	\item Design FIR filters using least-squares or windowing, specially focusing on filters with linear phase
	\item Learn the most import structures to implement FIR and IIR filters (transposed direct II structure, etc)
	\item Understand the effects of finite precision when the filter coefficients are quantized and roundoff errors occur during
	the filtering process
	\item Learn basic concepts of multirate processing, such as up and downsampling
\end{itemize}

\section{Contrasting Signals and Systems}

The previous discussion focused on \emph{signals}, while this chapter discusses how to model and work with \emph{systems}. System is a generic term that can be applied to any entity that converts one (or more) input signal(s) into one (or more) output signal(s). When there are multiple inputs and outputs, the system is called MIMO\index{Multiple input/multiple output (MIMO)}. This chapter will assume that the systems have a single input and a single output (SISO)\index{Single input/single output (SISO)}. It is conventional to call the input $x(t)$ or $x[n]$, for continuous and discrete-time signals, respectively, while the system output is denoted as $y(t)$ or $y[n]$. In general, the input to output mapping $y(t)=\calH\{x(t)\}$ will be described by an operator $\calH$ and depicted as 
\[
x[n] \rightarrow\boxed{\calH}\rightarrow y[n] \quad\textrm{or}\quad  x(t) \rightarrow\boxed{\calH}\rightarrow y(t).
\]

%As will be discussed in this chapter, 
In the special case of linear and time-invariant (LTI) systems (these concepts will be further discussed in Section~\ref{sec:lti}), a signal called impulse response $h(t)$ or $h[n]$ is capable of fully representing the behavior of the system (i.\,e., its output) to any input. In other words, the impulse response completely represents the LTI system. For this very special case of LTI systems, the input/output relation is represented by
\[
x[n] \arrowedbox{h[n]} y[n] \quad\textrm{or}\quad  x(t) \arrowedbox{h(t)} y(t)
\]
in the case of discrete or continuous-time systems, respectively.
%, respectively, because $h[n]$ (or $h(t)$) completely describes the system. 
Note that having a \textbf{signal} (the impulse response) representing a (LTI) \textbf{system} may be confusing. 

The Laplace transform $H(s)$ of the continuous-time impulse response $h(t)$ is called \emph{system function}\index{System function} or \emph{transfer function}\index{Transfer function}. In the discrete-time case, the system function is the Z transform $H(z)$ of $h[n]$. Similarly, the Fourier transforms $H(\aw)$ and $H(e^{j \dw})$ of the impulse responses are called \emph{frequency responses}\index{Frequency response}. Instead of the impulse response, the LTI system can be also represented via the system function as, for example in:
\[
x[n] \arrowedbox{H(z)} y[n] \quad\textrm{and}\quad  x(t) \arrowedbox{H(s)} y(t)
\]
or via its frequency response $H(\aw)$ or $H(e^{j \dw})$. \tabl{transf_of_h} summarizes these relations and nomenclature.

\begin{table}
\centering
\caption{Relations of the impulse response to the system function and frequency response of LTI systems.\label{tab:transf_of_h}}
\begin{tabularx}{\textwidth}{lCC|CC}
\toprule
Time & \multicolumn{2}{c|} {System function} & \multicolumn{2}{c} {Frequency response}  \\
 & Nomenclature & Output/input & Nomenclature & Output/input  \\ \midrule
Continuous & $H(s)$ Laplace~of~$h(t)$ & $H(s)=\frac{Y(s)}{X(s)}$ & $H(\aw)$ Fourier~of~$h(t)$ & $H(\aw)=\frac{Y(\aw)}{X(\aw)}$ \\ \midrule
Discrete & $H(z)$ \textrm{~~~~~~~~} Z~of~$h[n]$ & $H(z)=\frac{Y(z)}{X(z)}$ & $H(e^{j \dw})$ DTFT~of~$h[n]$ & $H(e^{j\dw})=\frac{Y(e^{j\dw})}{X(e^{j\dw})}$ \\ \bottomrule
\end{tabularx}
\end{table}

Both system function and frequency response relate the input to the output by a scaling factor (\emph{gain} and \emph{phase}) $\kappa$ that varies with the independent variable ($s$, $z$, $\dw$, etc.). For example, if the factor is $\kappa=3$ for a given situation, the output will be three times the input value. In other words, assume that $\kappa = H(\aw_0)=3$ for a given frequency $\aw = \aw_0$ rad/s. If the Fourier transform $X(\aw)$ of the input has a value $X(\aw_0)=7+j4$ at this frequency, the output at $\aw_0$ is $Y(\aw_0)=H(\aw_0) X(\aw_0) = 21+j12$. Note that, in general, both transfer functions and frequency responses are complex-valued.

The units for system functions and frequency responses depend on the corresponding units of the input and output signals. For example, $H(s)$ can be given in ohms if $x(t)$ and $y(t)$ are given in amperes and volts, respectively. 

An important special case of continuous-time LTI systems $H(s)$ are the ones described by linear differential equations with constant coefficients (coefficients that do not change over time). Similarly, the discrete-time LTI system $H(z)$ described by a \emph{linear constant-coefficient difference equation} (LCCDE)\index{Linear constant-coefficient difference equation (LCCDE)} 
is widely used in DSP. The LCCDE is often simply called \emph{difference equation}\index{Difference equation} and can be found via the inverse Z transform of $H(z)$.

Most physical systems have non-linearities and a behavior that changes with time. In other words, most physical systems are not LTI. But even if the system is not strictly LTI, in many important applications it is useful to model the system as such, and benefit from the large number of tools that exist to design and analyze LTI systems.


\section{A Quick Discussion About Filters}

This section presents a brief introduction to a special class of systems called LTI \emph{filters}\index{Filters}, which typically have the characteristic of being \emph{frequency-selective}. LTI systems will be further discussed in Section~\ref{sec:lti} but the goal here is to provide concrete examples on frequency-selective filters, due to their importance in DSP. Other filters will be discussed, but this section starts with the two most common filters: lowpass and highpass.
%bandpass and bandstop (or band-reject). 

The lowpass filter is characterized by attenuating (or rejecting) the frequency components that are above a given frequency called bandpass frequency $f_p$, while providing a gain $\kappa > 0$ to the frequency components from 0 to $f_p$. The name lowpass is used because the filter allows the low-frequency components of $x(t)$ to ``pass'' and compose the output. Similarly, the highpass tries to reject the components of $x(t)$ that are located from 0 to its $f_p$ in the frequency spectrum, while keeping those higher than $f_p$.

\begin{figure}[htb]
\centering
    \subfigure[Lowpass]{\label{fig:lowpass_spec}\includegraphics[width=5cm]{FiguresTex/lowpass_spec_V2}}
    \subfigure[Highpass]{\label{fig:highpass_spec}\includegraphics[width=5cm]{FiguresTex/highpass_spec_V2}} 
  \caption{Ideal magnitude specifications for lowpass and highpass filters.}
  \label{fig:low_highpass_specs}
\end{figure}

\figl{lowpass_spec} and \figl{highpass_spec} depict the ideal specification of low and highpass filters, respectively. These figures only illustrate the magnitude (gain $\kappa$) and, at this moment, the phase $\Theta$ is assumed to be zero. 

\bExample \textbf{Lowpass and highpass filtering examples}.
For example, assume a signal $x(t) = 3 \cos(2 \pi 100 t) + 8 \cos(2 \pi 200 t)$ ($t$ in seconds) is the input to the lowpass filter of \figl{lowpass_spec} with $\kappa=1$, $\Theta=0$ and $f_p = 150$ Hz. The output would be $y(t)=3 \cos(2 \pi 100 t)$ because the component of frequency 200 Hz would be filtered out. If the same $x(t)$ is the input to the highpass filter of \figl{highpass_spec} with $\kappa=4$, $\Theta=0$ and $f_p = 190$ Hz, the output would be $y(t)=32 \cos(2 \pi 200 t)$. In this case, besides eliminating the lowpass component, the filter imposed a gain of 4 to the amplitude of the 200 Hz component.
\eExample 

\figl{freqs_of_interest} shows the magnitude of the frequency response of a (second order analog) filter. This figure should be contrasted to the ideal case of \figl{low_highpass_specs}. In practice the filter gain cannot instantaneously change from 1 to 0 as idealized in \figl{low_highpass_specs}. Instead, this variation (the attenuation rolloff) depends on the filter order and creates a \emph{transition region} that is defined as the range of frequencies between $f_p$ and the stopband frequency $f_r$.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/freqs_of_interest}
\caption[{Magnitude of the frequency response of a practical analog filter in linear scale ($|H(f)|$, top) and in dB ($20 log_{10}|H(f)|$), which should be compared to the ideal case of \figl{low_highpass_specs}.}]{Magnitude of the frequency response of a practical analog filter in linear scale ($|H(f)|$, top) and in dB ($20\log_{10}|H(f)|$), which should be compared to the ideal case of \figl{low_highpass_specs}. Three frequencies are of interest: passband ($f_p=100$ Hz), cutoff ($f_c=158.5$ Hz) and stopband ($f_r = 500$ Hz) frequencies. \label{fig:freqs_of_interest}}
\end{figure}

\subsection{Cutoff and natural frequencies}

Besides $f_p$ and $f_r$, another frequency of interest is the so-called \emph{cutoff} frequency $f_c$. Assuming the filter gain at the passband is $\kappa$, the cutoff is the frequency for which the linear gain is $\kappa/\sqrt{2} \approx 0.707 \kappa$. %When $\kappa=1$ 
The cutoff indicates the frequency in which the filter attenuates a signal component to half of its power at the passband center. For example, assume an input signal $x(t)$ has a component $A \cos(2 \pi f_c t)$ with power $A^2/2$, which is passed through a highpass filter with unitary gain at passband and cutoff frequency $f_c$. This component will show up at the filter output as $\frac{A}{\sqrt{2}} \cos(2 \pi f_c t)$, which has power $\frac{A^2}{4}$, corresponding to half of the original power. In dB scale, the cutoff frequency corresponds to a gain of $20 \log_{10}(1/\sqrt{2}) \approx -3.01$ dB, as illustrated in \figl{freqs_of_interest} for a lowpass filter with gain $\kappa=1$ at DC.

The cutoff frequency should not be confused with the \emph{natural frequency}\index{Natural frequency}, which is detailed in \figl{natural_cutoff_frequency}. \tabl{special_freqs} is a useful reference for the special frequencies used in signal processing.

\subsubsection{Filter masks}

The filter designer often has a specification mask that should be obeyed. The passband and other special frequencies are used to described the mask.

\begin{figure}
\centering
\subfigure[Lowpass.]{\label{fig:lowpass-mask}\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/lowpass-mask}}\\
\subfigure[Highpass.]{\label{fig:highpass-mask}\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/highpass-mask}}
  \caption{Example of specification masks for designing low and highpass filters.\label{fig:masks}}
  %\label{fig:}
\end{figure}

\figl{lowpass-mask} and \figl{highpass-mask}  depict masks for low and highpass filters, respectively. In this case, the values \ci{Apass} and \ci{Astop} indicate the maximum and minimum attenuation in dB for the passband and stopband, respectively. These bands are indicated by the frequencies \ci{Fp} and \ci{Fr}, for pass and rejection bands.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/bandpass-mask}
\caption{Example of specification mask for designing bandpass filters.\label{fig:bandpass-mask}}
\end{figure}

Besides lowpass and highpass, some other popular filters are the so-called \emph{bandpass} and \emph{bandstop} (or band-reject) filters. \figl{bandpass-mask} shows a bandpass specification. 

The goal of this section was to provide an overview of filtering. 
%The next sections provide theoretical background.
Appendix~\ref{app:appSystemProperties} presents
a brief review of the most important properties of systems and the next section discusses LTI systems.

\section{Linear Time-Invariant Systems}
\label{sec:lti}

This section is dedicated to the study of linear and time-invariant (LTI) systems. 

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/ltidiagram}
\caption{Diagram of systems, emphasizing the linear and time-invariant (LTI) systems and the systems described by linear, constant-coefficient differential (or difference) equations (LCCDE).\label{fig:ltidiagram}}
\end{figure}

\figl{ltidiagram} depicts the position of LTI systems and also includes the important subset of systems described by linear, constant-coefficient differential (or difference, in discrete-time) equations (LCCDE). In this text, it is assumed that the LCCDE systems have zero initial conditions or, equivalently, that the system is at rest.

%In conclusion, a LCCDE with initial rest conditions gives us a causal and LTI system.

\subsection{Impulse response and convolution for LTI systems}
\label{sec:impresponse_convolution}

A LTI system is completely characterized by its impulse response.\footnote{Say you meet a person that tells you he/she is a LTI system. You should then ask his/her impulse response. If you know it, just relax because you can perfectly calculate the person's reaction (output) to any situation (input).} This fact is illustrated in the sequel using a simple example. Assume that a system $\calH$ is LTI and its impulse response $h[n]=2\delta[n] + 5\delta[n-1]$ is obtained by imposing an input $x[n]=\delta[n]$. For example, assuming $x[n]=4\delta[n]-3\delta[n-1]$, one has
\begin{align*}
y[n] & = \calH\{x[n]\} = \calH\{4\delta[n]-3\delta[n-1]\}\\
& = 4\calH\{\delta[n]\}-3 \calH\{\delta[n-1]\}\tag*{\text{(using linearity)}}\\
& = 4 h[n] - 3 h[n-1]\tag*{\text{(using time-invariance)}}\\
& = 4 (2\delta[n] + 5\delta[n-1]) - 3 (2\delta[n-1] + 5\delta[n-2])\\
& = 8\delta[n] + 14\delta[n-1]) - 15\delta[n-2].
\label{eq:example_lit}
\end{align*}
This example illustrates that, knowing $h[n]$ and using linearity and time-invariance, one can calculate the output.

In general, the input/output relation of a LTI depends on two facts:
\begin{itemize}
	\item As indicated by \equl{impulses}, any signal can be decomposed as the sum of impulses $\alpha \delta[n-n_0]$ that are shifted in time (by $n_0$) and scaled (by $\alpha$) in amplitude.
	\item By time-invariance, each impulse $\delta[n-n_0]$ generates a sequence $h[n-n_0]$ at the output and by linearity these sequences $\alpha h[n-n_0]$ can be scaled and summed to composed the output $y[n]$.
\end{itemize}
These two key facts lead to the \emph{convolution}\index{Convolution} operation:
%AK this align move the equations to next page
%\begin{eqnarray} %\tag cannot be used with eqnarray
\begin{align}
y[n] & = \calH\{x[n]\} = \calH\{\sum_{k=-\infty}^{\infty} x[k] \delta[n-k]\} \nonumber \\
& = \sum_{k=-\infty}^{\infty} x[k] \calH\{\delta[n-k]\}\tag*{\text{(using linearity)}} \nonumber \\
%& = \sum_{k=-\infty}^{\infty} x[k] h[n-k]\tag*{\text{(using time-invariance)}}  %could not use it together with equation number
& = \sum_{k=-\infty}^{\infty} x[k] h[n-k]\textrm{~~~~~~~~(using time-invariance)} \label{eq:convolution}
\end{align}
%\end{eqnarray}
Note how linearity and time-invariance were both invoked in this proof. The continuous-time convolution is similar:
\begin{equation}
y(t) =  \int_{-\infty}^{\infty} x(\tau) h(t-\tau) d\tau.
\label{eq:convolution_in_continuoustime}
\end{equation}
The convolution is so important that it is represented by the operator $\conv$. For example, $y[n]=x[n] \conv h[n]$ denotes the convolution of $x[n]$ and $h[n]$. Because $\conv$ is the same symbol used for multiplication in programming languages, the context has to distinguish them.
%whether it is convolution or multiplication.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/convolution_example}
\caption[{Example of convolution between $x[n]=2\delta[n]-3\delta[n-1]$ and $h[n]=\delta[n]-2\delta[n-1]+\delta[n-2]$.}]{Example of convolution between $x[n]=2\delta[n]-3\delta[n-1]$ and $h[n]=\delta[n]-2\delta[n-1]+\delta[n-2]$. The top-left and top-right plots are $x[n]$ and $y[n]$, respectively, while the other plots are the parcels of $x[n]$ (left) and the corresponding parcels of $y[n]$ (right).\label{fig:convolution_example}}
\end{figure}
%x=[2 -3]; nx=[0 1]; h=[1 -2 1]; nh=[0 1 2];


\figl{convolution_example} illustrates the convolution between two sequences. The interpretation is that $y[n]$ is composed by the sum of several scaled and shifted impulse responses. The plots at the right in \figl{convolution_example} indicate that $y[n]$ is the sum of the two parcels $2h[n]$ and $-3h[n-1]$. The \codl{convolution} provides an example of implementing convolution of discrete-time sequences,\footnote{On the Web one can find applets instructing about continuous and discrete convolution (see, e.\,g., \akurl{http://www.jhu.edu/~signals}{3jhu}).} which gives the same result as the (faster) \ci{conv} function in {\matlab}.

\includecode{MatlabOctaveFunctions}{convolution}


Note that if the samples of $x[n]$ and $h[n]$ are organized as elements of vectors of polynomial coefficients, convolving them is equivalent to multiplying the two polynomials. For the example in \figl{convolution_example} one can obtain the convolution result
by multiplying the equivalent polynomials $2x -3$ and $x^2 -2 x + 1$, which leads to $2x^3 -7 x^2 +8 x -3$, i.\,e., the coefficients are the samples of $y[n]$.

%\ url{http://www.jhu.edu/~signals/convolve/}
%? Discrete convolution
%Check \ url{http://www.jhu.edu/~signals/discreteconv2/index.html}.
%Some of its properties are listed in the sequel.

If the impulse response completely characterizes a LTI system, all its properties can be inferred from the corresponding impulse response. This is discussed in Appendix~\ref{sec:LTIproperties}.

\subsection{{\akadvanced} Convolution properties}

The convolution is:
\begin{itemize}
	\item Commutative: $a \conv b = b \conv a$
	\item Associative: $a \conv (b \conv c) = (a \conv b) \conv c$
	\item Distributive: $a \conv (b + c) = a \conv b + a \conv c$
\end{itemize}

Some other facts about the convolution:
\begin{itemize}
	\item If $N_1$ and $N_2$ are the duration in samples of two finite-length discrete-time sequences $a$ and $b$, then $a \conv b$ has duration $N_1+N_2-1$ samples
	\item The expression \equl{autocorrelation_energy} can also be written as
$R_{X}(\tau)  = x(\tau) ~\conv~ x^*(-\tau)$
	\item If $a[n]=b[n]\conv c[n]$, to obtain $a[n_a]$ for a specific instant $n_a$, one sums all products $b[n_b]c[n_c]$ for $n_a=n_b+n_c$
\end{itemize}

An example of this last fact is appropriate: if $x1[n]=\delta[n]+2\delta[n-1]+3\delta[n-2]$ and 
$x2[n]=5\delta[n]+6\delta[n-1]+7\delta[n-2]+8\delta[n-3]$, the output $y[n]=x1[n] \conv x2[n]$ is given by
\begin{verbatim}
y[0] = x1[0] x2[0] = 5
y[1] = x1[0] x2[1] + x1[1] x2[0] = 16
y[2] = x1[0] x2[2] + x1[1] x2[1] + x1[2] x2[0] = 34
y[3] = x1[0] x2[3] + x1[1] x2[2] + x1[2] x2[1] = 40
y[4] = x1[1] x2[3] + x1[2] x2[2] = 37
y[5] = x1[2] x2[3] = 24
\end{verbatim}
where one can note that the indexes of the input sequences sum up to the index of the output.
% (note that $*$ denotes multiplication in this case, not convolution).
Hence, one can alternatively implement convolution as in \codl{convolution2}.
\includecode{MatlabOctaveFunctions}{convolution2}
Note that \codl{convolution2} allows to explicitly deal with sequences that do not start at $n=0$. When using {\matlab}, it is often necessary to explicitly deal with and generate the time indexes.
%requires some ``bookkeeping'', i.\,e., that the user take care 

It can be observed from \codl{convolution2} that the first non-zero sample of the convolution between two sequences $x_1[n]$ and  $x_2[n]$ is located at $n=n_1 + n_2$, which are assumed to be the indexes of the first non-zero samples of the two sequences. Similarly, the last non-zero sample of the convolution is the sum of the indexes of the last non-zero samples of $x_1[n]$ and  $x_2[n]$. This can be seen by running \codl{snip_systems_ak_convolution}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_ak\_convolution}{snip_systems_ak_convolution} 
%\begin{lstlisting}
%x1=1:3; %define sequence x1
%x2=5:8; %define sequence x2
%nx1=-3:-1; %define abscissa for x1
%nx2=2:5; %define abscissa for x2
%subplot(311); stem(nx1,x1)
%subplot(312); stem(nx2,x2)
%[y,n]=ak_convolution2(x1,x2,-3,2); %calculate convolution
%subplot(313); stem(n,y) %show result with proper time axis
%\end{lstlisting}

\subsection{{\akadvanced} Convolution via correlation and vice-versa}
\label{sec:convolutionViaCorrelation}

Given that convolution and cross-correlation are tightly related, 
\codl{snip_systems_convolution_correlation} illustrates how the {\matlab} functions \ci{xcorr} and \ci{conv} can be used to calculate the other operation.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_convolution\_correlation}{snip_systems_convolution_correlation}

As indicated in \codl{snip_systems_convolution_correlation}, the key step to mimic a 
convolution via correlation or vice-versa is to flip the signal using $x(-t)$, use its 
complex conjugate and eventually shift by $t_0$ as  indicated in
\begin{equation}
\hat x(t) = x^*(-t + t_0).
\label{eq:convViaCorr}
\end{equation}
\ifml
\else
\equl{convViaCorr} is crucial to understand matched filtering, as discussed in Section~\ref{sec:optimalReceiversAWGN}.
\fi

\subsection{{\akadvanced} Discrete-time convolution in matrix notation}
\label{sec:convolutionAsMatrix}

Convolution can be denoted in matrix notation. This is especially convenient when dealing with finite-duration discrete-time signals. When these signals are represented by column vectors $\bx_1$ and $\bx_2$, a convolution matrix $\bH_1$ allows to obtain the convolution result $\by = \bx_1 \conv \bx_2$ as
\begin{equation}
\by = \bH_1 \bx_2,
\label{eq:convolutionInMatrixNotation}
\end{equation}
where $\bH_1$ is composed by the elements of $\bx_1$. It is also possible to create $\bH_2$ from the elements of $\bx_2$ and use $\by = \bH_2 \bx_1$.

This can be better understood with examples. Considering the goal is to obtain the convolution of two column vectors \ci{x1=[1; 2; 3]} and \ci{x2=[5; 6; 7; 8]} (previous example), instead of using \ci{y=conv(x1,x2)}, one can create a matrix 
\begin{verbatim}
hmatrix = [
     1     0     0     0
     2     1     0     0
     3     2     1     0
     0     3     2     1
     0     0     3     2
     0     0     0     3]
\end{verbatim}
with the command \ci{hmatrix = convmtx(x1,length(x2))}, and then calculate the convolution with \ci{y=hmatrix * x2}. Note that when using row vectors, the commands could be \ci{hm = convmtx(x1,length(x2)); x2*hm} (in this case, \ci{hm} would be the transpose of \ci{hmatrix}).
Application~\ref{app:blocksMatrixConvolution} discusses the issue of repeatedly processing blocks of samples using the convolution in matrix notation.
The matrices created by \ci{convmtx} are Toeplitz matrices and the command \ci{toeplitz.m}
can also be useful to compose convolution matrices.

\subsection{Approximating continuous-time via discrete-time convolution}
\label{sec:continuousViaDiscreteConvolution}

Some situations require approximating the continuous-time convolution denoted by 
\equl{convolution_in_continuoustime} via the discrete-time convolution in \equl{convolution}.
%An example is the analysis of communication systems such as in Section~\ref{}.
%
In such cases, the sampling interval $\ts$ should be used as a normalization factor, 
as indicated in \equl{continuous_convolution_via_discrete}. This factor is adopted
in the next example.

\bExample \textbf{Convolution of a pulse with itself leads to a triangular waveform}.
Assume a pulse $p(t) = 4\rect(5t-0.1)$  (see $\rect(\cdot)$ in Section~\ref{sec:rect_function}) with
 amplitude $A=4$~V and support $T_0=0.2$~s. 
The convolution $p(t) \conv p(t)$ corresponds to a triangular waveform: it is 0 for $t<0$ and assumes its maximum value at $A^2 T_0 = 4^2 \times 0.2 = 3.2$ at 0.2~s, which corresponds to the time that the two pulses
overlap completely and $A^2 T_0$ is the area of $p^2(t)$.
\figl{continuous_discrete_conv} depicts both the pulse and continuous-time convolution $p(t) \conv p(t)$.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/continuous_discrete_conv}
\caption{Convolution of a pulse $p(t) = 4 \rect(5t-0.1)$ with itself, obtained with \codl{snip_systems_continuous_discrete_conv}.\label{fig:continuous_discrete_conv}}
\end{figure}

But 
\figl{continuous_discrete_conv} was in fact obtained 
by representing $p(t)$ via its discrete-time version $p[n]$,
and  approximating the convolution $p(t) \conv p(t)$ with
%Appendix~\ref{sec:normalization_factors_psd_pdf}.
the scaled discrete-time convolution
$\ts (p[n] \conv p[n])$, i.\,e., \equl{continuous_convolution_via_discrete}.
\codl{snip_systems_continuous_discrete_conv} shows the first lines of
the script that generated \figl{continuous_discrete_conv}.
After running \codl{snip_systems_continuous_discrete_conv}, the vectors \ci{pulse} and \ci{triangle} 
 are shown in \figl{continuous_discrete_conv} with the proper time axes.

%As discussed in \exal{simulScaleShift}, 
%Appendix~\ref{sec:sinc}

\lstinputlisting[lastline=7,caption={MatlabOctaveCodeSnippets/snip\_systems\_continuous\_discrete\_conv},label=code:snip_systems_continuous_discrete_conv]{./Code/MatlabOctaveCodeSnippets/snip_systems_continuous_discrete_conv.m}

Note in line 6
the scaling factor $\ts$, which leads to the correct values, as indicated
by the datatip in \figl{continuous_discrete_conv}.
\eExample 

\subsection{Frequency response: Fourier transform of the impulse response}

As indicated in \tabl{transf_of_h}, the frequency response\index{Frequency response} of an LTI system is the Fourier transform of its impulse response. In continuous-time it will be denoted by $H(f)$ ($f$ in Hz) or $H(\aw)$ ($\aw$ in rad/s), while in discrete-time the notation is $H(e^{j\dw})$ ($\dw$ in rad). The frequency response is particularly useful because complex exponentials are eigenfunctions of LTI systems as explained in the sequel.

\subsubsection{Eigenfunctions}

Eigenfunctions are closely related to eigenvectors.
 if $\bA$ is a matrix (a linear transformation), a non-null vector $\bx$ is an eigenvector of $\bA$ if there is a scalar $\lambda$ such that
\[
\bA \bx = \lambda \bx.
\]
The scalar $\lambda$ is an eigenvalue of $\bA$ corresponding to the eigenvector $\bx$.
In general, the matrix $\bA$ generates a completely new vector $\by=\bA \bx$, i.\,e., $\by$ and $\bx$ have different directions and magnitudes.
However, if $\bx$ is an eigenvector of $\bA$, then $\by= \lambda \bx$, which means that the operation changed only the magnitude of $\bx$, leaving its direction unchanged (or possibly reversing it in case $\lambda < 0$).

Similarly, for any LTI system $\calH$ (continuous-time is assumed here), a complex exponential is an eigenfunction because the output is $y(t) = \lambda e^{j \aw_0 t}$ when the input is $x(t)=e^{j \aw_0 t}$. The frequency response is useful because the eigenvalue $\lambda = H(\aw_0)$ is the value of the frequency response $H(\aw)$ at the specific frequency $\aw = \aw_0$.
This implies that \emph{an LTI never creates new frequencies}, because it can simply change the magnitude and phase of frequencies that were presented at its input. This fact can be represented as
\[
e^{j \aw_0 t} \rightarrow\boxed{\calH}\rightarrow H(\aw_0) e^{j \aw_0 t}.
\]

It is then possible to analyze an LTI in the frequency domain by following the steps:
\begin{enumerate}
	\item Decompose the input as a sum (if the signal is periodic) or integral of complex exponentials. For that, one can use the Fourier series or transform $X(\aw) = \calF \{ x(t) \}$
	\item Obtain the frequency response, which provides the eigenvalues for all frequencies: $H(\aw) = \calF \{ h(t) \}$
	\item Conceptually, multiply each complex exponential $e^{j \aw_0 t}$ by its eigenvalue $H(\aw_0)$ and add the
	partial results to obtain the system output $y(t)$. In practice, this step is performed
	by 
	%Because the number of complex exponentials may be infinite, it is more convenient to obtain
	 $Y(\aw)=H(\aw) X(\aw)$ and then using the inverse transform to obtain $y(t) = \calF^{-1} \{ Y(\aw) \}$.
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/freq_response_onepole}
\caption[{Frequency response of $H(\aw) = \frac{1}{j \aw + 2}$ represented in polar form: magnitude (top) and phase (bottom).}]{Frequency response of $H(\aw) = \frac{1}{j \aw + 2}$ represented in polar form: magnitude (top) and phase (bottom). The data tips indicate the values for $\aw= \pm 4$~rad/s.\label{fig:freq_response_onepole}}
\end{figure}

\bExample \textbf{LTI filtering of continuous-time complex exponential.}
For example, consider that $x(t) = 6e^{j 4 t} + 6e^{-j 4 t} = 12 \cos(4t)$ is the input of an LTI with $h(t)=e^{-2t} u(t)$. The eigenvalues corresponding to each the complex exponential eigenfunctions with frequencies $\aw=\pm 4$ rad/s can be obtained from the frequency response $H(\aw) = \calF \{ h(t) \} = \frac{1}{j \aw + 2}$. The {\matlab} commands \ci{Omega=4; H=1/(j*Omega+2)} calculate $H(\aw)|_{\aw=4}=0.1-0.2j$ for the positive frequency. 
Because $h(t)$ is real-valued, the frequency response presents Hermitian symmetry and $H(\aw)|_{\aw=-4} = H^*(\aw)|_{\aw=4}$, i.\,e., $H(\aw)|_{\aw=-4}=0.1+0.2j$. The commands \ci{abs(H), angle(H)} can transform from Cartesian to polar: $H(\aw)|_{\aw=4} \approx 0.2236 e^{-j1.107}$ and $H(\aw)|_{\aw=-4} \approx 0.2236 e^{j1.107}$. Therefore, the output is $y(t) = (0.2236 e^{-j1.107}) 6e^{j 4 t} + (0.2236 e^{j1.107}) 6e^{-j 4 t} = 
1.32 e^{j(4t - 1.11)} + 1.32 e^{j(-4t + 1.11)} = 2.64 \cos(4t - 1.11)$. 

The effect of the LTI system was to impose a gain of $|H(4)|=0.2236$ and a phase of $\angle{H(4)}=-1.107$ rad.
\figl{freq_response_onepole} depicts the frequency response for a range $\aw=2\pi[-3,3]$ rad/s. Because of the Hermitian symmetry (when $h(t)$ is real), it is common to plot only the positive frequencies. For example, investigate the command \ci{freqs(1,[1 2])}, which shows the frequency response of $H(\aw) = \frac{1}{j \aw + 2}$ in a different format.
\eExample 

The previous discussion was restricted to exponentials of the form $e^{j \aw_0 t}$, but a general complex exponential $e^{st}$ (with $\sigma \ne 0$) is also an eigenfunction of any LTI and the eigenvalue is given by the Laplace transform $H(s)$, as represented by
\[
e^{s_0 t} \rightarrow\boxed{\calH}\rightarrow H(s_0) e^{s_0 t}.
\]

The frequency response $H(e^{j\dw})$ of discrete-time LTI systems is also very useful. When the time axis $t$ is discretized by sampling, i.\,e., $t=n \ts$, the complex exponential $e^{st}$ becomes $e^{s n \ts}$, which is more conveniently represented by $z^n$, where 
\[
z=e^{s \ts}
\]
with $z,s \in \complex$.

The discrete-time complex exponential $z^n$ is an eigenfunction of discrete-time LTI systems. To prove that, consider $x[n]=(z_0)^n, z_0 \in \complex$, then
\begin{align*}
y[n] &= \sum_{k=-\infty}^{\infty} (z_0)^k h[n-k] = \sum_{k=-\infty}^{\infty} h[k] (z_0)^{n-k} = (z_0)^n \sum_{k=-\infty}^{\infty} h[k] (z_0)^{-k}\\
&=  (z_0)^n H(z)|_{z=z_0}.
\end{align*}
The eigenvalue $H(z)|_{z=z_0}$ can be obtained from the transfer function
\[
H(z) = \sum_{n=-\infty}^{\infty} h[n] z^{-n},
\]
which is the Z-transform of the system's impulse response. As a special case, when $|z|=1$, the eigenfunction $e^{j \dw n}$ has its amplitude and phase eventually modified by a LTI as depicted by:
\[
e^{j \dw_0 n} \rightarrow\boxed{\calH}\rightarrow H(e^{j \dw_0}) e^{j \dw_0 n}. 
\]

\bExample \textbf{LTI filtering of discrete-time complex exponentials}.
This example describes how an LTI system filters a complex exponential.
Consider that $x[n] = 4 e^{j 0.5 \pi n} + 4 e^{-j 0.5 \pi n} = 8 \cos(0.5 \pi n)$ is the input of an LTI with $h[n]=0.7^n u[n]$. The eigenvalues can be obtained from $H(e^{j\dw}) = \calF \{ h[n] \} = \frac{1}{1 - 0.7e^{-j\dw}}$ when $\dw=0.5 \pi$ rad. The {\matlab} commands \ci{omega=pi/2; H=1/(1-0.7*exp(-j*omega))} calculate $H(e^{j\dw})|_{\dw=0.5\pi}=0.6711 - 0.4698j$. Because $h[n]$ is real-valued, the frequency response presents Hermitian symmetry. The commands \ci{abs(H), angle(H)} can transform from Cartesian to polar: $H(e^{j\dw})|_{\dw=0.5\pi} \approx 0.8192 e^{-j0.6107}$. Therefore, the output is $y[n] = (0.8192 e^{-j0.6107}) 4 e^{j 0.5 \pi n} + (0.8192 e^{j0.6107}) 4 e^{-j 0.5 \pi n} = 3.2769 e^{j(0.5 \pi n - 0.6107)} + 3.2769 e^{j(-0.5 \pi n + 0.6107)} = 6.5539 \cos(0.5 \pi n - 0.6107)$. The effect of the LTI system was to impose a gain of $0.8192$ and a phase of $-0.6107$ rad.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/freq_response_onepole_discrete}
\caption[{Frequency response represented in polar form: magnitude (top) and phase (bottom).}]{Frequency response of $H(e^{j\dw}) = \frac{1}{1 - 0.7e^{-j\dw}}$ represented in polar form: magnitude (top) and phase (bottom). The data tips indicate the values for $\dw=\pi/2$ rad.\label{fig:freq_response_onepole_discrete}}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/freqz_onepole_discrete}
\caption{Version of \figl{freq_response_onepole_discrete} obtained with the command \ci{freqz(1,[1 -0.7])}.\label{fig:freqz_onepole_discrete}}
\end{figure}

\figl{freq_response_onepole_discrete} depicts the frequency response for a range $\dw=[-15,15]$ rad. As expected, $H(e^{j\dw})$ is periodic because $H(e^{j\dw}) = H(e^{j(\dw+ 2\pi)})$. Given this periodicity and the Hermitian symmetry (when $h[n]$ is real), it is common to plot only the positive frequencies in the range $\dw=[0, \pi]$. For example, \figl{freqz_onepole_discrete} shows the plots obtained with the command \ci{freqz(1,[1 -0.7])}, which shows the frequency response of $H(e^{j\dw})$ with the magnitude in dB, the phase in degrees (instead of rad) and the abscissa in a ``normalized frequency'' $\dw/\pi$ that maps $[0, \pi]$ into $[0, 1]$.
\eExample 


\subsection{Fourier convolution property}
\label{sec:FourierConvolutionProperty}

The \emph{convolution property} of Fourier transforms states that the convolution between two signals has a Fourier transform given by the multiplication of their Fourier transforms. In other words, the convolution between these signals by can be obtained as the inverse Fourier transform of the multiplication of their Fourier transforms. For example, assuming continuous-time, the convolution $y(t)=x_1(t) \conv x_2(t)$ can be obtained with
\begin{equation}
y(t) = \calF^{-1} \{ X_1(f) X_2(f) \},
\label{eq:convolutionInFreqDomain}
\end{equation}
where $X_1(f)$  and $X_2(f)$ are the corresponding Fourier transforms.
Similarly, in discrete-time, the convolution result $y[n]$ can be written as
\begin{equation}
y[n] = x_1[n] \conv x_2[n] = \textrm{DTFT}^{-1} \{ X_1(e^{j \dw}) X_2(e^{j \dw}) \},
\label{eq:convolutionInFreqDomainDiscreteTime}
\end{equation}
where $X_1(e^{j \dw})$ and $X_2(e^{j \dw})$ are the corresponding DTFTs and, in this case, $\calF^{-1}$ denotes the inverse DTFT.

\bExample \textbf{Convolution using DTFTs and inverse DTFT.}
For example, if $x_1[n]=\alpha_1^n u[n]$ and $x_2[n]=\alpha_2^n u[n]$ are two discrete-time complex exponentials with $|\alpha_1|<1$ and $|\alpha_2|<1$, their DTFTs are $X_i(e^{j \dw})=1-\alpha_ie^{-j \dw},i=1,2$, respectively. The convolution result is
\begin{equation}
y[n] = x_1[n] \conv x_2[n]  = \frac{1}{\alpha_1 - \alpha_2} \left( \alpha_1^{n+1} u[n] - \alpha_2^{n+1} u[n]\right)
\label{eq:convolutionExampleDTFT}
\end{equation}
and can be obtained via the inverse DTFT of $Y(e^{j \dw})$, which is given by
\begin{equation}
Y(e^{j \dw}) = X_1(e^{j \dw}) X_2(e^{j \dw}) = \frac{1}{(1-\alpha_1e^{-j \dw})(1-\alpha_2e^{-j \dw})}.
\label{eq:convolutionExampleDTFT2}
\end{equation}
\equl{convolutionExampleDTFT} can be obtained by using partial fraction expansion of \equl{convolutionExampleDTFT2}.
\eExample

Due to the duality of Fourier transforms, the multiplication in time-domain corresponds
to convolution in frequency-domain between the respective spectra. This is called the
\emph{multiplication property} and corresponds to
\begin{equation}
x_1(t) x_2(t) \Leftrightarrow X_1(f) \conv X_2(f) = \int_{-\infty}^{\infty} X_1(p) X_2(f-p) \textrm{d}p.
%\label{eq:}
\end{equation}
in continuous-time.

In discrete-time, one needs to take in account that the spectra are periodic and the conventional convolution is not adequate when both signals are periodic. In fact, the
multiplication property for discrete-time signals is
\begin{equation}
x_1[n]  x_2[n] \Leftrightarrow \frac{1}{2\pi} \int_{<2\pi>} X(e^{j \dw})  X_2(e^{j (\dw-\theta)}) d\theta.
\label{eq:windowingConvolution}
\end{equation}
The integral in \equl{windowingConvolution} differs from a conventional convolution because it is calculated over only one period ($2 \pi$ rad in this case) and the result is normalized by this period. This modified convolution is denoted as $\cconv$ and
is called \emph{periodic}\index{Periodic convolution}, \emph{cyclic}\index{Cyclic convolution} or \emph{circular} convolution\index{Circular convolution}. As in a Fourier series expansion to obtain coefficients $c_k$, the associated period must be known in order to properly use $\cconv$. The normalization by the period can be incorporated in its definition, which leads to the following expression for the periodic convolution:
\begin{equation}
x_1(t) \cconv x_2(t) = \frac{1}{T} \int_{<T>} x_1(\tau)  x_2(t-\tau) \textrm{d}\tau,
\label{eq:circularConvolution}
\end{equation}
where both signals are periodic in $T$.

The definition of \equl{circularConvolution} allows to write \equl{windowingConvolution} as
\begin{equation}
x_1[n]  x_2[n] \Leftrightarrow X(e^{j \dw}) \cconv X_2(e^{j \dw}),
\label{eq:windowingConvolution2}
\end{equation}
where the period $2 \pi$ is implicit.

The periodic convolution corresponds to performing the conventional convolution (called ``linear'' convolution, in contrast to ``circular'') between $x_1(t)$ and $x'_2(t)$, where $x'_2(t)$ is a single period of $x_2(t)$ that is normalized by the period $T$.
The interval to define $x'_2(t)$ can be conveniently chosen as $x_2'(t)=x_2(t)/T, 0\le t < T$, or $0$ otherwise, or $x_2'(t)=x_2(t)/T, -T/2 \le t < T/2$, or $0$ otherwise.

As discussed in the sequel, the periodic convolution is typically associated to FFT-based processing.

\subsection{Circular and fast convolutions using FFT}
Recalling that the FFT corresponds to sampling the DTFT, \equl{convolutionInFreqDomainDiscreteTime} suggests that FFTs can be used to efficiently compute a convolution. 
However, even if $x[n]$ were not periodic, when its DTFT $X(e^{j \dw})$ is sampled (in frequency domain) by the FFT, the FFT values $X[k]$ are representing the spectrum of a periodic version of $x[n]$. Consequently, when FFTs substitute DTFTs in \equl{convolutionInFreqDomainDiscreteTime}, the result is not the linear but the circular convolution represented as
\begin{equation}
y[n] = x_1[n] \cconv x_2[n] = \textrm{FFT}^{-1} \{ X_1[k] X_2[k] \},
\label{eq:circularConvolutionWithFFT}
\end{equation}
where both FFTs must have the same length $N$, which plays the whole of the period of a circular convolution.

\codl{snip_systems_circularConvolution} provides an example of using \equl{circularConvolutionWithFFT}. Note how zero-padding is used to assure the element-wise  multiplication \ci{fft(x,N).*fft(h,N)} uses arrays with the same length. The result
of \codl{snip_systems_circularConvolution} confirms that, in general, linear and
circular convolution differ.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_circularConvolution}{snip_systems_circularConvolution} 

If the value of \ci{shouldMakeEquivalent} is made equal to 1, 
\codl{snip_systems_circularConvolution} returns the same results for both linear and
circular convolutions. In fact, when the FFT length is at least the number of non-zero
samples of the convolution output, the circular and linear convolution results coincide.
This suggests that \equl{circularConvolutionWithFFT} can be used to calculate a
linear convolution, provided that the FFT length is made long enough.

Obtaining a linear convolution via FFTs is trickier when one of the signals to be convolved has infinite duration. In this case, it is obviously not possible to find a large enough FFT
length to use \equl{circularConvolutionWithFFT}. However, if the other signal has
finite duration,  it is feasible and often used in practice to segment the long signal
into blocks and calculate the linear convolution by sequentially calculating one FFT
per block and properly combining the results. There are basically two alternatives to combine
the intermediate results that are called \emph{overlap-add} and \emph{overlap-save} methods,
 and are roughly equivalent.
 \codl{snip_systems_overlapAdd} illustrates the former.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_overlapAdd}{snip_systems_overlapAdd} 

A typical application of the overlap-add and overlap-save methods is to compute the output of a LTI system represented by a finite-duration impulse response (i.\,e., a FIR filter, as will be discussed in Section~\ref{sec:firFilters}). 
\codl{snip_systems_overlapAdd} provides an example where the impulse response has only three non-zero samples, and the long input signal is segmented in blocks of \ci{Nb=5} samples.

\section{{\akadvanced} Sampling and Signal Reconstruction Revisited}
\label{sec:samplingRevisited}

Having established the fundamentals of convolution and Fourier transforms, it is
possible now to sketch a proof of the sampling theorem (Theorem~\ref{th:sampling}) without difficulties.

\subsection{A proof sketch of the sampling theorem}

Sampling a signal $x(t)$ at each $\ts$ seconds (periodic and uniform sampling) can be modeled as the multiplication of $x(t)$ by a periodic function $p(t)$ with fundamental period $\ts$. For example, $p(t)$ can be a train of pulses with duty cycle $T_1$. However, it is mathematically convenient\footnote{It avoids the ``$\sin(x)/x$'' distortion in frequency domain associated to a \emph{pulse} train.} to adopt an \emph{impulse} train
\begin{equation}
p(t) = \sum_{k=-\infty}^\infty \delta(t-k \ts),
\label{eq:pulseTrain}
\end{equation}
which allows to model the sampled signal as
\begin{equation}
x_s(t) = x(t) p(t) = \sum_{k=-\infty}^\infty x(k \ts) \delta(t-k \ts),
\label{eq:sampledSignalGeneralRevisited}
\end{equation}
as anticipated in \equl{sampledSignalGeneral}.

From \equl{sampledSignalGeneralRevisited} and the Fourier convolution property discussed in
Section~\ref{sec:FourierConvolutionProperty}, the Fourier transform of $x_s(t)$ is
\begin{equation}
X_s(f) = X(f) \conv P(f),
\label{eq:sampledSignalGeneralRevisitedFrequency}
\end{equation}
where $X(f)=\calF \{ x(t) \}$ and $P(f)=\calF \{ p(t) \}$ is another impulse train, but in frequency domain. As indicated by \equl{fourierPropertyImpulseTrain}, the impulses in $P(f)$ are spaced by $\fs=1/\ts$ and have area $\fs=1/\ts$.

The convolution of $X(f)$ with the impulses in $P(f)$ creates infinite replicas of $X(f)$ at frequencies values that are multiples of $\fs$ as depicted in \figl{samplingSpectra}. As well-discussed in textbooks, if $\fs$ is not sufficiently large, these replicas will overlap and create aliasing. But in case $\fs > 2 F_\tmax$, all replicas are ``perfect'' copies of $X(f)$ scaled by $\ts$. The original spectrum can then be recovered by keeping one replica and eliminating the others. This filtering procedure is assumed here to be done with an ideal lowpass filter with bandwidth $\BW = F_\tmax$ and gain $\ts$. This ideal filter will then cancel the undesired replicas of $X_s(f)$ and recover $X(f)$ using precisely the scaling factor $\ts$.

\begin{figure}
\centering
\includegraphics[width=\textwidth,keepaspectratio]{FiguresTex/samplingSpectra}
\caption{Spectrum $X_s(f)$ of a sampled signal $x_s(t)$ obtained by the convolution between $X(f)$ and $P(f)$ as indicated in \equl{sampledSignalGeneralRevisitedFrequency}.\label{fig:samplingSpectra}}
\end{figure}

In summary, a lowpass signal $x(t)$ with maximum frequency $F_\tmax$, can be perfectly reconstructed from its sampled version $x_s(t)$ if the sampling frequency obeys $\fs > 2 F_\tmax$ (Theorem~\ref{th:sampling}) and the reconstructed signal $\hat x(t) = x(t)$ is obtained by passing $x_s(t)$ through an ideal lowpass filter with frequency response $H(f)$ having gain $\ts$ over the passband.

The spectrum $\hat X(f)$ of $\hat x(t)$ corresponds to $\hat X(f) = X_s(f) H(f)$.
The multiplication of $X_s(f)$ by $H(f)$ in frequency-domain corresponds to the convolution of $x_s(t)$ with the filter's impulse response $h(t) = \calF \{ H(f) \}$. From \equl{sinc_transform} and the duality property, $h(t) = \sinc(t/\ts)$ in this case.
Hence, this convolution is written as
\begin{equation}
\hat x(t) = x_s(t) \conv h(t) = \sum_{n=-\infty}^\infty x_s^a(n \ts) \sinc\left( \frac{t}{\ts}-n \right),
\label{eq:signalReconstructionViaSincs}
\end{equation}
which corresponds to the convolution of the scaled sinc $h(t)$ 
 with impulses\footnote{If needed, recall the notation for sampled signals in Section~\ref{sec:sampling_stage}.} of area $x_s^a(n\ts)$. The time shift by $n$ in \equl{signalReconstructionViaSincs} positions the sincs at $n \ts$, and was discussed in \exal{simulScaleShift}.
%, where $n \in \integers$, creates a replica $\sinc((t-n\ts)/\ts) = \sinc(t/\ts-n)$ at time $t=n \ts$ that is scaled by $x(n\ts)$.

\equl{signalReconstructionViaSincs} represents the reconstruction of a band-limited signal by the sinc interpolation of its samples and is called the \emph{Whittaker-Shannon interpolation formula}\index{Whittaker-Shannon interpolation formula}\index{Shannon-Whittaker interpolation formula}. 

Hence, when the sampling theorem is obeyed, the whole chain is as follows (the signals are depicted with their associated power in parenthesis as in \blol{powersFromCtoD}):
\begin{equation}
\underset{(\calP_c)}{x(t)} \arrowedbox{sampling} \underset{(\calP_s)}{x_s(t)} \arrowedbox{S/D} \underset{(\calP_d)}{x[n]} \arrowedbox{D/S} \underset{(\calP_s)}{x_s(t)} \arrowedbox{h(t)} \underset{(\calP_c)}{\hat x(t)}  = x(t)
\label{eq:samplingAndReconstruction}
\end{equation}
where $h(t) = \sinc(t/\ts)$ is the discussed ideal filter.

Using the notation suggested by \blol{samplingAndReconstruction}, the samples $x(n\ts)$ were converted
to areas $x_s^a(n\ts)$, which are converted to $x[n]=x_s^a(n\ts)$. Hence, \equl{signalReconstructionViaSincs} can be conveniently rewritten as
\begin{equation}
\hat x(t) = x_s(t) \conv h(t) =  \textrm{D/S} \{ x[n] \} \conv h(t) = \sum_{n=-\infty}^\infty x[n] \sinc\left( \frac{t}{\ts}-n \right).
\label{eq:signalReconstructionViaSincs2}
\end{equation}

\subsection{Energy and power of a sampled signal}
\label{sec:sampledSignalEnergyPower}

%As mentioned in Section~\ref{sec:impulseIsNotAFunction}, $\delta^2(t)$, 

The squared of the continuous-time impulse $\delta^2(t)$ is not defined. This creates a problem when one considers the energy or power of $\delta(t)$.
When $\delta(t)$ is interpreted as a pulse $p(t)$ with unit area and amplitude $1/\Delta$, one can argue that when $\lim_{\Delta \rightarrow 0}$ as in \equl{diracDeltaDefinition}, the resulting area of the squared pulse $p^2(t)$ is $1/\Delta$, which leads to $\delta^2(t) = \infty$. However, this would not be mathematically rigorous given that $\delta(t)$ is a distribution. Hence, the following route is taken here: instead of defining new \emph{transformations}\footnote{See, e.\,g., references in Appendix~\ref{sec:impulseIsNotAFunction}.} on the distribution $\delta(t)$, the instantaneous power of a sampled signal $x_s(t)$ is defined as the instantaneous power of its equivalent discrete-time signal $x[n]$ obtained via a S/D conversion, normalized by the associated $\ts$, i.\,e.
\begin{equation}
\calP_s \defeq \frac{\calP_d}{\ts}.
\label{eq:powerSampledSignal}
\end{equation}

For example, the pulse train of \equl{pulseTrain} has average power $\calP_s=1/\ts$ because when converted to discrete-time its power is $\calP_d=1$.

The same reasoning can be applied to sampled signals for which the independent variable is not $t$. The Fourier transform $P(f) = \{ p(t) \}$ of \equl{pulseTrain} has power $1/\ts$ because its discrete-frequency version has power $1/\ts^2$ and the normalizing factor is $1/\ts$ in this case. Note that, with this definition of instantaneous power of a sampled signal, the power of the impulse trains $p(t)$ and $P(f)$ are the same, as expected from \equl{parsevalPower}.

%The instantaneous power $p_x(t)$ of a sampled real signal $x_s(t)=\sum_{k=-\infty}^\infty x[k] \delta(t-k T_s)$ can be obtained by squaring the areas of each of its impulses:
%\begin{equation}
%p_x(t) = \left(x_s(t) \right)^2=\sum_{k=-\infty}^\infty x^2[k] \delta(t-k T_s).
%\label{eq:squaredSampledSignal}
%\end{equation}
%Over an interval $\ts$, the energy is $\int_{<\ts>} p_x(t) \textrm{d}t = x^2[k]$. Taking the average and normalizing by $\ts$ leads to
%Assuming that $x_s(t)$ was obtained from a D/C conversion of $x[n]$ with power $\calP_d$
%given by \equl{powerDiscreteTime}, the power of the sampled signal $\calP_s$ is
%\begin{equation}
%\calP_s = \frac{\ev[x^2[k]]}{\ts} = \frac{\calP_d}{\ts}.
%\label{eq:powerRelationPsPd}
%\end{equation}
%While the relation between $\calP_d$ and $\calP_s$ only depends on $\ts$, their relation to $\calP_c$ depends on how the sampling is performed when the processing chain is \blol{powersFromCtoD}, or on $h(t)$ when considering \blol{powersFromDtoC}.
%When sampling is modeled by the multiplication by a periodic impulse train, $x_s(t)$ is given by \equl{sampledSignalGeneral}. The corresponding impulse train has power $1/\ts$ but there is no general expression relating $\calP_c$ and $\calP_s$.
%
%As an example, consider 
%that $x(t)$ is a periodic and even pulse train with amplitude $A=8$~V, duty cycle $T_1=9$~s and period $T_0=18$~s. When sampled with $\ts=2$~s, $x_s(t)$ will have impulses at $t=0,2,-2$ that will repeat at each period $T_0$. In this case, $\calP_c = A^2/2 = 32$~W and $\calP_s = 3 A^2/T_0 = 3 \times 8^2 / 18 \approx 10.67$~W, while the sampling impulse train has power $1/\ts=0.5$~W.
%
%However, when the sampling theorem is obeyed, $\calP_c$ and $\calP_s$ in \blol{powersFromCtoD} are simply related by
%\begin{equation}
%\calP_s  = \frac{\calP_c}{\ts},
%\label{eq:powerRelationToBeProved}
%\end{equation}
%which is discussed in Section~\ref{sec:samplingRevisited}.


\subsection{Energy / power conservation after sampling and reconstruction}
\label{sec:energyConservationThroughSampling}

%With the help of the example in \figl{samplingSpectra}, it is possible to sketch a proof for \equl{powerRelationToBeProved}. In \figl{samplingSpectra}, the continuous-time signal $x(t)$ has energy $E_c = \int_{-\infty}^{\infty} |X(f)|^2 df = 2 A^2 F_\tmax$. This energy signal was transformed in a power signal via sampling and the spectrum $X_s(f)$ of the sampled signal is periodic with period $\fs = 1/\ts$ and power 

With the help of \equl{signalReconstructionViaSincs2}, it is possible to sketch a proof for \equl{energyConservationContinuousDiscreteTime}, which is valid when the sampling theorem is obeyed. 

\equl{signalReconstructionViaSincs2} states that any band-limited signal $x(t)$ can be represented by its samples $x[n]$. The interest here is to relate their respective power values $\calP_c$ and $\calP_d$. Assuming $x(t)$ is an energy signal and from \equl{sincOrthogonality}, its energy $E_c$ can be written as
\begin{equation}
E_c = \int_{-\infty}^{\infty} |x(t)|^2 \textrm{d}t = \int_{-\infty}^{\infty} \left|\sum_{n=-\infty}^\infty x[n] \sinc(t/\ts-n) \right|^2 \textrm{d}t = \ts \sum_{n=-\infty}^\infty |x[n]|^2 = \ts E_d,
\label{eq:energy_discretetime}
\end{equation}
where $E_d$ is the energy of $x[n]$. 
A similar reasoning can be applied to power signals. Rewriting \equl{power_continuous_time_signals} with $\Delta t = N \ts$ leads to
\begin{align*}
\calP_c &= \lim_{N \rightarrow \infty} \left[ \frac{1}{(2N+1)\ts} \int_{-N \ts}^{N \ts}{\left|\sum_{n=-N}^N x[n] \sinc(t/\ts-n) \right|^2}\textrm{d}t \right] \\
 &= \lim_{N \rightarrow \infty} \left[ \frac{\ts}{(2N+1)\ts} \sum_{n=-N }^{N }{|x[n]|^2} \right] \\
 &= \calP_d. \numberthis
\label{eq:powerRelationProved}
\end{align*} %\numberthis 	is handy, or \nonumber

%\equl{powerRelationToBeProved} is obtained from \equl{powerRelationPsPd} and 

From \equl{powerRelationProved} and \equl{powerSampledSignal}, 
\blol{samplingAndReconstruction} can be simplified as 
\begin{equation}
\underset{(\calP)}{x(t)} \arrowedbox{sampling} \underset{(\calP/\ts)}{x_s(t)} \arrowedbox{S/D} \underset{(\calP)}{x[n]} \arrowedbox{D/S} \underset{(\calP/\ts)}{x_s(t)} \arrowedbox{h(t)} \underset{(\calP)}{\hat x(t)}  = x(t),
\label{eq:samplingAndReconstructionSimplified}
\end{equation}
where $\calP = \calP_c = \calP_d$.

\subsection{Sampling theorem uses a strict inequality}

%(Nyquist) at page \pageref{th:sampling}. 
%
Some authors state this theorem as $\fs \geq 2 F_\tmax$, but in this case $F_\tmax$ would have to be interpreted as the frequency for which $X(f)$ does not have a discrete frequency component $\delta(f-F_\tmax)$. The confusion often arises when textbooks pictorially represent $X(f)$ with a triangle shape as in \figl{samplingSpectra} and, in this case, $F_\tmax$ is the ``maximum'' but $X(F_\tmax)=0$ such that $\fs \geq 2 F_\tmax$ ``works''. However, as the exercise of \equl{samplingCosineExample} suggests, it is not guaranteed to reconstruct a cosine of frequency $f_c$ if one takes its samples at rate $\fs = 2 f_c$.

Another source of confusion with respect to $\fs \geq 2 F_\tmax$ or $\fs > 2 F_\tmax$ is that when processing a signal sampled at $\fs$ with an FFT, the maximum frequency is the so-called Nyquist frequency $\fs/2$ of \tabl{nyquist_frequency}. Taking that $F_\tmax = \fs / 2$, it seems reasonable to adopt $\fs \geq 2 F_\tmax$. Note however that the FFT bin corresponding to the Nyquist frequency is representing all signal components within its width $\Delta_f$ and that, unless $X(f)$ has a discrete frequency component $\delta(f-F_\tmax)$ to create ambiguity as exemplified in \equl{samplingCosineExample}, there is no major practical issue.

%AK-TODO: Patricio and Rosal code about undersampling
%Matlab code at
%C:\akoveryears\ak2007\Projects\Brasilsat\Softwares\ExtraMatlabFiles
%C:\svns\laps\latex\dissertation\11_patricio_filtFI
%C:\akoveryears\ak2008\Students\Rosal
%C:\akoveryears\ak2008\Projects\Brasilsat\Code

\subsection{Undersampling or passband sampling}
\label{sec:undersampling}

Most digital signal processing (DSP) systems are designed to combat aliasing but there are exceptions. In digital communications, it is common to use aliasing to lower the frequency of a signal $x(t)$ in an operation known as \emph{undersampling}\index{Undersampling} or \emph{passband sampling}\index{Passband sampling}. Among other conditions, $x(t)$ has to be a passband signal with spectrum centered at (a relatively high) frequency $f_c$, but with (a relatively small) bandwidth $\BW < f_c$, such that $\fs > 2~\BW$. In this case, even if $\fs < 2 f_{\textrm{max}}$, where the maximum frequency is $f_{\textrm{max}} = f_c + 0.5 \BW$, $x(t)$ can still be reconstructed from a replica of its spectrum that was shifted in frequency.
% . Undersampling will be further discussed later on.

For example, consider a passband signal $x(t)$ with spectrum $X(f)$ with $\BW=25$~Hz and center frequency $f_c=70$~Hz, as depicted in \figl{filt_fpga_sinal_anlg}. This signal has $f_{\textrm{max}} = f_c + 0.5 \BW = 82.5$~Hz and using the sampling theorem as applied to lowpass signals one would be compeled to use $\fs > 2 \times  82.5 = 165$~Hz. However, using $\fs = 56$~Hz, for example, one can still recover the signal.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{FiguresTex/filt_fpga_sinal_anlg}
\caption{Passband signal with $\BW=25$~Hz and center frequency $f_c=70$~Hz.\label{fig:filt_fpga_sinal_anlg}}
\end{figure}

According to the classic model for the sampling operation, it corresponds to the convolution of $X(f)$ with an impulse train $P(f)$ such that $X_d(f) = X(f) \conv P(f)$ and $P(f)$ has impulses separated by $\fs = 56$. Hence, the sampled signal has spectrum $X_d(f)$ as depicted in \figl{filt_fpga_sinal_amos}. 

Assuming $x(t)$ is real and $X(f)$ has Hermitian symmetry, any of the two ``replicas''
in \figl{filt_fpga_sinal_anlg} could be used to reconstruct $x(t)$. Similarly, 
any of the six (among the infinite) replicas that are shown in \figl{filt_fpga_sinal_amos} could be used to reconstruct $x(t)$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{FiguresTex/filt_fpga_sinal_amos}
\caption[{Result of sampling $X(f)$ in \figl{filt_fpga_sinal_anlg} with $\fs=56$~Hz.}]{Result of sampling $X(f)$ in \figl{filt_fpga_sinal_anlg} with $\fs=56$~Hz, which places replicas at $f=\pm 14, \pm 42, \pm, 70, \pm 98, \ldots$, with only the first six shown in this figure.\label{fig:filt_fpga_sinal_amos}}
\end{figure}

For example, placing an ideal lowpass filter with cutoff frequency $\fs/2$ would obtain a version of $x(t)$ corresponding to a frequency downconversion of its spectrum $X(f)$ from 70 to 14~Hz.

The theory about undersampling indicates the range of $\fs$ that can be used in each situation and can be found in DSP textbooks.\footnote{For example, in \cite{Lyons10}.}

\subsection{Sampling a complex-valued signal}

The sampling theorem (Theorem~\ref{th:sampling}) assumed a real-valued signal, which consequently,
allowed to assume the spectrum magnitude is even (symmetric). In the more general case
of a complex-valued signal, the same principle of having spectrum replicas that cannot overlap
is valid, but the ``maximum positive frequency'' is not enough to determine the minimum $\fs$.

\figl{samplingNonSymmetrical} suggests an example where a complex-valued signal has spectrum
$X(f)$ with support from $-300$ to 100~Hz. A careless interpretation of the sampling theorem
could lead to the erroneous conclusion that $\fs > 2 \times 100$~Hz suffices to avoid aliasing.
But in this case $\fs > 400$~Hz is required to avoid the overlap of spectrum replicas.
\figl{samplingNonSymmetrical} adopts $\fs = 450$~Hz.

\begin{figure}
\centering
\includegraphics[width=\figwidthLarge,keepaspectratio]{FiguresTex/samplingNonSymmetrical}
\caption{Sampling with $\fs=450$~Hz a complex-valued signal with non-symmetrical spectrum.\label{fig:samplingNonSymmetrical}}
\end{figure}

Stating the sampling theorem for complex-valued signals requires more elaborated definitions of bandwidth (this is discussed in Section~\ref{sec:bandwidthDefinitions}). But \figl{samplingNonSymmetrical} (and \figl{samplingSpectra}) indicate that an efficient strategy is 
to follow the basic principle of avoiding aliasing after convolving the original spectrum
with the train of impulses spaced by $\fs$.


\subsection{Signal reconstruction and D/S conversion revisited}
%\section{Signal reconstruction via convolution}
\label{sec:signal_reconstruction}

Similar to sampling, the D/S conversion can now be better understood.
After that, the important topic of signal reconstruction is discussed.

\subsubsection{D/S conversion revisited}
\label{sec:dcConversionRevisited}

When a discrete-time signal $x[n]$ with DTFT $X(e^{j \dw})$ is converted into a sampled 
signal $x_s(t)$ with Fourier transform $X_s(\aw)$ via a D/S conversion, as discussed
in Section~\ref{sec:ds_conversion}, it has a frequency-domain description given by
\begin{equation}
X_s(\aw)  = X(e^{j \dw})|_{\dw = \aw \ts} = X(e^{j \aw \ts}).
\label{eq:sampledSignalSpectrum}
\end{equation}
In other words, the value of $X_s(\aw_0)$ for a specific frequency $\aw_0$ rad/s is obtained
from $X(e^{j \dw_0})$ where $\dw_0 = \aw_0 \ts$ rad, as dictated by
 \equl{freqdiscrete2continuous}.

The notation is such that the subscript in $X_s(\aw)$  indicates the Fourier transform
of a ``sampled'' signal or, alternatively, $X(e^{j \aw \ts})$ can be used. In both
cases, the reader should have in mind that a sampled signal has a periodic spectrum.

\equl{sampledSignalSpectrum} corresponds to scaling the abscissa of $X(e^{j \dw})$, originally
specified in rad, to create $X(e^{j \aw \ts})$ with an abscissa $\aw = \dw \fs$ in rad/s.
\figl{dc_conversion} provides an example of the spectra involved in this D/S conversion.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/dc_conversion}
\caption{Result of converting $x[n]$ with spectrum $X(e^{j \dw})$ into $x_s(t)$ with $X_s(\aw)=X(e^{j \aw \ts})$ via a D/S conversion using $\fs=10$~Hz.\label{fig:dc_conversion}}
\end{figure}

The datatips in \figl{dc_conversion} highlight that the value $|X(e^{j \dw_0})|=18.86$ at $\dw_0=1.571$~rad was converted to $|X(\aw_0)|$ where $\aw_0=\dw_0 \fs = 15.71$~rad/s.

The replicas that occur in the spectrum of a sampled signal are located in \emph{Nyquist zones}\index{Nyquist zone}, which are intervals of $\fs/2$ when the frequency $f$ is specified in Hertz. For example,
the first Nyquist zone is $[0, \fs/2[$, the second is $[\fs/2, \fs[$ and so on. When the frequencies
$\aw$ and $\dw$ are specified in rad/s and rad, the bandwidths of the Nyquist zones are
$\pi \fs$ and $\pi$, respectively.

In summary, $X_s(\aw)$ inherits the periodicity of $X(e^{j \dw})$ in spite of the notation indicating it
only by the subscript $s$ of ``sampled''. Therefore, in some situations, it is convenient to denote the spectrum of 
$x_s(t)$ as $X(e^{j \aw \ts})$, which indicates that the independent variable $\aw$ is in rad/s and
makes explicit that this spectrum is periodic. An alternative 
to describing the sampled-signal spectrum $X(e^{j \aw \ts})$ in rad/s is to use $\aw=2\pi f$ and
represent $X(e^{j 2 \pi f \ts})$ in Hz.

The adoption of a filter to eliminate or attenuate the periodic replicas of 
a sampled signal $X(e^{j \aw \ts})$ is discussed in the sequel.

\subsubsection{Signal reconstruction}
Digital signal processing systems that interface with the analog world typically require two analog filters: the anti-aliasing and reconstruction, as indicated in \figl{canonical_interface}.

The reconstruction process, which converts a sampled signal {\xs} into a continuous-time signal $x(t)$, is mathematically modeled by convolving $x_s(t)$ with a signal $h(t)$ to obtain $x(t) = x_s(t) \conv h(t)$. As discussed along the text, the signal $h(t)$ can be interpreted as the impulse response of a system and the reconstruction process can be pictorially depicted as:
\begin{equation}
x_s(t) \rightarrow\fbox{h(t)}\rightarrow x(t).
\label{eq:signalReconstruction}
\end{equation}

When one starts with the discrete-time signal $x[n]$, the D/A process has two stages: D/S conversion, that transforms the discrete-time $x[n]$ into a continuous sampled signal $x_s(t)$ and then processing (or ``filtering'') with $h(t)$ to obtain $x(t)$.

There are two important options for $h(t)$:

\begin{itemize}
        \item \emph{Zero-order hold} reconstruction: where $h(t) = u(t) - u(t-\ts)$ is a pulse with duration $\ts$ and amplitude 1
        \item \emph{Unitary energy} reconstruction: where $h(t)$ has unitary energy $E=1$ as, for example, the normalized pulse $h(t) = \frac{1}{\sqrt{\ts}} [u(t) - u(t-\ts)]$
\end{itemize}

As illustrated in \figl{zoh}, D/S followed by zero-order hold reconstruction is a simplified model for the actual process executed by a DAC chip. It is adopted here for simplicity. A consequence of ZOH
is that \equl{energyConservationContinuousDiscreteTime} holds, and the power in continuous
$\calP_c$ of $x(t)$ and discrete-time $\calP_d$ of $x[n]$ are the same.

In practice, the reconstruction process heavily depends on the respective analog filter and the DAC sampling frequency $\fs$. It is often necessary to use $\fs$ higher than the one suggested by the sampling theorem, to simplify the job that must be done by the reconstruction filter $H(f) = \calF \{h(t)\}$.

For a signal with approximate bandwidth BW, it is tempting to use $\fs = 2 \BW$, which 
is denoted as \emph{Nyquist sampling}\index{Nyquist sampling}. In this case the signal
is called \emph{critically-sampled}\index{Critically-sampled signal} and the reconstruction should be done by an ideal lowpass filter with passband from 0 to $\fs/2$. Even when 
$\fs > 2 \BW$, the transition band of the reconstruction filter should be small enough to significantly attenuate the neighboring spectrum replicas of the sampled signal at the output of the D/S conversion.

Recall that the spectrum replicas of $X(e^{j \aw \ts})$
are located at multiples of $2\pi \fs$~rad/s (see, e.\,g., \figl{dc_conversion}), which correspond to replicas at multiples of $\fs$ when considering $X(e^{j 2 \pi f \ts})$ with the frequency $f$ in Hz.
% (as suggested by \equl{freqdiscrete2continuous}).


\figl{canonicalFilterWithDC_CD} presents an extended version of \figl{canonicalDSPChain} 
	that incorporates the filters $A(s)$ and $R(s)$ (these two analog filters are
	also indicated in \figl{canonical_interface}).
While \figl{canonicalDSPChain}  assumed ZOH reconstruction, \figl{canonicalFilterWithDC_CD} 
illustrates the general case of an arbitrary reconstruction filter $h(t)$.

\begin{figure}
	\centering
		\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/canonicalFilterWithDC_CD}
	\caption{Extended version of \figl{canonicalDSPChain} using an arbitrary reconstruction filter $h(t)$
	and incorporating the filters $A(s)$ and $R(s)$.\label{fig:canonicalFilterWithDC_CD}}
\end{figure}

Note that there are two reconstruction filters in \figl{canonicalDSPChain}, represented
by $h(t)$ and $R(s)$. Their effect could be combined in only one filter with impulse
response $h_r(t)=h(t) \conv r(t)$, where $r(t)$ is the inverse Laplace transform of $R(s)$, but
it is often pedagogical to distinguish them as follows.

The internal filter $h(t)$ represents the filtering process that
occurs within a DAC chip. Modern DACs can already incorporate a sophisticated $h(t)$, but
is is assumed here that this is not the case, and $h(t)$ may implement ZOH or a filter
with small order. The ``external'' reconstruction filter $R(s)$ complements $h(t)$ and
provides improved rejection of the undesired spectrum replicas of $y_s(t)$ that may still
be present in $y(t)$. In summary, the main role of $h(t)$ is the conversion of the
sampled signal $y_s(t)$ into an analog signal $y(t)$, while $R(s)$ aims at achieving
the specified level of performance with respect to filtering out the replicas in $y_s(t)$.
The following example illustrates how signal reconstruction can be challenging in practice.

\bExample \textbf{Examples of signal reconstruction}.
\figl{dacFilter25khz} is the result of an example\footnote{\figl{dacFilter25khz}  and \figl{dacFilter80khz} were generated with \ci{figs\_systems\_showDACFilterEffect.m}, which uses both a low and high sampling frequencies to mimic the reconstruction via an analog filter.} where a random signal $y[n]$ with $\BW=25$~kHz and $\fs=200$~kHz, is converted to an analog signal $y(t)$. The reconstruction is performed by a DAC followed by a 5-th order analog filter $H(f)$, with cutoff frequency $f_c = \BW$.
This analog filter combines the effects of $h(t)$ and $R(s)$ in \figl{canonicalFilterWithDC_CD}.

 The top plot in \figl{dacFilter25khz} shows the magnitude of the DTFT $Y(e^{2 \pi \ts f})$ of $y_s(t)=\textrm{D/S}\{y[n]\}$,  superimposed to the frequency response $|H(f)|$ of the reconstruction filter. The multiples of $\fs$ are identified in the grid of dashed lines. The bottom plot shows the magnitude of
the resulting Fourier transform $Y(f)=\calF \{ y(t) \}$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{Figures/dacFilter25khz}
\caption{Reconstruction of a digital signal with $\BW=25$~kHz and $\fs=200$~kHz using an analog filter with cutoff frequency $f_c = 25$~kHz.\label{fig:dacFilter25khz}}
\end{figure}

\figl{dacFilter80khz} was obtained under the same conditions used for \figl{dacFilter25khz} but the signal bandwidth increased from 25 to 80~kHz. In this case, the filter did not significantly attenuate the two spectrum replicas in $Y(e^{2 \pi \ts f})$ that are neighbors of the one centered at $f=0$.  Thinking of an asymptotic Bode-diagram, one can expect the
fifth-order $|H(f)|$ to drop at 6~dB/octave per pole and, from 80 to 160~kHz, reach $5 \times (-6)=-30$~dB. The replica centered at 200~kHz, for example, has its band starting at 120~kHz, and the reconstruction filter has an attenuation of only (approximately) 20~dB at 120~kHz. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth]{Figures/dacFilter80khz}
\caption[{Same as \figl{dacFilter25khz}, but for a signal with $\BW=f_c=80$~kHz.}]{Same as \figl{dacFilter25khz}, but for a signal with $\BW=f_c=80$~kHz. In this case, the reconstruction generates significant out-of-band power due to the spectrum replicas.\label{fig:dacFilter80khz}}
\end{figure}

As illustrated by \figl{dacFilter80khz}, in practice, it is typically adopted a value for $\fs$ large enough due to the non-ideal reconstruction filter. In the example corresponding to \figl{dacFilter25khz}, in which the reconstruction seems adequate, the Nyquist frequency $\fs/2$ is four times the signal BW.
\eExample 

\subsubsection{Combined digital filtering and D/S steps}

When a digital filter $H_z(z)$ (subindices will be used here to avoid confusion and disambiguate $H(z)$ and $H(s)$) is part of an analog signal processing scheme such as the one depicted in
 \figl{canonicalFilterWithDC_CD}, it is sometimes useful to combine the roles of $H_z(z)$ and D/S into one
``analog'' filter with frequency response $H_z(e^{j \aw \ts})$. This can be interpreted as
a simple abscissa scaling in graphs such as \figl{dc_conversion}. Another view is that,
while \equl{sampledSignalSpectrum} refers to a sampled signal, a corresponding version
of an equivalent (analog) ``system'' is
\begin{equation}
H_s(\aw)  = H_z(e^{j \dw})|_{\dw = \aw \ts} = H_z(e^{j \aw \ts}).
\label{eq:digitalFilterAndDC}
\end{equation}

%\section{Analog Filters}
\section{{\akadvanced} First and Second-Order Analog Systems}

%AK-PUTBACK
%\section{Analog Filter Design}
%\subsection{Butterworth filters}
%\subsection{Frequency scaling}
%\subsection{Impedance matching networks}

Because they are basic building blocks for higher-order filters, first and second-order continuous-time (also called analog) systems are discussed in the sequel.

\subsection{First-order systems}
If the coefficients are real, a first-order system $H(s)=1/(s+a), a \in \Re$, (as the one whose frequency response is depicted in \figl{freq_response_onepole}) is restricted to have a lowpass frequency response. This $H(s)$ has a pole at $s=-a$ and a zero at $s=\infty$. Using $H(s)=a/(s+a)$ leads to a unitary gain at DC.
Placing a zero at $s=0$ leads to $H(s)=s/(s+a)$, which has a highpass response. 

A system $H(s)=1/(s+a)$ with ROC given by $\sigma > -a$ has an inverse Laplace transform $h(t) = e^{-a t} u(t)$, which can be written as $h(t) = e^{-t/\tau} u(t)$ where $\tau$ is the well-known \emph{time constant}\index{Time constant} \akurl{http://web.mit.edu/2.151/www/Handouts/FirstSecondOrder.pdf}{3mit}.
After an interval of one time constant, the impulse response has decayed to 36.8\% of its initial value.

If the ROC of $H(s)=a/(s+a)$ includes $s=j \aw$, the frequency response in this case is $H(\aw)=a/(j\aw+a)$.
As illustrated by \equl{cutoffOfPole}, the magnitude $|H(\aw)|=a/\sqrt{a^2+\aw^2}$ at $\aw=a$ is then $|H(a)|=1/\sqrt{2}$, which is the cutoff frequency $\aw_c=a$ because it corresponds to a decrease of 3~dB in power.

\subsection{Second-order systems}
\label{sec:secondOrderSystems}
The system function $H(s)$ of a second-order system (SOS), also called a \emph{two-pole resonator}, has a denominator that can be written as $s^2 + 2 \alpha s + \aw_n^2$. When there are two zeros at $\infty$
and the gain at DC is unitary ($H(s)|_{s=0}=1$), the SOS is:
\begin{equation}
H(s) = \frac{\aw_n^2}{s^2 + 2 \alpha s + \aw_n^2},
\label{eq:resonator}
\end{equation}
where $\aw_n$ is the \emph{natural frequency}\index{Natural frequency} and $\alpha$ the \emph{decay rate}\index{Decay rate} parameter, which is useful for defining the \emph{damping ratio} $\zeta = \alpha / \aw_n$. Hence, \equl{resonator} can be rewritten as:
\begin{equation}
H(s) = \frac{\aw_n^2}{s^2 + 2 \zeta \aw_n s + \aw_n^2}.
\label{eq:secondOrder1}
\end{equation}

The values of $\alpha$, $\zeta$ and $\aw_n$ can be related to several characteristics of a SOS. For example, $\alpha$ represents the rate of exponential decay of oscillations when the system input is a unit step $u(t)$. 
%Besides, they help to categorize different classes of second-order systems. 
And depending on  $\zeta$, three categories of second-order systems are defined:
\begin{itemize}
	\item $\zeta = 1$: \emph{critically damped} SOS, with a double real pole at $- \aw_n$;
	\item $\zeta > 1$: \emph{overdamped} SOS, with real poles at $- \zeta \aw_n \pm \aw_0$;
	\item $0 \le \zeta < 1$: \emph{underdamped} SOS, with complex conjugate poles.
\end{itemize}
Note that these three categories depend only on the denominator of $H(s)$.

The numerator of $H(s)$ represents another degree of freedom. Changing this numerator, leads to distinct SOS.
%The numerator of \equl{resonator} was chosen such that the gain at DC is unitary ($H(s)|_{s=0}=1$). 
An alternative to \equl{resonator} (or, equivalently, \equl{secondOrder1}) is
\begin{equation}
H(s) = \frac{s}{s^2 + 2 \zeta \aw_n s + \aw_n^2},
\label{eq:secondOrder2}
\end{equation}
which has a zero at the origin ($H(s)|_{s=0}=0$). Yet, other 
two options of SOS can be defined as
\begin{equation}
H(s) = \frac{s^2}{s^2 + 2 \zeta \aw_n s + \aw_n^2}
\label{eq:secondOrder3}
\end{equation}
and
\begin{equation}
H(s) = \frac{2 \zeta \aw_n s + \aw_n^2}{s^2 + 2 \zeta \aw_n s + \aw_n^2}.
\label{eq:secondOrder4}
\end{equation}
\tabl{sosNumTypes} summarizes these options for the numerator of a SOS.

\begin{table}
\centering
\caption{Some distinct options for the numerator of a SOS.\label{tab:sosNumTypes}}
\begin{tabular}{|l|c|c|}
\hline
Characteristic & Numerator & Reference \\ \hline
Two-zeros at $\infty$ and unitary DC gain & $\aw_n^2$ & \equl{secondOrder1} \\ \hline
One zero  at DC & $s$ & \equl{secondOrder2} \\ \hline
Two zeros at DC & $s^2$ & \equl{secondOrder3} \\ \hline
One finite zero at $-\aw_n/(2 \zeta)$ & $2 \zeta \aw_n s + \aw_n^2$ & \equl{secondOrder4} \\ \hline
\end{tabular}
\end{table}

Using the quadratic formula to find the roots of a SOS, the poles are $s_0=-\alpha \pm \sqrt{\alpha^2 - \aw_n^2}$, which can be rewritten as $s_0=-\alpha \pm j \sqrt{\aw_n^2 - \alpha^2}$. 
\figl{natural_cutoff_frequency} summarizes these relations.
Note that $\aw_n = |s_0|$ and, unless $\alpha = 0$ (poles on the $j \aw$ axis), the natural frequency $\aw_n$ is different from the center frequency of the pole $\aw_0 = \imag{p} = \sqrt{\aw_n^2 - \alpha^2} = \aw_n \sqrt{1 - \zeta^2}$. 

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./FiguresTex/natural_cutoff_frequency}%
\caption{Relations between natural frequency $\aw_n$, pole center frequency $\aw_0$,  
decay rate $\alpha$ and damping ratio $\zeta$ for a pair of complex conjugate poles.\label{fig:natural_cutoff_frequency}}
\end{figure}

Note that for a first-order system $H(s)=a/(s+a)$ with real-valued coefficient $a$, the pole is real, its center frequency is $\aw_0 = 0$, and the natural and cutoff frequencies $\aw_n = \aw_c = a$ coincide. For a second-order system, the expression for the cutoff frequency $\aw_c$ is more elaborated. Considering \equl{resonator}, the cutoff is
\begin{equation}
\aw_c = \aw_n \sqrt{ 1 - 2 \zeta^2 + \sqrt{4\zeta^4 - 4\zeta^2 + 2}},
%\label{eq:}
\end{equation}
which can be found by using $|H(s)|_{s=j \aw_c}=1/\sqrt{2}$.
In this case, $\aw_c = \aw_n$ when $\zeta=1/\sqrt{2}$.

\bExample \textbf{On the damping ratio of a SOS}.
\figl{secondOrderSys} illustrates the influence of the damping ratio using the values
$\zeta = 0.5, 0.707, 1$ and 2.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/secondOrderSys}
\caption{Magnitude of the frequency response for the SOS expressed by \equl{secondOrder1} and \equl{secondOrder4}.\label{fig:secondOrderSys}}
\end{figure}

\figl{secondOrderSys} also compares \equl{secondOrder1} and \equl{secondOrder4}, which are two of the options contrasted in \tabl{sosNumTypes}.
\eExample

\figl{sosParameters} illustrates some key time-domain performance parameters for an
underdamped system obtained when the input $x(t)=u(t)$ is a step function. The \emph{rise time} $t_r$ is the interval for the step response to rise from 10 to 90\% of its final value.
The \emph{settling time} $t_s$ is the interval to have the output within a given range, which is 5\% in \figl{sosParameters}. The \emph{overshoot} is the peak amplitude and occurs
at the \emph{peak time} $t_p$.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./FiguresTex/sosParameters}
\caption{Time-domain performance parameters for an
underdamped system based on its step response.\label{fig:sosParameters}}
\end{figure}

\tabl{sosParameters} summarizes some parameters for the system $H(s)$ described by
\equl{secondOrder1}, which can be found\footnote{These results can be found in
textbooks of control systems or, e.\,g., \akurl{http://www.ece.rutgers.edu/~gajic/psfiles/chap6.pdf}{3con}.} from the inverse Laplace transform of $Y(s) =  H(s)/s$ given that $X(s)=1/s$ is the transform of $u(t)$. The tolerance $\epsilon$ is used to obtain $t_s$ and a typical
value is $\epsilon = 0.05$.

\begin{table}
\centering
\caption{Parameters of a second-order system as described by \equl{secondOrder1}.\label{tab:sosParameters}}
\begin{tabular}{|l|c|c|}
\hline
Performance parameter & Expression & For $\zeta=\sqrt{2}/2 \approx 0.707$ \\ \hline
Peak time  & $t_p = \pi / \aw_0 = \pi / (\aw_n \sqrt{1 - \zeta^2})$ & $t_p =4.4/\aw_n$ \\ \hline
Rise time & $t_r \approx (2.23 \zeta^2 - 0.078 \zeta + 1.12)/\aw_0$ & $t_r =3/\aw_n$ \\  \hline
Settling time & $t_s = - \ln(\epsilon \sqrt{1-\zeta^2})/(\zeta \aw_n)$ & $t_s =4.7 / \aw_n $ \\ \hline
Overshoot &  $\textrm{ov}=e^{- (\zeta \pi)/\sqrt{1 - \zeta^2}}$ & ov=4.32\% \\ \hline
3-dB bandwidth $\aw_c$ (rad/s) & $\aw_c = \aw_n \sqrt{1-2 \zeta^2 + \sqrt{2 - 4\zeta^2 + 4 \zeta^4}}$ & $\aw_c=\aw_n$ \\ \hline
\end{tabular}
\end{table}

\tabl{sosParameters} indicates that, for an \emph{underdamped} system, the rise time when
$\zeta=\sqrt{2}/2$ is $t_r=3/\aw_n$. As expected, all three time parameters are inversely
proportional to the natural frequency $\aw_n$.

\codl{snip_systems_sos_parameters} indicates the commands to calculate the parameters,
emphasizing the factors that depend on $\zeta$ (\ci{zeta}) only.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_sos\_parameters}{snip_systems_sos_parameters} 

Matlab has the \ci{stepinfo} function, which can be used to obtain most of the
parameters in \tabl{sosParameters} as follows:
\begin{lstlisting}[language=Matlab]
zeta=0.5 %damping ratio, e.g. sqrt(2)/2 = 0.707;
wn=2 %natural frequency in rad/s
sys = tf([wn^2],[1 2*zeta*wn wn^2]); %define the transfer function
S=stepinfo(sys,'RiseTimeLimits',[0.1,0.9], ...
    'SettlingTimeThreshold',0.05) %get parameters with stepinfo
step(sys) %plot step response to check results if want
\end{lstlisting}


%\begin{table}
%\centering
%\caption{Parameters of a second-order system as described by \equl{secondOrder4} (derivados pelo Carlos).\label{tab:sosParameters4Carlos}}
%\begin{tabular}{|l|c|c|}
%\hline
%Performance parameter & Expression & For $\zeta=\sqrt{2}/2 \approx 0.707$ \\ \hline
%Peak time  & $t_p = \dfrac{\big(\tan^{-1} \bigg(\frac{-2 \zeta \sqrt{1-\zeta^2}}{-2\zeta^2 + 1}\bigg) + \pi\big)}{\omega_d}$  & $AK-TODO$\\ \hline
%Rise time & $AK-TODO$ & $AK-TODO$ \\  \hline
%Settling time & $t_s = - \ln\dfrac{(\epsilon \sqrt{1-\zeta^2})}{\zeta \omega_n+ 2 \omega_n \zeta^2}$ & $AK-TODO$ \\ \hline
%Overshoot &  $\textrm{ov}=e^{- \zeta \omega_n t_p}$ & $AK-TODO$\\ \hline
%Bandwidth (in rad/s) & $AK-TODO$ & $AK-TODO$ \\ \hline
%\end{tabular}
%\end{table}


%\begin{table}
%\centering
%\caption{Parameters of a second-order system as described by \equl{secondOrder4}.\label{tab:sosParameters4}}
%\begin{tabular}{|l|c|c|}
%\hline
%Performance parameter & Expression & For $\zeta=\sqrt{2}/2 \approx 0.707$ \\ \hline
%Peak time  & AK-TODO & $t_p =/\aw_n$ \\ \hline
%Rise time & AK-TODO & $t_r =/\aw_n$ \\  \hline
%Settling time & AK-TODO & $t_s =  / \aw_n $ \\ \hline
%Overshoot &  AK-TODO & ov=\% \\ \hline
%Bandwidth (in rad/s) & AK-TODO & $\BW_r=\aw_n$ \\ \hline
%\end{tabular}
%\end{table}


The system bandwidth will be discussed in the next section.
\tabl{sosParameters} informs that, when $\zeta=\sqrt{2}/2$, the 3-dB bandwidth $\aw_c$ in radians per second of
the SOS given by \equl{secondOrder1} is simply its 
natural frequency $\aw_n$. In fact, as $\zeta$ varies from 0.5 to 0.8, which are
the values typically used,
its $\aw_c$ varies from $1.27 \aw_n$ to $0.87 \aw_n$. This justifies using
$\aw_n$ as a rough approximation of bandwidth for \equl{secondOrder1}.

\section{{\akadvanced} Bandwidth and Quality Factor}

\subsection{Bandwidth and Quality Factor of Poles}
\label{sec:bw_qfactor_poles}
% from https://ccrma.stanford.edu/~jos/fp/Relating_Pole_Radius_Bandwidth.html

The overall behavior of a filter is dictated by the combined effect of its poles and zeros.
The position of a pole in the $s$ or $Z$ plane determines how it influences the overall system function ($H(s)$ or $H(z)$, respectively). 

It can be shown that asymptotically each pole contributes with a roll-off of 20~dB per decade (equivalent to 6 dB per octave) and a zero with an increase of 20~dB per decade.  This asymptotic behavior is used to obtain graphs known as \emph{Bode diagrams}\index{Bode diagram}. This clearly indicates that 
the sharpness of a frequency response (shorter transition regions) can be improved by increasing the order of the corresponding system function $H(s)$ (or $H(z)$), i.\,e., adding poles and/or zeros. 
However, increasing the order of an analog filter requires more components (capacitors, opamps, etc.) and for a digital filter it requires more multiplications and additions. 
Hence, it is important to make the best use of poles (and zeros) and, motivated by that, to have figures of merit for them. 

Besides its frequency, the bandwidth and quality factor of a single pole or zero are important to assess its influence.
Poles will be emphasized in this section, but similar definitions are applied to zeros.
This section discusses characteristics of a single pole while Section~\ref{sec:bw_qfactor_filters} extends the definitions to filters.

\subsubsection{Bandwidth of a pole}

The bandwidth of a pole is illustrated in \figl{bw_threedb} and defined here as $\BW = f_2 - f_1$, where the cutoff frequencies $f_1$ and $f_2$ are those in which the gain $|H(f_0)|$ at the center frequency $f_0$ (that may be different than the natural frequency $f_n$) has decreased to $|H(f_0)|/\sqrt{2}$. The factor $1/\sqrt{2}$ corresponds to $20 \log_{10}(1/\sqrt{2}) - 3.0103$~dB, or approximately $-3$~dB. Therefore, this definition is called the \emph{3-dB bandwidth} or \emph{half-power} BW.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/bw_threedb}
\caption{Bandwidth $\BW = f_2 - f_1$ defined by the cutoff frequencies where the gain falls $-3$~dB below the reference value at $f_0$.\label{fig:bw_threedb}}
\end{figure}

\subsubsection{Pole bandwidth in continuous-time}
Assuming a first-order system $H(s) = - \alpha / (s - p)$, where $p = \alpha + j \aw_0$ is the pole, the bandwidth of $p$ in Hz is given by
\begin{equation}
\BW = \frac{ |\alpha| }{\pi}.
\label{eq:bw_related_to_radius}
\end{equation}
This result can be obtained by noting that for any frequency $\aw$
\[
|H(\aw)| = \frac{|\alpha|}{ \sqrt{(\aw - \aw_0)^2 + \alpha^2} }.
\]
Hence, the squared gain at the specific pole frequency $\aw = \aw_0$ is
\[
|H(\aw_0)|^2 = \frac{\alpha^2}{ (\aw_0 - \aw_0)^2 + \alpha^2 } = 1
\]
and the squared gains at cutoff frequencies $\aw = \aw_0 \pm \alpha$ fall to $1/2$ as required by the definition of cutoff frequency:
\begin{equation}
|H(\aw_0 \pm \alpha)|^2 = \frac{\alpha^2}{ (\aw_0 \pm \alpha - \aw_0)^2 + \alpha^2 } = \frac{1}{2}.
\label{eq:cutoffOfPole}
\end{equation}

Hence, a range from $\aw_0 - \alpha$ to $\aw_0 + \alpha$ is determined by the two cutoff frequencies of a pole at $\aw_0$ such that its bandwidth $\BW$ is $2 |\alpha|$ rad/s. Dividing by $2\pi$ leads to $\BW = 2 |\alpha| / (2 \pi)$ in Hz, as indicated in \equl{bw_related_to_radius}.
%points where $|H(\aw_0 \pm \alpha)| = \frac{1}{\sqrt{2}}$, 
A similar relation holds in discrete-time, as follows.

\subsubsection{Pole bandwidth in discrete-time}
Using the previous results and assuming the transformation $z = e^{s \ts}$ of \equl{relating_s_z}, the pole $p = \alpha + j \aw_0$ in the plane $s$ is mapped into the pole $z=r e^{j \dw_0}$ in the plane $Z$ via
\[
r e^{j \dw_0} = e^{p \ts} = e^{(\alpha + j \aw_0) \ts} = e^{\alpha \ts} e^{j \aw_0 \ts}.
\]
Hence, $r = e^{\alpha \ts}$ and $\dw_0 = \aw_0 \ts$. Assuming $\alpha < 0$ for a causal and stable $H(s)$, \equl{bw_related_to_radius} indicates that $\alpha = - \BW \pi$, such that $r = e^{\alpha \ts} = e^{- \BW \pi \ts}$ and
\begin{equation}
\label{eq:pole_bw_discretetime}
\BW = - \frac{ \ln (r)}{ \pi \ts}
\end{equation}
in Hz for a pole $r e^{j \dw_0}$ in the $Z$ plane with $r < 1$.

One can rewrite \equl{pole_bw_discretetime} as $\ln(r) = - \BW \pi \ts$ and
use the Taylor series expansion of $\ln(\cdot)$ around a value $x=a$
\[
\ln(x) = \ln(a) + \frac{1}{a} (x-a) - \frac{1}{2a^2} (x-a)^2 + \frac{1}{3a^3} (x-a)^3 - \ldots,
\]
with $a=1$, to keep only the first two terms and achieve
\[
\ln(r) \approx \ln(1) + \frac{1}{1} (r-1) = r -1.
\]
Hence, the approximation $r-1 \approx - \BW \pi \ts$ allows to write
\begin{equation}
\BW \approx \frac{ (1-r) \fs}{ \pi },
\label{eq:approximation_pole_bw_discretetime}
\end{equation}
which can be used when $r$ is close to 1.

\ignore{
\begin{lstlisting}
p1=-300+j*4000; p2=-300-j*4000; %pole and its conjugate
A=poly([p1 p2]); %convert it to a polynomial A(s)
Q=sqrt(A(3)) / A(2) %find the Q-factor for the two poles
Q=abs(p1) / (2*(-real(p1))) %alternative calculation of Q
Q=4000 / (2*(abs(real(p1)))) %alternative calculation of Q
Q=abs(p1)/ (2*(abs(real(p1)))) %alternative calculation of Q
\end{lstlisting}
}
%See https://ccrma.stanford.edu/~jos/fp/Two_Pole_Partial_Fraction_Expansion.html#fig:tppfe

\subsubsection{Quality factor of poles}

Besides the number of poles and zeros, another factor that influences 
the sharpness of a frequency response 
is the \emph{quality factor}\index{Quality factor} (or $Q$-factor) of each 
%complex conjugate pair of 
pole, which is defined as
\begin{equation}
Q \defeq \frac{f_n}{ \BW},
\label{eq:qFactorPoleDefinition}
\end{equation}
where $f_n = \aw_n / (2 \pi)$ is the natural frequency in Hz and $\BW=f_2 - f_1$ is the 3-dB bandwidth (also in Hz), as indicated in \figl{bw_threedb}.

For a second-order resonator, such as \equl{resonator}, the quality factor can be shown \akurl{http://www.dsprelated.com/dspbooks/filters/Relating_Pole_Radius_Bandwidth.html}{3dsp} to be
\begin{equation}
Q = \frac{\aw_n}{2 \alpha}.
\label{eq:q_factor_second_order}
\end{equation}

Appplication~\ref{app:poleQFactor} presents a discussion on the influence of the Q-factor in frequency responses.

\subsection{Bandwidth and Quality Factor of Filters}
\label{sec:bw_qfactor_filters}

The previous definitions of bandwidth and Q-factor of poles can be extended to systems, especially frequency-selective filters such as bandpass and lowpass.

\subsubsection{Bandwidth definitions for signals and systems}
\label{sec:bandwidthDefinitions}
For a generic system there are definitions for the bandwidth ($\BW$)\index{Bandwidth definitions} other than the 3-dB bandwidth of \figl{bw_threedb}. In all cases, if the frequency response has Hermitian symmetry, only the positive frequencies are taken in account.

A generalization of BW is to adopt the $X$-\emph{dB bandwidth} where $X$ is the attenuation of interest, such as 1~dB.

Alternatively, the \emph{absolute bandwidth} can be applied when the signal is bandlimited. It is simply defined as the frequency range in which the spectrum is non-zero. For example, $H(f)=\textrm{rect}(f)$ has $\BW=0.5$. Note that in this case the 3-dB BW definition would not be appropriate.

Another alternative definition of $\BW$ is the so-called \emph{null-to-null} or \emph{zero-crossing} BW. In the case of a lowpass spectrum, the BW is determined by the first null of $|H(\aw)|$ (or $|H(\dw)|$ in discrete-time). For example, a frequency response given by the $H(f)=\sinc(fT)$ of \equl{sinc_transform} has $\BW = 1/T$ according to this definition.
For a passband spectrum, the two neighbor nulls of the center frequency determine BW.
For example, the BW of $H(f)=\sinc(fT-5T)$ is $\BW = 2/T$.

When a signal is complex-valued, its spectrum $X(f)$ does not have to exhibit Hermitian symmetry.
In this case, the sampling theorem and other results depend on the \emph{double-sided}\index{Double-sided bandwidth} or \emph{bilateral}\index{Bilateral bandwidth} bandwidth, which takes in account the support of $X(f)$ from negative to positive frequencies. 

For example, first consider the ``conventional'' case of $X(f)$ corresponding to a real-valued ideal lowpass filter with unitary gain from $-200$ to 200~Hz: its BW is 200~Hz, and its double-sided bandwidth is 400~Hz. Contrast now with a complex-valued signal with $X(f)$ being a square pulse from $-300$ to 100~Hz as depicted in \figl{samplingNonSymmetrical}. In the latter case, the double-sided bandwidth continues to be 400~Hz but the conventional BW fails to indicate, for example, the minimum $\fs$ according to the sampling theorem. In this case, the sampling theorem could be stated with
respect to the double-sided bandwidth.

Another approach when dealing  with complex-valued signals is to assume that $X(f)$ is zero for negative frequencies (such
signals are called
\ifml
``analytic''\index{Analytic signal}).
\else
``analytic''\index{Analytic signal}, as discussed in Section~\ref{qamDemodPhaseSplitter}). 
\fi
In this case, the bandwidth BW takes in account only positive
frequencies, as usual, but it may be misleading the fact that the sampling theorem 
in this case of an analytic signal can be stated as
\begin{equation}
\fs > \BW.
\label{eq:samplingTheoremComplexSignals}
\end{equation}
For example, if $X(f)$ is non-zero from DC to 300~Hz, sampling the corresponding complex-valued
(analytic) time-domain signal $x(t)$ at $\fs = 350$~Hz suffices to avoid aliasing. In this case,
the first spectrum replica at positive frequencies has support from 0 to 300~Hz, the second one
from 350 to 650~Hz, and so on, indicating that there is no aliasing.

A bandwidth definition very useful when dealing with filtered white noise is the \emph{equivalent noise bandwidth} (ENBW). The ENBW of a filter $H(f)$ is the bandwidth of a fictitious ideal (e.\,g., lowpass or bandpass) filter $H_e(f)$ with rectangular spectrum that obeys
\begin{equation}
\int_{-\infty}^{\infty} |H(f)|^2 \textrm{d}f = \int_{-\infty}^{\infty} |H_e(f)|^2 \textrm{d}f
\label{eq:enbwProperty}
\end{equation}
and has the maximum value $H_{\textrm{max}}$ of $|H_e(f)|$ is equal to the maximum of $|H(f)|$.
The name ENBW is because both filters lead to the same output power when the input is white noise (discussed in details later, in Section~\ref{sec:ctWhitePSD}
) such that the equivalent filter can substitute the original (and more complicated one) while leading, for instance, to the same SNR in a simulation or theoretical development.

An example is provided in \figl{digi_comm_enbw}, which was obtained with Matlab's function\footnote{The function \ci{enbw} was designed for lowpass (windows) spectra with the maximum value at DC. It does not work for passband spectra.} \ci{enbw} as informed in \codl{figs_digicomm_enbw}. In this case, the ENBW was $102.4/50 \approx 2$ times larger than the cutoff frequency. %Increasing the filter order makes its frequency response closer to a rectangular shape and decreases this factor.

\lstinputlisting[caption={Code/MatlabBookFigures/figs\_digicomm\_enbw},label=code:figs_digicomm_enbw,linerange={1-8},firstnumber=1]{./Code/MatlabBookFigures/figs_digicomm_enbw.m}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/digi_comm_enbw}
\caption{DTFT magnitude in dB of a 4-th order Butterworth filter with cutoff frequency of 50~Hz and the equivalent ideal filter with absolute bandwidth of 102.4 Hz.\label{fig:digi_comm_enbw}}
\end{figure}

Observing \figl{digi_comm_enbw}, due to the rectangular shape of 
$|H_e(f)|$, 
\[
\int_{-\infty}^{\infty} |H_e(f)|^2 \textrm{d}f = 2 \textrm{~ENBW} \times H_{\textrm{max}}^2.
\]
Hence, using \equl{enbwProperty} this can be written as:
\begin{equation}
\textrm{ENBW} = \frac{\int_{-\infty}^{\infty} |H(f)|^2 \textrm{d}f}{2H_{\textrm{max}}^2},
\label{eq:enbw}
\end{equation}
which for real signals simplify to
\begin{equation}
\textrm{ENBW} = \frac{\int_{0}^{\infty} |H(f)|^2 \textrm{d}f}{H_{\textrm{max}}^2}.
%\label{eq:}
\end{equation}

\subsubsection{Quality factor for filters}
\label{sec:qForFilters}

The Q-factor of a pole, as defined in \equl{qFactorPoleDefinition}, can be easily adapted to a system with a frequency response that allows obtaining its 3-dB bandwidth $\BW$ (see \figl{bw_threedb}) such as a bandpass filter.\footnote{Actually, the Q factor is used in several areas of engineering and physics.} In this case, the pole natural frequency $f_n$ in \equl{qFactorPoleDefinition} is substituted by the filter's center frequency $f_c$ such that
\begin{equation}
Q \defeq \frac{f_c}{ \BW}.
\label{eq:qFactorFilterDefinition}
\end{equation}
The inverse of $Q$, i.\,e.,  $\BW / f_c$ is called \emph{fractional bandwidth}\index{Fractional bandwidth} and often specified in percentage.

\section{Importance of Linear Phase (or Constant Group Delay)}

In some applications such as analog signal transmission and audio amplification, the system 
should ideally have an output $y(t)$ identical to its input $x(t)$. Given that obtaining 
$y(t)=x(t)$ is often unfeasible due to propagation delays inherent of communication channels and
electronic systems, a more realistic target is to obtain $y(t)=x(t-t_0)$, a delayed version of
the input. From the time-shift Fourier property (see Appendix~\ref{sec:fourierProperties}), the
delay by $t_0$ corresponds to a linear phase $e^{-j 2 \pi f t_0}$ in frequency domain. Hence,
having a linear phase is an important property of a system to achieve distortionless transmission, i.\,e., letting a signal pass without distortion.

A linear phase filter with frequency response $H(\aw)$ has a phase $\Theta = \angle{H(\aw)}$  that corresponds to a line segment in the passband (the phase behavior in the stopband is considered irrelevant). In practice, the line has a negative slope because a positive slope would correspond to a non-causal behavior (the output would be an anticipated version $x(t+t_0)$ of the input signal). 
Hence, it is convenient to define the \emph{group delay}\index{Group delay} as:
\begin{equation}
\tau_g(\aw) = - \frac{d \Theta}{d \aw}.
\label{eq:groupDelay}
\end{equation}
Similarly, the discrete-time version is
\begin{equation}
\tau_g(e^{j\dw}) = - \frac{d \Theta}{d \dw}
\label{eq:discreteTimeGroupDelay}
\end{equation}
with $\Theta = \angle{H(e^{j\dw})}$. 

\bExample \textbf{Impact of the system phase on the output signal}.
To help understanding the practical importance of a system with linear phase, it is adopted a periodic pulse $x[n]$ with $N_1=15$ (number of non-zero samples in a period) and period $N=50$ (see Example~\ref{ex:periodic_pulse}). The experiment is to obtain $Y[k]$ by multiplying the DTFS $X[k]$ of $x[n]$ by $e^{-j 2\pi N_0 k/ N}$, where $N_0=4$. \codl{snip_systems_linear_phase_system} carries out the operation, including the conversion of $Y[k]$ to $y[n]$.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_linear\_phase\_system}{snip_systems_linear_phase_system}
%\begin{lstlisting}
%%Specify: N-period, N1/N-duty cicle, N0-delay, k-frequency
%N=50; N1=15; N0=4; k=0:N-1;
%xn=[ones(1,N1) zeros(1,N-N1)]; %x[n]
%Xk=fft(xn)/N; %calculate the DTFS of x[n]
%phase = -2*pi/N*N0*k; %define linear phase 
%Yk=Xk.*exp(j*phase); %impose the linear phase
%yn=ifft(Yk)*N; %recover signal in time domain
%\end{lstlisting}
\figl{ex_linear_phase} illustrates the result: $y[n]=x[n-N_0]$ is a perfect delayed version of $x[n]$. The middle plot shows the phase that was added to the original phase of $X[k]$. The magnitude of $X[k]$ was left unchanged but that would not be enough to keep the shape of $x[n]$ in case the phase was not linear with frequency. 

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/ex_linear_phase}
\caption{Effect of adding a linear phase $e^{-j 2\pi N_0 k/ N}$ ($N_0=4$ and $N=50$) to the DTFS of $x[n]$ resulting in a delayed version $y[n]=x[n-4]$ .\label{fig:ex_linear_phase}}
\end{figure}

To be contrasted with the linear phase case, \figl{ex_nonlinear_phase} provides an example where the phase is 0, 2 or $-2$ rad, as depicted in the top-most plot. In order to assure that $y[n]$ is real, the phase is an odd function to preserve the Hermitian symmetry. 

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/ex_nonlinear_phase}
\caption{Effect of adding the specified nonlinear phase (top) to the pulse in \figl{ex_linear_phase}, which leads to a distorted signal (bottom).\label{fig:ex_nonlinear_phase}}
\end{figure}

Note that in the case of \figl{ex_nonlinear_phase}, the pulses in $x[n]$ are severely distorted due to the effect of a non-linear phase.
\eExample 

Considering continuous-time (same is valid for discrete-time), a linear phase $e^{-j 2 \pi f t_0}$ avoids distorting an input signal $x(t)$ because the system delays all components of $x(t)$ by the same time interval $t_0$. To obtain that, a linear phase system adds to the input signal component with 
frequency $f_i$, a phase $\Theta_i = - 2 \pi f_i t_0$ that is proportional to $f_i$ (the larger
$f_i$, the larger the phase $\Theta_i$ added by the system). The following example concerns this aspect.

\bExample \textbf{A linear phase allows components with distinct frequencies to be delayed by the same angle}.
For example, assume components corresponding to $\aw=200$ and 400 rad/s with $t_0=4$~s. The phases $\Theta = \aw t_0$ are 800 and 1600 rad, respectively. On the other hand, if these two components pass a system that adds a fixed phase $\Theta=1200$ rad to both, the delays would be 6 and 3 s, respectively, which would potentially distort the delayed version with respect to the original signal $x(t)$. 
\eExample 

For non-linear phase filters, the group delay must be interpreted as an average delay that is frequency-dependent (compare \figl{groupDelayLinearPhase} and \figl{groupDelayNonLinearPhase}).\footnote{After discussing digital filters, their phase and group delay will be exemplified in
\figl{groupDelayLinearPhase} and \figl{groupDelayNonLinearPhase}.}

\bExample \textbf{Gaussian filter and minimum group delay}.
In some applications, such as digital communications, minimizing the group delay is important to achieve low latency. In this case, the \emph{Gaussian filter}\index{Gaussian filter} is competitive because it has
the minimum possible group delay. As a consequence, this filter has no overshoot to an input $u(t)$ while minimizing the rise and fall time intervals. Its non-causal (and ideal, which requires truncation in practice) impulse response is
\begin{equation}
h(t) = \frac{\sqrt{\pi}}{\alpha} e^{-\left( \frac{\pi t}{\alpha} \right)^2},
\label{eq:gaussianFilterImpulseResponse}
\end{equation}
where $\alpha$ controls the 3-dB bandwidth BW. More specifically, $\alpha$ depends on the product of BW and the symbol period $\tsym$ as follows:
\begin{equation}
\alpha = \frac{1}{\BW~\tsym} \sqrt{\frac{\ln 2}{2}}.
%\label{eq:}
\end{equation}
The frequency response of this filter is $H(f) = e^{- \alpha^2 f^2}$ and decreases with $f$ but is not zero for finite $f$.
\eExample 

\section{{\akadvanced} Filtering technologies: Surface acoustic wave (SAW) and others}
\label{sec:filteringTechnologies}

There are several technologies to build an electronic filter. 
%In fact, modern equipment seldom use simple RLC filters built with off-the-shelf parts.
The options include ceramic, microelectromechanical system (MEMS), yttrium iron garnet (YIG), SAW and many others. Even if the scope is reduced to radio-frequency (RF) communications, the diversity of filters is huge. Consequently, parameters such as the Q-factor, center frequency $f_c$ and \emph{insertion loss} $\textrm{IL}(f)$ (which is the inverse of the insertion gain $\textrm{IG}(f)=1/\textrm{IL}(f)$, as discussed in Section~\ref{sec:insertionLoss}), vary widely. This section
provides some practical examples.

\bExample \textbf{Performance of commercial SAW filter}.
\figl{saw_filter2} illustrates the typical performance of a commercial SAW filter\footnote{The Vanlong BP60160, \akurl{http://www.vanlong.com/products/std70mhz.htm}{3van}.} with some of its specifications listed in \tabl{sawFilterSpecs}.
The insertion gain $\textrm{IG}(f)$ is shown in \figl{saw_filter2} at 1 and 10~dB per division (div) at the left plot and superimposed (at 1~dB per division) to the group delay at the right plot. 

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/saw_filter2}
\caption{Performance of a commercial SAW filter. The insertion gain $\textrm{IG}(f)$ at two resolutions at the left plot and superimposed to the group delay at the right.\label{fig:saw_filter2}}
\end{figure}

%The $\textrm{IG}(f)$ at 10~dB per div is adjusted to a reference of zero ??
As also indicated in \tabl{sawFilterSpecs}, \figl{saw_filter2} indicates that 
\ci{df1}, \ci{df3} and \ci{df40} are the bandwidths considering the attenuation at 1, 3 and 40~dB, respectively, with values 13.08, 13.88 and 16.6~MHz. The insertion loss $\textrm{IL}(f)|_{f=f_c}$ at $f_c=70.12$~MHz is 24.7~dB. The group delay plot only shows the variation at the order of ns, but \tabl{sawFilterSpecs} indicates the delay as 1.6~$\mu$s.
As indicated in Section~\ref{sec:insertionLoss}, $\textrm{IG}(f)$ is sometimes called IL and \figl{saw_filter2} uses a negative \ci{IL} to express that the filter attenuates the power by 24.7~dB as properly expressed in \tabl{sawFilterSpecs}.

\begin{table}
\centering
\caption{Specifications of a commercial SAW filter.\label{tab:sawFilterSpecs}}
\begin{tabular}{|l|c|c|c|}
\hline
Parameter & Minimum & Typical & Maximum \\ \hline
Center frequency at $25^o$~C ($f_c$ or \ci{f0}, MHz) & 69.8 & 70 & 70.2 \\ \hline
Insertion loss at $f_c$ (\ci{IL}, dB) & - & 24.7 & 28.0 \\ \hline
1-dB bandwidth (\ci{df1}, MHz) & 12.75 & 13.0 & - \\ \hline
3-dB bandwidth (\ci{df3}, MHz) & 13.0 & 13.8 & - \\ \hline
40-dB bandwidth (\ci{df40}, MHz) & - & 16.6 & 17.1  \\ \hline
Passband variation (ripple, dB) & - & 0.5 & 0.6 \\ \hline
Group delay variation (ns) &  - & 25 & 50 \\ \hline
Absolute delay ($\mu$s) & - & 1.6 & - \\ \hline
\end{tabular}
\end{table}

The Q-factor of the filter depicted in \figl{saw_filter2} is $Q=70.12/13.88 \approx 5$ and provides more than 60~dB of attenuation at the stopband. Building such (relatively) highly-selective filters is easier when the center frequency $f_c$ is fixed than when it has to vary, for tuning purposes. For example, the filter of \figl{saw_filter2} is supposed to operate with $f_c=70$~MHz, which is an intermediate frequency (IF) used in wireless signal reception. When a receiver has to operate over a frequency range, a common strategy is to pre-select the frequency band of interest using a so-called RF-filter with center frequency $f_c^{\textrm{RF}}$ (e.\,g., 2.1~GHz) and convert it via frequency shifting to the IF (e.\,g., 70~MHz), when it is then better filtered and further processed. 
\eExample

A common IF for broadcast AM radio is $f_c=455$~kHz, which motivates the next filter example.

\bExample \textbf{Commercial ceramic filter}.
\figl{ceramic_filter} illustrates the performance of a commercial ceramic filter\footnote{Filter SFPKA455KE4A-R1 in page 5 of
\akurl{http://www.murata.com/products/catalog/pdf/p05e.pdf}{3mur}.} that can be used with an IF of 455~kHz and has some specifications listed in \tabl{ceramicFilterSpecs}. 
Note that this filter's stopband is not flat, especially within the range from 600 to 700~kHz. This makes the specifications harder to interpret than for an ideal bandpass filter.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/ceramic_filter}
\caption[Performance of a commercial ceramic filter.]{Performance of a commercial ceramic filter. The right plot is a zoom of the attenuation at the left plot, centered at $f_c=455$~kHz and includes the group delay with values ranging between approximately 50 and 120~$\mu$s in passband.\label{fig:ceramic_filter}}
\end{figure}

%AK-TODO discusss issue of saying 6 dB BW

\begin{table}
\centering
\caption{Specifications of a commercial ceramic filter where $f_n = 455$~kHz is the nominal frequency and $f_c$ is the center of the 6-dB BW.\label{tab:ceramicFilterSpecs}}
\begin{tabular}{|l|c|}
\hline
Parameter & Value \\ \hline
Center frequency $f_c$ (kHz) & $455 \pm 1.5$ \\ \hline
Minimum 6-dB bandwidth (kHz) & $f_n \pm 7.5$ \\ \hline
Maximum stop bandwidth, within 40 dB (kHz) & $f_n \pm 15.0$ \\ \hline
Minimum stopband attenuation within $f_n \pm 100.0$ kHz (dB) & 27 \\ \hline
Maximum insertion loss (at minimum loss point, dB) &  6 \\ \hline
Maximum ripple within $f_n \pm 5.0$ kHz (dB) & 1.5 \\ \hline
\end{tabular}
\end{table}

Caution has to be exercised when interpreting the ``Maximum stop bandwidth'' in \tabl{ceramicFilterSpecs}, which indicates
the filter attenuates 40~dB at the end of a band centered on $f_n$ and with a bandwidth of approximately 30~kHz. But this attenuation then decreases. According to \tabl{ceramicFilterSpecs}, an attenuation of at least 27~dB is guaranteed only within $455 \pm 100$~kHz.
\ifml
\else
Broadcast AM radio will be discussed in Section~\ref{sec:amModulation}.% in the context of amplitude modulation
\fi
\eExample

%AK-IMPROVE %\section{Conversions From Discrete to Continuous-Time And Vice-Versa}
\ignore{
%\section{Conversions From Discrete to Continuous-Time And Vice-Versa}
%
There are two methodologies for designing digital filters: a) the $\fs$ that will be used in the actual implementation is given and b) there is no \emph{a priori} knowledge about $\fs$ and the filter is specified in the discrete-time domain. In case b), $\fs$ is typically assumed equal to 1~Hz.
%
Bilinear Design Summary
\begin{itemize}
	\item Calculate pre-warping analog cutoff frequency
	\item Denormalise filter transfer function using pre-warping cutoff
	\item Apply bilinear transformation and simplify
	\item Use inverse Z-transform to obtain difference equation
\end{itemize}
%
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_filter\_design\_methods}{snip_systems_filter_design_methods} 
}

%\subsection{Continuous and discrete-time frequency responses}

\section{Introduction to Digital Filters}
\label{sec:digital_filters}

\subsection{Designing simple filters using specialized software}

Before studying digital filters in more details, it is useful to briefly discuss analog filters because most of the times they are required in digital signal processing systems as will be discussed here.

\subsubsection{Example of analog filter design}
\label{sec:analogFilterDesign}

There are integrated circuits (IC) that implement analog filters. They can also be built using discrete components: resistors, capacitors, inductors and operational amplifiers (opamps). The \emph{design}\index{Filter design} of a filter corresponds to obtaining its system function. In the case of analog filters, there are many recipes (well-established algorithms, also called approximations) to obtain a system function $H(s)$ that obeys the specified requirements in a project. The most common approximations are the Butterworth, Chebyshev, Bessel and Cauer (or elliptic).

Having $H(s)$, the next step is the \emph{realization}\index{Filter realization} of the filter using a circuit, i.\,e., establishing the topology and the components of the circuit that (approximately) implement $H(s)$. These two steps are the subject of many textbooks on filter design. Here, the approach will be to simply provide an example using the FilterPro software from Texas Instruments \akurl{www.ti.com/filterpro-dt}{3tif}. This software allows to obtain the schematic and components.\footnote{FilterPro has been deprecated in favor of WEBENCH at \akurl{http://www.ti.com/lsds/ti/analog/webench/webench-filters.page}{3twe}.}


Analog filters that use only resistor, capacitors and inductors are called \emph{passive} filters. An alternative to avoid using inductors\footnote{For the readers without background in electrical circuits but interested in the subject, some basic concepts can be found on the Web, e.\,g. \akurl{http://en.wikipedia.org/wiki/Voltage_divider}{3ele}.} 
are active filters, which are typically based on opamps. 
When the target is an IC, not a circuit with discrete components, inductors are avoided due to
the difficulty of their integration using current microelectronics technology. When an analog
 filtering operation is required in an IC, an active filter is typically adopted and this
is the only class of filters supported by FilterPro.

\bExample \textbf{Example of analog filter designed with FilterPro}.
The following example illustrates the design of an analog filter with a desired cutoff frequency $f_c=1$~kHz, which is assumed to define the passband, such that the passband frequency is $f_p = f_c$. A second-order lowpass filter with passband frequency of approximately 977.2 Hz was obtained with FilterPro and its schematic\footnote{The FilterPro  options were a lowpass Butterworth filter with passband frequency of 1 kHz, with a fixed order of 2 and components with 10\% of tolerance. When using the exact values of the calculated resistances and capacitances (two of 11.25 K$\Omega$, one of 5.63 K$\Omega$, 10 nF and 40 nF), the cutoff frequency is exactly 1 kHz, but this is typically not feasible in practice, i.\,e., commercial components are available only within a finite set of values and have non-zero tolerance.} is shown in \figl{analog_filter}.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/analog_filter}
\caption{Lowpass active analog filter designed with Texas Instruments' FilterPro software.\label{fig:analog_filter}}
\end{figure}

The system function $H(s)$ can be obtained by analyzing the circuit in \figl{analog_filter}. After that, $H(s)$ can be compared to the ideal one obtained e.\,g. with {\matlab} via the command:
\ci{[Bs,As]=butter(2,2*pi*1000,'s')}
that gives
\begin{equation}
H(s) = \frac{B(s)}{A(s)} = \frac{3.9478\times 10^7}{s^{2}  + 8.8858 \times 10^3 s  + 3.9478\times 10^7}.
\label{eq:butter_analog}
\end{equation}
The arguments of the function \ci{butter} are the filter order and the cutoff frequency in rad/s (not Hz). Note the required \ci{'s'} to indicate to {\matlab} that the goal is to design an analog, not a digital filter. 
\eExample 


\bExample \textbf{Further examples of analog filter design in Matlab}.
\label{ex:analogFilterDesigns}
%\subsection{Designing filters that obey specifications}
%\subsection{Designing analog filters}
\codl{figs_systems_elliptic} illustrates how to design some analog filters with a given specification for its magnitude
using Matlab (this code is not compatible with Octave).
The frequency response magnitude and phase of the four designed filters are depicted in \figl{somefilters}.

\lstinputlisting[caption={MatlabBookFigures/figs\_systems\_elliptic.m},label=code:figs_systems_elliptic,linerange={1-9,12-18,21-29,33-37},firstnumber=1]{./Code/MatlabBookFigures/figs_systems_elliptic.m}

\begin{figure}
\centering
    \subfigure[Lowpass elliptic ($N=8$).]{\includegraphics[width=5cm]{Figures/lowpassElliptic}}    
    \subfigure[Highpass elliptic ($N=8$).]{\includegraphics[width=5cm]{Figures/highpassElliptic}}
    \subfigure[Bandpass elliptic ($N=16$).]{\includegraphics[width=5cm]{Figures/bandpassElliptic}}
    \subfigure[Bandpass Butterworth ($N=86$).]{\includegraphics[width=5cm]{Figures/bandpassButterworth}}    
  \caption{Frequency response of filters designed in \codl{figs_systems_elliptic}. The magnitude specification masks are indicated. Note that the phase was unwrapped with the command \ci{unwrap} for better visualization.}
  \label{fig:somefilters}
\end{figure}

It should be noted from \codl{figs_systems_elliptic} and \figl{somefilters} that a bandpass filter has twice the order that is specified as an input parameter (and obtained by \ci{buttord} and \ci{ellipord} in this example). Note also that, in practice, an analog filter typically has an order between 1 to 10. For example, an order $N=86$ is not feasible for practical implementation with an analog circuit.
\eExample 

\subsubsection{Example of digital filter design}

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresNonScript/canonical_interface}
\caption[{The canonical interface of a digital filter $H(z)$ with the analog world via A/D and D/A processes.}]{The canonical interface of a digital filter $H(z)$ with the analog world via A/D and D/A processes. The analog $A(s)$ and $R(s)$ are the anti-aliasing and external reconstruction filters, respectively.\label{fig:canonical_interface}}
\end{figure}

For comparison, it is interesting to design now an equivalent digital filter $H(z)$ to substitute its analog counterpart corresponding to $H(s)$. But in order to interface $H(z)$ with the external (analog) world, four extra blocks are required, as illustrated in \figl{canonical_interface}. The filters $A(s)$ (anti-aliasing) and $R(s)$ (external reconstruction) are lowpass analog filters with passband frequency equal to half of the sampling frequency, which is assumed here to be $\fs = 10$~kHz. Their gains are not important at this moment and in practice they are determined by the interfacing electronics.
Assuming the converters and analog filters are properly defined, it remains to design $H(z)$, which can be done with the command
\begin{lstlisting}
[Bz,Az]=butter(2,(1000/5000))
\end{lstlisting}
that leads to
\begin{equation}
H(z) = \frac{B(z)}{A(z)} = \frac{0.067455  + 0.134911  z^{-1} + 0.067455 z^{-2}}{1.0  -1.14298 z^{-1}  + 0.41280 z^{-2}}.
\label{eq:butter_digital}
\end{equation}
It is important to note that when designing an analog filter, the second argument of \ci{butter} is $\aw$ in rad/s. For a digital filter, as explained in Section~\ref{sec:frequency_normalization}, {\matlab} uses the normalized frequency $f_N = \dw/\pi$.
% given by \equl{matlabNormalizedFrequency} or \equl{inHzMatlabNormalizedFrequency}. 
In this case, the Nyquist frequency $\fs/2$ is 5~kHz and using \equl{inHzMatlabNormalizedFrequency}, $f=1000$~Hz is normalized by the Nyquist frequency in the command \ci{[Bz,Az]=butter(2,(1000/5000))}.

It will be later discussed in this chapter that the function \ci{butter} used the bilinear transformation to convert $H(s)$ of \equl{butter_analog} into $H(z)$ of \equl{butter_digital}.

From \tabl{transf_of_h} one knows the definition $H(z)=Y(z)/X(z)$ and this should not be confused with $H(z)=B(z)/A(z)$ in \equl{butter_digital}.  In many applications $H(z)$ is a \emph{rational function} (a ratio of two polynomials), with $B(z)$ specifying the numerator and $A(z)$ the denominator. In general, $B(z) \ne Y(z)$ and $A(z) \ne X(z)$. 

Digital filters described by a rational function $H(z)$ can be conveniently implemented via a LCCDE, as discussed next.

\bExample \textbf{Obtaining the difference equation and implementing a digital filter}.
For example, assume that a LTI system with impulse response $h[n]=0.2^n u[n]$ outputs $y[n]=\delta[n] - 0.3\delta[n-1]$ when the input is $x[n]=\delta[n] - 0.5\delta[n-1] + 0.06\delta[n-2]$. In this case, $X(z)=1 -0.5z^{-1} + 0.06z^{-2}$ and $Y(z)=1-0.3z^{-1}$, which leads to $H(z)=1/(1-0.2z^{-1})$ with $B(z)=1$ and $A(z)=1-0.2z^{-1}$. With this concept in mind, \equl{butter_digital} can be written as
\begin{align*}
&\mathrel{\phantom{=}}X(z) (0.067455  + 0.134911  z^{-1} + 0.067455 z^{-2})\\
&= Y(z) (1.0  -1.14298 z^{-1}  + 0.41280 z^{-2})
\end{align*}
and taking the Z-inverse one can obtain the difference equation corresponding to \equl{butter_digital}:
\begin{align*} %&\mathrel{\phantom{=}}
y[n]&=0.067455x[n] + 0.134911x[n-1] + 0.067455x[n-2] \\
&\mathrel{\phantom{=}} + 1.14298 y[n-1] - 0.41280 y[n-2].
\end{align*}
The amazing fact is that this can be implemented with a simple code, similar to \codl{digitalfilter}.

\begin{lstlisting}[caption=Digital filter implementation in pseudo-code.,label={code:digitalfilter}]
initialization() { %all previous values are assumed 0
	  xnm1=0 %variable to store the value of x[n-1]
	  xnm2=0 %x[n-2]
	  ynm1=0 %y[n-1]
	  ynm2=0 %y[n-2]
}
processSample() {
	  xn=readFromADConverter() %read input sample from ADC
	  yn=0.067455*xn + 0.134911*xnm1 + 0.067455*xnm2 + 
	  	 1.14298 ynm1 - 0.41280 *ynm2
	  writeToDAConverter(yn) %write output sample into DAC
	  ynm2=ynm1 %update for next iteration. Note the order of ...
	  ynm1=yn   %updates: avoid overwriting a value ...
	  xnm2=xnm1 %that should be used later
	  xnm1=xn
}
\end{lstlisting}
where one assumes a timer\footnote{In practice a hardware interrupt is periodically generated according to a timer. The code that implements the filter is part of the interrupt service routine (ISR).} invokes the function \ci{processSample} at a rate of $\fs$ (10 kHz in this case). 

%get code from:
%\begin{verbatim}
%C:\svns\laps\latex\dslbook\Code\MatlabBookFigures\figs_systems_filters.m
%\end{verbatim}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/comparing_filters}
\caption{Comparison of two analog filters with its digital counterpart of \equl{butter_digital}. The ``ideal'' analog corresponds to \equl{butter_analog} while the ``10\% tolerance'' corresponds to its realization using the schematic of \figl{analog_filter}.\label{fig:comparing_filters}}
\end{figure}

\figl{comparing_filters} compares the discussed filters, illustrating that $H(z)$ has a lowpass frequency response that is similar in terms of passband frequency to the original $H(s)$. For frequencies above the passband, $H(z)$ attenuates more than $H(s)$, but this is not deleterious for a lowpass filter. Later, in Section~\ref{sec:bilinear_properties}, this behavior will be clarified, which is intrinsic of having the bilinear transformation mapping the behavior of the analog filter at infinite frequencies into $\dw=\pi$~rad (or $\fs/2$~Hz) for $H(z)$, i.\,e., mapping $H(\aw)|_{\aw \rightarrow \infty}$ into $H(e^{j\dw})|_{\dw = \pi}$.

If $\fs$ is changed, the frequency response of the overall system in \figl{canonical_interface} is scaled according to $\aw = \dw \fs$ (\equl{freqdiscrete2continuous}). For example, decreasing $\fs$ from 10 to 5~kHz would divide by two the cutoff frequency of the overall system.
\eExample 

%Use here figures that relate the analog and their digital counterparts. Show the Matlab code for the digital filters.


%\bApplication \textbf{Distinct ways of specifying the passband ``ripple'' / deviation.} 
\subsection{Distinct ways of specifying the ``ripple'' / deviation in filter design}
As illustrated in \figl{somefilters}, practical filters have a frequency response that deviates from
the ideal ones. Instead of a flat magnitude over the whole passband and zero gain in the stopband,
as depicted in \figl{low_highpass_specs}, a non-ideal filter has deviations from the target values
and eventual ``oscillations'' called ``ripples''\index{Ripple in frequency response}.
When designing a filter, it is possible to impose limits to these ripples, but this procedure
is different for the passband and stopband ``ripples'' / deviations, as discussed next.

There are several ways of specifying the maximum ``ripple'' %$R_{\dB}$ 
or deviation at the passband of a filter. For example, it can be specified in dB or linear scale, which are called here $R_p$ and $D_p$, respectively (as suggested by Matlab). If one assumes a lowpass filter with a unitary gain at DC, the maximum linear deviation $D_p$ can be assumed to be within $[1-D_p, 1+D_p]$ or $[1- D_p, 1]$. The latter option is particularly useful if the filter magnitude monotonically decreases as for the Butterworth filters and is assumed here, meaning that the frequency response is 1 at DC and decreases in the passband to a value not smaller than $1-D_p$. For example, a specification can be that the passband magnitude does not deviate 5\% of the gain at DC. In this case, $D_p = 0.05$ and the minimum magnitude at passband is $1-D_p = 0.95$. Often, this value is specified in dB as 
\[
R_p = -20 \log_{10} (1-D_p).
\]
If the deviation is within $[1- D_p, 1]$ in linear scale, then it is confined to $[-R_p, 0]$ in dB.

At the stopband, the magnitude in linear scale $D_s$ is also a ``deviation'' because the reference value is 0 (the desired magnitude at stopband). In other words, for the stopband, the deviation coincides with the magnitude itself and its value in dB is given by 
\[
R_s = -20 \log_{10} D_s.
\]
Often, the filter magnitude has a maximum value of 0~dB and the values of $R_p$ and $R_s$ \emph{are positive}.

Matlab uses the same convention adopted here in its routines. This is exemplified in \codl{snip_systems_IIR_design}, which uses
the method \ci{buttord} to find the order of a Butterworth filter that complies with the project specifications.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_IIR\_design}{snip_systems_IIR_design}
%\begin{lstlisting}
%Wp=0.3; %normalized passband frequency (rad / pi)
%Ws=0.7; %normalized stopband frequency (rad / pi)
%Rp=1; %maximum passband ripple in dB
%Rs=40; %minimum stopband attenuation in dB
%[n,Wn] = buttord(Wp,Ws,Rp,Rs); %find the Butterworth order
%[B,A]=butter(3*n,Wn); %design the Butterworth filter
%zp=exp(j*pi*Wp); zs=exp(j*pi*Ws); %Z values at Wp and Ws
%linearMagWp = abs(polyval(B,zp)/polyval(A,zp)) %mag. at Wp
%linearMagWs = abs(polyval(B,zs)/polyval(A,zs)) %mag. at Ws
%D_p = 1-linearMagWp %linear deviation at Wp (linear)
%R_p = -20*log10(1-D_p) %deviation (ripple) at Wp (dB)
%D_s = linearMagWs %linear deviation at Ws (linear)
%R_s = -20*log10(D_s) %deviation (ripple) at Ws (dB)
%\end{lstlisting}
The required filter order is \ci{n=4}, and the obtained results are:
\ci{linearMagWp=0.9105}, \ci{linearMagWs=0.01}, 
\ci{Dp\_result=0.0895}, \ci{Rp\_result=0.8147}, \ci{Ds\_result=0.01} and \ci{Rs\_result=40.0}.
They can be interpreted as follows. Matlab designed the method \ci{buttord} to meet
exactly the requirements for the stopband, while giving an eventual slack to the requirements
for the passband. Hence, we can see that \ci{Rs\_result=40.0}~dB is exactly the specified attenuation
at the stopband. This corresponds to a linear gain \ci{linearMagWs=0.01} of the filter's frequency
response $H(e^{j \dw})$ at the beginning of the stopband. Using mathematical notation: $H(e^{j \dw})|_{\dw=0.7 \pi} = 0.01$ 
in linear scale, 
which corresponds to $20 \log_{10} H(e^{j \dw})|_{\dw=0.7 \pi} = 20 \log_{10} (0.01) = -40$~dB.

The filter gain is 1 at DC, and it decreases until reaching \ci{linearMagWp=0.9105} in the end of its passband.
 In our notation, $H(e^{j \dw})|_{\dw=0.3 \pi} = 0.9105$, 
which corresponds to $20 \log_{10} H(e^{j \dw})|_{\dw=0.3 \pi} = 20 \log_{10} (0.9105) = -0.8144$~dB.
This value does not coincide with \ci{Rp\_result=0.8147} due to the rounding to four decimal places.
Using \ci{20*log10(linearMagWp)} gives the precise gain value of $-0.8147$~dB. Note that the deviation in the passband
of 0.8147~dB from the ideal 0~dB is within the maximum value of 1~dB imposed to \ci{buttord}. There
is a slack of $1-0.8147=0.1853$~dB.

In summary, it is a bit confusing how Matlab and other software define the input parameters
to functions such as \ci{buttord}. But one just needs to interpret the ripples in dB as obtained
from ``deviations'' of the ideal values of 1 and 0 in passband and stopband, respectively.

%\eApplication


\subsection{LCCDE digital filters}

A digital filter is a discrete-time system that deals with quantized input $x_q[n]$ and output $y_q[n]$ signals (quantized amplitudes). A possible practical scenario for a digital filter is the one shown in \figl{canonical_interface}. There are many different kinds of digital filters but this section assumes filters that are based on linear constant-coefficient difference equations (LCCDE). As indicated in \figl{ltidiagram}, when zero initial conditions are assumed, the LCCDE systems are a special case of LTI systems. But when one takes in account the non-linear quantization effects that occur in practical CPUs (or digital signal processors) due to their limited precision, the overall system is not strictly LTI. In spite of that, it is convenient to isolate the effects of quantization and finite-precision arithmetics, and study digital filters as LTI LCCDE systems. Later on, the impact of quantizing the filter coefficients and the roundoff errors due to finite-precision can be incorporated. Therefore, hereafter, the term digital filter refers to discrete-time LTI LCCDE systems. Besides, the systems are assumed to be causal.

A system that has the output and input signals related by a LCCDE such as
\begin{equation}
\sum_{k=0}^N a_k y[n-k] = \sum_{k=0}^M b_k x[n-k]
\label{eq:lccde}
\end{equation}
can be represented by (taking the Z-transform on both sides of the LCCDE)
\[
Y(z) \sum_{k=0}^N a_k z^{-k} = X(z) \sum_{k=0}^M b_k z^{-k}.
\]
In this case, the transfer function is 
\begin{equation}
H(z) = \frac{Y(z)}{X(z)} = \frac{\sum_{k=0}^M b_k z^{-k}}{\sum_{k=0}^N a_k z^{-k}},
\label{eq:hz_transfer_function}
\end{equation}
which, after eventual algebraic simplifications, can be written as the ratio 
\begin{equation}
H(z)=\frac{B(z)}{A(z)}
\label{eq:transferfunction}
\end{equation}
of two polynomials in $z$. Calculating the zeros $z_k$ (roots of the numerator $B(z)$) and the poles $p_k$ (roots of the denominator $A(z)$), one can write
\[
H(z) = g \frac{\prod_{k=1}^M (z-z_k)}{\prod_{k=1}^N (z-p_k)},
\]
where $g$ is a gain. The coefficients of \equl{lccde} can be real or complex. For simplicity, they will be assumed real ($a_k, b_k \in \Re$) unless otherwise stated. In this case, $g \in \Re$ and the roots $z_k, p_k$ are real or, for each root $r e^{j\theta}$, its complex conjugate $r e^{-j\theta}$ is also a root such that the product $(z-re^{j\theta})(z-re^{-j\theta})=z^2+2r\cos(\theta)+r^2$ leads to a second-order polynomial with real coefficients.

It is useful to know that the value of $H(z)$ at $z=1$ and $z=-1$ correspond to the values of the frequency response $H(e^{j\dw})$ at $\dw=0$ (DC) and $\dw=\pi$ rad, respectively, which can be written as
\begin{equation}
H(z)|_{z=1} = H(e^{j\dw})|_{\dw=0} \textrm{~~~and~~~} H(z)|_{z=-1} = H(e^{j\dw})|_{\dw=\pi}.
\label{eq:z_dc_nyquist_freqs}
\end{equation}
For example $H(z)=3+z^{-1}$ has a frequency response with $H(e^{j\dw})|_{\dw=0} = 3+1=4$ and $H(e^{j\dw})|_{\dw=\pi} = 3-1=2$.

\bExample \textbf{Specifying a filter in Hz, rad or normalized frequency}.
When designing digital filters, it is important to remember the fundamental relation $\aw = \dw \fs$, first presented in \equl{freqdiscrete2continuous}. \figl{specs_three} illustrates how this relation can be used to convert a specification for a lowpass analog filter with passband and stopband frequencies of 500 and 850 Hz, respectively, into a specification of a digital filter with $\fs=2000 Hz$, which leads to passband and stopband frequencies of $2 \pi 500 / 2000 \approx 1.57$ and $2 \pi 850 / 2000 \approx 2.67$ rad, respectively.

\begin{figure}[htbp]
\centering
    \subfigure[Analog ($f$ in Hz)]{\label{fig:spec-in-hz}\includegraphics[width=5cm,keepaspectratio]{FiguresNonScript/spec-in-hz}}
\subfigure[Digital ($\dw$ in rad)]{\label{fig:spec-in-rad}\includegraphics[width=5cm,keepaspectratio]{FiguresNonScript/spec-in-rad}}
\subfigure[Digital ($f_N$ in $\dw/\pi$)]{\label{fig:spec-in-matlab}\includegraphics[width=5cm,keepaspectratio]{FiguresNonScript/spec-in-matlab}}
  \caption{Comparison of analog filter specification (a) and two corresponding digital versions assuming $\fs=2000$ Hz: (b) was obtained with $\aw = \dw \fs$ and (c) uses the convention adopted in {\matlab}.}
  \label{fig:specs_three}
\end{figure}

\figl{specs_three} also shows the convention used by {\matlab}, which gets rid of the irrational number $\pi$ by using a normalized axis $f_N=\dw/\pi$, as listed in \tabl{nyquist_frequency}. In this case, $\aw = \dw \fs$ can be written as $2 \pi f = f_N \pi \fs$, which simplifies to obtaining $f_N = f/(\fs/2)$ with the normalization of $f$ by the Nyquist frequency $\fs/2$. For example, $f=500$ Hz in \figl{spec-in-hz} turns into $f_N = 500/1000=0.5$ in \figl{spec-in-matlab}. This normalization by the Nyquist frequency is sensible because digital filters are always constrained to work with frequencies of at most $\fs/2$~Hz, which corresponds to $\pi$~rad.
\eExample 

\subsection{FIR, IIR, AR, MA and ARMA systems}
\label{sec:arma}

An important characteristic of a digital filter is the \emph{duration of its impulse response}, which allows to classify it as a finite impulse response (FIR) or infinite impulse response (IIR) filter. For example, a filter with $h_f[n]=3\delta[n]+2\delta[n-1]$ is FIR and a filter with $h_i[n]=0.8^n u[n]$ is IIR. Note that if one plots $h_i[n]$ in a computer, due to the limited precision, the values of $h_i[n]$ would underflow to zero for large enough $n$, but theoretically this impulse response has infinite duration.

Another characteristic is whether the filter is \emph{recursive} or not. A recursive filter has the output $y[n]$ depending on past values of the output itself ($y[n-1], y[n-2], \ldots$) while the output of a non-recursive filter depends only on the input $x[n]$ and its past values (assuming causality). The system function $H(z)$ of a non-recursive filter has $N=0$ in \equl{hz_transfer_function} and no finite poles.

In practice, FIR can be associated to non-recursive filters and IIR to recursive filters. But recursiveness and impulse response duration are two distinct concepts! It is possible to 
create a recursive filter with an impulse response of finite duration using cancellation of poles and zeros, for example.
In spite of that, the design of recursive filters is often called ``IIR design''. Similarly, ``FIR design'' is the
topic that concerns the design of non-recursive filters. This widely adopted jargon is used in this text.

IIR and FIR filters will be discussed in the sequel. While the subject is presented in the context of digital filtering, many conclusions are valid for systems in general. In other words, IIR and FIR systems can be found in applications in which the goal is not filtering, but system identification, equalization, control, tracking, statistics, etc. In some of these areas, $H(z)$ is not classified as FIR or IIR, but as MA, AR or ARMA ``model''.

A moving-average (MA) model $H(z)$ is a non-recursive FIR filter and an autoregressive (AR) filter is a special case of a recursive IIR filter for which $M=0$ in \equl{hz_transfer_function}. The AR IIR filter does not have finite zeros other than at the origin $z=0$ and is given by
\begin{equation}
H(z) = g\frac{z^N}{\prod_{k=1}^N (z-p_k)} = \frac{g}{1+a_1 z^{-1} +a_2 z^{-2} + \ldots  +a_N z^{-N}}.
\label{eq:ar_model}
\end{equation}
Note in \equl{ar_model} that the gain value $g$ for the numerator is chosen such that the denominator coefficient $a_0$ of \equl{hz_transfer_function} is equal to 1. The numerator $z^N$ leads to $N$ zeros at $z=0$ and makes $H(z)$ causal.

The autoregressive-moving-average (ARMA) model is a generic $H(z)$ model composed by an AR (denominator) and MA (numerator) sections. There are many generalizations of these basic models, which are adopted in areas such as statistics. \emph{Parametric modeling}\index{Parametric modeling} is the task of, according to a predefined criterion such as least-squares, fit the data to the assumed model. In spite of being defined in distinct scenarios, filter design and parametric modeling share several fundamental results.

%\subsubsection{Ideal filters} Low, high, band-pass, band-stop (or band-reject)

Before discussing the design of IIR and FIR filters, it is useful to 
note the importance of frequency scaling.

\subsection{Filter frequency scaling}
\label{sec:frequencyScaling}
Computational routines often produce a prototype with normalized frequencies (e.\,g., a cuttof frequency of 1~rad/s) that are then converted (eventually by another routine) to the required frequencies via \emph{frequency scaling}\index{Frequency scaling}. 

With frequency scaling one can modify a filter to obtain a different cutoff frequency. For example, if $H(s)$ has a cutoff of $\aw_c$ rad/s, the transformation $s \rightarrow s/k$ moves this cutoff frequency to $k \aw_c$ rad/s. The operation is similar to using $x(2t)$ to double the ``speed'' of a signal represented by $x(t)$, which leads to $x(2t)$ having half of the support of $x(t)$. Similarly, $x(t/2)$ has twice the support of $x(t)$.
More specifically, if the prototype $H(s)$ has a cutoff frequency of 1~rad/s, then
\begin{equation}
G(s) = H(s)|_{s=s/\aw_c}
\label{eq:frequencyScaling}
\end{equation}
scales the complex plane, and consequently $s=j \aw$, such that $\aw_c$ is the 
cutoff frequency of the new filter $G(s)$.

There are computational routines that provide a prototype filter with (normalized) cutoff frequency of $1$~rad/s, which can then be converted to a new filter with an arbitrary cutoff frequency $\aw_c$ with the transformation $s \rightarrow s/\aw_c$. For example, this change in cutoff frequency can be done in Matlab by applying the function \ci{lp2lp} to a lowpass analog prototype (current version of Octave misses this functionality) as indicated in \codl{snip_systems_filter_conversion}.

\includecodepython{MatlabOnly}{snip\_systems\_filter\_conversion}{snip_systems_filter_conversion}

Note that \ci{lp2lp} requires the prototype to have a cutoff frequency of $\aw_c = 1$~rad/s (besides being an analog lowpass filter). 

\bExample \textbf{Frequency conversion when the prototype does not have a cutoff frequency of 1~rad/s}.
Assume that a frequency conversion must be applied to $H(s)=100/(s+100)$ such that
its cutoff frequency of $\aw_c = 100$~rad/s is shifted to 30~rad/s. Because this
is a simple first-order filter, the final result $G(s)=30/(s+30)$ can be immediately
obtained. But the whole procedure is described because it is useful for higher-order filters.

A convenient step is to move the cutoff frequency of $H(s)$ to 1~rad/s using
a substitution of $s$ by $100s$, which leads to
\[
F(s) = H(100s)=\frac{100}{100s+100} = \frac{1}{s+1}.
\]
Now, the final filter can be obtained by expanding $\aw_c$ to 30 by substituting
$s$ in $F(s)$ by $s/30$:
\[
G(s) = F(s/30) = \frac{1}{\frac{s}{30}+1} = \frac{30}{s+30}.
\]
Alternatively, one could directly use $G(s)=H(100s/30)$.
These frequency conversions are widely used in the design of IIR filters as will be discussed.
\eExample

\subsection{Filter bandform transformation: Lowpass to highpass, etc.}

There are many transformations to convert a \emph{prototype}\index{Prototype filter} (or reference) filter into another one. 
Section~\ref{sec:frequencyScaling} briefly discussed frequency scaling.
It is also possible to convert a lowpass prototype filter into a highpass, bandpass and bandstop, for example, which is called \emph{bandform transformation}\index{Bandform transformation}. 

When dealing with analog filters, it is often necessary to apply \emph{impedance scaling}\index{Impedance scaling} to have the filter input and/or output with a desired impedance. The intent in this case is not to change the transfer function, but to perform impedance matching. All these three kinds of filter transformations are well-discussed in the literature\footnote{See references in \akurl{en.wikipedia.org/wiki/Prototype_filter}{3tra}.}. In the sequel, bandform transformation is briefly discussed.

Bandform transformations can be used to create a bandpass from a lowpass prototype, for example. These transformations differ considerably for analog and digital frequencies, especially due to the fact that a digital filter has a periodic spectrum.\footnote{Compare Matlab's documentation for ``Digital frequency transformations'' and ``Analog filter transformation'', currently at \akurl{http://www.mathworks.com/help/toolbox/dsp/ug/bsva1v1.html}{3mad} and  \akurl{http://www.mathworks.com/help/toolbox/signal/ref/f9-131178c.html\#bqmx16g}{3maa}, respectively.} Having a lowpass analog prototype, a Matlab user\footnote{As indicated, the current version of Octave misses this functionality.} can use the functions \ci{lp2bp}, \ci{lp2bs} and \ci{lp2hp} to convert it to bandpass, bandstop and highpass filters, respectively. 

Sometimes frequency scaling is incorporated to bandform transformation such that a routine converts a lowpass prototype with normalized 1~rad/s cutoff frequency into another filter with desired bandwidth and frequencies. For example, the Matlab's function \ci{lp2bp} converts a normalized lowpass into a bandpass with the desired center frequency and bandwidth as illustrated in \codl{snip_systems_lowpass_to_bandpass}.
\includecodepython{MatlabOnly}{snip\_systems\_lowpass\_to\_bandpass}{snip_systems_lowpass_to_bandpass}
%\begin{lstlisting}
%N = 4; %prototype order (bandpass order is 2N = 8)
%[B,A] = butter(N,1,'s'); %prototype with cutoff=1 rad/s
%[Bnew,Anew]=lp2bp(B,A,50,30); %convert to cutoff=50 rad/s
%freqs(Bnew,Anew) %plot mag (linear scale) and phase
%\end{lstlisting}

The functions \ci{lp2bp}, \ci{lp2bs}, \ci{lp2hp} and \ci{lp2lp} are restricted to analog filters. Transformations of digital filters can be achieved via a mapping provided by \emph{allpass filters}\index{Allpass filters}, which are filters that have unitary magnitude for all frequencies and just change the phase of the input signal. Besides frequency transformation, allpass filters are adopted for phase equalization and other applications. Some Matlab functions that provide support to digital filter transformations are \ci{iirlp2bs}, \ci{iirlp2bp}, \ci{iirlp2xn}, \ci{iirftransf}, \ci{allpasslp2xn} and \ci{zpklp2xn}.

For example, \codl{snip_systems_bandstop_conversion} compares the conversion of a Butterworth and a elliptic lowpass prototypes, both with cutoff frequency $\dw_c=0.5$~rad, into stopband versions with cutoff frequencies $\dw_1=0.5$ and $\dw_2=0.8$~rad. The two prototypes are \emph{halfband filters}\index{Halfband filters} because their cutoff frequencies is $0.5$~rad.

\includecodepython{MatlabOnly}{snip\_systems\_bandstop\_conversion}{snip_systems_bandstop_conversion}
%\begin{lstlisting}
%Wc=0.5; %cutoff (normalized) frequency of prototype filter
%Wstop = [0.5 0.8]; %stopband cutoff frequencies
%N=3; %order of prototype filter
%[B,A] = butter(N,Wc); %Butterworth prototype
%[Bnew,Anew]=iirlp2bs(B,A,Wc,Wstop); %Bandstop Butterworth
%[B2, A2] = ellip(N, 3, 30, Wc); %Elliptic prototype
%[Bnew2,Anew2]=iirlp2bs(B2,A2,Wc,Wstop); %Bandstop elliptic
%fvtool(Bnew2, Anew2, Bnew, Anew); %compare freq. responses
%\end{lstlisting}

For the comparison, it was used the Matlab's \emph{Filter Visualization Tool}\index{Filter Visualization Tool (Matlab)} (\ci{fvtool}). Note that the fourth argument of \ci{ellip} is the passband frequency (not necessarily the cutoff frequency). In this case, a maximum ripple of 3~dB was specified using the second argument of \ci{ellip} to have the passband frequency coinciding with the cutoff frequency \ci{Wc} ($\dw_c=0.5$~rad). Besides, a minimum attenuation of $30$~dB was required for the stopband of the elliptic lowpass prototype.

As another example, the following Matlab code converts a Butterworth lowpass prototype with cutoff frequency $\dw_c=0.5$~rad into a bandpass with cutoff frequencies $\dw_1=0.25$ and $\dw_2=0.75$~rad.
\begin{lstlisting}
N = 5; %filter order
[B,A] = butter(N,0.5); %prototype with cutoff=0.5 rad
[Bnew,Anew]=iirlp2xn(B,A,[-0.5 0.5],[0.25 0.75],'pass');
\end{lstlisting}
The third and fourth arguments of function \ci{iirlp2xn} (\ci{[-0.5 0.5]} and \ci{[0.25 0.75]}) indicate frequency points in the original filter and to what frequencies they should be mapped. In this case, $\dw=-0.5$~rad should be mapped to $\dw=0.25$~rad and $\dw=0.5$ to $\dw=0.75$~rad.

Instead of creating a prototype and then converting to the desired filter, most routines allow to directly design the filter.
The following commands illustrate the design of highpass, bandpass and bandstop elliptic filters.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_elliptic\_filter\_design}{snip_systems_elliptic_filter_design}
%\begin{lstlisting}
%Rp=0.1; %maximum ripple in passband (dB)
%Rs=30; %minimum attenuation in stopband (dB)
%[B,A]=ellip(N,Rp,Rs,0.4,'high'); %highpass, cutoff=0.4 rad
%[B,A]=ellip(N,Rp,Rs,[0.5 0.8],'bandpass'); %BW=[0.5, 0.8]
%[B,A]=ellip(N,Rp,Rs,[0.5 0.8],'stop'); %BW=[0.5, 0.8]
%\end{lstlisting}

Similarly, FIR design routines provide support to directly designing filters other than lowpass ones. For example,  \codl{snip_systems_FIR_filter_design} designs a stopband filter. Note the normalization by the Nyquist frequency \ci{Fs/2}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_FIR\_filter\_design}{snip_systems_FIR_filter_design}
%\begin{lstlisting}
%Fs = 44100; %sampling freq. (all freqs. in Hz)
%N  = 100;   %filter order (N+1 coefficientes)
%Fc1  = 8400;  % First cuttof frequency
%Fc2  = 13200; % Second cuttof frequency
%B  = fir1(N, [Fc1 Fc2]/(Fs/2), 'stop', hamming(N+1));
%\end{lstlisting}

Because there are plenty of routines for bandform transformation, the discussion about IIR and
FIR filter design concentrates on lowpass filters.

\section{IIR Filter Design}
\label{sec:iirFilters}

There are two main categories of IIR filter design: \emph{direct} and \emph{indirect} methods. In the latter category, one first designs a continuous-time system function $H(s)$, and in a second step converts $H(s)$ into a discrete-time filter $H(z)$. In contrast, the direct methods obtain $H(z)$ directly from the specifications.

\subsection{Direct IIR filter design}

The direct methods for IIR design are tightly related to parametric power spectrum estimation. For instance, the Yule-Walker  IIR filter design method is related to the techniques discussed in Section~\ref{sec:arModelingPSD}.

The Yule-Walker method is implemented in the Matlab function \ci{yulewalk}. Its input is a simplified description of the desired discrete-time frequency response magnitude $|H(e^{j\dw})|$, which can be arbitrary. In other words, $|H(e^{j\dw})|$ is not constrained to be the standard lowpass, highpass, bandpass or bandstop responses, which is an advantage of this method. Yule-Walker basically consists in discretizing the specification of $|H(e^{j\dw})|$ to obtain $|H[k]|$, finding the inverse DFT of $|H[k]|$ and solving modified Yule-Walker equations to obtain the filter coefficients~\cite{Friedlander84}. The phase information cannot be specified. The code below illustrates how Matlab can design a digital filter with two passbands and order 16:
\begin{lstlisting}
N = 16; %filter order
f = [0 0.3 0.3 0.6 0.6 0.8 0.8 1]; %frequencies
m = [1 1   0   0   1   1   0   0]; %magnitudes (linear scale)
[B,A] = yulewalk(N,f,m); %get H(z)=B(z)/A(z)
[h,w] = freqz(B,A,128); %frequency response with 128 points
plot(w/pi,20*log10(abs(h))) %plot magnitude in dB
\end{lstlisting}

\subsection{Indirect IIR filter design}

When the desired filter should have standard lowpass, highpass, bandpass or bandstop responses, indirect design methods are typically adopted. The reason is that there are many well-established recipes for designing analog filters $H(s)$ and also methods for mapping $H(s)$ into a discrete-time $H(z)$ filter. 

The indirect IIR filter design has the following steps:
\begin{enumerate}
	\item convert the filter specification from discrete to continuous-time,
	\item design the continuous-time $H(s)$,
	\item convert $H(s)$ into $H(z)$.
\end{enumerate}

Some practical analog filter designs (the step that obtains $H(s)$) are discussed in
\exal{analogFilterDesigns}. The emphasis in the next paragraphs is the last step: conversion of $H(s)$ into $H(z)$.

Note that, after $H(s)$ is converted into $H(z)$, the frequency response $H(\aw)$ of the former
is mapped to $H(e^{j \dw})$. When the goal of $H(z)$ is to perform filtering, it is often of  interest to compare $H(e^{j \dw})$ and $H(\aw)$. In this case, as 
discussed in Section~\ref{sec:dcConversionRevisited}, the D/S conversion of the discrete-time
impulse response corresponding to $H(e^{j \dw})$ can be used to obtain $H(e^{j \aw \ts})$ in rad/s, 
which can then be more directly compared with the original $H(\aw)$ that is also in rad/s.
In such situation, it may be worth to use subscripts $H_s(\aw)$ and $H_z(e^{j \aw \ts})$ to 
identify the original and D/S converted frequency responses, respectively. Given that 
$H_z(e^{j \aw \ts})$ is periodic, a reasonable goal may be to seek a good approximation
over the first Nyquist zone, i.\,e.
\begin{equation}
H_z(e^{j \aw \ts}) \approx H_s(\aw), \textrm{~~~over~~~} 0 \le \aw < \pi \fs.
\label{eq:goalOfConversions}
\end{equation}

Five methods to convert a continuous-time $H(s)$ into a discrete-time $H(z)$ transfer function are
briefly discussed in the sequel with an emphasis on the bilinear method, which is the most
used in designing standard filters (lowpass, highpass, etc.). 

\subsection{Methods to convert continuous into discrete-time system functions}
\label{sec:HsIntoHz}

\subsubsection{Matched Z-transform or pole-zero matching}

The \emph{matched Z-transform} method, also called \emph{pole-zero matching}
method, consists in simply using \equl{relating_s_z} to map all finite\footnote{As discussed in \cite{Hori92}, a variation of the method maps poles and zeros at $\infty$ to $z=-1$.} poles and zeros of $H(s)$ into new poles and zeros, in the z-plane, which then compose $H(z)$. In other words, assuming $s_i$ is the $i$-th zero or pole of $H(s)$, it is mapped into a corresponding zero or pole of $H(z)$ given by $z_i = e^{s_i \ts}$, where $\ts$ is the sampling interval.

After the mapping is performed, a scaling factor is
used such that the magnitude of $H(z)$ is the same of $H(s)$ for a specific frequency of
interest. For example, for lowpass systems, the DC gain is adjusted to have $H(z)|_{z=1} = H(s)|_{s=0}$.

\bExample \textbf{Matched Z-transform of a first-order system}.
The matched Z-transform of $H(s)=a/(s+a)$ is discussed in this example.
The pole at $s=-a$ is mapped to a pole at $z=e^{-a \ts}$ such that $H(z)=1/(z-e^{-a \ts})$. 
Note that $H(s)$ at DC is 1, while $H(z)$ at DC is $H(z)|_{z=1}=1/(1-e^{-a \ts}) \ne 1$.
Now, one can choose a scaling factor $g$ to create a new $H(z)=g/(z-e^{-a \ts})$
that has $H(z)|_{z=1}=H(s)|_{s=0}=1$.

In this case, the scaling factor is $g=1-e^{-a \ts}$ and the new $H(z)$ is
\begin{equation} 
H(z) = \frac{1-e^{-a \ts}}{z-e^{-a \ts}} = \frac{(1-e^{-a \ts})z^{-1}}{1-e^{-a \ts}z^{-1}}.
\label{eq:matchedZtransformExample}
\end{equation}
The extra delay $z^{-1}$ imposed by the numerator of $H(z)$ in \equl{matchedZtransformExample} is 
often eliminated and the final result is
\begin{equation} 
H(z) = \frac{1-e^{-a \ts}}{1-e^{-a \ts}z^{-1}},
\label{eq:matchedZtransformExample2}
\end{equation}
which minimizes the delay in the corresponding difference equation.
\eExample

%\subsubsection{Pole-zero placement}
When pole-zero \emph{matching} (matched Z-transform) starts from direct specification of zeros and poles, it
is sometimes called pole-zero \emph{placement}\index{Pole-zero placement}. The following example of designing a notch
filter illustrates this situation.

\bExample \textbf{Using pole-zero placement, design a 2nd-order notch filter to minimize the impact of interference at a given frequency}.
The value $f_0$ of the  frequency used by electrical power systems depends on the country and is typically $f_0=50$ or 60 Hz.
The power line signal can cause strong interference at this specific frequency and its harmonics.
For instance, a circuit that digitizes an ECG signal may have to filter out the interference at $f_0$ to obtain better results.
Here we are interested in using pole-zero placement to design a \emph{notch} filter\index{Notch filter} to reduce the
power at $f_0$. Four project guidelines will be used:

\begin{itemize}
	\item The filter will have two complex-conjugate zeros $z_1$ and $z_2$ and two complex-conjugate poles $p_1$ and $p_2$.
	\item The digital frequencies $\dw$ (the angles) of poles and zeros are the same. Given~\equl{freqdiscrete2continuous}, which
	can be written as $2 \pi f = \dw \fs$, 
	the frequency $f_0$ specified in Hz can be mapped to the digital domain as $\dw_0 = 2 \pi f_0 / \fs$, in radians.
	\item The zeros have magnitudes equal to one because they must be located on the unit circle.
	\item The poles magnitude $r$ depend on $\BW$, which is the 3~dB filter bandwidth. More specifically, $r$ can be obtained from \equl{pole_bw_discretetime} as
	\[
	r = e^{- \BW \pi / \fs},
	\]
	or from \equl{approximation_pole_bw_discretetime}, which leads to
	\[
	r \approx 1 - \frac{\BW}{\fs} \pi.
	\]
	The latter is a good approximation when $r$ is in the range $[0.9, 1[$.
\end{itemize}

These guidelines lead to the following pairs of zeros: $z_1 = e^{j \dw_0}$ and $z_2 = e^{-j \dw_0}$, while the poles are $p_1 = r e^{j \dw_0}$ and $p_2 = r e^{-j \dw_0}$.

\codl{snip_systems_notch} provides an example of notch filter design assuming $f_0=60$~Hz.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_notch}{snip_systems_notch}

The frequency response shows that this filter can attenuate the signal by more than 20~dB around the frequency $f_0$.
However, when one decreases $\BW$ to avoid filtering out components other than $f_0$, the poles get closer to the unit circle,
which may lead to unstable filters when implemented in an embedded system. For instance, if $\BW=4$~Hz, then $r=0.9691$.
If $\BW$ is decreased to $\BW=0.1$~Hz, then $r=0.9992$ and the filter attenuation is rather small because the poles are almost canceling the zeros.
\eExample

Similar to the matched Z-transform, the next filter design techniques assume $H(s)$ is available.

\subsubsection{Impulse invariance}

The \emph{impulse invariant} transformation method is based on
the criterion of sampling the continuous-time impulse response such that $h[n] = \ts h(n \ts)$, where $h(t)$ and $h[n]$ are the impulse responses
 of $H(s)$ and $H(z)$, respectively, with
\begin{equation}
H(z) = \ts \calZ \{ h(t)|_{t=n\ts}\}
\label{eq:impulseInvariant}
\end{equation}
and $\calZ \{ \cdot \}$ denoting the Z-transform. 

The impulse invariant method is restricted to strictly proper $H(s)$ (degree of numerator smaller than the denominator's). Besides, special care must be exercised when $H(s)$ has multiple-order poles.\footnote{See \cite{Cavicchi96,Nelatury07}, which discuss pitfalls of Matlab's \ci{impinvar} function.
You can also try using \ci{As=poly([-2 -2 -3])} in \codl{snip_systems_impulseinvariance} to
observe that \ci{impinvar} does not work well with a multiple-order pole ($s=-2$ in this case).
}

\bExample \textbf{Example of $H(s)$ to $H(z)$ conversion using impulse invariance}.
Assume that $\ts=0.5$ and $H(s) = 4/[(s+2)(s+3)]$. Performing a partial fraction expansion leads to
\[
H(s) = \frac{4}{s+2} + \frac{-4}{s+3},
\]
which, assuming a causal system (to define the ROC), corresponds to 
\[
h(t) = \calL^{-1} \{ H(s) \} = 4[ e^{-2t}  -  e^{-3t}] u(t).
\]
The impulse invariance is obtained after a S/D process. Using the simplified
notation of \exal{simplifiedCD}, one can write
\[
h[n] = \ts h(t)|_{t=n \ts}  = 0.5 \times 4 \left[\left(e^{-2\ts}\right)^n  - \left(e^{-3 \ts} \right)^n \right] u[n] 
\]
and then, using a procedure similar to the one in \exal{exampleZ}, leads to
\begin{align}
H(z) = \calZ \{ h[n] \} &= \frac{2}{1 - e^{-2\ts} z^{-1}} - \frac{2}{1 - e^{-3\ts} z^{-1}} =
\frac{2}{1 - 0.3679 z^{-1}} - \frac{2}{1 - 0.2231 z^{-1}} \nonumber \\
&= \frac{0.2895 z^{-1}}{1 - 0.5910 z^{-1} + 0.0821 z^{-2}}.
\end{align}
Note that the poles $s=-2$ and $s=-3$ in the s-plane were converted to
$z=e^{-2\ts}$ and $z=e^{-3\ts}$ in z-plane, respectively, which corresponds to using
\equl{relating_s_z} of the matched Z-transform method. And the numerators of the parcels
of $H(z)$ are the pole residues scaled by $\ts$.
\codl{snip_systems_impulseinvariance} illustrates the use of \ci{impinvar} in this example.
%to obtain an impulse-invariant $H(z)$.
In this specific case, the impulse invariant and matched Z-transform methods lead
to the same poles of $H(z)$.

\lstinputlisting[lastline=8,caption={MatlabOctaveCodeSnippets/snip\_systems\_impulseinvariance},label=code:snip_systems_impulseinvariance]{./Code/MatlabOctaveCodeSnippets/snip_systems_impulseinvariance.m}

The result of \codl{snip_systems_impulseinvariance} is a \co{MSE = 7.7342e-34}, which indicates
that $h[n] \approx \ts h(n \ts)$, as desired.
\eExample

When $H(s)$ has only single-order poles, the impulse invariant transformation method can
be summarized as:
\begin{enumerate}
	\item a partial fraction expansion of $H(s)$ to obtain the residue $r_i$ of each pole $p_i$ and
	\item create the parcels of $H(z)$ with poles $z=e^{p_i \ts}$ and respective
	numerators $\ts r_i$.
\end{enumerate}

\subsubsection{Step invariance}

The \emph{step invariant} transformation method is similar to the impulse invariant but uses
the step responses in continuous $q(t)$ and discrete-time $q[n]$ instead of the impulses responses. Similar to the impulse response $h(t)$, which is the system output when the input is $\delta(t)$, the step response $q(t)$ is the system output when the input is $u(t)$. The goal of this transformation is to achieve 
\begin{equation}
q[n] = q(t)|_{t=n\ts},
\label{eq:step_invarianceCriterion}
\end{equation}
which can be written as 
\begin{equation}
\calZ^{-1} \left\{ H(z) \frac{1}{1-z^{-1}} \right\} = \calL^{-1} \left. \left\{ H(s) \frac{1}{s} \right\} \right|_{t=n\ts},
\label{eq:stepInvariance}
\end{equation}
where the S/D process is denoted in the simplified notation of \exal{simplifiedCD}. 

\equl{stepInvariance} used the fact that $1/s$ and $1/(1-z^{-1})$ are the Laplace and Z transforms
of $u(t)$ and $u[n]$, respectively.
Taking the Z-transform of both sides of \equl{stepInvariance} leads to
\begin{equation}
H(z) = \calZ \left\{ \calL^{-1} \left. \left\{ H(s) \frac{1}{s} \right\} \right|_{t=n\ts} \right\} (1-z^{-1}).
\label{eq:stepInvariance2}
\end{equation}

The script \ci{snip\_systems\_stepinvariance.m} illustrates the step invariance method for a single pole $H(s)=a/(s+a)$.

\subsubsection{Backward difference and forward difference}

The \emph{backward difference} and the \emph{forward difference} methods consist in substituting $s$ by $(1-z^{-1})/\ts$ and $(1-z^{-1})/(\ts z^{-1})$, respectively. They are motivated by the
solution of differential equations using finite differences. For digital filter design these methods
are outperformed by others, and have more pedagogical than practical importance.

\subsubsection{Bilinear or Tustin's}

In the \emph{bilinear} transform\index{Bilinear transformation}  (also called Tustin's)\index{Tustin's bilinear method} method, $s$ is substituted by $(2/\ts)(z-1)/(z+1)$.
%Note that all three methods  does \emph{not} work by substituting $s = \ln(z)/\ts$ in $H(z)$, but by
%converting poles and zeros individually. In contrast, all other methods that are discussed in
%the sequel substitute $s$ by a rational function of $z$.


Before discussing the bilinear transformation in more details, 
the next subsection provides an example of each of the major IIR design techniques.

%\subsection{Summary of methods to convert $H(s)$ into $H(z)$}
\subsection{Summary of methods to convert continuous-time system function into discrete-time}

In previous sections, several methods to convert $H(s)$ into $H(z)$ were discussed.
\tabl{HsIntoHz} summarizes their application to a first order analog $H(s)=a/(s+a)$, with $a \in \Re$. 
Recall from \figl{natural_cutoff_frequency}, that for 
 $H(s)=a/(s+a)$, the pole center frequency is $\aw_0 = 0$, and the natural and cutoff frequencies $\aw_n = \aw_c = a$ coincide.

The matched Z-transform in \tabl{HsIntoHz} used \equl{matchedZtransformExample2}.

\begin{table}
\begin{center}	
	\caption{Methods to convert $H(s)$ into $H(z)$.\label{tab:HsIntoHz}}
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Mapping} & \multirow{2}{*}{$H(z)$ when} \\
       &         & $H(s)=\frac{a}{s+a}$ \\ \hline
			\multirow{2}{*}{Matched Z-transform } & Map poles or zeros at & \multirow{2}{*}{$\frac{(1-e^{-a\ts})z^{-1}}{1-e^{-a \ts}z^{-1}}$}\\
& $s=a$ to $z=e^{a\ts}$ &  \\ \hline			
Impulse-invariant & $H(z) = \ts \calZ \left\{ \calL^{-1} \left. \left\{ H(s) \right\} \right|_{t=n\ts} \right\}$ & $\frac{\ts a}{1-e^{-a \ts}z^{-1}}$ \\ \hline
Step-invariant & $H(z) = \calZ \left\{ \calL^{-1} \left. \left\{ H(s) \frac{1}{s} \right\} \right|_{t=n\ts} \right\} (1-z^{-1})$ & $\frac{(1-e^{-a \ts})z^{-1}}{1-e^{-a \ts}z^{-1}}$ \\ \hline
			Backward difference & $s=\frac{1-z^{-1}}{\ts}$ & $\frac{a}{\frac{1-z^{-1}}{\ts}+a}$ \\ \hline
Bilinear & $s=\frac{2}{\ts}\left(\frac{1-z^{-1}}{1+z^{-1}}\right)$ & $\frac{a}{\frac{2}{\ts}\left(\frac{1-z^{-1}}{1+z^{-1}}\right)+a}$ \\ \hline
%\multirow{2}{*}{Pre-warped bilinear} & $s=\frac{2}{\ts}\left(\frac{1-z^{-1}}{1+z^{-1}}\right)$ and & \multirow{2}{*}{$\frac{ \tan\left(\frac{a \ts}{2}\right)}{\left(\frac{1-z^{-1}}{1+z^{-1}}\right)+\tan\left(\frac{a \ts}{2}\right)}$} \\
%& $\aw_a = \frac{2}{\ts} \tan \left( \frac{\ts \aw_d}{2} \right)$ &  \\ \hline
\end{tabular}
\end{center}
\end{table}

Due to its importance, the bilinear transformation is further discussed in the next sections.
Section~\ref{sec:bilinear_def_properties} concerns the definition of the bilinear transformation
and its main properties. 
The discussion in Section~\ref{sec:bilinear_def_properties} considers that $H(s)$ and $\fs$ 
have already been chosen and focuses on finding $H(z)$ through the bilinear transformation, and understanding
this mapping. 
Section~\ref{sec:bilinear_design} complements this discussion by detailing the design
of system functions $H(z)$ using the bilinear transformation. This includes choosing $\fs$, 
designing $H(s)$ and finally obtaining $H(z)$.

\section{Bilinear Transformation: Definition and Properties}\index{Bilinear transformation}
\label{sec:bilinear_def_properties}

The most popular approach to designing conventional lowpass, highpass, bandpass and bandstop IIR filters is the \emph{bilinear} transformation because it is capable of preserving in $H(z)$, most of the good features of the original $H(s)$. In this method, the discrete-time system function is obtained by:
\begin{equation}
H(z) = H(s)|_{s=\frac{2}{\ts} \frac{z-1}{z+1} } = H(s)|_{s=2 \fs \frac{z-1}{z+1} }.
\label{eq:bilinearTransform}
\end{equation}
The bilinear is not a ``transform'' that relies on basis functions such as Fourier and Z. Instead, it
is a simple \emph{tranformation} from $H(s)$ in continuous-time to $H(z)$ in discrete-time.\footnote{In spite of 
being a mapping (transformation) from $H(s)$ to $H(z)$, 
calling it ``bilinear \emph{transform}'' is common in the DSP jargon.}
The following examples illustrate the bilinear transformation.

\bExample \textbf{Examples of bilinear transformations}.
\label{ex:bilinearExamples}
Assuming $\fs = 1000$~Hz, the bilinear transformation of $H(s) = \frac{1}{s + 2}$ (used in \figl{freq_response_onepole}) is 
\begin{equation}
H(z) = \frac{1}{2\fs \frac{(z-1)}{z+1} + 2} = \frac{1}{2002} \left( \frac{z+1}{z-\frac{1998}{2002}} \right) = 4.995\times 10^{-4} \left( \frac{z+1}{z-0.998} \right).
\label{eq:bilinear_simples_example}
\end{equation}
This operation can be very tedious when the order of $H(s)$ is high. But Python (\ci{scipy.signal.bilinear}) and {\matlab} have the handy \ci{bilinear} function. The result in \equl{bilinear_simples_example} could be obtained with \ci{[HzNum,HzDen]=bilinear(1,[1 2],1000)} in Matlab and \ci{[HzNum,HzDen]=bilinear(1,[1 2],1/1000)} in Octave. In Python, the commands are:
\begin{lstlisting}[language=Python]
import scipy.signal
Bs=1; # numerator B(s) in continuous-time
As=[1, 2];  # denominator A(s) in continuous-time
Fs=1000 # sampling rate in Hz
[Bz, Az]=scipy.signal.bilinear(Bs, As, Fs)
print("Numerator B(z) =", Bz, " and Denominator A(z) =", Az)
\end{lstlisting}

Note that while Python's and Matlab's \ci{bilinear} functions assume the sampling frequency $\fs$ as an input parameter, Octave adopts the sampling interval $\ts=1/\fs$. The first (Python's and Matlab's) syntax will be adopted here unless otherwise stated.

As an alternative to dealing with polynomials $B(s)$ and $A(s)$, it is possible to use \ci{bilinear} with the roots of these polynomials, by providing the poles and zeros of $H(s)=B(s)/A(s)$. In Matlab, the same $H(z)$ in \equl{bilinear_simples_example} could be obtained using the poles and zeros of $H(s)$, with: \ci{[HzZeros,HzPoles,gain]=bilinear([],-2,1,1000)}.

As a second example, consider the conversion of $H(s)=(s-3)/(s^2+4s+5)$ (the poles are $-2 \pm j$ and the zero is $3$) assuming $\fs=10$ Hz:
\[
H(z) = \frac{ {\frac{2}{\ts} \frac{z-1}{z+1}} - 3}{ \left({\frac{2}{\ts} \frac{z-1}{z+1}}\right)^2+4 \left({\frac{2}{\ts} \frac{z-1}{z+1}}\right)+5 } = \frac{0.035   -0.012 z^{-1}   -0.047z^{-2}}{1   -1.629z^{-1} + 0.670z^{-2}}
\]
which can be obtained in Matlab with the command \ci{[HzNum,HzDen]=bilinear([1 -3],[1 4 5],10)}.\\An alternative call would be
\ci{[HzZeros,HzPoles,gain]=bilinear(3,transpose([-2+j -2-j]),1,10)} that leads to
\begin{equation}
H(z) = \frac{0.035(z+1)(z-1.353)}{(z-0.81+j0.082)(z-0.81-j0.082)}.
\label{eq:example_of_hz_zeros}
\end{equation}
But note that when using the function \ci{bilinear} with poles and zeros in Matlab, one needs to use column instead of the row vectors used with transfer functions.
\eExample 


\bExample \textbf{Bilinear transformation of second order $H(s)$ systems using symbolic math}.
Symbolic math can be used to conveniently perform algebraic manipulations, such as the bilinear transformation.
In Matlab, the second order $H(s)$ given by \equl{secondOrder1} can be converted to a discrete-time version $H(z)$ using \codl{snip_systems_sos_bilinear}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_sos\_bilinear}{snip_systems_sos_bilinear} 

The bilinear transformation of \equl{secondOrder1} is obtained as
\begin{equation}
H(z) = \frac{\aw_n^2 \ts^2 (z+1)^2}{(4 + 4 \zeta \aw_n \ts + (\aw_n \ts)^2) z^2 + (2(\aw_n \ts)^2-8) z + 4 - 4 \zeta \aw_n \ts + (\aw_n \ts)^2},
\label{eq:sosBilinear1}
\end{equation}
which can be simplified using \equl{freqdiscrete2continuous}, i.\,e., substituting $\aw_n \ts = \dw_n$ to obtain
\begin{equation}
H(z) = \frac{\dw_n^2 (z+1)^2}{(4 + 4 \zeta \dw_n  + \dw_n^2) z^2 + (2\dw_n^2-8) z + 4 - 4 \zeta \dw_n + \dw_n^2}.
\end{equation}

Similarly, \codl{snip_systems_sos_bilinear} can be used to find the bilinear transformation of 
\equl{secondOrder4} as
\begin{equation}
H(z) = \frac{ (4 \zeta \dw_n+\dw_n^2)z^2 + 2\dw_n^2 z + \dw_n(\dw_n -4 \zeta)} { (4+4 \zeta \dw_n + \dw_n^2)z^2 + (2 \dw_n^2-8)z + 4 - 4 \zeta \dw_n + \dw_n^2}.
\label{eq:sosBilinear4}
\end{equation}

\codl{snip_systems_check_sosbilinear} can be used to verify \equl{sosBilinear1} and \equl{sosBilinear4}

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_check\_sosbilinear}{snip_systems_check_sosbilinear} 

Together, \codl{snip_systems_sos_bilinear} and
\codl{snip_systems_check_sosbilinear} indicate the power of combining symbolic math
and numerical functions.
%\begin{equation}
%H(z) = \frac{(z+1)   \dw_n (4 \zeta z - 4 \zeta + \dw_n z + \dw_n)}{ (4+4 \zeta \dw_n + \dw_n^2)z^2 + (2 \dw_n^2-8)z + 4 - 4 \zeta \dw_n + \dw_n^2}.
%\end{equation}
%\label{eq:}
\eExample 

\subsubsection{Derivation of the bilinear transformation}

There are several ways to explain and/or motivate the bilinear method. One of them is by modifying the matched Z-transform of \equl{relating_s_z}, which relates z and s by $z = e^{s\ts}$. Multiplying this relation by $e^{-s\ts/2} / e^{-s\ts/2}$ and using the Taylor expansion $e^x = 1 + x + x^2/2 + \ldots$ truncated to the first order $e^x \approx 1 + x$, leads to\footnote{This derivation is just illustrative and the bilinear can also be obtained by applying trapezoidal numerical integration. See alternatives in Problem~\ref{prob:series_expansion}}
\begin{equation}
z \defeq e^{s\ts} = \frac{e^{s\ts/2}}{e^{-s\ts/2}} \approx \frac{2 + s \ts}{2 - s \ts}.
\label{eq:bilinear_small_fs}
\end{equation}
Solving for $s$, one obtains
% and expanding the logarithm in a series leads to:
%\begin{align*}
%s &= \frac{1}{\ts} \ln(z)\\
%&= \frac{2}{\ts} \left[ \frac{z-1}{z+1} + \frac{1}{3} \left(\frac{z-1}{z+1}\right)^3 + \frac{1}{5} %\left(\frac{z-1}{z+1}\right)^5 + \frac{1}{7} \left(\frac{z-1}{z+1}\right)^7 + \ldots \right].
%\end{align*}
the bilinear transformation: %corresponds to the first-order approximation:
\begin{equation}
s \approx \frac{2}{\ts} \left[ \frac{z-1}{z+1} \right]
\label{eq:bilinear}
\end{equation}
and, therefore, the conversion is performed with \equl{bilinearTransform}.

The approximation $e^x \approx 1 + x$ requires $x$ to be close to 0, which in the bilinear transformation corresponds to having $s \ts \approx 0$, i.\,e., having $\ts$ small enough (a high sampling frequency) for the values of $s$ of interest.

\subsection{Bilinear mapping between s and z planes and vice-versa}

%AK: moved before to help positioning the figure
\begin{figure}[!htb]
  \begin{center}
    \subfigure[z in unit circle, $\fs=8$~Hz]{\label{fig:bilinear_map1}\includegraphics[width=6cm,keepaspectratio]{Figures/bilinear_map1}} \textrm{~~~~~~~~~~}
    \subfigure[z in unit circle, $\fs=150$~Hz]{\label{fig:bilinear_map2}\includegraphics[width=6cm,keepaspectratio]{Figures/bilinear_map2}}
    \\
    \subfigure[$s=j \aw$, $\fs=8$~Hz]{\label{fig:bilinear_map3}\includegraphics[width=6cm,keepaspectratio]{Figures/bilinear_map3}}
		\textrm{~~~~~~~~~~}
    \subfigure[s in left and right sides, $\fs=8$~Hz]{\label{fig:bilinear_map4}\includegraphics[width=6cm,keepaspectratio]{Figures/bilinear_map4}}
  \end{center}
  \caption{Examples (a) and (b) of bilinear mappings from the unit circle in z to s plane. Mappings (c) and (d) of chosen points in s to z plane. Each
	example shows the points identified by numbers in their original and mapped planes.\label{fig:bilinear_maps}}  
\end{figure}

\equl{bilinearTransform} imposes a mapping between the s and z planes that depends only on $\fs$.
The code \ci{ak\_map\_s\_into\_z.m} and \ci{ak\_map\_z\_into\_s.m} allows to explore the mapping from s into z and vice-versa, respectively. The first mapping (from \equl{bilinearTransform}) is
\begin{equation}
s=\frac{2}{\ts} \frac{z-1}{z+1}
\label{eq:bilinear_z_to_s}
\end{equation}
and the mapping from s to z can be derived by isolating z:
\begin{equation}
z= \frac{2+\ts s}{2- \ts s}.
\label{eq:bilinear_s_to_z}
\end{equation}
%which was already presented in \equl{bilinearPoleZeroMap}.

\bExample \textbf{Interpreting the mapping imposed by the bilinear transformation}.
According to \equl{bilinearTransform}, the bilinear transformation obtains the value of $H(z_0)$ 
for a given $z_0$, from the value of $H(s)|_{s=\frac{2}{\ts} \frac{z_0-1}{z_0+1} }$.
For instance, assuming $z_0 =  3+j4$ and $\ts=0.5$~seconds, then $H(z_0) = H(s)|_{s=0.75+j0.25}$.
To avoid confusion and disambiguate $H(z)$ and $H(s)$, it is useful to use a subindex and denote them $H_z(z)$ and $H_s(s)$, respectively. The result of the previous example can now be conveniently written as $H_z(3+j4) = H_s(0.75+j0.25)$.
\eExample 

\figl{bilinear_maps} shows some examples of the mapping imposed by the bilinear transformation. \figl{bilinear_map1} illustrates how 11 points uniformly spaced in the unit circle of the plane z are mapped to s when assuming $\fs = 8$~Hz. Their corresponding positions in both planes are indicated by the numbers (point 1 in z plane is mapped into point 1 in s plane, and so on). For instance, the pair of points ``1'' show that $z=1$ (DC) is mapped into $s=0$. The points in the range ``2'' to ``6'' correspond to positive frequencies, while points ``7'' to ``11'' represent negative frequencies. As a point gets closer to $z=-1$ (that is $\dw=\pi$) in plane z, its corresponding mapping into the s plane converges to $s=j\infty$ or $s=-j\infty$, depending if the limit is approached from a positive or negative frequency, respectively.

\figl{bilinear_map2} uses the same choice of points in z as in \figl{bilinear_map1}, but now with $\fs$ increased from $8$ to 150~Hz. Comparing these two figures, one can notice that the same point ``6'' in plane z leads to $s=j \aw_0$ where $\aw_0$ is larger than 2000 rad/s in \figl{bilinear_map2} and less than 120 rad/s in \figl{bilinear_map1}.

\figl{bilinear_map3} shows that the points in $s=j\aw$ are mapped into the z unit circle. \figl{bilinear_map4} illustrates that points at the left half S-plane are mapped inside the unit circle in z, while the points at the right half s plane are outside this circle. This fact explains the mentioned property of the bilinear transformation: causal and stable filters $H(s)$ are mapped to causal and stable filters $H(z)$.

Careful observation of \figl{bilinear_map3} allows to conclude that a linear spacing among the points in the s plane corresponds to a non-linear spacing in the z plane. This fact is further studied in the next subsection.

\subsection{Non-linear frequency warping imposed by bilinear}

Assume the bilinear transformation was used to map $H_s(s)$ into $H_z(z)$ (the subscripts will make easier to distinguish the two transfer functions). The current task is to observe the relation between frequencies $\dw$ and $\aw$ in the discrete and continuous-time, respectively, when the bilinear is used. This mapping between $\dw$ and $\aw$ was illustrated in \figl{bilinear_map1} to \figl{bilinear_map3}.

By the definition of bilinear in \equl{bilinearTransform}, the mapping between the 
frequency responses can be found by substituting 
$s=j \aw$ and $z=e^{j\dw}$ 
 into \equl{bilinear_z_to_s}. This leads to
\begin{equation}
j \aw = \frac{2(e^{j\dw}-1)}{\ts(e^{j\dw}+1)} = \frac{2 j \sin(\dw/2)}{\ts \cos(\dw/2)},
\label{eq:bilinearIntermediateResult}
\end{equation}
where the second equality follows from the tricks of \equl{complex_exp_trick_sin} and \equl{complex_exp_trick_cos} applied to numerator and denominator, respectively. Consequently, \equl{bilinearIntermediateResult} can be written as
\begin{equation}
\aw = \frac{2}{\ts} \tan \left( \frac{\dw}{2} \right) = 2 \fs \tan \left( \frac{\dw}{2} \right),
\label{eq:prewarping}
\end{equation}
which indicates how the bilinear relates $\aw$ (rad/s) and $\dw$ (rad).
When the bilinear transformation relates $H(z)$ and $H(s)$, the system function $H(z)$ behaves at frequency $\dw$ in
the same way that $H(s)$ behaves at frequency $\aw$. 

%\begin{equation}
%H_z(e^{j\dw}) = H_s \left( j \frac{2}{\ts} \tan \left( \frac{\dw}{2} \right) \right) = H_s \left( j \aw \right).
%\label{eq:bilinearIntermediateResult}
%\end{equation}
%corresponds to
%\[
%H_z(e^{j\dw}) = H_s \left( \frac{2(e^{j\dw}-1)}{\ts(e^{j\dw}+1)} \right),
%\]
%According to \equl{bilinearIntermediateResult}, the bilinear relates $\aw$ (rad/s) and $\dw$ (rad) as

In spite of this nonlinear frequency warping imposed by the tangent in \equl{prewarping}, the ``shape'' of $H(z)$ is typically a good approximation of $H(s)$. 
For example, the ripples are maintained.
In particular, ``equal ripple'' filters such as Chebyshev and elliptic have the ripples preserved. 

As mentioned, an arbitrary $H(s)$ such as a differentiator (see its mask in \figl{differentiator_mask}) may get severely distorted when converted to $H(z)$ using the bilinear transformation.
Hence, 
%there are other ways of converting from $s$ to $z$ such as the \emph{impulse invariant} and \emph{matched Z-transform} methods, but 
the bilinear is the most used in practice to design the conventional lowpass, highpass, bandpass and bandstop IIR filters, which aim at a flat magnitude over the passband.

\begin{figure}
\centering
\includegraphics[width=\figwidthLarge,keepaspectratio]{FiguresNonScript/frequency_axes_bilinear_simpler}
\caption{Version of \figl{frequency_axes} in which the mapping between $\aw$ and $\dw$ uses the bilinear transformation instead of the fundamental equation $\aw = \dw F_s$ of \equl{freqdiscrete2continuous}.\label{fig:frequency_axes_bilinear_simpler}}
\end{figure}

From \equl{prewarping}, it can be shown that the sampling ($\fs$) and Nyquist ($\fs/2$) frequencies, are mapped approximately to the digital frequencies $\dw=2.5$ and 2~rad, respectively. This is illustrated in \figl{frequency_axes_bilinear_simpler}.
When  $\aw \rightarrow \infty$, it is mapped by the bilinear into $\dw = \pi$~rad. In summary,
the specific values $ 0, \fs/2, \fs, \infty$  of the continuous-time frequency $f=\aw/(2\pi)$ are mapped, respectively into $\dw=0, 2, 2.5$ and $\pi$.

Note that $\aw = \dw F_s$ (\equl{freqdiscrete2continuous}), represented in \figl{frequency_axes}, is used when mapping from continuous-time to discrete-time via periodic sampling. The mapping between $\aw$ and $\dw$ illustrated in \figl{frequency_axes_bilinear_simpler} is imposed by the bilinear transformation and happens in a very distinct situation: when using \equl{bilinearTransform} to convert $H(s)$ into $H(z)$. Some examples are provided in the next paragraphs to illustrate the bilinear frequency warping.

\bExample \textbf{Nonlinear relation among frequencies obtained with the bilinear transformation}.
\figl{bilinearWarp} illustrates the plot obtained with \codl{snip_systems_bilinearmap}, which
assumes $\fs=0.5$~Hz. Note that the three passbands have the same bandwidth in $\aw$ ($b_1=b_2=b_3=0.4$~rad/s), but the bilinear mapping generates three distinct corresponding bandwidths $B_1 > B_2 > B_3$ in $\dw$.
The ``nonlinear warping'' that led to different bandwidths, would also occur in the other way around: considering a situation with equal bandwidths in $\dw$.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/bilinearWarp}
\caption{Bilinear leads to a nonlinear warping between $\aw$ (rad/s) and $\dw$ (rad).
This example uses $\fs=0.5$~Hz such that $\aw=\tan(\dw/2)$.\label{fig:bilinearWarp}}
\end{figure}

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_bilinearmap}{snip_systems_bilinearmap} 

More specifically, executing \codl{snip_systems_bilinearmap} leads to:
\begin{verbatim}
b1,b2,b3=0.4 rad/s (all have same value)
B1,B2,B3=0.48996, 0.2263, 0.11891 rad, respectively
\end{verbatim}
which confirms the nonlinear frequency warping indicated in \figl{bilinearWarp}.
\eExample 

\bExample \textbf{Example of how frequencies $\dw$ and $\aw$ are warped when related via the bilinear transformation}.
The warping of \equl{prewarping} is illustrated in \figl{bilinear_freqs} for $\fs = 100$ Hz. 

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/bilinear_freqs}
\caption{Relation imposed by the bilinear transformation between $\aw$ (rad/s) and $\dw$ (rad)  for $\fs = 100$ Hz. In this case, the frequency $\aw_0=540.4$ rad/s is mapped to $\dw_0=2.433 \pm k 2 \pi$ rad.\label{fig:bilinear_freqs}}
\end{figure}

For instance, with $\fs=100$~Hz, a given continuous-time angular frequency $\aw_0=540.4$ rad/s is mapped into $\dw_0=2.433$ rad (or equivalently the multiples $\dw_0 \pm k 2 \pi, k \in \integers$), i.\,e., $H_s(540.4) = H_z(e^{j(2.433 \pm k 2 \pi)})$. \figl{bilinear_freqs} shows the discrete-time frequencies (angles) $\dw$ corresponding to $k=-1,0,1$. 

Using the plot for $k=0$, $\aw_s=2 \pi \fs=2 \pi 100 \approx 628.3$~rad/s is mapped, as expected from \equl{prewarping}, to $\dw \approx 2.5$~rad. As discussed, the bilinear maps frequencies from $f=[0, \fs]$~Hz to $\dw=[0, 2.5]$~rad, and $f>\fs$ to $\dw=]2.5, \pi]$~rad.
\eExample 

\bExample \textbf{Using 2-d and 3-d graphs to observe how the bilinear maps s and z planes}.
\label{ex:bilinearGraphs}
This is another example that indicates how $H(s)$ is mapped into the z-plane by the bilinear transformation. 
Consider the system function 
\begin{equation}
H(s)=101(s^2-2s+1)/(s^3+3s^2+103s+101),
\label{eq:hs_for_bilinear}
\end{equation}
which can be obtained using 
\begin{lstlisting}
    Hs_num=101*poly([1 1]);
    Hs_den=poly([-1 -1+1j*10 -1-1j*10]);
\end{lstlisting}
and has $\aw=10$~rad/s as the highest pole frequency. 
The poles at $-1 \pm j10$ create a peak at $\aw=10$~rad/s, which has a frequency of $10/(2\pi) \approx 1.6$~Hz as shown in \figl{bilinear_example_differentiator}. 
This $H(s)$ was chosen because it is not a standard filter realization with a flat passband,
as illustrated in \figl{bilinear_example_differentiator}. It is not evident in \figl{bilinear_example_differentiator}, but $|H(f)|=0$ when $f \rightarrow \infty$ due to the asymptotic decay with $-20$~dB per decade imposed by the denominator order being 3 while the numerator's is 2.

%\figwidthSmall
\begin{figure}[htbp]
\centering
\includegraphics[width=5cm,keepaspectratio]{Figures/bilinear_example_differentiator}
\caption{$|H(f)|$ corresponding to $H(s)=101(s-1)(s-1)/[(s+1)(s+1-j10)(s+1+j10)]$ of \equl{hs_for_bilinear}, to illustrate the bilinear transformation.\label{fig:bilinear_example_differentiator}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/fs_on_bilinear_freqz}
\caption{Frequency responses from $H(z)$ obtained via the bilinear transformation of $H(s)$ in \equl{hs_for_bilinear} using $\fs=$ 1, 3, 5 and 7~Hz.\label{fig:fs_on_bilinear_freqz}}
\end{figure}

\figl{fs_on_bilinear_freqz} shows the magnitude (in dB) of four frequency responses
from distinct $H(z)$ obtained with the bilinear transformation of $H(s)$ using the following sampling frequencies $\fs$: 1, 3, 5 and 7~Hz. The plots illustrate aspects such as that the behavior
of $|H(f)|$ when $f \rightarrow \infty$ (goes to $- \infty$ in dB scale, in this case) is mapped to $\dw=\pi$~rad.

Comparing \figl{bilinear_example_differentiator} and \figl{fs_on_bilinear_freqz}, one can observe that the bilinear transformation may impose severe modifications when converting the function from the s to the z plane. That should be expected because it is desired to map the infinitely long $j \aw$ axis into the unit circle $e^{j \dw}$ and, to accomplish that, 
the bilinear mapping 
corresponds to the nonlinear \emph{warping} of $j \aw$ imposed by \equl{prewarping}. 

Observing \figl{fs_on_bilinear_freqz} for the extreme case of $\fs=1$~Hz and recalling
that $\fs$ is always mapped
to approximately $\dw=2$~rad, the pole peak at $f=1.6 > \fs$ in \figl{bilinear_example_differentiator} 
is then mapped to $\dw \in \textrm{~} ]2, \pi]$~rad. In this case, the linear increase
with frequency
of the magnitude in dB scale observed in $|H(f)|$ from $f=0$ to 1.6~Hz is severely distorted in $|H(e^{j\dw})|$.

%\includecode{MatlabOctaveFunctions}{bilinearPlotZPlane}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthLarge,keepaspectratio]{Figures/impact_fs_on_bilinear}
\caption[{Magnitude (in dB) for bilinear transformations of $H(s)$.}]{Magnitude (in dB) for bilinear transformations of $H(s)$ in \equl{hs_for_bilinear} for $\fs=1$  (top-left), 3 (top-right), 5 (bottom-left) and 7~Hz. The values that $H(z)$ assumes for the unit circle $|z|=1$ are indicated by the black contours. The corresponding 2-d frequency response magnitudes are depicted in \figl{fs_on_bilinear_freqz}.\label{fig:impact_fs_on_bilinear}}
\end{figure}


The 3-d plots of $|H(z)|$ for the whole Z plane can further illustrate the impact of $\fs$ in
the bilinear transformation.
The function \ci{ak\_bilinearPlotZPlane.m}
was used to obtain \figl{impact_fs_on_bilinear}. Given $H(s)$, varying $\fs$ locates the
three poles in different positions. While for $\fs=1$~Hz the pair of complex-conjugate poles
have angles closer to $\pm \pi$~rad, the angles move towards 0 when $\fs$ increases, such that
they are approximately $\pm \pi/2$~rad when $\fs=5$~Hz. This 3-d view complements the information
in \figl{fs_on_bilinear_freqz}.
\eExample
%which illustrates the impact of $\fs$ on $H(z)$.
%For each value of $z=z_0$, one gets $H(z)|_{z=z_0}$ by transforming $z_0$ into a value $s_0=(2 \fs) \frac{z_0-1}{z_0+1}$ and obtaining $H(z)|_{z=z_0} =  H(s)|_{s=s_0}$.

%\subsubsubsection{The ``pre-warping'' idea:}\index{Pre-warping}

\subsection{Tracking the frequency warping provoked by bilinear}

The next paragraphs discuss how the frequencies $\aw_c$ of a continuous-time filter $H(s)$ are warped by \equl{prewarping} and mapped into frequencies $\dw$ of discrete-time
$H(z)$, and then mapped back to continuous-time frequencies $\aw_d$ when using \equl{freqdiscrete2continuous}. Hence, one is dealing with
two frequency normalizations: 
$\aw = \frac{2}{\ts} \tan \left( \frac{\dw}{2} \right)$, which is imposed by the bilinear, and $\aw = \dw \fs$, which
is due to the uniform sampling.\footnote{Both $\aw_c$ and $\aw_d$ are given in radians per second, while $\dw$ is given in radians.}

Therefore, when an IIR filter $H(z)$ is designed using $H(s)$ as starting point and later implemented in a system with sampling frequency $\fs$, there are three frequency categories of interest:
\begin{itemize}
	\item $\aw_a$: the analog angular frequencies associated to the analog filter $H(s)$,
	\item $\dw$: the digital frequencies of $H(z)$ (or $H(e^{j \dw})$)
	\item $\aw_d$: other analog frequencies, which are related to $\dw$ via $\aw = \fs \dw$ as indicated in \equl{digitalFilterAndDC}.
\end{itemize}
It is useful to adopt a more elaborated
notation to relate these frequencies to their corresponding frequency responses. With this
goal, the system functions $H(s)$ and $H(z)$ will be denoted as $H_s(s)$ and $H_z(z)$, respectively.

As used before, the frequency response of the analog filter is $H_s(\aw_a)$, while $H_z(e^{j \dw})$
is adopted for the digital filter. When $H_z(z)$ is followed by a D/S step as in
\equl{digitalFilterAndDC}, the equivalent frequency response of this combined action will be denoted here as $H_z(e^{j \aw_d \ts})$.
The subscripts of $\aw_a$ and $\aw_d$ suggest: from ``analog'' and ``digital'' filters, respectively
(they are both ``analog'' frequencies, in rad/s, such that $\aw_d$ is \textbf{not} a digital frequency).

When dealing with the bilinear transformation and knowing the $\fs$ value that will be used, it is then useful to
rewrite \equl{prewarping} using $\aw_d = \fs \dw$ (as it was done to obtain \equl{sampledSignalSpectrum}), which leads to
\begin{equation}
\aw_a = \frac{2}{\ts} \tan \left( \frac{\ts \aw_d}{2} \right).
\label{eq:prewarping2}
\end{equation}
\equl{prewarping2} indicates that, when the bilinear transformation is adopted, $H(s)$ behaves at frequency $\aw_a$ in the same way that the continuous-time equivalent version of $H(z)$ behaves at frequency $\aw_d$. In other words, the two systems have the same gain and phase at frequencies $\aw_a$ and $\aw_d$, respectively.
The goal of the next example is to expose this relation. % between $\aw_a$ and $\aw_d$ via the bilinear transformation.

\bExample \textbf{Frequencies $\aw_a$ and $\aw_d$ in bilinear warping}.
\label{ex:bilinearWarpingFrequencies}
As an example of analog system, \codl{snip_systems_sampling} obtains
\begin{equation}
H_s(s) = \frac{-13600(s-1)}{s^5+4.2s^4+421.81s^3+1638.2 s^2 + 8407 s + 13600}
\label{eq:example_of_Hs}
\end{equation}
%\begin{align}\nonumber
%H_s(s) &=\\
%&\frac{-1.36\times(s-1)}{s^5+4.2s^4+421.81s^3+1.638 \times 10^{3} s^2 + 8.407\times 10^{3} s + 1.36\times 10^{4}}
%\label{eq:example_of_Hs}
%\end{align}
and converts it to $H_z(z)$ using \ci{bilinear}. The filter $H_s(s)$ is lowpass and its pole
with highest frequency is at $\aw=20$~rad/s, which is $f=20/(2\pi)\approx 3.2$~Hz. The adopted
sampling frequency was $\fs=10$ Hz.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_sampling}{snip_systems_sampling}
%\begin{lstlisting}
%a=1; %zeros in the s-plane
%b=-2; c=-1+j*4; d=-0.1+j*20; %poles in the s-plane
%Hs_num=poly(a); %numerator of H_s(s)
%Hs_den=poly([b c conj(c) d conj(d)]); %H_s(s) denominator
%k=Hs_den(end)/Hs_num(end); %calculate factor 
%Hs_num=k*Hs_num; %force a gain=1 at DC (s=0)
%Fs=10; %sampling frequency (Hz)
%[Hz_num, Hz_den] = bilinear(Hs_num, Hs_den, Fs); %bilinear
%\end{lstlisting}

\figl{bilinear_freq_responses} shows\footnote{Note that \figl{bilinear_freq_responses} is an extended version of \figl{dc_conversion}. The same frequency responses were used and only the y-axis ranges differ.}
 the magnitude of frequency responses of (from top to bottom) $H_s(\aw_a)$, $H_z(e^{j\dw})$ and $H_z(e^{j\aw_d \ts})$.

Note that the frequency axis of $H_z(e^{j\dw})$ has been warped\footnote{And, consequently, the axis of $H_z(e^{j\aw_d \ts})$ too, given that it is obtained via a simple linear scaling $\dw=\aw_d \ts$.} according to \equl{prewarping2}.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/bilinear_freq_responses}
\caption[{Frequency responses of $H_s(\aw_a)$ given by \equl{example_of_Hs}, $H_z(e^{j\dw})$ obtained via \ci{bilinear} and $H_z(e^{j\aw_d \ts})$ (from top to bottom).}]{Frequency responses of $H_s(\aw_a)$ given by \equl{example_of_Hs}, $H_z(e^{j\dw})$ obtained via \ci{bilinear} with $\fs=10$~Hz and $H_z(e^{j\aw_d \ts})$ (from top to bottom). Note that the intrinsic bilinear frequency warping converts $\aw_a=20$ rad/s into $\aw_d=15.71$ rad/s.\label{fig:bilinear_freq_responses}}
\end{figure}

The pole at $\aw_a = 20$ rad/s was mapped to
\[
\aw_d = \frac{2}{\ts} \tan^{-1} \left( \frac{\ts \aw_a}{2} \right) = \frac{2}{0.1} \tan^{-1} \left( \frac{0.1 \times 20}{2} \right) \approx 15.71 \textrm{~~rad/s},
\]
which can be checked via the commands:
\begin{lstlisting}
wa=20; Ts=1/10; wd=2/Ts*atan(Ts*wa/2)
\end{lstlisting}
In some situations, it is desirable to avoid that a frequency of interest ($\aw_a = 20$~rad/s in this case) becomes mapped to another frequency $\aw_d \ne \aw_a$. In such cases, a \emph{pre-warping}
procedure can be adopted, as will be discussed in Section~\ref{sec:bilinear_for_iir}.
\eExample

\subsection{{\akadvanced} Properties of the bilinear transformation}
\label{sec:bilinear_properties}

The bilinear transformation has two important properties for digital filtering applications:
\begin{itemize}
	\item it is capable of maintaining 
the order of $H(z)$ equals to the order of the original proper $H(s)$, and
  \item if $H(s)$ is causal and stable, then so it is $H(z)$.
\end{itemize}

It has also other interesting properties.
The bilinear makes any point in the s-plane to be mapped onto a unique point in the z-plane and vice-versa. It establishes a correspondence between the ``analog'' DC $s=0$ and the ``digital'' DC $z=1$. Similarly, it maps the highest digital frequency $\dw=\pi$ into the highest analog frequency $s=j\infty$.

%Comparing the two previous case, one can note that 
The bilinear transformation generates $H(z)$ with the same number of poles as in $H(s)$. 
In fact, a finite pole or zero at $s=s_0$ is mapped to a pole or zero at 
\begin{equation}
z= \frac{2+\ts s_0}{2- \ts s_0}.
\label{eq:bilinearPoleZeroMap}
\end{equation}
From \exal{bilinearExamples}, one can check that 
\ci{s=-2+j;Ts=0.1;z=(2+Ts*s)/(2-Ts*s)} outputs \co{0.81 + 0.082j}, as indicated in \equl{example_of_hz_zeros}. 

If $\ts$ is too small (make $\ts \rightarrow 0$ in \equl{bilinearPoleZeroMap}), the bilinear maps the pole (or zero) into $z=1$, which
can significantly distort the original $H(s)$. On the other hand, if $\ts \rightarrow \infty$, it
would be mapped to $z=-1$.

As depicted in \figl{bilinear_fs}, when the bilinear is used for designing IIR filters, the value of $\ts$ is used twice and cancels out. In these cases, an arbitrary and reasonable (not too large or small) value such as $\ts=1$ can be used. In cases in which $\ts$ does not cancel out (e.\,g., in some designs of a digital controller), it is important to choose a value that preserves in $H(z)$ the characteristics of interest in $H(s)$.

The bilinear can create extra zeros at $z=-1$. This behavior can be studied by applying the bilinear transformation to $H(s)=1/(s-s_0)$. For higher order $H(s)$, most of the extra zeros at $z=-1$ are canceled.\footnote{Inspect the source code of a \ci{bilinear} function to check the algorithm that deals with this cancellation. For instance, 
look at function \ci{normalize} called by Python's \ci{bilinear} at \akurl{https://github.com/scipy/scipy/blob/v1.13.0/scipy/signal/_filter_design.py}{3bin}.
}

Another aspect is that, when mapping a given $H(s)$ to $H(z)$ using the bilinear, their values can be \emph{forced to match at a single frequency value}. For filters having a single cutoff frequency such as lowpass and highpass filters, this frequency is often the chosen one.

When using the bilinear, the values of $H(s)$ and $H(z)$ can coincide in only one frequency value, but when designing IIR filters, one can obtain $H(s)$ already taking in account that the bilinear will distort the frequencies. This can be done by ``pre-warping'' the frequencies of interest and is not limited to a single frequency. In other words, it is possible to use ``pre-warping'' in all frequencies of interest (e.\,g., passband and stopband frequencies for a lowpass) to obtain $H(s)$ and, after this stage, face the bilinear restriction of matching $H(s)$ and $H(z)$ in a single frequency.
% (this strategy will be detailed when \equl{prewarping} is presented).

%For filters that have more than one cutoff frequency, the original $H(s)$ must
%be designed taking in account that the bilinear will distort the frequencies. Otherwise,
%after $H(s)$ is designed and ready to be mapped to $H(z)$, only one frequency can be chosen to
%have the two frequency responses (from $H(s)$ and $H(z)$) coinciding.


\section{System Design with Bilinear Transformation}
\label{sec:bilinear_design}

This section discusses the design of system functions $H(z)$ using the bilinear transformation. 
While $H(s)$ and $\fs$ were assumed to be known in the previous section, here 
the focus is on design, even in case $H(s)$ is not provided.

\begin{figure}
\centering
\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/bilinear_fs}
\caption{Three categories of bilinear applications and the respective choices of the sampling frequency $\fs$ in the bilinear transformation depends on the application, and can be arbitrary in the case of digital filter design.\label{fig:bilinear_fs}}
\end{figure}

The bilinear applications can be grouped in three main categories, as depicted in \figl{bilinear_fs}:
\begin{itemize}
\item \textbf{Mimic $G(s)$}: A continuous-time system function $G(s)$ already exists and the goal is to find $H(z)$ (and $\fs$)
that behaves, eventually in both time and frequency domains, as close as possible to $G(s)$. In this case, the value of $\fs$ must be chosen according to the characteristics of the involved signals and impact the hardware (ADC, DAC and other components) used in the actual digital control system deployment. 
This is the case in some digital control applications. For instance, $G(s)$ may be a proportional-integral-derivative (PID) controller\index{PID controller} that needs to be converted to a discrete-time $H(z)$ and implemented in digital hardware (see, e.\,g., \akurl{https://ctms.engin.umich.edu/CTMS/index.php?example=MotorSpeed&section=ControlDigital}{3pid}). 
This category will be
called here \emph{mimic $G(s)$}. We will denote the original system function as $G(s)$ to help distinguishing it from a continuous-time function $H(s)$ that is
designed by the bilinear transformation user.
\item \textbf{Match single frequency}: In some applications, the goal is to use the bilinear transformation
to find $H(z)$ that matches the behavior of a pre-existing continuous-time system function $G(s)$ at a single frequency.
In this case, $G(s)$ may be a lowpass filter, for instance, and the matching frequency can be the cutoff frequency $\aw_c$ of $G(s)$.
The bilinear sampling rate $\fs$ can be used to find $H(z)$ with the property of having a cutoff frequency
$\dw_c$ that maps exactly at $\aw_c = \fs \dw_c$.
\item \textbf{IIR filter design}: This category concerns the design of IIR filters $H(z)$ with
standard frequency responses: lowpass, highpass, bandpass and bandstop. In this case, even
if the filter specification was given in continuous-time, it can be converted to a discrete-time specification.
From this discrete-time specification, the designer can arbitrarily
choose the value of $\fs$ to perform pre-warping, and then design $H(s)$ using well-known filter approximation methods
such as Butterworth and Chebyshev. With $\fs$ and $H(s)$, the discrete-time filter $H(z)$
is finally obtained through the bilinear transformation.
\end{itemize}

As highlighted in \figl{bilinear_fs}, the designer can choose an arbitrary value for the sampling frequency $\fs$ when the bilinear is used to design a digital filter from a pre-warped continuous-time system function $H(s)$. In contrast, in applications that fall within the \emph{mimic} $G(s)$ category, $\fs$ should be carefully chosen. When the
goal is use the bilinear to match a single frequency, a specific value of the sampling frequency can be used to
define the matching frequency.


The next three subsections will discuss each of the
categories in \figl{bilinear_fs}.

\subsection{Bilinear for IIR filter design}\index{Pre-warping}
\label{sec:bilinear_for_iir}

The design of IIR filters using the bilinear transformation relies on first 
designing an analog filter $H(s)$ using a classic approximation method.
The most used methods are listed in \tabl{filter_approximations}. 
In applications in which the phase linearity is important, good options
are the Bessel and Butterworth. However, their magnitude response does not
represent a magnitude roll-off as steep as the other options.
If the phase does not need to be approximately linear, 
Cauer (or elliptic) filters achieve a sharper roll-off and, consequently,
a lower filter order for a given project specification.
However, Cauer and Chebyshev Type I present ripple along the passband frequencies,
which may be a problem in some applications.
When compared to Chebyshev Type I, the Chebyshev Type II approximation has the advantage of 
having ripple only in the stopband.

\begin{table}
    \centering
    \caption{Main approximations adopted in the design of continuous-time filters $H(s)$ with their pros and cons
		with respect to phase and magnitude (mag.).}
    \label{tab:filter_approximations}
    \begin{tabular}{ccc}
        \toprule
        Approximation & Pros & Cons \\
        \midrule
				Bessel & Phase linearity & smooth (poor) mag. roll-off  \\
        Butterworth  & Flat passband mag. & gradual (poor) mag. roll-off \\
        Chebyshev Type I  & Steeper mag. roll-off  & ripple in passband \\
        Chebyshev Type II & Steeper mag. roll-off  & ripple only in stopband \\
        Cauer (or elliptic) & Sharp (good) mag. roll-off & Non-linear phase and ripples \\
        \bottomrule
    \end{tabular}
\end{table}

This subsection assumes the goal is to design an IIR filter $H(z)$ by applying the bilinear transformation to the designed $H(s)$.
The next paragraphs will emphasize the importance of pre-warping frequencies and finding the order of the analog filter $H(s)$ that complies with the project requirements.

In IIR filter design, there is no concern about the value of the sampling frequency.
% in spite of the non-linear frequency warping imposed by the bilinear transformation.
In this case, before obtaining the $H(s)$ that will be adopted for the
bilinear transformation, it is important to \emph{pre-warp}\index{Pre-warping in bilinear transformation} all frequencies of interest
using \equl{prewarping}. This way, the non-linear behavior imposed by
the bilinear transformation (see, 
e.\,g. \figl{bilinear_freqs} and \figl{bilinearWarp}) is pre-compensated by the filter designer. 
%Pre-warping is used to properly specify any of the frequencies of interest that should be used in the design of $H(s)$.

A key message is: 
\begin{itemize}
\item \emph{When designing a filter using the bilinear transformation, all frequencies of interest must be pre-warped and the design of $H(s)$ based on these modified frequencies. When $H(z)$ is obtained, the bilinear
transform will ``undo'' the pre-warping and locate the frequencies in the desired positions.}
\end{itemize}

\figl{bilinear_iir} summarizes the steps when using the bilinear transformation for IIR design.
Note that the value of the sampling rate $\fs$ is chosen in Step 2 and reused in Step 4, which then
cancels out the influence of $\fs$. This will be discussed later, after some examples of IIR design
with bilinear.

\begin{figure}
\centering
\includegraphics[width=\figwidthLarge,keepaspectratio]{FiguresNonScript/bilinear_iir}
\caption{Steps of IIR filter design using the bilinear transformation when the filter requirements are in discrete-time (frequencies in radians).\label{fig:bilinear_iir}}
\end{figure}


The following two examples illustrate pre-warping and finding the minimum order of a filter that
complies with the project requirements.
These projects use the \ci{bilinear} function for pedagogical reasons. But instead of using this function, one could directly design IIR filters using higher-lever functions such as \ci{butter}, \ci{cheby2} and others, which 
automatically invoke a bilinear routine.

\bExample \textbf{Design a digital lowpass filter using the bilinear transformation}.
\label{ex:bilinear_simple}
The task is to design a Butterworth lowpass digital filter $H_z(z)$ with 
passband $\dw_p=\pi/5$ and stopband $\dw_s=3\pi/10$ frequencies, respectively, both
in rad. Note that the specifications are in discrete-time. 
The maximum and minimum attenuations in pass and stopbands are $A_p=4$~dB and 
$A_s=10$~dB, respectively.

In order to use classic methods for designing analog filter $H(s)$, the discrete-time frequencies
$\dw_p$ and $\dw_s$ are converted to continuous-time assuming an arbitrary sampling frequency. Here we will assume
$F'_s=0.5$~Hz for convenience. Hence, using \equl{prewarping} the frequencies of interest are pre-warped to $\aw_p=\tan(\dw_p/2) \approx 0.325$ and $\aw_s=\tan(\dw_r/2) \approx 0.509$~rad/s.

To find the minimum order of a Butterworth that obeys the requirements, one can use:
\begin{lstlisting}
Ap=4; % maximum attenuation in passband (in dB)
As=10; % minimum attenuation in stopband (in dB)
wp=0.325; % passband frequency in rad/s
ws=0.509; % stopband frequency in rad/s
[n,w_cutoff] = buttord(wp,ws,Ap,As,'s')
[Bs,As]=butter(n,w_cutoff,'s')
\end{lstlisting}
The second-order Butterworth with
3-dB cutoff frequency of 0.294~rad/s obeys the corresponding mask and has the system function 
$H_s(s)=0.086/(s^2 + 0.416s + 0.086)$.
In Python,\footnote{When using \ci{buttord} and \ci{butter}, Python does not lead to the same results as {\matlab}~\akurl{https://stackoverflow.com/questions/28056404/matlab-and-scipy-give-different-results-for-buttord-function}{3bor}.} the commands are:
\begin{lstlisting}[language=Python]
import scipy.signal
Ap=4; # maximum attenuation in passband (in dB)
As=10; # minimum attenuation in stopband (in dB)
wp=0.325; # passband frequency in rad/s
ws=0.509; # stopband frequency in rad/s
[n,w_cutoff] = scipy.signal.buttord(wp ,ws ,Ap ,As ,analog=True)
[Bs, As]= scipy.signal.butter(n, w_cutoff, analog=True)
\end{lstlisting}
Note that, while {\matlab} identifies a continuous-time system with \ci{'s'}, Python uses \ci{analog=True}.

Having obtained $H_s(s)$,
using \equl{bilinearTransform} with $F'_s=0.5$~Hz leads to
\begin{align*}
H(z) &= \frac{0.086}{ \left( \frac{z-1}{z+1} \right)^2 + 0.416 \left( \frac{z-1}{z+1} \right) + 0.086} \\
 &= \frac{0.086(z+1)^2}{ \left( z-1 \right)^2 + 0.416 \left(z-1\right)\left(z+1\right) + 0.086 \left(z+1\right)^2} \\
 &\approx \frac{0.0575 + 0.1150z^{-1} + 0.0575 z^{-2} }{1   -1.2166 z^{-1} +    0.4466 z^{-2} }.
\end{align*}
The frequency response of the designed $H(z)$ has magnitudes $10 \log_{10}|H_z(e^{j\dw_p})|^2 = -3.96$ and 
$10 \log_{10}|H_z(e^{j\dw_r})|^2 = -10$~dB, which obey the requirements. Note that there is some slack
at $\dw_p$ while the filter matches exactly\footnote{Matlab tries to match the requirement at the stopband frequency while Python's SciPy tries to match the requirement at the passband frequency~\akurl{https://stackoverflow.com/questions/28056404/matlab-and-scipy-give-different-results-for-buttord-function}{3bor}.} the requirement at $\dw_s$.
\eExample

\bExample \textbf{Design a digital bandpass filter using the bilinear transformation}.
The goal is to design a filter with a bandpass from $0.2\pi$ to $0.4\pi$~rad, and stopbands from 0 to $0.1\pi$ and from $0.5\pi$ to $\pi$~rad.
The filter must have less than 3~dB of ripple in the passband and at least 30~dB of attenuation in the stopbands.
In this example, we will adopt $F'_s=0.5$~Hz (an arbitrary value) and the Chebyshev Type II approximation, which does not have the ripples at the passband that Chebyshev Type I has.

\codl{snip_systems_iir_bilinear} indicates how to perform this filter design
using \matlab.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_iir\_bilinear}{snip_systems_iir_bilinear} 

In case the sampling frequency $F'_s$ is changed (line \ci{Fs=0.5}) in \codl{snip_systems_iir_bilinear}, one can observe
that this does not alter the final filter $H(z)$ given by \ci{[Bz, Az]}.
\eExample 

\subsubsection{Bilinear sampling rate is arbitrary in IIR filter design}

\exal{bilinearGraphs} shows the strong impact that $\fs$ may have when the bilinear transformation is applied to a given continuous-time system $H(s)$. However, when using the bilinear in the specific task of designing a digital filter, the chosen value for $\fs$ will cancel out and have no influence on the design. 
This was mentioned in \figl{bilinear_fs} and can be experimentally observed by changing $F'_s$
in \exal{bilinear_simple} or in \codl{snip_systems_iir_bilinear}. This fact is demonstrated here
through the design of a simple first-order digital filter $H(z)$ with cutoff frequency $\dw_c$.

The following code uses symbolic math to obtain $H(s)$ after pre-warping and, then using bilinear to obtain $H(z)$.
\begin{lstlisting}[language=Matlab]
%define s, z, natural frequency, damping ratio and sampling interval
syms Wc wc b0 a0 s z Fs %all as symbolic variables
%1) Filter requirements: 1st-order Butterworth, cutoff freq. Wc rad
%2) pre-warping
wc = 2*Fs*tan(Wc/2);
%3) Design H(s) with pre-warped frequencies. A first-order
% Butterworth with cutoff frequency of 1 rad/s is H(s)=1/(s+1).
Hs_prototype = 1/(s+1);
% Use frequency scaling s=s/wc to move the cutoff from 1 to wc rad/s
Hs=subs(Hs_prototype,s,s/wc); % Continuous-time H(s).
pretty(Hs) % Use pretty(Hs) to see it
%4) Bilinear transformation
Hz=subs(Hs,s,(2*Fs)*((z-1)/(z+1))); %bilinear: s <- 2/Ts*(z-1)/(z+1)
Hz=simplify(Hz); %simplify the expression 
disp('H(z) obtained with the bilinear transform:')
pretty(Hz) %show it using an alternative view
\end{lstlisting}

We want to show that the value of the sampling rate $F'_s$ can be chosen arbitrarily in this IIR design. There is only one frequency of interest in this case to be pre-warped, which leads to $\aw_c=2 F'_s \tan(\dw_c/2)$.
Starting with the first-order Butterworth prototype $H_p(s)=1/(s+1)$ with cutoff frequency equals to 1~rad/s,
one uses frequency scaling to move the cutoff frequency to the pre-warped frequency $\aw_c$. Hence, after Step 3 of the code (which implements Step 3 of \figl{bilinear_iir}), one has:
\[
H(s) = H_p \left( \frac{s}{\aw_c} \right) = \frac{1}{\frac{s}{\aw_c}+1}= \frac{1}{\frac{s}{2 F'_s \tan(\dw_c / 2) }+1}.
\]
After the bilinear transformation in Step 4, the code outputs:
\[
H(z) = H(s)|_{s=2\fs \frac{z-1}{z+1}} =  \frac{1}{\frac{z-1}{\tan(\dw_c / 2)(z+1) }+1}.
\]
One can observe that $H(z)$ does not depend on $F'_s$ because the value of $F'_s$ was canceled. This also
happens for filters of order larger than 1.

\bExample \textbf{Another example of the arbitrary $F'_s$ getting canceled out}.
\label{ex:fs_canceled_out}
Assume a second-order digital lowpass filter $H(s)=1/(s^2 + \sqrt{2}s + 1)$ (a normalized Butterworth) meets 
the requirements in continuous-time. We want to use the bilinear transformation to obtain $H(z)$ with a 3-dB cutoff frequency
of $\pi/4$~rad.

We can choose any convenient value for $F'_s$ and to illustrate that,
instead of using a specific numeric value, the development
will be made for a generic $F'_s$. 

\equl{prewarping} is adopted to perform pre-warping, which leads to
\[
\aw_c =  2 F'_s \tan \left( \frac{\pi/4}{2} \right) \approx 0.8284 F'_s.
\]
Then, \equl{frequencyScaling} is used to perform frequency scaling and move the cutoff frequency $\aw_c=1$~rad/s of
the prototype filter to $\aw_c = 0.8284 F'_s$. Substituting $s$ by $s/\aw_c = s /(0.8284 F'_s)$
leads to
\[
H(s) = \frac{1}{\left( \frac{s }{0.8284 F'_s} \right)^2 + \sqrt{2} \left( \frac{s }{0.8284 F'_s} \right) + 1}
\]
Now, the bilinear can be applied to $H(s)$, which leads to
\[
H(z) = H(s)|_{s=\left(2 F'_s \frac{z-1}{z+1} \right)} = \frac{0.0976 z^2 +0.1953 z +0.0976}{z^2 - 0.9428 z + 0.3333}.
\]
Note that $F'_s$ was canceled out because it only appeared in $H(s)$ dividing $s$, which is
later substituted by $2 F'_s \frac{z-1}{z+1}$.
\eExample

In conclusion, the $F'_s$ value adopted for the bilinear can be different from the one actually used when implementing the digital filter $H(z)$ (for instance, in an embedded system with ADC and DAC chips). We will use $F'_s$ to denote the value used in the bilinear transformation and reserve $\fs$ to the adopted sampling frequency when implementing $H(z)$, as depicted in \figl{frequency_axes_bilinear}.

\begin{figure}
\centering
\includegraphics[width=\figwidthLarge,keepaspectratio]{FiguresNonScript/frequency_axes_bilinear}
\caption{Version of \figl{frequency_axes} and \figl{frequency_axes_bilinear_simpler} that considers both 
the bilinear transformation and the conversion of continuous to discrete-time via $\aw=\dw \fs$.\label{fig:frequency_axes_bilinear}}
\end{figure}

Hence, in this very specific application of bilinear, the sampling frequency $F'_s$ can be chosen arbitrarily. This may sound confusing\footnote{For instance, the bilinear is discussed in \cite{Oppenheim09} always in the context of IIR filter design and $\fs$ is assumed to be always an arbitrary value.} because, as discussed here, there are other applications of the bilinear transformation in which the value of $\fs$ must be carefully chosen.
In IIR design, one just needs to avoid choosing a too small value for $F'_s$, which could eventually lead to numerical issues in the design of $H(s)$ due to an extreme pre-warping of frequencies of interest.
 Hence, for convenience, $F'_s$ is chosen to be 0.5, 1 or 2~Hz in IIR design.

\figl{frequency_axes_bilinear} takes in account that two
sampling frequencies can be used in a bilinear transformation: $F'_s$ and $\fs$.
The former is used in the transform itself while the latter
is adopted for the actual implementation of $H(z)$ in a filtering hardware such as the one depicted in \figl{canonical_interface}.

\bExample \textbf{Design an IIR filter with the bilinear transformation using frequency scaling of an analog prototype filter}.
The task is to design on Matlab (Octave has a slightly different syntax) a third-order lowpass Butterworth digital filter $H(z)$ using the bilinear transformation of
$H_p(s) = 1/(s^3+2s^2+2s+1)$ (that can be obtained with \ci{[Bs,As]=butter(3,1,'s')}), which has a cutoff frequency $\aw_c = 1$~rad/s.
Other project requirements are a sampling frequency of $\fs=500$ Hz and $H(z)$ with a cutoff frequency $f_d = 100$~Hz (i.\,e., $\aw_d = 2\pi f_d \approx 628.3$~rad/s).

Pre-warping must be used because $\aw_c=1$~rad/s is the cutoff frequency of the prototype $H_p(s)$ that 
has to be mapped to $f_d=100$~Hz when $H(z)$ is implemented.

%$f_d$ and $\fs$ can
%be considered as part of $\phi$ in continuous-time. Converting to discrete-time provides
%$\dw = \aw_d / \fs \approx 1.26$ rad.

%In this case, the answer to Block 5 in \figl{bilinear_steps} is ``no'', such that Block 6 could
%be used. But

We will arbitrarily adopt $F'_s = \fs = 500$~Hz.
The goal is to find the frequency $\aw_a$ to which $\aw_d = 2  \pi f_d$ is mapped to.
\equl{prewarping2} leads to
\[
\aw_a = 2 \fs \tan \left( \frac{\aw_d}{2 \fs} \right) = 2 \times 500  \tan \left( \frac{2 \pi 100}{2 \times 500} \right) \approx 726.52~\textrm{rad/s},
\]
which corresponds to $115.63$~Hz.

Now, frequency scaling must be applied to $H_p(s)$ in order to move its cutoff frequency from $\aw_c=1$ to $\aw_a=726.52$~rad/s. Substituting $s \leftarrow s'/ 726.52$  in $H_p(s)$ leads to
\[
H(s) = \frac{3.8348 \times 10^8}{s^3+ 1.453 \times 10^3 s^2+ 1.0557 \times 10^6 s+ 3.8348 \times 10^8}.
\]
Applying the bilinear to $H(s)$ with $F'_s = \fs = 500$~Hz leads to
$H(z) = (0.0985 + 0.2956 z^{-1} +  0.2956z^{-2} + 0.0985 z^{-3})/(1   -0.5773 z^{-1} +    0.4218 z^{-2} -0.0563 z^{-3})$. Evaluating $|H(z)|$ at $z=e^{j\dw}$, with $\dw = 2 \pi f_d / \fs$, one obtains $1/\sqrt{2}$. This indicates that, due to the pre-warping, the bilinear positioned the cutoff of $H(z)$ at the desired value of 100~Hz.

The next paragraphs illustrate how a computer can be used to carry out the whole procedure.
A first approach is to design an analog filter by first pre-warping the cutoff frequency and then use the bilinear:
\begin{lstlisting}
fs=500; %sampling frequency (Hz)
[Bs,As]=butter(3,2*pi*115.63,'s'); %analog filter
[Bz,Az]=bilinear(Bs,As,fs); %convert analog to digital
z=exp(1j*2*pi*100/500); abs(polyval(Bz,z)/polyval(Az,z))%check cutoff
\end{lstlisting}
The second and simpler approach (at least from the user's perspective) is to directly use a routine that incorporates pre-warping and bilinear transformation, such as \ci{butter}:
\begin{lstlisting}
Fs=500;fd=100; fnormalized=fd/(Fs/2); [B2,A2]=butter(3,fnormalized)
\end{lstlisting}
(recall that {\matlab} deals with digital filters using frequencies in Hertz normalized by the Nyquist frequency as informed in \tabl{nyquist_frequency}). 
A comparison would show that \ci{[Bz,Az]} and \ci{[B2,A2]} have approximately the same elements.

A third way of dealing with a software routine that directly designs a digital filter is
to convert the specification to discrete-time. In this case, $\dw = \aw_d / \fs \approx 1.2566$~rad
is the desired cutoff frequency and $H(z)$ can be obtained with:
\begin{lstlisting}
Fs=500;Wd=2*pi*100/Fs;[B3,A3]=butter(3,Wd/pi)
\end{lstlisting}

Note that in this and in the previous (second) approach, the function \ci{butter.m} was not informed
about $\fs$. Consequently, when the pre-warping was done inside \ci{butter.m}, it used
an arbitrary value of $F'_s$ to pre-warp, then obtained $H(s)$, and converted using bilinear with $F'_s$, such that the value of $F'_s$ gets canceled out, as previously explained.
\eExample

\subsubsection{Bilinear IIR design can start from continuous-time specifications}

In previous IIR designs, the filter specification was provided in the discrete-time domain.
However, the design of a digital filter may also start from specifications in the analog domain.
\figl{bilinear_steps} summarizes a project flow that
includes both options when designing a filter $H(z)$ using the bilinear transformation.

%Later, an alternative project flow is discussed, in which $H(s)$ is already provided.

\begin{figure}
\centering
\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/bilinear_steps}
%\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/bilinear_steps}
\caption{Steps for designing an IIR filter $H(z)$ using the bilinear transformation, assuming a lowpass filter. 
The set of requirements can be in discrete ($\Phi$) or continuous-time ($\phi$) domain.\label{fig:bilinear_steps}}
\end{figure}

\figl{bilinear_steps} considers the possibilities of having a set of
filter requirements specified either in continuous ($\phi$) or discrete-time ($\Phi$). 
For example, a discrete-time lowpass filter
can be specified by $\Phi=\{\dw_p,\dw_r,A_p,A_r\}$, where the subscripts $p$ and $r$
distinguish the frequencies and attenuations of the passband and stopband, respectively.
Similarly, $\phi=\{F_p,F_r,A_p,A_r,\fs\}$ is a possible lowpass filter requirement in 
continuous-time, as depicted in \figl{lowpass-mask}. 
Note that $\phi$ includes the sampling frequency $\fs$ used in the hardware implementation 
of the digital filter while $\Phi$ does not.

In case the IIR design starts with filter requirements
provided in discrete-time, $\fs$ is not part of the design process and $F'_s$ is chosen arbitrarily.
The designer executes the steps ``$3-4-5$'' depicted in \figl{bilinear_steps},
as done in \exal{bilinear_simple}.


%\textbf{Anticipating the D/S conversion after $H(z)$ via $\aw=\dw \fs$:}

%With \figl{bilinear_freqs} in mind, note a natural next step: having a digital frequency $\dw_0=2.433$ rad, one can use \equl{freqdiscrete2continuous} to obtain $\aw_0 = \dw_0 \fs = 2.433 \times 100 = 243.3$ rad/s, which corresponds to $f_0 = \aw_0 / (2 \pi) \approx 38.72$ Hz. Hence, it is useful to anticipate this step, as discussed in the sequel.

%In summary, the pre-warping of the bilinear transformation can be described by \equl{prewarping}
%or, alternatively, \equl{prewarping2}. 

The next example illustrates a design using bilinear in which the specification $\phi$ of the filter
$H(z)$ to be designed is given in continuous-time, which includes the sampling frequency $\fs$ that will actually be used when implementing the filter.

%\bExample \textbf{Design a digital lowpass filter using the bilinear transformation with the
%specification in continuous-time}.
\bExample \textbf{Bilinear design of IIR filter from continuous-time specifications}.
\label{ex:ct_bilinear_example}
%\bExample \textbf{Design a digital lowpass filter using the bilinear transformation with the specification in continuous-time}.
%\label{ex:ct_bilinear_example}
The task is to design an elliptic lowpass digital filter $H_z(z)$ that when 
implemented on a hardware operating at $\fs=500$~Hz obeys the requirements
$\phi = \{100, 150, 4, 20, 500 \}$, where $\phi=\{F_p,F_r,A_p,A_r,\fs\}$ with
frequencies given in Hz.
%$\phi=\{\aw_p,\aw_r,A_p,A_r,\fs\}$.

According to Block 2 of \figl{bilinear_steps}, using $\aw=\fs \dw$ (\equl{freqdiscrete2continuous}) and the definition
of angular frequency $\aw = \fs f$, the continuous-time specifications
$\phi$ can be converted to the discrete-time specification $\Phi= \{0.4 \pi, 0.6 \pi, 4, 20\}$, where $\Phi=\{\dw_p,\dw_r,A_p,A_r\}$
with frequencies given in radians.
After this, one can continue to use $\fs=500$~Hz or a new value for the sampling frequency
can be chosen. For pedagogical purposes, a new value $F'_s=0.5$~Hz is chosen here.
Note that an important point is to use $\fs=500$~Hz when implementing $H_z(z)$ on ``hardware'', not $F'_s$, which
will be exclusively used for the design of $H(z)$.

From \equl{prewarping} with $F'_s=0.5$, the frequencies of interest are pre-warped to $\aw_p=\tan(\dw_p/2) \approx 0.7265$ and $\aw_r=\tan(\dw_r/2) \approx 1.3764$~rad/s. 
Given the pre-warped specifications in continuous-time $\phi' = \{0.7265, 1.3764, 4, 20, 0.5 \}$, one can  obtain
the second-order elliptic filter $H_s(s)=(0.1s^2+0.24)/(s^2 + 0.3542s + 0.3803)$ that obeys the corresponding mask. 
This design of the analog filter $H_s(s)$ can be done with classical approximations and respective software,
such as using \ci{ellipord} and \ci{ellip} in {\matlab} (see \codl{snip_systems_iir_elliptic}).

\equl{bilinearTransform} (with $F'_s=0.5$~Hz) allows to convert $H_s(s)$ into $H_z(z) = (0.196 + 0.1614 z^{-1} +  0.196z^{-2})/(1   -0.7145 z^{-1} +    0.5915 z^{-2})$, which has $10 \log_{10}|H_z(e^{j\dw_p})|^2 = -4$ and 
$10 \log_{10}|H_z(e^{j\dw_r})|^2 = -29.961$~dB. Note that there is significant slack
(almost 10~dB) at $\dw_r$ while the approximation matches exactly the requirement at $\dw_p$.
\codl{snip_systems_iir_elliptic} indicates how to perform this filter design
using Matlab.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_iir\_elliptic}{snip_systems_iir_elliptic} 

Executing \codl{snip_systems_iir_elliptic} allows to confirm that, due to pre-warping, the values of
\ci{Hs\_at\_wp} and \ci{Hs\_at\_wr} coincide with \ci{Hz\_at\_Wp} and \ci{Hz\_at\_Wr}, respectively.
\eExample 

As discussed in the previous example, even when the filter specification is given in continuous-time,
before the bilinear, the analog filter $H(s)$ can be designed with several pre-warped frequencies of interest.
For instance, two frequencies were pre-warped in \exal{ct_bilinear_example}.
A different situation occurs when the bilinear must be
applied to a given $G(s)$. In this case, there is only one ``degree of freedom'': the choice
of $F'_s$. Hence, it is not possible to use pre-warping to match 
the behavior of $G(s)$ and $H(z)$ at several frequencies. But it is possible to match
their behavior in a single frequency. This is the topic of the next subsection.

\subsection{Bilinear for matching a single frequency}
%\subsubsection{Using the bilinear sampling frequency to match frequencies}

When $G(s)$ and $\fs$ are given and there is a single frequency of interest to be ``matched'' in the continuous and discrete-time domains, it is possible to use $F'_s$ to take pre-warping in account and reach a matching between $G(s)$ and $H(z)$.
This ``matching'' frequency strategy can also be used to design IIR filters when there is a single frequency of interest.

The single ``match'' frequency $\aw_m$ is $\aw_m = \aw_a = \aw_d$~rad/s. 
 From \equl{prewarping2}:
\begin{equation}
\aw_m = 2 \fs \tan \left(  \frac{\aw_m}{2 \fs} \right),
\label{eq:matching_freq_bi}
\end{equation}
which can be writte as
\[
\fs =  \frac{\aw_m}{ 2\tan \left(  \frac{\aw_m}{2 \fs} \right) }.
\]
The trick is to consider that $\fs$ at the left side is a new value $F'_s$, which can be chosen to impose the matching:
\begin{equation}
F'_s =  \frac{\aw_m}{ 2\tan \left(  \frac{\aw_m}{2 \fs} \right) },
\end{equation}
while $\fs$ in the tangent is the provided (original) $\fs$ value.
Instead of using the angular frequency $\aw_m =  2 \pi f_m$, one can use the linear frequency $f_m = \aw_m / (2 \pi)$ Hz.
In summary, the matching can be imposed by\footnote{In Matlab (version R2022a), inspecting the source code of the \ci{bilinear} function with
the command \ci{type bilinear} lists the command \ci{sampleFreq = pi*preWarp/tan(pi*preWarp/sampleFreq)}, which corresponds
to \equl{bilinearMatchingFreq}.}
\begin{equation}
F'_s =  \frac{\aw_m}{ 2\tan \left(  \frac{\aw_m}{2 \fs} \right) } = \frac{\pi f_m}{ \tan \left(  \frac{\pi f_m}{\fs} \right) }.
\label{eq:bilinearMatchingFreq}
\end{equation}

To avoid the discontinuity of $\tan(\pi/2)$, one has to choose $\frac{\pi f_m}{\fs}<\pi/2$, which
corresponds to $\fs > 2 f_m$, which is consistent with the sampling theorem. As expected, having
a pre-defined $\fs$ restricts $f_m < \fs/2$ to be less than the Nyquist frequency.

Hence, instead of using \equl{bilinearTransform}, it is possible to
 incorporate pre-warping such that $\aw_m$ is the match frequency using:
\begin{equation}
H(z) = G(s)|_{s=\frac{\aw_m}{ \tan \left(  \frac{\aw_m}{2 \fs} \right) } \left( \frac{z-1}{z+1} \right) }.
\label{eq:bilinearWithMatchingViaFs}
\end{equation}

\bExample \textbf{Simple example of matched $G(s)$ and $H(z)$ at specific frequency $f_m$~Hz.}
Assume $f_m = 10$~Hz and $H(z)$ was obtained from 
$G(s)$ using \equl{bilinearWithMatchingViaFs} with sampling frequency $\fs=30$~Hz.
Using \equl{bilinearMatchingFreq}, the value
of $H(z)$ at the angle $\dw_m=2\pi f_m/\fs \approx 2.1$~rad matches the value of $G(s)|_{s=j20\pi}$
at $f_m=10$~Hz. 

In case $\fs=60$~Hz and $f_m = 10$~Hz, then it is at the angle $\dw_m=2\pi f_m/\fs \approx 1.05$~rad that the value of $H(z)$ matches $G(s)|_{s=j20\pi}$.

In general, for any value of $f_m < \fs/2$, $f_m$ is a matching frequency when the bilinear uses \equl{bilinearMatchingFreq}.
\eExample 

\figl{bilinear_givenHs} is the equivalent of \figl{bilinear_steps} for scenarios of bilinear use 
in which the goal is to match a single frequency and a continuous-time system function $G(s)$ is provided.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/bilinear_givenHs}
%\includegraphics[width=\textwidth,keepaspectratio]{FiguresNonScript/bilinear_givenHs}
\caption{Steps for matching a single frequency $\aw_m$ using the bilinear transformation when a continuous-time system function $G(s)$ and $\fs$ are provided.\label{fig:bilinear_givenHs}}
\end{figure}


Similar to what was done starting from \equl{matching_freq_bi}, \equl{prewarping} can also be used to find $F'_s$ that maps a specific pair $\dw_d$ and $\aw_a$ as follows:
\begin{equation}
F'_s = \frac{\aw_a}{ 2\tan \left(  \frac{\dw_d}{2} \right) }.
\label{eq:bilinearPairWithMatchingViaFs}
\end{equation}

Adapting \equl{goalOfConversions} to the current notation, the goal of pre-warping a single frequency
is to obtain:
\begin{equation}
H_z(e^{j \aw_d \ts}) \approx H_s(\aw_a).
%\label{eq:bilinearTarget}
\end{equation}
over the first Nyquist zone (see Section~\ref{sec:dcConversionRevisited}).

Some extra examples of the bilinear
transform are discussed in the sequel.

\bExample \textbf{Pre-warping for matching a single frequency applied to the system of \exal{bilinearWarpingFrequencies}}.
Matching a single frequency using the \ci{bilinear} function is easily done in {\matlab}. For example, the change in frequency of the pole at $\aw_a=20$ rad/s in \figl{bilinear_freq_responses} can be avoided using
\ci{[Hz\_num, Hz\_den] = bilinear(Hs\_num, Hs\_den, Fs, 20/(2*pi))} instead of \ci{[Hz\_num, Hz\_den] = bilinear(Hs\_num, Hs\_den, Fs)}.
\figl{bilinear_responses_prewarped} illustrates the result of pre-warping. Note that, because in this case $H_s(s)$ is already given, only for a single frequency the bilinear can achieve $\aw_a=\aw_d$, while all others are non-linearly shifted.

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/bilinear_responses_prewarped}
\caption{Version of \figl{bilinear_freq_responses} for which \ci{bilinear} used pre-warping for obtaining $\aw_a=\aw_d=20$ rad/s.\label{fig:bilinear_responses_prewarped}}
\end{figure}

In this example, there was a single frequency of interest to be matched. If there are several frequencies of interest, they all need to be pre-warped, before the design of $H(s)$.
\eExample


%Whenever possible, it is convenient to start from a specification in discrete-time domain when
% designing an IIR filter using the bilinear. In this case, as mentioned,
%the value of $\fs$ (or $\ts$) is arbitrary as further discussed in the following example.

\bExample \textbf{Design an IIR filter by matching a single frequency}.
%\bExample \textbf{IIR design using bilinear when starting from a specification in discrete-time}.
The task here is the same as in \exal{fs_canceled_out}: design a second-order digital lowpass filter with a 3-dB cutoff frequency
of $\pi/4$~rad by using the bilinear transformation on the prototype filter $H(s)=1/(s^2 + \sqrt{2}s + 1)$.
There is a single frequency 
$\aw_m$ to be matched, which is the cuttof frequency of $H(s)$. The behavior of $H(s)$ at $\aw_m$ should match the behavior at $\dw_m = \pi/4$ of $H(z)$.
Using \equl{bilinearPairWithMatchingViaFs} to match these frequencies leads to $F'_s = 1/(2 \tan(\pi/8)) \approx 1/0.8284 \approx 1.2071$~Hz. Using \equl{bilinearTransform} leads to
\begin{align*}
H(z) &= \frac{1}{ \left( \frac{2 \times 1.2071 (z-1)}{z+1} \right)^2 + \sqrt{2} \left( \frac{2 \times 1.2071 (z-1)}{z+1} \right) + 1} \\
 &\approx \frac{0.0976 + 0.1953z^{-1} + 0.0976 z^{-2} }{1   -0.9428 z^{-1} +    0.3333 z^{-2} }.
\end{align*}

Using Matlab, the following commands implements this shortcut:
\begin{lstlisting}
Fs = 1/(2*tan(pi/8)); [Bz,Az]=bilinear(1,[1 sqrt(2) 1],Fs)
\end{lstlisting}
which does not explicitly use pre-warping in the \ci{bilinear} function, but
incorporates it on the choice of $F'_s$.
\eExample

\bExample \textbf{Pre-warped bilinear applied to a first-order system}.
\tabl{HsIntoHz} summarized several methods to convert a first-order $H(s)$ into $H(z)$.
Here we extend this table by creating \tabl{HsIntoHz_prewarped}, which incorporates the pre-warped bilinear transformation.

\begin{table}
\begin{center}	
	\caption{Pre-warped bilinear as a method to convert $H(s)$ into $H(z)$.\label{tab:HsIntoHz_prewarped}}
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Mapping} & \multirow{2}{*}{$H(z)$ when} \\
       &         & $H(s)=\frac{a}{s+a}$ \\ \hline
Bilinear & $s=\frac{2}{\ts}\left(\frac{1-z^{-1}}{1+z^{-1}}\right)$ & $\frac{a}{\frac{2}{\ts}\left(\frac{1-z^{-1}}{1+z^{-1}}\right)+a}$ \\ \hline
\multirow{2}{*}{Pre-warped bilinear} & $s=\frac{2}{\ts}\left(\frac{1-z^{-1}}{1+z^{-1}}\right)$ and & \multirow{2}{*}{$\frac{ \tan\left(\frac{a \ts}{2}\right)}{\left(\frac{1-z^{-1}}{1+z^{-1}}\right)+\tan\left(\frac{a \ts}{2}\right)}$} \\
& $\aw_a = \frac{2}{\ts} \tan \left( \frac{\ts \aw_d}{2} \right)$ &  \\ \hline
\end{tabular}
\end{center}
\end{table}

The bilinear with pre-warping in \tabl{HsIntoHz_prewarped} is based on first modifying $H(s)$ using
\equl{prewarping2}. In other words, the pole with cutoff frequency $\aw_c=a$ in $H(s)$ is pre-warped to obtain
$\aw_d = a$ in the end, after the conversion using bilinear. This is done with $\aw_a = (2/\ts) \tan(\ts a/2)$
and frequency scaling, substituting $s$ by $s/\aw_a$. This leads to
\[
H(s)=\frac{\frac{2}{\ts} \tan\left(\frac{a \ts}{2}\right)}{s+\frac{2}{\ts} \tan\left(\frac{a \ts}{2}\right)}.
\]
Applying the bilinear mapping to this $H(s)$ leads to the result in \tabl{HsIntoHz}.

An alternative shortcut is to use \equl{bilinearWithMatchingViaFs} with $\aw_m = a$~rad/s as the ``matching frequency''.
This also leads to the result in \tabl{HsIntoHz} 
\eExample 

%AK TO_DO:
%\subsubsection{Designing our own conversion from $s$ to $z$ (analog to digital)}


%AK-PUTBACK - acho que a impulse invariance eh a abaixo.
%First attempt: move $\aw/2$ to $\pi$.
%seria legal compara-la.

%\subsection{Bilinear $H(z)$ filter design when $H(s)$ is already provided}
%\subsubsection{Other scenarios for the use of bilinear}
\subsection{Bilinear for mimicking G(s)}
\label{sec:bilinearAdvanced}

In the following discussion, $G(s)$ is already provided and the bilinear should be used to transform
it, or a scaled version of $G(s)$, into $H(z)$. An example of application is
the task of obtaining a discrete-time version $H(z)$ of a given continuous-time controller $G(s)$ that is applied to a \emph{plant}. In this case, the sampling frequency can be chosen
according, e.\,g., to the sampling theorem (\equl{samplingTheoremRealSignals}).

Consider a control system (see 
\akurl{https://ctms.engin.umich.edu/CTMS/index.php?example=MotorSpeed&section=ControlDigital}{3pid}) that uses a PID
controller\index{PID controller} given by
\[
G(s) = 100 + \frac{200}{s} + 10s.
\]
The dominant pole of the plant $P(s)$ to be controlled\footnote{To simplify the discussion, the plant $P(s)$ is not presented here.} is $s=2$, which suggests a settling time of approximately 2 seconds.
Therefore, the sampling frequency is chosen as $\fs=20$~Hz, which is fast enough when compared to the time constants
involved in the dynamics of $G(s)$.
Using bilinear transformation, the digital version of this PID controller is
\[
H(z) = \frac{505 z^2 - 790 z + 305}{z^2 - 1},
\]
which should be adjusted for the task of automatic control until the system presents good behavior in closed loop.

In the case of mimicking $G(s)$, there is not a small set of frequencies of interest as in IIR filter design. The most effective strategy to 
improve the bilinear transformation and better mimic $G(s)$ is to increase $\fs$ to reach a good approximation in \equl{bilinear_small_fs}.

\section{FIR Filter Design}
\label{sec:firFilters}

This section discusses filters that have impulse responses with finite duration.

\subsection{A FIR filter does not have finite poles}

Essentially, most FIR filters are non-recursive and have a transfer function that can be written as $H(z)=B(z)$, i.\,e., the denominator is $A(z)=1$ in \equl{transferfunction}.
Given this denominator, all poles of FIR filters are located in $z=0$ or $|z|=\infty$ (while IIR filters do not have this restriction). For example, $H(z)=1+z^{-1}+z^{-2}$ has a pole of order (or multiplicity) 2 at $z=0$. You can check that with \ci{zplane([1 1 1],1)} on {\matlab}\footnote{Note that Matlab shows the pole multiplicity in \ci{zplane} while Octave version 3.2.3 does not.} or note it by rewriting as $H(z)=(z^{2}+z+1)/z^2$.
Another example is the noncausal system $H(z)=z^{2}+z+1$, which has a pole of multiplicity 2 at infinity.\footnote{See \akurl{http://en.wikipedia.org/wiki/Pole_(complex_analysis)}{3poi} for a brief discussion about poles at infinity.} 

\subsection{The coefficients of a FIR coincide with its impulse response}
An interesting property of FIR filters is that the coefficients of $H(z)=B(z)$ (also called ``taps''\index{Tap (filter)}) are exactly the non-zero values of the filter's impulse response $h[n]$. For example, using the inverse Z-transform on $H(z)=3+4z^{-1}+5z^{-2}$, one obtains $h[n]=\calZ^{-1}\{H(z)\}=3\delta[n]+4\delta[n-1]+5\delta[n-2]$.

This property sometimes leads to confusing commands. For instance, the arguments \ci{Bz} and \ci{Az} to method \ci{freqz(Bz,Az)}
are polynomials in Z transform domain. But for FIR filters, it is possible to substitute \ci{Bz} by the impulse
response \ci{hn} as follows:
\begin{lstlisting}
hn=[2, 1, 3]; % define some arbitrary FIR impulse response
freqz(hn, 1); % find the DTFT corresponding to H(z)=B(z)/A(z)
\end{lstlisting}
Note in this code that $A(z)=1$ for an FIR filter.

\subsection{Algorithms for FIR filter design}

\begin{figure}
\centering
\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/differentiator_mask}
\caption{Mask for a differentiator filter specified with the syntax for arbitrary filter magnitudes.\label{fig:differentiator_mask}}
\end{figure}

There are several algorithms for both FIR and IIR.
Instead of using pre-defined functions such as Butterworth's, a FIR design is typically done with alternative methods, which seek to optimize a given criterion. The most common algorithm is the \emph{least-squares}\index{Least-squares FIR design}, which is implemented in {\matlab} via \ci{firls}. Another algorithm is \emph{Parks-McClellan}, implemented in Matlab's \ci{firpm}, but not available in Octave 3.2.3.

These and other optimization-based algorithms are executed according to a syntax that allows specifying an arbitrary function, not restricted to be lowpass, highpass, etc.
\figl{differentiator_mask} illustrates the specification of a desired magnitude response of a \emph{differentiator}\index{Differentiator}, which has a gain in passband that is proportional to frequency. 
An arbitrary function will be specified in $N_B$ bands of interest. For each band $b$, four parameters are necessary: the start $f_b^s$ and end $f_b^e$ frequencies, the desired function values  (the magnitude in this context) $A_b^s$ and $A_b^e$ at the start and end frequencies. An optional parameter is the weight of each band (e.\,g., W1 and W2 in \figl{differentiator_mask}). The larger the weight, the higher the importance given to that band in the design process.

In {\matlab}, the function \ci{fir2} implements another algorithm for designing a FIR with arbitrary frequency response magnitude, using a different syntax. Instead of working with bands, \ci{fir2} allows the specification of the desired magnitude at frequencies of interest.

An example of using least-squares for FIR design with \ci{firls} is discussed in the sequel.

\subsection{FIR design via least-squares}

The least-squares method tries to minimize the total squared error between the desired function and the obtained approximation over the specified band. It is possible to give a different weight to each band. The larger the weight, the smaller the error in that band. A vector \ci{W} with weights is an optional input parameter of \ci{firls}.

For example, an ideal lowpass with cutoff $\dw=\pi/2$ rad can be specified with $N_B=2$ bands. The  (normalized) frequencies could be $f_1^s=0, f_1^e=0.5, f_2^s=0.5, f_2^e=1$ and the amplitudes $A_1^s=1, A_1^e=1, A_2^s=0, A_2^e=0$. These values are organized in vectors
\ci{F=[0 0.5 0.5 1]} and \ci{A=[1 1 0 0]} and a $M=100$-order FIR can be obtained with \ci{B=firls(100,F,A)}.
Note that \ci{A} and \ci{F} have the same length, which must be $2 N_B$ (even number), and \ci{W}  must have length equal to $N_B$ if specified. The default is to weight all bands equally, which corresponds to \ci{W} with all elements equal to 1.

\begin{figure}
 \centering
    \subfigure[1st passband prioritized]{\label{fig:fir_first_band_important}\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/fir_first_band_important}}
    \subfigure[2nd passband prioritized]{\label{fig:fir_second_band_important}\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/fir_second_band_important}}
  \caption[{Frequency response of filters obtained with \ci{firls}.}]{Frequency response of filters obtained with \ci{firls}. In (a), the first passband had a larger weight than the second, while in (b) it was the opposite.}
  \label{fig:fir_bands}
\end{figure}

\figl{fir_bands} compares the frequency responses of two filters obtained with \ci{firls}. \figl{fir_first_band_important} was obtained with \codl{snip_systems_firls}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_firls}{snip_systems_firls}
%\begin{lstlisting}
%f=[0 0.25 0.3 0.55 0.6 0.85 0.9 1]; %frequencies
%A=[1 1 0 0 1 1 0 0]; %amplitudes
%M=20; %filter order
%W=[1000 1 1 1]; %weights, first passband is prioritized
%B=firls(M,f,A,W);
%freqz(B);
%\end{lstlisting}
The horizontal lines were superimposed to indicate the bands in which the gain should be 0 dB.
\figl{fir_second_band_important} was obtained with the same commands, but changing \ci{W} to \ci{W=[1 1 1000 1]}. Note that the impact of \ci{W} is visible with the band that is not prioritized having larger deviations.

\subsection{FIR design via windowing}
\label{subsec:fir_windowing}

The \emph{windowing} method for FIR design is implemented in \ci{fir1}.
In terms of computational cost, the method is very simple: the FIR impulse response $h[n]$ is obtained by multiplying the impulse response of an ideal filter (which has an infinite duration) by a finite-length sequence $w[n]$ called \emph{window} with $M+1$ non-zero samples. Recall that the coefficients $B(z)$ of a FIR coincide with its $h[n]$.
For the case of an ideal lowpass filter, the impulse response is given by
\begin{equation}
h_{\textrm{LP}}[n] = \frac{\sin(\dw_c n)}{\pi n},
\label{eq:idealLowpass}
\end{equation}
which is the inverse DTFT of a ``rectangular pulse'' in frequency-domain.

The impulse response for the ideal highpass filter can be obtained by
noting that $H_{\textrm{HP}}(e^{j \dw}) = 1 - H_{\textrm{HP}}(e^{j \dw})$ and converting this
to time-domain leads to 
\begin{equation}
h_{\textrm{HP}}[n] = \delta[n] - h_{\textrm{LP}}[n].
\label{eq:idealHigpass}
\end{equation}

The steps to obtain a FIR filter $h_w[n]$ of order $M$ (assumed to be an even number here) via the windowing method are:
\begin{enumerate}
	\item \textbf{Obtain the impulse response for the corresponding ideal filter}: $h[n]$ has an infinite
	duration but the next  steps will require only $M+1$ of its coefficients.
	\item \textbf{Choose a window}: Pick a window $w[n]$ with $M+1$ non-zero coefficients in the range $[-M/2, M/2]$.	
	\item \textbf{Get a filter with finite order $M$ (i.\,e., $M+1$ coefficients)}: Multiply the chosen window $w[n]$ by the ideal filter impulse response $h[n]$ to get a temporary
	\begin{equation}
	h^t_w[n] = h[n] w[n].
	%\label{eq:}
	\end{equation}	
	\item \textbf{Make the filter causal}: Delay $h^t_w[n]$ such that the first $h_w[n]$ coefficient is at $n=0$:
	\begin{equation}
	h_w[n] = h^t_w[n - M/2].
	%\label{eq:}
	\end{equation}
\end{enumerate}

The DTFT of $h[n]$, $w[n]$, $h^t_w[n]$ and $h_w[n]$ are $H(e^{j\dw})$, $W(e^{j\dw})$, $H^t_w(e^{j\dw})$ and $H_w(e^{j\dw})$, respectively. \figl{fir_design} illustrates the effect of multiplying a signal
($h[n]$ in this case) by a finite-duration window. This is a very useful approach for modeling
the process of truncating an infinite-duration signal for further processing. This model
allows to calculate, for instance, the spectrum of the finite-duration signal based on
the convolution between the original and the window spectra. This is useful not only
in FIR design, but many other situations such as segmenting a signal into frames.

\begin{figure}
\centering
\includegraphics[width=\figwidthLarge,keepaspectratio]{FiguresTex/fir_design}
\caption{FIR design described in both time and frequency domains.\label{fig:fir_design}}
\end{figure}

In windows-based FIR design, the filter frequency response $H_w(e^{j\dw})$ is the
 result of the (circular) convolution
between $H(e^{j\dw})$ and $W(e^{j\dw})$, as indicated in \figl{fir_design}. The ripples
(ringing) 
observed in $H_w(e^{j\dw})$ are located at the transitions of $H(e^{j\dw})$ and
indicate that the main lobe of $W(e^{j\dw})$ dictates the transition band and
the amplitude of $W(e^{j\dw})$ sidelobes determine the attenuation at the stopband.


The properties of the filter are completely dependent on the adopted window type. The most common windows are named after their inventors: Hamming, Blackman, Kaiser and Hann (also called the hanning window). Among these four, the Kaiser is the only one with an extra degree of freedom provided by a parameter $\beta$. The other three are completely specified by the order $M$. 

The rectangular window is useful to better understand the windowing process. The rectangular
is 1 for $M+1$ samples and 0 otherwise.
For example, a $4$-th order lowpass filter with $\dw_c = \pi/3$ can be designed with a rectangular windows according to \codl{snip_systems_rectangular_window}.

\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_rectangular\_window}{snip_systems_rectangular_window}



There is support in {\matlab} for obtaining windows other than the rectangular. For example, the command \ci{w=window(@hamming,10)} obtains a Hamming window with 10 non-zero samples, which can be used to obtain a FIR of order $M=9$ with cutoff frequency $\dw=\pi/2$ rad via the command \ci{B=fir1(9,0.5,w)}.

The adopted window determines the frequency response of the associated FIR designed via windowing.
Section~\ref{sec:windows} discusses the characteristics of windows in more details. 

One tricky feature of \ci{fir1} is that it generates filters with cutoff frequency $\dw_c$ that corresponds to a gain of $-6$~dB (0.5 in linear scale). In contrast, the cutoff frequency of a filter obtained with \ci{butter} and other IIR filter design functions corresponds to a gain of $-3$~dB ($1/\sqrt{2}$ in linear scale). Another aspect is that, if the order $M$ is too small when using \ci{fir1}, the gain at $\dw_c$ may not reach $-6$~dB. This also happens with other filter design procedures.

\subsection{Two important characteristics: FIRs are always stable and can have linear phase}

Because practical FIR filters have all $M+1$ coefficients with finite values, \equl{bibo_stable} indicates that all FIR filters are BIBO stable. Besides, FIR filters can easily have linear phase, which is a desirable property in many applications (such as telecommunications, as will be discussed in Application~\ref{app:linear_filtering}). It can be proved that the linear phase is achieved if and only if there is symmetry in $h[n]$ about some value $n_s$. For example, a FIR $H(z)=1.5 -2 z^{-1} + 5 z^{-2} - 2 z^{-3} + 1.5 z^{-4}$ of order $M=4$ has 5 non-zero samples in $h[n]$ and exhibit symmetry about $n_s=2$, which corresponds to $h[2]=5$, such that $h[n]=h[4-n]$. 
The phase is linear over frequency because, for the given example:
\begin{align}
H(e^{j\dw}) & = \calF\{h[n]\} \\
 & = \calF\{1.5\delta[n] - 2 \delta[n-1] + 5 \delta[n-2] - 2 \delta[n-3]\nonumber\\
 & \phantom{\mathrel{=}{}} + 1.5 \delta[n-4]\} \\
 & =  1.5 - 2 e^{-j\dw} + 5 e^{-j2\dw} - 2 e^{-j3\dw} + 1.5 e^{-j4\dw} \\
 & =  e^{-j2\dw} \left( 1.5e^{j2\dw} - 2 e^{j\dw} + 5 - 2 e^{-j\dw} + 1.5 e^{-j2\dw} \right) \\
 & =  e^{-j2\dw} \left[ 1.5 (e^{j2\dw}+e^{-j2\dw}) -2 (e^{j\dw}+e^{-j\dw}) + 5 \right]\\
 & =  e^{-j2\dw} \left[ 3 \cos(2\dw) -4 \cos(\dw) + 5 \right]\\
 & =  e^{-j2\dw} A(\dw),
\label{eq:linear_phase_example}
\end{align}
where $A(\dw)=3 \cos(2\dw) -2 \cos(\dw) + 5$ is a real function that provides the magnitude $|H(e^{j\dw})|$, such that the phase $e^{-j2\dw}$ of $H(e^{j\dw})$ is linear $-2 \dw$, given by the factor . %In this case, if 

As \equl{linear_phase_example} indicates, a symmetric FIR can always be decomposed as
\[
H(e^{j\dw}) = e^{-j \dw \tau_g} A(\dw),
\]
where $A(\dw)$ is a real function and $\tau_g$ is the \emph{group delay}, to be further discussed along with \equl{discreteTimeGroupDelay}.

\subsection{Examples of linear and non-linear phase filters}

When the system has linear phase it is relatively easy to calculate its group delay because it coincides with the slope of a straight line. \codl{groupDelayLinearPhase} uses the FIR from \equl{linear_phase_example} to show an example of calculating the slope from two points \ci{k1} and \ci{k2}, comparing it with the result of the \ci{grpdelay} function:

\lstinputlisting[caption=Group delay estimation for linar phase system ,label=code:groupDelayLinearPhase]{./Code/MatlabOctaveCodeSnippets/snip_systems_groupdelay.m}
%\begin{lstlisting}[caption=Group delay estimation for linear phase system, label=code:groupDelayLinearPhase]
%h=[1.5 -2 5 -2 1.5]; %coefficients of symmetric FIR
%N=8; %number of FFT points
%Fs=44100; %sampling frequency (Hz)
%H=fft(h,N); %calculate FFT of N points
%f=Fs/N*(0:N/2); %create abscissa in Hz, where Fs/N is the bin width
%p = unwrap(angle(H(1:N/2+1))); %calculate the phase in rad
%plot(f,p), xlabel('f (Hz)'), ylabel('Phase (rad)'), pause
%k1=2; k2=4; %choose two points of the line
%%Calculate derivative as the slope. Convert from Hz to rad/s first:
%groupDelay = -atan2(p(k2)-p(k1),2*pi*(f(k2)-f(k1))) %in seconds
%groupDelayInSamples=round(2*groupDelay*Fs)/2; %quantize with step=0.5
%grpdelay(h,1,f(1:N/2+1),Fs); %find delay for positive frequencies
%\end{lstlisting}
Line 11 used \ci{round} to avoid numerical errors. The trick of multiplying by 2 and after rounding divide by 2 allows to take into account that a symmetric FIR, unless the estimation was troubled due to the existence of zeros on the unit circle, has group delay that can be represented with a quantizer of step $\Delta=0.5$ sample.

%AK-PUTBACK
%\subsection{Pole-zero representation}

%AK-PUTBACK
%\subsection{Geometric evaluation of the Fourier and Z transforms from the pole-zero plot}

\figl{groupDelayLinearPhase} and \figl{groupDelayNonLinearPhase} were obtained by executing \codl{ak_plotGroupDelay} for the filters 
\ci{[0.3 -0.4 0.5 0.8 0.5 -0.4 0.3]} and \ci{[0.3 -0.4 0.5 0.8 -0.2 0.1 0.5]}, respectively.
\lstinputlisting[caption=Code for plotting the group delay and phase.,label=code:ak_plotGroupDelay]{./Code/MatlabOnly/ak_plotGroupDelay.m}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/groupDelayLinearPhase}
\caption{Group delay and phase for a channel represented by a symmetric FIR with linear phase and constant group delay of 3 samples.\label{fig:groupDelayLinearPhase}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/groupDelayNonLinearPhase}
\caption{Group delay and phase for a channel represented by a non-symmetric FIR \ci{h=[0.3 -0.4 0.5 0.8 -0.2 0.1 0.5]} with non-linear phase.\label{fig:groupDelayNonLinearPhase}}
\end{figure}

The filter of \figl{groupDelayLinearPhase} has a linear phase while the one in \figl{groupDelayNonLinearPhase} does not.
It can be seen from \figl{groupDelayNonLinearPhase} that even causal systems can have a negative group delay for some frequencies.

\subsection{Zeros close to the unit circle may impact the phase linearity}
\label{sec:symmetricFIRnonlinearPhase}

An often overlooked fact is that
%that symmetric coefficients is not a sufficient condition for a FIR presenting linear phase in a graph due to the influence of $A(\dw)$. 
if $A(\dw)$ approaches zero, the phase graph is not guaranteed to be visualized as a linear function of $\dw$. For example, the first filter (from \equl{linear_phase_example}) in the script below shows a  linear phase via the \ci{freqz} function, but the second does not due to a zero at approximately $e^{\pm j 1.994}$:
\begin{lstlisting}
h1=[1.5 -2 5 -2 1.5]; freqz(h1); pause %has linear phase
h2=[-0.7 6 4 6 -0.7]; freqz(h2); %does not have linear phase
abs(roots(h2)), angle(roots(h2));%h2 has zeros on unit circle!
\end{lstlisting}
The function \ci{grpdelay} properly treats the zeros on the unit circle and indicates $\tau_g=2$ samples for both \ci{h1} and \ci{h2} filters.
Because conventional filters such as lowpass and bandpass do not have zeros at the passband, the symmetry of their coefficients guarantees linear phase within the band of interest and the discussed behavior often manifest itself only in the stopband.

Another way of interpreting the problem caused by zeros on the unit circle is to note that
they lead to a zero magnitude. If the magnitude of a complex number is zero or close 
enough to zero, its phase will not be important.

\subsection{Four types of symmetric FIR filters}

Depending on the symmetry, there are four types of linear-phase FIR filters.
The filter of \equl{linear_phase_example} is classified as a linear-phase FIR of type I.
In general, a type I has an order $M$ that is even (the length of $h[n]$ is odd) and is symmetric $h[n]=h[M-n]$. \tabl{fir-types} summarizes all types.

\begin{table}
\centering
\caption[Types of linear-phase FIR filters]{Types of linear-phase FIR filters. It is assumed that the filters are causal with the first non-zero sample at $n=0$. Type III filters have $h[M/2]=0$.\label{tab:fir-types}}
\begin{tabularx}{\textwidth}{c>{\centering}p{2.8cm}CCC}
\toprule
Type & Symmetry & Order ($M$) & $H(e^{j\dw})|_{\dw=0}$ (DC) & $H(e^{j\dw})|_{\dw=\pi}$ (Nyquist) \\
\midrule
I & Symmetric: $h[n]=h[M-n]$ & even & any value & any value\\
II & Symmetric: $h[n]=h[M-n]$ & odd & any value & 0\\
III & Anti-symmetric: $h[n]=-h[M-n]$ & even & 0 & 0\\
IV & Anti-symmetric: $h[n]=-h[M-n]$ & odd & 0 & any value\\
\bottomrule
\end{tabularx}
\end{table}

Recall that, because $H(e^{j\dw})|_{\dw=0} = H(z)|_{z=1}$, the behavior of a FIR filter at DC can be obtained with 
\[
H(z)|_{z=1}= \sum_{k=0}^M b_k = \sum_{n=0}^M h[n].
\]
Hence, the anti-symmetric filters in \tabl{fir-types} are restricted to have a zero at DC.
Similarly, at the Nyquist frequency $\dw=\pi$ rad, $H(e^{j\dw})|_{\dw=\pi} = H(z)|_{z=-1}$, and for a type III FIR with order $M$ even:
\begin{align*}
H(z)|_{z=-1}&= \sum_{k=0}^M b_k = \sum_{k=0}^{M/2} [b_{2k} (-1)^{2k} + b_{2k+1} (-1)^{2k+1}]\\
&= \sum_{n=0}^{M/2} (h[2n] - h[2n+1]) = h[M/2]=0,
\end{align*}
where the first parcels in the summations correspond to even ($2k$ or $2n$) and the second parcels to odd coefficients.
This kind of relation is better visualized via examples as in \codl{snip_systems_FIR_types}. 

\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_FIR\_types}{snip_systems_FIR_types}
%\begin{lstlisting}
%hI=[1 2 3 2 1] %Type I FIR
%hII=[1 2 2 1]  %Type II FIR
%hIII=[1 2 0 -2 -1] %Type III FIR
%hIV=[1 2 -2 -1] %Type IV FIR
%disp(['I: DC=' num2str(polyval(hI,1)) ...
%    ' Nyquist=' num2str(polyval(hI,-1))])
%disp(['II: DC=' num2str(polyval(hII,1)) ...
%    ' Nyquist=' num2str(polyval(hII,-1))])
%disp(['III: DC=' num2str(polyval(hIII,1)) ...
%    ' Nyquist=' num2str(polyval(hIII,-1))])
%disp(['IV: DC=' num2str(polyval(hIV,1)) ...
%    ' Nyquist=' num2str(polyval(hIV,-1))])
%\end{lstlisting}
Running \codl{snip_systems_FIR_types} outputs 
\begin{verbatim}
I: DC=9 Nyquist=1
II: DC=6 Nyquist=0
III: DC=0 Nyquist=0
IV: DC=0 Nyquist=2
\end{verbatim}
as expected from \tabl{fir-types}. Because the gain at DC is zero, linear-phase FIR filters of type III and IV are useful to operate as differentiator filters, for example.

\figl{fir_symmetries} shows the impulse and frequency responses for the four types of symmetric
FIR filters exemplified in \codl{snip_systems_FIR_types}. Note that the type II FIR in this
case (\ci{hII=[1 2 2 1]}) has three zeros on the unit circle (use \ci{abs(roots(hII))}) 
and its phase has a discontinuity in spite of a constant group delay (use \ci{grpdelay(hII)}), 
as warned in Section~\ref{sec:symmetricFIRnonlinearPhase}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidthLarge,keepaspectratio]{Figures/fir_symmetries}
\caption{Impulse and frequency responses for the four types of symmetric FIR filters
exemplified in \codl{snip_systems_FIR_types}.\label{fig:fir_symmetries}}
\end{figure}

\section{Realization of Digital Filters}

Having $H(z)$, the next step is to choose a \emph{realization}\index{Filter realization}. For example, the difference equation $y[n] = 5x[n] - 5x[n-1]$ could be implemented as $y[n] = 5 (x[n] - x[n-1])$ to save computation. In this case, instead of two multiplications followed by a subtraction, the alternative requires only one subtraction and one multiplication. However, a difference equation is not adequate to distinguish realizations. For example, algebraically, they are of course the same: $y[n] = 5 x[n] - 5x[n-1] = 5 (x[n] - x[n-1])$. In order to specify realizations of digital filters, a flow chart or diagram is used. The ``delay block'' is represented by \fbox{$z^{-1}$}, such that its output is the previous value of the input. For example:
\[
x[n] \rightarrow\boxed{z^{-1}}\rightarrow x[n-1]
\]
and
\[
y[n] \rightarrow\boxed{z^{-1}}\rightarrow \boxed{z^{-1}} \rightarrow \boxed{z^{-1}} \rightarrow y[n-3].
\]
The output of each delay block needs to be stored in RAM memory, such that the number of delay blocks should be minimized if one aims at minimizing memory consumption. The \emph{filter memory}\index{Filter memory} is the set of values taken from the output of the delay blocks, which are stored for using in the next iteration.

Other blocks are the multiplier and adder. These three help to indicate the order of operations when implementing a difference equation. \figl{fir_realizations} depicts the two realizations mentioned previously. A careful observation concludes that such diagrams still have some ambiguity with respect to the order of operations but they are more descriptive than a difference equation.

\begin{figure}
\centering
    \subfigure[]{\label{fig:filter_realization_1}\includegraphics[width=4.5cm,keepaspectratio]{FiguresTex/filter_realization_1}}
\textrm{~~~~~~}    \subfigure[]{\label{fig:filter_realization_2}\includegraphics[width=5.5cm,keepaspectratio]{FiguresTex/filter_realization_2}}
  \caption{Two distinct realizations of $y[n]=5x[n] - 5x[n-1]$.}
  \label{fig:fir_realizations}
\end{figure}

At this point, it may not be clear that the value of $y[n]$ can be distinct in the two cases of \figl{fir_realizations}. This may happen when one compares these two filter realizations using a finite number of bits to represent numbers. For example, assume that the hardware used to implement the equations provides only 8 bits to represent integer numbers in the range $[-128,127]$ in two's complement. Say that $x[0]=85$ and $x[1]=92$ and the goal is to obtain $y[1]$. Using the realization of \figl{filter_realization_2}, the difference $x[1] - x[0]=92-85=7$ is multiplied by 5 and $y[n]=35$. In contrast, using \figl{filter_realization_1} leads to an overflow when calculating $5 x[0] = 425$, which would require at least 10 bits because converting from decimal to binary leads to $425_{\textrm D} = 01\textrm{~}1010\textrm{~}1001_{\textrm B}$, with the most significant bit representing the sign. Therefore, unless the hardware was able to implement \emph{saturation} and represent this product as 127, the result with finite precision would be $5 x[0]=-87_{\textrm D}=1010\textrm{~}1001_{\textrm B}$. Similarly, $5 x[1]=-52$ when represented with 8 bits and, consequently, $y[1]=-87 - (-52)=-35$ in this case. This simple example should suffice to illustrate the importance of the filter realization when the precision is finite. There are many structures for obtaining realizations of digital filters. The most popular will be presented in the sequel.

\subsection{Structures for FIR filters}

\begin{figure}
  \begin{center}
    \subfigure[Direct form]{\label{fig:fir_direct_form}\includegraphics[width=\figwidthSmall]{FiguresTex/fir_direct_form}}  
    \subfigure[Symmetric]{\label{fig:fir_direct_form_symmetric}\includegraphics[width=\figwidthSmall]{FiguresTex/fir_direct_form_symmetric}}
  \caption{Two structures for FIR realizations.\label{fig:fir_structures}}  
  \end{center}
\end{figure}


The most straightforward implementation of a FIR is called \emph{direct form} or \emph{tapped delay line}, which is depicted in \figl{fir_direct_form} for $H(z)=1 + 3.5 z^{-1} + 5.8 z^{-2} - 6 z^{-3}$.

Many FIR filters are designed to have linear phase and, consequently, have symmetry in their coefficients,\footnote{Matlab's \ci{fdatool}, via its menu Edit, provides support to converting the structure and also to showing it.} as indicated in \tabl{fir-types}. 
This has been taken into account in the \emph{symmetric} FIR structure of \figl{fir_direct_form_symmetric}, which implements $H(z)=2 - 3.5 z^{-1} + 8 z^{-2} - 8 z^{-4} + 3.5 z^{-5} - 2 z^{-6}$.

\subsection{Structures for IIR filters}

\begin{figure}  
  \begin{center}
\subfigure[Direct form I]{\label{fig:iir_direct_1}\includegraphics[width=6cm]{FiguresTex/iir_direct_1}}
    \subfigure[Preparation for direct form II]{\label{fig:iir_direct_2_preparation}\includegraphics[width=6cm]{FiguresTex/iir_direct_2_preparation}}
    \subfigure[Direct form II]{\label{fig:iir_direct_2}\includegraphics[width=5.5cm]{FiguresTex/iir_direct_2}}
  \end{center}
  \caption{Two alternatives for implementing the digital filter of \equl{iir_example}.}
  \label{fig:iir_realizations}
\end{figure}

IIR filters are often implemented using the \emph{transposed direct II} structure. It is convenient first to describe the \emph{direct I} structure via an example. Assume the task is to implement:
\begin{equation}
H(z) = \frac{B(z)}{A(z)} = \frac{0.6  + 0.1  z^{-1} + 2.3 z^{-2} + 4.5 z^{-3}}{1.0  -1.2 z^{-1}  + 0.4 z^{-2} + 5.2 z^{-3} + 1.1 z^{-4}}.
\label{eq:iir_example}
\end{equation}
\figl{iir_direct_1} depicts the implementation of $H(z)$ using the direct form I structure. Note that 7 delay blocks were used, which corresponds to requiring 7 memory locations to store their outputs. In case each number is represented by 16 bits, this would require 14 bytes. It is possible to reduce this number to 8 bytes using a direct form II structure, as depicted in \figl{iir_direct_2}.

The direct form II can be derived from the form I by noting that $H(z)$ is a LTI system and the following is valid:
\begin{align*}
&\mathrel{\phantom{\equiv}} x[n] \arrowedbox{B(z)} z[n] \arrowedbox{A(z)}  y[n] \\
&\equiv  x[n] \arrowedbox{A(z)} w[n] \arrowedbox{B(z)}  y[n].
\end{align*}

However, there is a disadvantage in implementing the poles first, as done in the direct form II: the signal can saturate in intermediate stages of filtering. When using the direct form I, the zeros tend to attenuate the signal and, after this attenuation, the poles amplify some frequency components. The net result should be no overflows. On the other hand, the direct form II first amplifies via the poles and later attenuates via the zeros. To circumvent this disadvantage while retaining the property of requiring the minimum amount of delay blocks, the direct form II can be transposed.
In other words, while the form I implements $B(z)$ first (corresponding to the zeros), the form II implements $A(z)$ first. This new order allows sharing the delay blocks and reduces their number to the order of the filter ($M=4$ in the example of \figl{iir_realizations}). \figl{iir_direct_2_preparation} shows the intermediate step when one goes from direct form I to II.

\begin{figure}
\subfigure[Intermediate step]{\label{fig:iir_transposed_preparation}\includegraphics[width=5cm]{FiguresTex/iir_transposed_preparation}}
\centering
\textrm{~~~~~~}    \subfigure[Reorganized]{\label{fig:iir_transposed_final}\includegraphics[width=5cm]{FiguresTex/iir_transposed_final}}
  \caption{IIR of        \equl{iir_example} implemented with the transposed direct form II. The intermediate diagram in (a) is obtained by transposing \figl{iir_direct_2}, while (b) simply reorganizes it.}
  \label{fig:iir_transposed}
\end{figure}

The \emph{transposed direct form II} is based on the fact that with three steps: a) inverting the orientation of all ``arrows'' in signal flow graph, b) swapping input $x[n]$ and output $y[n]$, and c) swapping adders by signal ``splitters'' and vice-versa, one obtains a structure that implements the same difference equation of the original structure.

If the IIR filter has order $M>2$ it is common to organize its poles and zeros as \emph{second order sections} (SOS)\index{Second order sections (SOS)}. For example, a filter $H(z)$ of order $M=7$ can be implemented as a \emph{cascade} connection $H(z)=H_1(z) H_2(z) H_3(z) H_4(z)$ of three SOS $H_1(z), H_2(z), H_3(z)$ and one first-order section $H_4(z)$ as indicated by:
\begin{align*}
&\mathrel{\ifpdf \phantom{=} \else \fbox{\phantom{=}} \fi}x[n] \arrowedbox{H(z)} y[n]\\
&\equiv x[n] \rightarrow\boxed{H_1(z)} \boxed{H_2(z)} \rightarrow \boxed{H_3(z)} \rightarrow \boxed{H_4(z)} \rightarrow y[n].
\end{align*}
It is also possible to decompose $H(z)$ into SOS sections that are organized in \emph{parallel}, such that $H(z)=G_1(z) + G_2(z) + G_3(z) + G_4(z)$ for this example, with $G_4(z)$ being a first-order section. The commands \ci{residue} and \ci{residuez}\index{Residue} are helpful to obtain the partial fractions expansion in the Laplace and Z transform domains, respectively, and are useful when designing the parallel structure. Using \ci{residuez} and assuming that $H(z)$ is given by \equl{iir_example}, one can find the parallel structure with the commands:
\includecodelong{MatlabOctaveCodeSnippets}{snip\_systems\_residuez}{snip_systems_residuez}
%\begin{lstlisting}
%b=[0.6 0.1 2.3 4.5];
%a=[1 -1.2 0.4 5.2 1.1];
%[r,p,k]=residuez(b,a); %fraction expansion in Z domain
%%note that using [b,a]=residuez(r,p,k) reconstructs b,a
%%hence, create two SOS with pairs of fractions:
%[b1,a1]=residuez([r(1) r(2)],[p(1) p(2)],0)
%[b2,a2]=residuez([r(3) r(4)],[p(3) p(4)],0)
%\end{lstlisting}
The hint is that \ci{residuez} can be used for decomposing in fractions and also for organizing pairs of fractions as second-order sections. The last two commands in the previous code reorganizes pairs of fractions, i.\,e., it implements
\[
\frac{r_1}{1-p_1 z^{-1}} + \frac{r_2}{1-p_2 z^{-1}} = \frac{r_1+r_2 - (r_1 p_2 + r_2 p_1)z^{-1}}{(1-p_1 z^{-1})(1-p_2 z^{-1})},
\]
where $r_i$ is the residue of pole $p_i$.
Therefore, \equl{iir_example} can be written as
\[
H(z) = \frac{-0.1637 + 1.3542 z^{-1}}{1 -2.6669 z^{-1} +  4.0399 z^{-2}} + \frac{0.7637 + 1.0226 z^{-1}}{1 + 1.4669    z^{-1} + 0.2723z^{-2}}.
\]

The cascade structure with each SOS implemented with a transposed direct form II can be considered the most popular realization and will be emphasized here.

Decomposing $H(z)$ in sections allows an easier control of issues provoked by using finite precision. For example, the poles with higher quality factor $Q$ can be placed last in the cascade and pairs of zeros ``paired'' (or ``matched'') to pairs of poles to minimize the chances of signal saturation at intermediate stages.

\begin{table}
\centering
\caption[{\matlab} functions to convert among the formats: transfer function (tf), zero-pole (zp) and second order sections (sos)]{{\matlab} functions to convert among the formats: transfer function (tf), zero-pole (zp) and second order sections (sos). The number two is a mnemonic for ``to''.\label{tab:tfzpsos}}
\begin{tabular}{cccc}
\toprule
 & tf & zp & sos
\\\midrule
tf & & tf2zp & tf2sos
\\
zp  & zp2tf & & zp2sos
\\
sos  & sos2tf & sos2zp &
\\\bottomrule
\end{tabular}
\end{table}

{\matlab} offers several functions to deal with SOS, as indicated in \tabl{tfzpsos}. The description of $H(z)$ with $S$ sections uses a matrix of dimension $S \times 6$, where each row describes a SOS
\[
H_s(z) = \frac{b_0  + b_1  z^{-1} + b_2 z^{-2}}{1.0  + a_1 z^{-1}  + a_2 z^{-2}}
\]
via a vector $[b_0, b_1, b_2, 1, a_1, a_2]$. For example, \equl{iir_example} can be decomposed in SOS with:
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_SOS\_decomposition}{snip_systems_SOS_decomposition}
%\begin{lstlisting}
%B=[0.6 0.1 2.3 4.5]; %numerator
%A=[1 -1.2 0.4 5.2 1.1]; %denominator 
%[z,p,k]=tf2zp(B,A); %convert to zero-pole for improved 
%[sos,g] = zp2sos(z, p, k) %to SOS, g is the overall gain
%\end{lstlisting}
The Octave implementation of the \ci{zp2sos} function is slightly different from Matlab's. The gain \co{g=0.6} obtained with the previous commands is the same for both Matlab and Octave. But the \co{sos} matrix in Matlab is:
\begin{verbatim}
sos =    0    1.0000    1.3689    1.0000    1.4669    0.2723
    1.0000   -1.2022    5.4790    1.0000   -2.6669    4.0399
\end{verbatim}
while in Octave it is:
\begin{verbatim}
sos = 1.00000  -1.20220   5.47898   1.00000  -2.66692   4.03987
      1.00000   1.36887  -0.00000   1.00000   1.46692   0.27229
\end{verbatim}
The comparison shows that the order of SOS is inverted: by default Matlab orders the sections with poles closest to the unit circle (higher $Q$) last in the cascade while Octave performs the opposite. Another difference is the representation of the SOS with a numerator $(1 + 1.3689z^{-1})$ of order 1. The Matlab's version is
\[
H(z) = 0.6 \left(\frac{1 + 1.3689z^{-1}}{1 + 1.4669z^{-1} + 0.2723z^{-2}}\right) \left(\frac{1 -1.2022z^{-1} + 5.4790 z^{-2}}{1 - 2.6669z^{-1} + 4.0399z^{-2}}\right),
\]
which is represented using transposed direct form II sections in \figl{cascade_sos}.

\begin{figure}
\centering
%\includegraphics[width=\figwidth,angle=270,keepaspectratio]{FiguresTex/cascade_sos} 
\includegraphics[width=\figwidth,keepaspectratio]{FiguresTex/cascade_sos} 
\caption{Realization of \equl{iir_example} with transposed direct form II second order sections.\label{fig:cascade_sos}}
\end{figure}

It is important to note that using zero/pole ($[z,p,k]$ syntax) to design IIR filters is better than using transfer functions with respect to numerical accuracy. After obtaining $[z,p,k]$, one can analyze or implement the filter using SOS (via \ci{zp2sos}). For filters of order higher than 7, numerical problems may happen when forming the transfer function ($[b,a]$ syntax).\footnote{See Matlab's documentation for the \ci{filter} function for more details.}

\subsection{Running a digital filter using filter or conv}
\label{subsec:compensating_filtering}

Any LTI system $H(z)$ can be represented by an impulse response $h[n]$. Having $h[n]$, one can use convolution to obtain $y[n] = x[n] \conv h[n]$. On a computer, it may be necessary to truncate an infinite duration $h[n]$. {\matlab} has the function \ci{impz} that, based on $H(z)$, can obtain a vector \ci{h} representing $h[n]$. However, if $H(z)=B(z)/A(z)$ represents a LCCDE, it is typically more efficient to implement an IIR via its difference equation with \ci{y=filter(B,A,x)} than via convolution with \ci{y=conv(x,h)}. When comparing the result of these two options, it is important to note that \ci{conv} outputs a vector of length $N_y = N_x + N_h -1$, where $N_x$ and $N_h$ are the lengths of \ci{x} and \ci{h}, respectively, while \ci{filter} always has $N_y=N_x$ because it truncates the tail of the corresponding convolution.

\subsection{{\akadvanced} Effects of finite precision}

When implementing a digital filter using finite precision there are two aspects to consider:
\begin{itemize}
	\item The quantization of the filter coefficients of $H(z)$ leads to another transfer function $H_q(z)$ with, eventually, displaced poles and zeros, frequency response, etc.
	\item During runtime, the result of multiplications and additions are quantized and this error impacts the system output. This is called round-off or rounding error.
\end{itemize}

It is possible to implement digital filters using floating-point arithmetics. This is performed by {\matlab}, for example, which can operate with double precision (each number represented by 64 bits). This is considered the representation with infinite precision here. For the study of finite precision effects, it will be assumed a fixed-point representation.

\subsubsection{Quantization of the filter coefficients via examples}

This section briefly discusses the issue of quantizing the filter coefficients via examples. Recall from Section~\ref{sec:fixedPoint} that $b_i$ and $b_f$ are the number of bits assigned to the integer and fractional part of the number, respectively.
%(using Octave)

In order to study the effect of coefficient quantization, a 8-th order bandpass filter was used. It was designed with the commands in \codl{snip_systems_coefficient_quantization}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_coefficient\_quantization}{snip_systems_coefficient_quantization}
%\begin{lstlisting}
%Apass=5; %maximum ripple at passband (dB)
%Astop=40; %minimum attenuation at stopband (dB)
%Fs=500; % sampling frequency
%Wr1=10/(Fs/2); %normalized frequencies (from 0 to 1)
%Wp1=20/(Fs/2); %frequencies in Hz are divided by 
%Wp2=120/(Fs/2);%the Nyquist frequency Fs/2
%Wr2=140/(Fs/2);
%[N,Wp]=ellipord([Wp1 Wp2],[Wr1 Wr2],Apass,Astop) %order,Wp
%[z,p,k]=ellip(N,Apass,Astop,Wp); %design digital filter
%[B, A]=zp2tf(z,p,k); %zero-pole to transfer function
%\end{lstlisting}

After the filter is designed, with its coefficients represented in double precision, Matlab or Octave can be used to find their fixed point representations.

Using Matlab, the coefficients of the filter can be quantized with the commands in \codl{snip_systems_Matlab_coef_quant}.

\includecodepython{MatlabOnly}{snip\_systems\_Matlab\_coef\_quant}{snip_systems_Matlab_coef_quant}

Using Octave, the quantization code is \codl{snip_systems_Octave_coef_quant}.

\includecodepython{OctaveOnly}{snip\_systems\_Octave\_coef\_quant}{snip_systems_Octave_coef_quant}
%\begin{lstlisting}
%is=7; % number of bits for integer part
%ds=8; % number of bits for decimal part
%Bq=fixed(is,ds,B);
%Aq=fixed(is,ds,A);
%\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/quantized_filter1}
\caption{The original magnitude response of the 8-th order filter and its 16-bits per coefficient Q7.8 quantized version ($b_i=7$ and $b_f=8$).\label{fig:quantized_filter1}}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/quantized_filter1_zp}
\caption{Zeros and poles for the original and quantized filters discussed in \figl{quantized_filter1}.\label{fig:quantized_filter1_zp}}
\end{figure}


\figl{quantized_filter1} shows the magnitude responses of the original and quantized filters, and one can see that there is a discrepancy close to DC.
\figl{quantized_filter1_zp} shows zeros and poles for the same pair of filters. One can note that the pair of poles close to DC get transformed into real poles. This impacts the frequency response and creates the mentioned discrepancy.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/quantized_filter2}
\caption{Magnitude of quantized filter with Q10.15 using 26 bits ($b_i=10$ and $b_f=15$) and the original filter of order 8 in \figl{quantized_filter1}.\label{fig:quantized_filter2}}
\end{figure}

%\lstinputlisting[caption={MatlabBookFigures/figs\_systems\_fixed\_point},label=code:fixedpoint]{./Code/MatlabBookFigures/figs_systems_fixed_point.m}

\figl{quantized_filter2} is the result of increasing the number of bits to $b=26$, with $b_i=10$ and $b_f=15$ when quantizing the eighth-order filter. It can be observed that good results are obtained. However, $b=26$ bits does not always guarantee good results. The following example illustrates that a good choice of $b$ depends on aspects such as the filter order.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/quantized_filter3}
\caption{Magnitude responses for original filter of order 14 and its quantized version with $b=26$ bits ($b_i=10$ and $b_f=15$).\label{fig:quantized_filter3}}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/quantized_filter3_zp}
\caption{Zeros and poles for the corresponding filter in \figl{quantized_filter3}. Note the occurrence of poles outside the unit circle, which make the quantized filter  unstable.\label{fig:quantized_filter3_zp}}
\end{figure}

Changing the filter mask of the original 8-th filter to impose \ci{Astop=80} increases the filter order from 8 to 14. Now, quantization with 26 bits generates an unstable filter as illustrated in \figl{quantized_filter3} and \figl{quantized_filter3_zp}.

\subsubsection{Roundoff errors during the signal processing}

%
The analysis of the quantization error created along the filtering process is more involved than observing the effect of filter coefficients quantization. Note that the term roundoff is used even when truncation is the actual operation, as done by Octave's fixed point package.

When interested on evaluating the roundoff errors, it does not suffice to pass objects of the fixed point class (called FixedPoint and embedded.fi in Octave and Matlab, respectively) to a specific function. For example, considering  \ci{filter}, the internal operations within this function may not have its results quantized. To study the roundoff errors, one typically has to write a function that internally represents the numbers in fixed point too.
\codl{snip_systems_filtering_precision} and \codl{roundoffErrors} illustrate this point. Octave was adopted for the former code and Matlab for the latter. The commands for dealing with fixed point in both are discussed in Section~\ref{sec:fixedPoint}.

%provide concrete examples of the need for taking care of roundoff errors along the signal processing flow.

\includecodepython{OctaveOnly}{snip\_systems\_filtering\_precision}{snip_systems_filtering_precision}

\codl{snip_systems_filtering_precision} uses \ci{filter} and illustrates how the signals and coefficients can be quantized. When observing the signal \ci{y5}, one may consider that all the processing used fixed point numbers in Q3.5 format. However, the internal variables used by filter were not quantized.
In other words, internally, \ci{filter} does not uses finite precision arithmetics. This is circumvented in \codl{roundoffErrors}.

%\begin{figure}
%\centering
%\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/octave_fixed1}
%\caption{Original signal and after quantization, where the input dynamic range saturated the quantizer.\label{fig:octave_fixed1}}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=\figwidthSmall,keepaspectratio]{FiguresNonScript/octave_fixed2}
%\caption{Quantized signal filtered by two distinct filters: one with the original and another with quantized coefficients.\label{fig:octave_fixed2}}
%\end{figure}

%The  was used to generate \figl{octave_fixed1} and \figl{octave_fixed2}.

\lstinputlisting[caption={MatlabBookFigures/figs\_systems\_roundoffErrors},label=code:roundoffErrors,linerange={1-1,4-27,39-56}]{./Code/MatlabBookFigures/figs_systems_roundoffErrors.m}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/roundoffErrors}
\caption{Example of filter outputs with floating-point double precision and fixed-point using Q2.3 generated with \codl{roundoffErrors}.\label{fig:roundoffErrors}}
\end{figure}

\codl{roundoffErrors} was used to generate \figl{roundoffErrors} with three signals: one obtained with fixed-point processing and two other not-quantized signals, which were obtained with \ci{myFilter} and Matlab's \ci{filter} for validation purposes. It can be seen that the non-quantized signals differ only during the transient part, as commented in the source code of \ci{myFilter}. Other than that, the signals are equivalent. 

Observe that \ci{myFilter} provides support to using a special register called \emph{accumulator}\index{Accumulator DSP register}, which is typical in digital signal processor chips, and in this case uses twice the number of bits of the other registers. For example, when the regular registers have 16 bits, the accumulator may have 32 bits, to minimize the roundoff error. The number representation adopted during the internal processing within \ci{myFilter} is determined by the type (or class) of the inputs passed to the function. In the first call to \ci{myFilter} in \codl{roundoffErrors}, the variable \ci{zeroReference} and all others are fixed point objects and the function is careful to not change this condition. For example, using a command such as \ci{acc = 0} would make acc a ``double'' variable, not an object of the fixed point class anymore. In the second call, all \ci{myFilter} inputs are in double precision and the result is equivalent to \ci{filter} after the transient.

It can be seen from \figl{roundoffErrors} that the quantized output has restricted range due to the small number of bits assigned to the integer part according to the Q2.3 representation. The reader is invited to run the code with a larger value for \ci{bi} and even modify it to properly deal with the transient in a Transposed Direct-II realization.

%AK-PUTBACK 
\ignore{
%\subsection{Workflows for projecting digital filters}
%
The next paragraphs will discuss the project of IIR filters using the bilinear transformation. It is assumed the digital filtering setup in \figl{canonical_interface}, which has many degrees of freedom. Consequently, there are various ways of organizing the steps for designing the system. Also, there are different alternatives for specifying the requirements. Of course, the project of a digital filter aims to find $H(z)$ that obeys all the predefined project requirements. These requirements can be described, for example, in terms of the frequency response $H(e^{j\dw})$ (its magnitude, phase, etc.), impulse response $h[n]$ or the output of $H(z)$ to the step function $u[n]$.
The whole procedure of a filter design will be called here a \emph{workflow}, and it depends on the requirements and used steps.
%
The traditional filter design workflow is discussed first. It assumes there is a mask specification for the frequency response magnitude $|H(e^{j\dw})|$ in the discrete-time domain, such as the ones in \figl{masks}.
%
and some of the most used 
%
\begin{table}
\centering
\caption{Oi.\label{tab:}}
\begin{tabular}{|l|c|r|}
Workflow \# & Input / requirements on & Main steps  \\
1 & mask on $|H(e^{j\dw})|$ & 1) Use a standard approximation to obtain $H(z)$. 2) Choose realization. 3) Choose $\fs$ \\
2 & mimic $H(s)$ & 1) Choose $\fs$ and convert to $H(z)$ using bilinear.
%\hline
\end{tabular}
\end{table}
%
with anti-aliasing and reconstruction filters. 
%
Alternatively, the project can be based on specifications for the frequency response $H(\aw)$ in the analog domain, 
%
For example, one can say the passband frequency is $x$ Hz, the stopband frequency is $y$ Hz, and so on. In this case, the basic relation
\equl{freqdiscrete2continuous} can be used to convert the frequencies in rad/s or Hz to rad. Following this approach, it is reasonable to assume that frequency response specifications are in terms of $H(e^{j\dw})$, i.\,e., in the discrete-time domain.
%
Here it is assumed that there is a mask specification for the frequency response magnitude, which can be described either in the continuous $|H(\aw)|$ or discrete-time $|H(e^{j\dw})|$ domain.
%
The previous Sometimes the project requirements are imposed in the z domain. For example, a mask can be imposed to $|H(z)|$
}
%AK-PUTBACK - note that there is a section about it in the chap about digicomm
%\section{The ideal system and the importance of linear phase}
%\subsection{Group delay}

%\section{Converting from discrete to continuous time}
%\section{Discrete to continuous-time conversion}
%\section{Revisiting Sampling and the Reconstruction (Interpolation) of Sampled Signals}
%\section{Revisiting the Reconstruction (Interpolation) of Sampled Signals}


%Check if you understand the code  (need to take out the frequency part of this code):
%AK_PUTBACK
%\lstinputlisting[caption={MatlabOctaveBookExamples/ex\_systems\_conversion\_discrete\_continuous},label=code:ex_systems_conversion_discrete_continuous]{./Code/MatlabOctaveBookExamples/ex_systems_conversion_discrete_continuous.m}


%\bApplication \textbf{Minimum phase systems}.
\section{{\akadvanced} Minimum phase systems}
\label{app:minimumPhase}
 A LTI discrete-time system $H(z)$ is causal and stable only if all its poles are inside the unit circle, but these two requirements do not impose restrictions on the zeros. The zeros of a causal and stable $H(z)$ can be anywhere in the Z plane. In many cases, it is useful to impose that the zeros must also be inside the unit circle. For a continuous-time system $H(s)$, the equivalent requirement is to have all zeros inside the left-part of the S plane. A minimum-phase system has this property.

A LTI is \emph{minimum-phase}\index{Minimum-phase systems} if both the system $H$ itself and its inverse $1/H$ are causal and stable. Assuming, discrete-time, because the poles of the inverse $1/H(z)$ are the zeros of $H(z)$, only a system with all zeros inside the unit circle has a causal and stable inverse. Similarly, a continuous-time system $H(s)$ is minimum-phase only if all its zeros are at the left half of the S-plane.

A non-minimum-phase system $H_{\textrm{non}}(z)$ can always be transformed into a minimum-phase $H_{\textrm{min}}(z)$ that has the same magnitude, i.\,e., $|H_{\textrm{non}}(z)| = |H_{\textrm{min}}(z)|$. This is achieved by replacing the zeros of $H_{\textrm{non}}(z)$ that are outside the unit circle by their conjugate reciprocals.\index{Reciprocal of a complex number}\footnote{The reciprocal of a complex number $c$ is $1/c$.} That is, replace all zeros $z_0$ with $|z_0|>1$ by $1/z_0^*$.
For example, if $z_0=2 e^{j \pi/2}$ is a zero of $H_{\textrm{non}}(z)$, it should be replaced by $1/z_0^* = 0.5 e^{j \pi/2}$.
The function \ci{ak\_forceStableMinimumPhase.m} implements this method and also decreases the magnitude of zeros on top of the unit circle.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/minphase_example}
\caption{Frequency responses of $H_{\textrm{non}}(z)$ and its minimum-phase counterpart $H_{\textrm{min}}(z)$.\label{fig:minphase_example}}
\end{figure}

\lstinputlisting[caption={MatlabBookFigures/figs\_systems\_minphase},label=code:minphase,linerange={1-10}]{./Code/MatlabBookFigures/figs_systems_minphase.m}

%\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_minphase}{snip_systems_minphase}

\figl{minphase_example} was obtained with \codl{minphase}.
Note that the magnitude is the same but the phase is distinct, with $H_{\textrm{min}}(z)$ having smaller values.

Minimum-phase systems have interesting properties. As the name indicates, among all systems with a given magnitude, the minimum-phase is the one with the phase closest to zero. \figl{minphase_example} illustrates this behavior, which can also be observed via the group delay, as in
\figl{minphase_groupdelay}.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/minphase_groupdelay}
\caption{Group delay for the non-minimum and minimum phase systems of \figl{minphase_example}.\label{fig:minphase_groupdelay}}
\end{figure}

Besides, the impulse response $h[n]$ of a minimum-phase system has its energy concentrated toward time $n=0$
more than any other causal signal having the same magnitude spectrum.
%\eApplication


\section{{\akadvanced} Multirate Processing}
\label{sec:updownsampler}
%See \cite{Mitra01}, page 660.

Many modern DSP systems adopt distinct sampling rates along the processing chain, using interpolators and decimators. Two basic blocks of these \emph{multirate processing}\index{Multirate processing}  systems are the upsampler and downsampler.

\subsection{Upsampler and interpolator}

The \emph{upsampler}\index{Upsampler} has an output $q[m] = x[m/L]$ for any $m$ that is an integer multiple of $L$ and 0 otherwise. It is represented by
\[
x[n] \arrowedbox{\uparrow{L}} q[m].
\] 
Note that distinct indexes $m$ and $n$ are used to emphasize the sampling rates are not the same.

The upsampling by $L$ is not a time-invariant operation (in spite of being linear), so the system is not LTI. When the input is a WSS process, the output process cannot be assumed WSS.

The Fourier transform of the upsampler output is given by: 
\begin{equation}
Q(e^{j\dw} )= X(e^{jL\dw}),
\label{eq:upsampledPSDDiscreteTime}
\end{equation}
which corresponds to scaling the abscissa $\dw$ by the factor $L$ such that $Q(e^{j\dw} )$ is a  compressed version\footnote{The same effect of ``speeding up'' $x(t)$ with $x(Lt)$ as in \exal{simulScaleShift}.} of $X(e^{j\dw})$. 

\figl{upsampler_spectrum} provides an example where the original spectrum $X(e^{j\dw})$ is an
ideal lowpass filter with cutoff frequency $\dw_c=\pi/4$~rad. After upsampling by $L=4$, the
resulting $Q(e^{j\dw})$ has not a lowpass spectrum anymore. In fact, the new ``high frequency''
components in $Q(e^{j\dw})$ are required to enable the amplitudes of the upsampler output $q[m]$ to vary from/to zero between two original samples of the (relatively ``smooth'') input $x[n]$.

\begin{figure}
\includegraphics[width=\columnwidth]{./FiguresNonScript/upsampler_spectrum}
\caption[Original spectrum $X(e^{j\dw})$ (top) and its upsampled version $Q(e^{j\dw}) = X(e^{jL\dw})$ with $L=4$ (bottom).]{Original spectrum $X(e^{j\dw})$ (top) and its upsampled version $Q(e^{j\dw}) = X(e^{jL\dw})$ with $L=4$ (bottom). For convenience, the abscissa was normalized by $\pi$ such 
that the value (in red) of $X(e^{j\dw})$ at $\dw=2\pi$~rad ($f=2$) appears in $\dw=\pi/2$~rad ($f=0.5$) for $Q(e^{j\dw})$.\label{fig:upsampler_spectrum}}
\end{figure}

Notice that there is no aliasing or change in spectrum amplitude involved in this process, but within $[-\pi,\pi[$~rad, $Q(e^{j\dw} )$ has $L$ versions of what was the original $X(e^{j\dw} )$ in the range $[-\pi,\pi[$~rad as emphasized
in \figl{upsampler_spectrum2}. In this example, with $L=4$, the equivalent of four replicas of the original spectrum can be found within $[-\pi,\pi[$~rad: the ones centered at $f=-0.5,0,0.5$ and half of the replicas in $f=-1$ and 1.

\begin{figure}
\begin{center}
\includegraphics[width=12cm]{./FiguresNonScript/upsampler_spectrum2}
\caption{Zoom of the bottom plot of \figl{upsampler_spectrum}: due to
the upsampling by $L=4$, $Q(e^{j\dw})$ has four replicas of the original lowpass spectrum.\label{fig:upsampler_spectrum2}}
\end{center}
\end{figure}


The autocorrelations $R_x[l/L]$ and $R_q[l]$ of the signals at, respectively, the input and output of an upsampler are related by
\begin{equation}
R_q[l]  = \left\{\begin{array}{cl} R_x[l/L],& l=0, \pm L, \pm 2L, \ldots\\ 0, & \text{otherwise.} \\ \end{array}\right. 
\label{eq:upsampledXcorr}
\end{equation}

Assuming the entire bandwidth of the input is of interest, in order to eliminate all images within the interval $[-\pi, \pi[$ except the one centered at $\dw = 0$ (DC), a lowpass filter $H_i(z)$ should have a stopband starting at $\pi/L$. Otherwise, \emph{imaging}\index{Imaging due to upsampling} will occur. 
%Furthermore, this lowpass filter should have a gain equal to $L$.
Hence, an \emph{interpolator} is composed of an upsampler and a lowpass filter, which is represented by
\[
x[n] \rightarrow\boxed{\uparrow{L}}\rightarrow q[m] \rightarrow\boxed{H_i(z)}\rightarrow \tilde x[m].
\] 

\subsection{Downsampler and decimator}

Meanwhile, the \emph{downsampler}\index{Downsampler} can be represented by
\[
z[m] \arrowedbox{\downarrow{M}} y[n].
\] 
and has an output $y[n] = z[nM]$. Its spectrum can be expressed as:
\begin{equation}
Y(e^{j\dw}) = \frac{1}{M} \sum_{k=0}^{M-1} Z(e^{j(\dw-2 \pi k)/M})
\label{eq:downsamplerSpectrum}
\end{equation}
In other words, the spectrum $Y(e^{j\dw})$ of the downsampled signal is the sum of $M$ stretched versions of the spectrum $Z(e^{j\dw})$ of the input signal $z[m]$. 

As discussed in \exal{simulScaleShift}, the transformation $(\dw-2 \pi k)/M$ of the independent  
variable $\dw$ corresponds to stretching the frequency axis by $M$ and shifting this intermediate result by $2\pi k$~rad. As indicated in \equl{downsamplerSpectrum}, these $M$ individual versions are summed and the result is scaled by $1/M$.

\figl{downsampling_example} illustrates an example in which the original spectrum $Z(e^{j\dw})$ is downsampled by $M=3$. As indicated by \equl{downsamplerSpectrum}, $Y(e^{j\dw})$ is composed by $M=3$ parcels and each one represented by one color in \figl{downsampling_example}. 

\begin{figure}
\includegraphics[width=12cm]{./Figures/downsampling_example}
\caption{Original spectrum $Z(e^{j\dw})$ (top) and the result $Y(e^{j\dw})$ (bottom) of downsampling it by $M=3$.\label{fig:downsampling_example}}
\end{figure}

If one considers a single parcel $Z(e^{j(\dw-2 \pi k)/M})$, it can be observed that a parcel itself is not periodic in $\dw=2\pi$, as required for any discrete-time signal spectrum. For example, $Z(e^{j\dw/3})$(in blue) alone does not have a period of $2\pi$.  The replica at $\dw=2\pi$~rad of the original spectrum is shifted to $M \times 2\pi = 6 \pi$ in $Y(e^{j\dw})$ and, consequently, $Z(e^{j\dw/3})$ does not have a replica at $2 \pi$ nor $4 \pi$, but only at $\dw=6 \pi$ and multiples.
But when the parcel $Z(e^{j\dw/3})$ is combined with $Z(e^{j(\dw-2 \pi)/3})$ (in red) and $Z(e^{j(\dw-4 \pi)/3})$ (in black), the resulting spectrum indeed indicates that $Z(e^{j\dw})$ has a period of $2 \pi$.

Since the maximum frequency that can be represented
after downsampling is reduced by a factor of $M$, any frequency beyond $\pi/M$ in the input will be aliased into the central period of the downsampled signal, as indicated by the $M-1$ replicas in \equl{downsamplerSpectrum}. Hence, a \emph{decimator} is composed of an anti-alising filter $H_d(z)$  designed with stopband edge at $\pi/M$ and a downsampler, as illustrated in
\[
\tilde z[m] \rightarrow\boxed{H_d(z)}\rightarrow z[m] \rightarrow\boxed{\downarrow{M}}\rightarrow y[n].
\] 

Note that an interpolator uses a lowpass filter after the upsampling operation to combat
imaging, while a decimator uses the lowpass filter before downsampling in order to control aliasing.

\ignore{
In practice, for FSRC the anti-imaging and anti-aliasing filters Hi(z) and Hd(z) are combined into a single filter H(z) for complexity savings. Then, H(z)  is often designed with a cutoff frequency at 
F_cutoff=min < (\pi/L,\pi/M),
(3)
the minimum value between \pi/L and \pi/M [21]. 
It is of interest to calculate the power spectral density (PSD) at the output of H(z). However, the signal z[m] is not wide-sense stationary (WSS) but cyclostationary and a more elaborated approach is needed [13,20].
A common trick in this case is to apply to z[m] a random time-shift, uniformly distributed over 0 to L-1, such that the resulting signal is WSS and the concept of PSD applies [13]. This is equivalent to calculating a time-average of the autocorrelation of z[m] and obtaining the PSD from this average. Following this approach similar to the derivation in Appendix A of [15], the average PSD of the signal z[m] at the output of H(z) can be expressed as:
F_zz (e^jO)=1/L |H(e^jO )|^2 F_xx (e^jOL),	(4)
Where F_xx (e^jO) is the PSD of x[m'], which is flat for a PAM transmitter whose symbols are i.i.d.
As also derived in [15], given the downsampler does not introduce new non-stationarity [20], the PSD Fyy at the output of the downsampler is given by:
F_yy (e^jO )=1/M ?_(n=0)^(M-1) M<<< F_zz (e^(jO+j 2\pin/M) ) ?,	(5)
where F_zz (e^jO) is the PSD of the higher-rate filtered signal at the input of the downsampler, which is given in (4).
}

\section{Applications}

\bApplication \textbf{Implementing a ``universal'' analog filter using the PC sound board and Playrec}.
\label{app:playrec}
This application practices the system illustrated in \figl{canonical_interface}, using a PC to implement the system $H(z)$ and the sound board for A/D and D/A conversions. It relies on a sound library to continuously read input samples from the ADC and send output samples to the DAC. There are many libraries for doing that, such as Java Sound, which has the advantage of a clean API and running on Windows, Linux and other operating systems. But a requirement for this example was to implement the signal processing in Matlab and Octave. Because of that, Playrec~\akurl{http://www.playrec.co.uk}{1pre} was adopted. The code based on Playrec is not optimized for this application because it continuously and dynamically allocates and frees memory. A more optimized implementation would use something as a circular buffer. But Playrec is flexible in the sense that allows using all {\matlab} powerful functions and can be compiled to both. In our case, the experiments were done using Playrec compiled for Octave on Linux. The reader is referred to Playrec's web site at \akurl{http://www.playrec.co.uk}{1pre}  for installation instructions and it is assumed from now on that Playrec has been already installed and tested.

\codl{universalChannelInitialization} lists the first lines of the initialization code, where the sampling frequency $\fs$ is defined. Hence, the application bandwidth is $\fs/2$. It should be mentioned that the values of the two first variables in \codl{universalChannelInitialization} may vary among different computers. The user must use Playrec's scripts \ci{select\_play} and \ci{select\_rec} to investigate the proper identifiers (ID's) in his/her system.

\lstinputlisting[caption={MatlabOctaveThirdPartyFunctions/ak\_universalChannelInitialization.m},label=code:universalChannelInitialization,linerange={1-7}]{./Code/MatlabOctaveThirdPartyFunctions/ak_universalChannelInitialization.m}

\codl{universalChannel1} provides an example of filtering using Playrec. Note that the first instruction is to run \codl{universalChannelInitialization}  to initialize Playrec. Just after that, a Butterworth filter of order 30 is designed, with cutoff frequency $\fs/4$~Hz, which corresponds to $\dw=\pi/2$~rad (or $0.5$ after normalizing by $\pi$, as required by {\matlab}). The array \ci{memory} is created in the next line, to store the samples in the filter's memory. The actual filtering processing is in line 23, within an eternal loop, and repeatedly updates \ci{memory}.

\lstinputlisting[caption={MatlabOctaveThirdPartyFunctions/ak\_universalChannel1.m},label=code:universalChannel1]{./Code/MatlabOctaveThirdPartyFunctions/ak_universalChannel1.m}

Expanding the setup in Application~\ref{app:latency} to use two computers, with audio cables to connect the ADC and DAC of the first to the DAC and ADC of the second, respectively, it is possible to implement and test several DSP algorithms. For example, while one computer implements the analog filter based on $H(z)$, Audacity can be used to playback a file with a signal $x[n]$ composed by the sum of two sinusoids to be the filter input, and simultaneously obtain the filter output $y[n]$.

\ifml
Another example is suggested in \codl{universalChannel2}, which shows how to add random noise to the filter's output.
\else
Another example is suggested in \codl{universalChannel2}, which shows how to add random noise to the filter's output (this corresponds to the channel in \figl{frequency_selective_channel}).
\fi

\lstinputlisting[caption={MatlabOctaveThirdPartyFunctions/ak\_universalChannel2.m},label=code:universalChannel2,linerange={21-27},firstnumber=21]{./Code/MatlabOctaveThirdPartyFunctions/ak_universalChannel2.m}

Note that the variable \ci{debug} in \codl{universalChannelInitialization} can be used to evaluate if the system is capable of not dropping samples (given the chosen $\fs$ and DSP processing).
\eApplication 

\bApplication \textbf{Implementing a ``universal'' analog filter using the PC sound board and Java code}.
\label{app:javaUniversalChannel}
This application discusses another implementation of a universal channel. Instead of the Playrec {\matlab} code adopted in Application~\ref{app:playrec}, Java is used here. The advantage of using Playrec is that the channel can be based on virtually all {\matlab} available functions. However, in many cases the channel just needs to implement a filter. In this situation, the task of compiling Playrec can be avoided. The portability of Java makes easier to run the filter using the provided code, which can have its source code modified to add noise, etc. Alternatively, the already
compiled \ci{DigitalFilter.jar} can be executed, which requires only having Java installed on
your machine.

The DigitalFilter Java project can be found at folder \ci{Code/Java\_Language} of the companion software.
The code can be executed from a command prompt, going to folder
\ci{Code/Java\_Language/DigitalFilter/dist} and typing 
\begin{lstlisting}
java -jar DigitalFilter.jar
\end{lstlisting}
which will open the GUI depicted in \figl{DigitalFilterJava}.
The assumed sampling frequency is $\fs=44.1$~kHz,
which can be modified by editing the source code
and recompiling.

\begin{figure}
\centering
\includegraphics[width=\figwidthLarge]{./FiguresNonScript/DigitalFilterJava}
\caption{Screenshot of the DigitalFilter GUI after user informed the coefficients of the filter obtained with {\matlab} command \ci{[B,A]=butter(4,0.5)}.\label{fig:DigitalFilterJava}}
\end{figure}

The GUI has two text areas to specify the numerator
$B(z)$ (top area) and denominator $A(z)$ (bottom area) of a
filter's system function $H(z)=B(z)/A(z)$. The coefficients must be separated by a blank space.
\figl{DigitalFilterJava} shows the coefficients of an IIR filter obtained with {\matlab} command \ci{[B,A]=butter(4,0.5)}.
The difference equation is implemented using a Direct Form II Transposed, as the \ci{filter} function in {\matlab}.

After clicking \ci{Change filter} and then the \ci{Start} button, the signal acquired by the sound board ADC will be continuously filtered by $H(z)$ and the filter output sent to the sound board DAC,
according to the scheme depicted in \figl{canonical_interface}.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./FiguresNonScript/filterWith2Computers}
\caption{Result of experiment with \codl{snip_systems_realtimeLoopback} and soundboards of two computers: an analog bandpass filter implemented with the canonical interface of \figl{canonical_interface}.\label{fig:filterWith2Computers}}
\end{figure}

\figl{filterWith2Computers} shows the plot obtained with the Matlab code of \codl{snip_systems_realtimeLoopback} (a modified version of \codl{snip_signals_realtimeLoopback}). Two computers were used and two audio cables connected the ADC (mic in) of the first computer to the DAC (speaker out) of the second computer and vice-versa. The first computer executed the DigitalFilter Java code while the second executed \codl{snip_systems_realtimeLoopback}. The gains (volumes) of both computers were adjusted to avoid signal saturation. The filter that the Java code implemented was obtained with the command \ci{[B,A]=cheby1(4,1,[0.2 .5])} (the coefficients are not the ones in \figl{DigitalFilterJava}), which corresponds to a bandpass filter. 

\includecodepython{MatlabOnly}{snip\_systems\_realtimeLoopback}{snip_systems_realtimeLoopback}

The shape of this bandpass filter can be observed in \figl{filterWith2Computers} but the stopband attenuation is not as good as the theoretical one (obtained with \ci{freqz}, for example). A caveat of the experiment can be observed from the top plot of \figl{filterWith2Computers}, which shows the signal acquired by the ADC of the second computer. The zero-valued samples in the middle of the noise signal are due to Matlab not being able to output WGN samples (\exal{autocorrelationWhite} discusses WGN), record the input signal and performing the extra processing without this delay. A workaround is to use another software to output the WGN and use Matlab only to record the samples, process them and plot the results.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./FiguresNonScript/filterWith2ComputersAudacity}
\caption{Result similar to \figl{filterWith2Computers} but without silence intervals in acquired signal (top plot).\label{fig:filterWith2ComputersAudacity}}
\end{figure}

\figl{filterWith2ComputersAudacity} illustrates the result of a second experiment, where Audacity was used to send WGN samples to the DAC of the second computer. This can be conveniently done by using Audacity's menu \ci{Generate} and choosing \ci{Noise -> White}. Then, the option \ci{Loop Play} of menu \ci{Transport} can be used to conveniently play the noise in an eternal loop. The second computer also executed \codl{snip_systems_realtimeLoopback} but with its fourth and fifth lines commented out such that Matlab simply reads from the ADC and shows the plots. It can be seen that there are no silence gaps as occurred in \figl{filterWith2Computers}. 

The user is invited to implement a system with larger attenuation in the stopband. One aspect is to use $H(z)$ with larger order. Another aspect is to get a better estimate of the spectrum by changing the values of \ci{numSamples} and input parameters of \ci{psd} (or \ci{pwelch}) in \codl{snip_systems_realtimeLoopback}. For example, it may help to increase the number of FFT points as discussed in Chapter~\ref{ch:frequency}.
\eApplication 

%\bApplication
%\textbf{Relating power in continuous and discrete-time when the reconstruction filter has unitary energy.}
%\label{app:dc_conversion}
%
%In Section~\ref{sec:cont_disc_power}, a ZOH reconstruction was assumed. Alternatively, 
%if the reconstruction filter has an impulse response $h(t)$ with unitary energy, a  reasoning similar to the one adopted in Section~\ref{sec:cont_disc_power} leads to the conclusion that the power is $\calP_c = \calP_d / \ts$. Because of this, some textbooks denote $\calP_d$ as the average energy (joules) instead of power (watts). This ambiguity can also be interpreted as a consequence of the fact that a discrete-time signal $x[n]$ does not have explicit information about time. Hence, the interpretation of its power in Watts requires a model or assumption for the reconstruction filter $h(t)$, which must incorporate the whole reconstruction process, including filters and amplifiers. In this text the assumed model is zero-order hold reconstruction.
%\eApplication %(Cioffi).


\bApplication \textbf{Influence of the poles quality factor on frequency response}.
\label{app:poleQFactor}
The goal here is to illustrate how the pole Q-factor influences a frequency response.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/qfactor_resonator}
\caption[{Magnitudes of frequency responses for two resonators with $H(s)$ as in \equl{resonator}.}]{Magnitudes of frequency responses for two resonators with $H(s)$ as in \equl{resonator}. The top plot corresponds to poles $p=-300 \pm j4000$ with $Q\approx 6.6854$ and $|H(\aw)|_{\aw=4000}=16.52$ dB. The bottom plot corresponds to $p=-3 \pm j4000$ with $Q\approx 666.67$ and $|H(\aw)|_{\aw=4000}=56.48$ dB. The two data tips indicate approximately 3-dB frequencies.\label{fig:qfactor_resonator}}
\end{figure}
%The 3-dB bandwidth is (approximately)  $[3678, 4276.3]$ rad/s and 

First, \codl{snip_systems_Qfactor} shows how to calculate $Q$ for a second-order system in {\matlab} using \equl{q_factor_second_order}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_Qfactor}{snip_systems_Qfactor}
%\begin{lstlisting}
%p1=-300+j*4000; p2=-300-j*4000; %pole and its conjugate
%A=poly([p1 p2]); %convert it to a polynomial A(s)
%Omega_n = sqrt(A(3)); alpha = A(2)/2; %use definitions
%Q = Omega_n / (2*alpha) %calculate Q-factor
%\end{lstlisting}
In this case, the poles are located at the frequency $\pm 4000$ rad/s and have $Q \approx 6.6854$.
For this example, the 3-dB bandwidth is defined (approximately) by $\aw_1 = 3678$ and $\aw_2 = 4276.3$ rad/s, which gives $\BW=(\aw_2-\aw_1)/(2\pi) \approx 95.2$ Hz. If we change the poles to $-3 \pm j4000$, the $Q$-factor increases to $Q \approx 666.67$ and $\BW$ decreases to approximately 6 Hz. \figl{qfactor_resonator} compares these two situations.

The approximations used in analog filter design differ in the strategy for locating the poles (and zeros). Typically, the approximations that use poles with larger $Q$-factor have better magnitude but worse phase responses. The \codl{showQfactors} compares the poles and their $Q$ values for the Butterworth, Chebyshev (Type 1) and Cauer (or elliptic), which are listed
in \tabl{filter_approximations}.

\includecode{MatlabOctaveFunctions}{showQfactors}

The output of \codl{showQfactors} is the following:
\begin{verbatim}
- Butterworth filter:
Poles=-96.5926 +/- j25.8819 => Q=0.51764
Poles=-70.7107 +/- j70.7107 => Q=0.70711
Poles=-25.8819 +/- j96.5926 => Q=1.9319
- Chebyshev 1 filter:
Poles=-23.2063 +/- j26.6184 => Q=0.76087
Poles=-16.9882 +/- j72.7227 => Q=2.198
Poles=-6.2181 +/- j99.3411 => Q=8.0037
- Elliptic filter:
Poles=-34.9934 +/- j49.0885 => Q=0.86137
Poles=-9.154 +/- j92.2792 => Q=5.0651
Poles=-1.3808 +/- j100.0259 => Q=36.2244
\end{verbatim}
These results indicate that elliptic filters are more aggressive with respect to pole positioning (the poles are closer to the $j \aw$ axis) and have sharper magnitude responses. Butterworth filters, on the other hand, have poles with lower $Q$ than Chebyshev and elliptic filters.

%AK: it did not match with the site calculation:
%can be calculated\footnote{Using, e.\,g., \ url{http://www.sengpielaudio.com/calculator-cutoffFrequencies.htm}.} as $\aw_1 = 3712.0$ and $\aw_2 = 4310.3$ rad/s, which gives a bandwidth of $\BW=(\aw_2-\aw_1)/(2\pi) \approx 95.2$ Hz.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/freq_response_butter6ord}
\caption[{Poles (left) and magnitude $|H(\aw)|$ (right) for the sixth-order Butterworth filter of \codl{showQfactors}.}]{Poles (left) and magnitude $|H(\aw)|$ (right) for the sixth-order Butterworth filter of \codl{showQfactors}. The top plots are obtained with the pair of poles with the smallest $Q$, the middle plots with four poles and the bottom plots with all six poles.\label{fig:freq_response_butter6ord}}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/freq_response_cheby16ord}
\caption[{Similar to \figl{freq_response_butter6ord}, but with a sixth-order Chebyshev Type 1 filter.}]{Similar to \figl{freq_response_butter6ord}, but with a sixth-order Chebyshev Type 1 filter. Note the poles are closer to the $j \aw$ axis than for the Butterworth filter and there are ripples in the passband (one peak for each pole).\label{fig:freq_response_cheby16ord}}
\end{figure}

\figl{freq_response_butter6ord} and \figl{freq_response_cheby16ord} compare the Butterworth and Chebyshev approximations using sixth-order filters. Both lead to \emph{all-poles}\index{All-poles} transfer functions, because there are no finite zeros (all zeros are at $\aw = \pm \infty$). Finite zeros offer an additional degree of freedom but typically create additional abrupt changes in phase.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/freq_response_ellip6ord}
\caption[{Zero-pole plot (top) and magnitude in dB (bottom) for the sixth-order elliptic filter of \codl{showQfactors}.}]{Zero-pole plot (top) and magnitude in dB (bottom) for the sixth-order elliptic filter of \codl{showQfactors}.
This filter has zeros at $\aw = \pm 104.6303$, $\pm 116.8868$, $\pm 242.7541$ rad/s, each one imposing a ``valley'' in the magnitude response, as the one indicated for $\aw = 104.6303$.\label{fig:freq_response_ellip6ord}}
\end{figure}

The elliptic filters use finite zeros and achieve very short transition regions. \figl{freq_response_ellip6ord} illustrates a sixth-order elliptic filter to be compared to \figl{freq_response_butter6ord} and \figl{freq_response_cheby16ord} (their bottom-most plots).
\eApplication 

\ignore{
For a system in zero-pole form, supply column vector arguments z and p to zplane:
zplane(z,p)
For a system in transfer function form, supply row vectors b and a as arguments to zplane:
zplane(b,a)
}


\bApplication \textbf{Estimating the frequency response of a sound board using a cable.}
The idea here is to estimate an impulse response $\delta(t)$ and then obtain a frequency response $H(f) = \calF \{\delta(t)\}$, continuing the tests conducted in Application~\ref{app:latency} (the same topic in be further discussed in Application~\ref{app:smoothingFFT}). 

Note that 
%in Application~\ref{app:smoothingFFT} the selected segment started at sample $n=12650$ to avoid small amplitudes before the system response actually begins. In fact, 
shifting a signal in time does not alter the magnitude of the spectrum. But choosing the ``time origin'' is important 
%, which was the only result generated in the previous application. In contrast, the current task includes 
for the spectrum phase. 
For the specific signal described in Application~\ref{app:latency}, 
the first task is then to isolate one of the four ``impulses'' (recall they are just gross approximations to $\delta(t)$). Here it is chosen the segment that starts at sample $n=11026$, which is when the second impulse $\delta[n]$ is positioned.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall]{./Figures/loopbackFreqResponsePhase}
\caption{Sound system phase frequency response $\angle{H(f)}$ estimated from an impulse response.\label{fig:loopbackFreqResponsePhase}}
\end{figure}

\codl{snip_systems_phase_estimation} estimates the phase from the estimated $H(f)$ considering the response to the second impulse $\delta[n]$ at $n=11026$.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_phase\_estimation}{snip_systems_phase_estimation}
\figl{loopbackFreqResponsePhase} shows the obtained phase.
Because the phase is linear, the group delay can be estimated as suggested in
\codl{groupDelayLinearPhase}, by choosing two points and calculating the line slope in line 12.
In this case the result was 45.573~ms, which is consistent with the estimate
conducted in time-domain in Application~\ref{app:latency}.
\eApplication

\bApplication \textbf{Interpreting the impulse response for line topology identification in DSL.}
In DSL, loop qualification consists in determining if a given line under analysis is able to support the desired DSL service. Identifying the topology of a given telephone line means identifying its line sections and is helpful for the loop qualification. These sections can be serial sections or \emph{bridge taps}\index{Bridge taps}. Roughly speaking, a bridge tap is a derivation that is not terminated and causes unwanted signal reflections (echoes). Both types of sections have essentially two attributes: the length of the section and the type of cable used, which here is assumed to be entirely described by the gauge (or equivalently, the diameter) of the conductors. Therefore, the topology of a line under analysis can be considered as a set
$\Theta  = \left\{ {{\theta _1},\;{\theta _2},\; \ldots ,\;{\theta _{{n_s}}}} \right\}$,
where the subset $\theta_{k}$, with $k = 1, 2, ..., n_{s}$, represents the $k$-th line section from a port at the central office to the port at the costumer premises equipment, and $n_{s}$ is the number of sections that composes the subscriber line. Each subset $\theta_{k}$ is composed by the parameters
${\theta _k} = \left\{ {{\tau _k},{l_k},{\phi _k}} \right\}$
where $\tau _k \in \{\textrm{``serial'',``bridge tap''}\}$, $l_k$, and $\phi _k$ are, respectively, the type of section, its length, and the cable gauge.

For many practical lines, the impulse response is composed by a series of positive (or upward) pulses delayed in time. Each succeeding pulse is generated by the part of the signal transmitted through the line discontinuities and multi-reflections suffered by the input signal. Each ``path'' taken by the input signal into the line defines a pulse in the output.

The following discussion will suggest how the impulse response can be interpreted.
\figl{DELTpaths} (top) depicts an example of a topology with six line sections and two bridge taps in the third and fifth line section, respectively. \figl{DELTpaths} (bottom) also shows the first four (strongest) possible paths that the input signal can follow, from port 1 to port 2. \figl{DELTsignal} shows the corresponding (simulated) impulse response $h(t)$ of the line topology depicted in \figl{DELTpaths}. The relation between the signal paths and the pulses in the $h(t)$ can be observed from the arrival times of the positive pulses, $t_1$ to $t_4$.

\begin{figure}[!htbp]
	\center
%\subfigure[]{\label{fig:DELTtopology}\includegraphics[scale=0.29]{Figures/DELTtopology.eps}}
	\subfigure[]{\label{fig:DELTpaths}\includegraphics[width=\figwidth]{FiguresNonScript/DELTpaths.eps}}
	\subfigure[]{\label{fig:DELTsignal}\includegraphics[width=\figwidthLarge]{FiguresNonScript/DELTsignal.eps}}
	\caption[DSL line topology and corresponding impulse response]{DSL line topology and corresponding impulse response: (a) from top to bottom: topology and some of the possible signal paths; (b) corresponding impulse response with points indicating the estimated time of arrival of the pulses.}
	\label{fig:DELTexample}
\end{figure}

For example, the length of path A can be estimated by processing $h(t)$ to extract the time of arrival $t_A$ of the first pulse. Having the time, the path length is obtained by considering that the signal propagates at velocity $v_p = 0.67 c$, where $c \approx 3 \times 10^8$ m/s is the speed of light in vacuum. For example, assume the total serial length $500+300+350+850=2,000$~m of the line is not known and must be automatically estimated by the DSL service provider.
Assume an estimate of $h(t)$ was available and $t_A=9.35$~$\mu$s is obtained by signal processing (in our case, by inspection of \figl{DELTsignal}). Hence, the estimate for the total length is $L_A = t_{A} v_p \approx 1,879.4$~m.
The length of the bridge taps can also be estimated along this line of reasoning, assuming $L_A$ has already been estimated.

The presence of each bridge tap originates a new path and, as a consequence, a new pulse in the impulse response. In the example, the corresponding paths are B and C, which correspond to the arrival times $t_{B}$ e $t_{C}$) of the second and third pulses, respectively. Given that the shortest bridge tap (the fifth section) has length $l_5$, the total length traveled by its corresponding pulse can be written as
${L_B} = {L_A} + 2{l_5}$.
Thus, the length of the first bridge tap can be estimated by
$\hat {l_5} = \frac{{\left( {{t_B} - {t_A}} \right)}}{2}{v_p}$.
Use the same reasoning to estimate the length of the longest bridge tap (third section). 

Topology identification is an involved problem. For example, the path D is caused by multiple reflections, which can cause several problems. And the presented analysis has restrictions. For example, it is not possible to determine the position of bridge taps from the impulse response. If the position of the bridge taps in \figl{DELTpaths} change, the path traveled by the signal remains the same and, consequently, its signature on $h(t)$ does not change.
\eApplication

\bApplication
\textbf{Understanding and compensating the delay imposed by a system.}
\label{app:groupdelay}
All practical systems impose some delay or distortion to signals. For example, in communication systems the channel is typically  \emph{frequency-selective}\index{Frequency-selective channel}, meaning that distinct frequency components of the signal get attenuated and delayed in a way that the output signal is a distorted version of the input signal. Besides this effect, systems impose a delay (the output is observed only after an interval of time from the instant the input was generated, as studied in Application~\ref{app:sound_board_quantizer}) that is often characterized by its group-delay. Better understanding how to predict this delay when the system is LTI is the subject of the following discussion.

\begin{figure}
\centering
    \subfigure[Impulse response]{\label{fig:butter_8_impulse_response}\includegraphics[width=7.5cm]{Figures/butter_8_impulse_response}}
    \subfigure[Group delay]{\label{fig:butter_8_group_delay}\includegraphics[width=7.5cm]{Figures/butter_8_group_delay}}
  \caption[{Impulse response and group delay of IIR filter obtained with \ci{[B,A]=butter(8,0.3)}.}]{Impulse response and group delay of IIR filter obtained with \ci{[B,A]=butter(8,0.3)}. The impulse response was truncated to 50 samples.}
  \label{fig:butter_8_example}
\end{figure}

\begin{figure}
\centering
    \subfigure[Input signal]{\label{fig:butter_8_input_signal}\includegraphics[width=\figwidthSmall]{Figures/butter_8_input_signal}}
    \subfigure[FFT of the input]{\label{fig:butter_8_input_fft}\includegraphics[width=\figwidthSmall]{Figures/butter_8_input_fft}}
  \caption{Input signal $x[n]$ and its FFT.}
  \label{fig:butter_8_input}
\end{figure}

An example of filtering is provided in the sequel using an 8-th order IIR filter obtained and analyzed with \codl{snip_systems_filtering}.
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_filtering}{snip_systems_filtering}
%\begin{lstlisting}
%[B,A]=butter(8,0.3); %IIR filter
%h=impz(B,A,50); %obtain h[n], truncated in 50 samples
%grpdelay(B,A); %plots the group delay of the filter
%x=[1:20,20:-1:1]; %input signal to be filtered, in volts
%X=fft(x)/length(x); %FFT of x, to be read in volts
%y=conv(x,h); %convolution y=x*h
%\end{lstlisting}
The truncated impulse response of this filter is depicted in \figl{butter_8_impulse_response} and the group delay in \figl{butter_8_group_delay}. An input signal \ci{x} with a triangular shape was generated and is depicted in \figl{butter_8_input} together with its FFT.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/filtered_triangle}
\caption{Input $x[n]$ and output $y[n]$ signals obtained via convolution with truncated $h[n]$.\label{fig:filtered_triangle}}
\end{figure}

The result of the convolution \ci{y=conv(x,h)} is shown in \figl{filtered_triangle}. The input has peaks at $n=19$ and $n=20$, such that \co{x[19]=x[20]=20}, while the output peaked at $n=25$ with \co{y[25]=19.7}.
This delay of 5 samples could be inferred by observing that the peak of \ci{h} in
\figl{butter_8_impulse_response} is located at sample $n=6$. A more accurate analysis can be done via the filter's group delay. In the case of a linear-phase FIR, the group delay is constant (the same number of samples) in the whole passband. But the current IIR has a frequency dependent group delay and the spectrum of the input signal needs to be analyzed via \figl{butter_8_input}.

Note that most of the input signal power is concentrated at DC and the FFT coefficient $X[k]$ for $k=1$, which corresponds to an angle $\dw=2 \pi / 40 = 0.1571$ rad, given that \ci{length(x)=40} is the FFT length. From \figl{butter_8_group_delay} or directly calculating \ci{gd=grpdelay(B,A,[0 0.1571])}, it can be observed that the group delay $\tau_g(\dw)$ of the filter for components at $k=0$ and $k=1$ is $\tau_g(0)=5.03$ and $\tau_g(0.1571)=5.1043$ samples, respectively. When using \figl{butter_8_group_delay}, note that $\dw$ should be normalized and the value read at $\dw / \pi = 0.05$. The group delay at the frequencies of interest is approximately 5 samples, which explains the delay imposed by the filter in \figl{filtered_triangle}.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/filtered_triangle_with_filter}
\caption{Input $x[n]$ and output $y[n]$ signals obtained via \ci{filter}.\label{fig:filtered_triangle_with_filter}}
\end{figure}

Instead of using convolution, the output was obtained with \ci{y=filter(B,A,x)}. In this case, the vector \ci{y} is the one depicted in \figl{filtered_triangle_with_filter}, which should be compared to \figl{filtered_triangle}. Because $N_x=40$ and $N_h=50$ samples, the convolution in \figl{filtered_triangle} led to $N_y=40+50-1=89$ samples. In contrast, \figl{filtered_triangle_with_filter} shows that \ci{filter} makes sure that $N_y=N_x=40$ samples. To visualize the tail of the convolution, one should add zero samples to the end of \ci{x}. For example, the extra 20 samples in \ci{y=filter(B,A,[x zeros(1,20)])} are enough to visualize the ``triangle'' in \ci{y}.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/filtered_triangle_aligned}
\caption{Output $y[n]$ aligned with the input $x[n]$ and corresponding error $x[n]-y[n]$.\label{fig:filtered_triangle_aligned}}
\end{figure}

In some situations it is necessary to compare the error between the input and filtered output signals. But this requires compensating the filter delay and eliminating the convolution tail.
\figl{filtered_triangle_aligned} was obtained with:
\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_compensate\_grpdelay}{snip_systems_compensate_grpdelay}
%\begin{lstlisting}
%x=[1:20,20:-1:1]; %input signal to be filtered, in volts
%[B,A]=butter(8,0.3); %IIR filter
%h=impz(B,A,50); %truncate h[n] using 50 samples
%y=conv(x,h);  %convolution y=x*h
%y=transpose(y); %make x and y of same dimension
%meanGroupDelay=5 %estimated "best" filter delay
%y(1:meanGroupDelay)=[]; %compensate the filter delay
%y(length(x)+1:end)=[]; %eliminate convolution tail
%mse=mean( (x-y).^2 ) %calculate the mean squared error
%SNR=10*log10(mean(x.^2)/mse) %estimate signal/noise ratio
%stem(0:length(x)-1,x); hold on
%stem(0:length(y)-1,y,'r','Marker','x');
%stem(0:length(y)-1,x-y,'k','Marker','+');
%\end{lstlisting}
The mean squared error (MSE) in this case is 0.0363, which is equivalent to an SNR of 35.97~dB.

Decreasing the filter cutoff frequency to $\dw=0.1$ rad via \ci{[B,A]=butter(8,0.1)} and adjusting the variable \ci{meanGroupDelay} to 16 samples (\ci{meanGroupDelay=17} leads to a slightly better result) makes the SNR decrease to 23.22~dB because more high frequency components of \ci{x} are filtered out.
\eApplication

\bApplication \textbf{Taking the filter memory in account.}
\label{app:filterMemoryInAccount}
When using \ci{filter} in block processing, it is important to take the filter's memory in account. By default, this memory is assumed to be zero (the system is relaxed), but this can be modified with the command \ci{filtic}, which allows to set the filter's initial conditions.

\begin{figure}
\centering
\includegraphics[width=\figwidthSmall,keepaspectratio]{Figures/filtered_triangle_without_memory}
\caption{Filtering in blocks of $N=5$ samples but not updating the filter's memory.\label{fig:filtered_triangle_without_memory}}
\end{figure}

Instead of using \ci{y=filter(B,A,x)} to completely filter \ci{x} in one function call, it is useful to know how to segment the input \ci{x} in blocks of $N$ samples and repeatedly invoke \ci{filter} until filtering all blocks. This is illustrated in \codl{snip_systems_filtering_blocks}.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_filtering\_blocks}{snip_systems_filtering_blocks}
%\begin{lstlisting}
%N=5; %block length
%numBlocks=floor(length(x)/N); %number of blocks
%Zi=filtic(B,A,0); %initialize the filter memory with zero samples
%y=zeros(size(x)); %pre-allocate space for the output
%for i=0:numBlocks-1
%    startSample = i*N + 1; %begin of current block 
%    endSample = startSample+N -1; %end of current block
%    xb=x(startSample:endSample); %extract current block
%    [yb,Zi]=filter(B,A,xb,Zi);%filter and update memory
%    y(startSample:endSample)=yb; %update vector y
%end
%\end{lstlisting}
These commands obtain the same results of \figl{filtered_triangle_with_filter} because the filter memory is updated via \ci{[yb,Zi]=filter(B,A,xb,Zi)}. 
However, a simple modification from this command to \ci{yb=filter(B,A,xb)}, which disregards the filter memory leads to the result in \figl{filtered_triangle_without_memory}. In this case, the filter has zeroed memory at the beginning of each block of $N=5$ samples. The strong distortion on $y[n]$ in this case indicates that it is crucial to take care of updating the filter memory when dealing with signal processing in blocks.
\eApplication

\bApplication \textbf{Processing sample blocks using convolution in matrix notation.}
\label{app:blocksMatrixConvolution}

As \equl{convolutionInMatrixNotation} indicates, the convolution between two finite-length signals can be represented in matrix notation. However, in many applications, block processing is adopted, where one of the signals has an infinite duration and is segmented into successive blocks of $N$ samples, as in \equl{block_processing}.

\codl{snip_systems_matrixBlockConvolutions} illustrates the extra manipulations that are required to use a matrix notation for the convolution of each block and get the correct convolution result.

\includecodepython{MatlabOctaveCodeSnippets}{snip\_systems\_matrixBlockConvolutions}{snip_systems_matrixBlockConvolutions} 

The basic idea is that the end samples (``tails'') of the partial convolution results need to be properly added to the final result. In the example of \codl{snip_systems_matrixBlockConvolutions}, the impulse response has \ci{Nh=3} non-zero samples and the block length is \ci{Nb=5}. The convolution matrix \ci{hmatrix} has dimension $7 \times 5$ and each partial convolution result corresponds to \ci{Nbout=7} samples. For instance, the last two samples of \ci{yblock} of the first block need to be added to the first two samples of \ci{yblock} obtained of the second block, and so on. This is similar to the overlap-add method illustrated in \codl{snip_systems_overlapAdd} but without the FFT-based processing.
\eApplication

\bApplication
\textbf{Power and energy at the output of a LTI system.}
In many applications it is important to estimate the power at the output of a system. The following result is valid for LTI systems.

\bTheorem \textbf{Power at the output of a discrete-time LTI system}.
\label{th:output-power-of-lti}
\begin{equation}
\calP_y = \calP_x E_h + 2 \sum_{i=0}^L \sum_{j=0, j \ne i}^L h_i h_j R_x(j-i)
\label{eq:power_lti}
\end{equation}
where $R_x(j-i) = \ev[\langle x[n],x[n-(j-i)]\rangle]$ is the empirical (time-averaged) autocorrelation and $E_h = \sum_{i=0}^L h_i^2$ is the energy of $h[n]$. If $R_x(k)=0, \forall k$ and $k \ne 0$, then $\calP_y = \calP_x E_h$.

Proof sketch:
Assume a short impulse response $h[n]=h_0 + h_1 \delta[n-1] + h_2 \delta[n-2]$ with $L=2$. Take three consecutive output values starting at an arbitrary time instant, such as $n=5$, for example. These values will be
$y[5]=x[5]h_0 + x[4]h_1 + x[3]h_2$, $y[6]=x[6]h_0 + x[5]h_1 + x[4]h_2$ and $y[7]=x[7]h_0 + x[6]h_1 + x[5]h_2$. Calculating the energy $E_y$ for these three samples is enough to provide insight on the solution:

\IfPackageLoadedTF{tex4ht}{
	\begin{align*}
		E_y &= y^2[5]+y^2[6]+y^2[7] \\
		&= h_0^2 \bigl(x^2[5]+x^2[6]+x^2[7]\bigr) + h_1^2 \bigl(x^2[4]+x^2[5]+x^2[6]\bigr) \\
		&\mathrel{\phantom{=}}+h_2^2 \bigl(x^2[3]+x^2[4]+x^2[5]\bigr) + 2 h_0 h_1 \bigl(x[5]x[4]+x[6]x[5]+x[7]x[6]\bigr)\\
		&\mathrel{\phantom{=}}+ 2 h_0 h_2 \bigl(x[5]x[3]+x[6]x[4]+x[7]x[5]\bigr)\\
		&\mathrel{\phantom{=}}+ 2 h_1 h_2 \bigl(x[4]x[3]+x[5]x[4]+x[6]x[5]\bigr). 
	\end{align*}
}{
	\begin{align*}
		E_y &= y^2[5]+y^2[6]+y^2[7] \\
		&= h_0^2 \bigl(x^2[5]+x^2[6]+x^2[7]\bigr) + h_1^2 \bigl(x^2[4]+x^2[5]+x^2[6]\bigr) \\
		&\mathrel{\phantom{=}}+h_2^2 \bigl(x^2[3]+x^2[4]+x^2[5]\bigr) + 2 h_0 h_1 \bigl(x[5]x[4]+x[6]x[5]+x[7]x[6]\bigr)\\
		&\mathrel{\phantom{=}}+ 2 h_0 h_2 \bigl(x[5]x[3]+x[6]x[4]+x[7]x[5]\bigr) + 2 h_1 h_2 \bigl(x[4]x[3]+x[5]x[4]\\
		&\mathrel{\phantom{=}}+x[6]x[5]\bigr). 
	\end{align*}
}

Taking all samples in account and using expectation, one can obtain a relation between the output and input energies $E_y$ and $E_x$, respectively:
\[
E_y = E_x E_h + 2 \sum_{i=0}^L \sum_{j=0, j \ne i}^L h_i h_j \langle x[n],x[n-(j-i)] \rangle
\]
or, alternatively
\[
\calP_y = \calP_x E_h + 2 \sum_{i=0}^L \sum_{j=0, j \ne i}^L h_i h_j R_x(j-i)
\]
given that $\calP = E/N$.

If the input $x[n]$ does not have correlation among its samples, then it is a ``white'' signal (see Section~\ref{sec:examplesAutocorrelation}) with $R_x(k)=0$ for $k \ne 0$, such that
\begin{equation}
\calP_y = \calP_x E_h.
\label{eq:powerOutputLTI}
\end{equation}
\eTheorem

For example, assume that $h[n]=1 - 0.5 \delta[n-1]$, such that $E_h = 1.25$ and $\calP_y = \calP_x E_h + 2 h_0 h_1 R_x(1)$. Assuming the input is AWGN with power $\calP_x=1$, then \equl{powerOutputLTI} leads to $\calP_y = \calP_x E_h = 1.25$ . The following code illustrates this fact:
\begin{lstlisting}
N=1000000; x=randn(1,N); h=[1 -0.5]; y=filter(h,1,x);
Eh=sum(h.^2), Px=mean(x.^2), Py=mean(y.^2)
\end{lstlisting}
However, if $x[n]$ is not white, $R_x(k)$ will impact the result. For the same filter, if $x[n]=1, \forall n$, then $R(1)=1$ and $\calP_x=1$, such that $\calP_y = 0.25$. As a third example, consider the samples of a binary $x[n]$ are i.i.d with equiprobable 0 and 1 values. In this case, $R(1)=1/4$ and $\calP_x=0.5$, such that $\calP_y = 0.5 \times 1.25 + 2(-0.5) \times 1/4 = 0.375$. The following commands
\begin{lstlisting}
N=1000000;x=round(rand(1,N));h=[1 -0.5];y=filter(h,1,x);
Eh=sum(h.^2), Px=mean(x.^2), Py=mean(y.^2)
\end{lstlisting}
can confirm the results.
\eApplication

\bApplication \textbf{The delay and attenuation imposed by a channel}.
%\section{Channel Delay and Attenuation}
\label{app:linear_filtering}
%\label{sec:delayAttentuation}
%\subsection{Delay caused by channels}
Here the LTI system is assumed to be a communication channel.
When the channel has a flat magnitude and a linear phase (or, equivalently, a constant group delay) over the bandwidth of the transmitted signal, the received signal is a delayed version of the transmitted signal. In this special case, the delay can be described by a single value. In general however, each frequency component is delayed by a different amount and this relation is described by the group delay function in \equl{groupDelay}.

As discussed, roughly, the group delay is the time between the channel's initial response and its peak response or, in other words, the group delay of a filter is a measure of the average delay of the filter as a function of frequency. Hence, for an arbitrary (non-constant) group delay, distinct frequency components of the transmitted signal get delayed by different delays, the signal gets distorted and there is not a single delay value to characterize the transmission. In practical communication systems, an estimator keeps tracking at the receiver of the best time instant to compensate the delay provoked by the channel. And in simulations, it is common to resort to a brute force approach, where this ``optimum'' delay compensation instant is searched by trying distinct values in a range and choosing the one that minimizes a figure of merit such as the bit error rate.

When the channel simulation requires filtering to shape the magnitude of the signal spectra but allows linear phase, it is convenient to use FIR filters with linear phase because the delay they impose is well-defined as indicated in \figl{groupDelayLinearPhase}.
Hence, in many cases the LTI channel is assumed to be causal and represented by a finite-length impulse response $h[n]=h_0 \delta[n] + h_1 \delta[n-1] + \ldots + h_L \delta[n-L]$ with $L+1$ non-zero values. The integer $L$ is the channel \emph{order} because in the Z-domain, $H(z)=h_0 + h_1  z^{-1} + \ldots + h_L  z^{-L}$.
Assuming a linear phase FIR of $L$-th order, where $L$ is even (or equivalently, the filter has an odd number of coefficients), the delay is simply $L/2$.

Recall that, for FIR filters, the impulse response coincides with the filter coefficients. Adopting vectors, the impulse response can be represented by \ci{h=[h0 h1 ... hL]} and the command \ci{y=filter(h,1,x)} used because the numerator $B(z)$ and denominator $A(z)$ of $H(z)=B(z)/A(z)$ are \ci{h} and $1$, respectively.
%For example, \figl{groupDelayLinearPhase} illustrate the input and output signals corresponding to the commands
%\begin{lstlisting}
%h=[0.3 -0.4 0.5 0.8 0.5 -0.4 0.3];
%x=[0 0 0 1 0 0 0 0 0 0 0];
%y=filter(h,1,x);
%\end{lstlisting}


\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/compensatingFIRChannel}
\caption[{Linear phase FIR channel obtained with \ci{h=fir1(10,0.8)}.}]{Linear phase FIR channel obtained with \ci{h=fir1(10,0.8)}. The output signal was scaled to have the same power as the input.\label{fig:compensatingFIRChannel}}
\end{figure}

For example, consider the linear phase channel obtained with \ci{h=fir1(10,0.8)}.
\figl{compensatingFIRChannel} depicts its impulse response, group delay and the output $y[n]$ for an input $x[n]$ representing symbols $+1,-1,+1,+1,-1,-1$ with an oversampling of $L=4$ samples per symbol and a shaping pulse with four samples equal to 1.

\figl{compensatingFIRChannel} allows to observe that the input gets delayed by the group delay of \ci{h}, which in this case is 5. Note that the input peak at \ci{x[0]} corresponds to the output sample \ci{y[5]}. This simple signal delay happens because because \ci{h} in this example was chosen to be symmetric and have a linear phase, such that the group delay is 5 samples for all frequencies as indicated in \figl{compensatingFIRChannel}. In terms of seconds, a group delay of 5 samples corresponds to $5 \ts = 5/\fs$, where $\fs$ is given in Hz.
In this case, the first 5 samples of the output should be eliminated for aligning with $x[n]$. For example, the commands \ci{y(1:5)=[], x(end-4:end)=[]} could be used to align the corresponding vectors and keep them with the same number of samples.

However, in a more general scenario, the group delay varies with frequency, such as for the non-symmetric FIR filter illustrated in \figl{groupDelayNonLinearPhase}.

%\subsection{Power attenuation caused by channels}
Typically a channel attenuates the input signal and decreases its power.
When the channel is LTI, the output power $P_y$ is related to the input power $P_x$ as informed by Theorem~\ref{th:output-power-of-lti} (page \pageref{th:output-power-of-lti}).

When comparing the input $x[n]$ and output $y[n]$ waveforms, it is useful to know the ratio $g = P_y / P_x$ and multiply $x[n]$ by $\sqrt{g}$ in order to compare signals with equivalent amplitude ranges. 
However, in practice, it is hard to determine $g$ and typically an adaptive automatic gain control (AGC) is employed. 

Another example of compensating the delay and gain imposed by a channel is discussed in the sequel.

%\subsection{Example of compensating the delay and attenuation}

If useful, one can mimic a perfect automatic gain control (AGC) and force the output to have the same power as the input with the following commands \ci{Px=mean(x.\^{}2), Py=mean(y.\^{}2), y = sqrt(Px/Py)*y}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\figwidth,keepaspectratio]{Figures/compensatingButterworthChannel}
\caption[{IIR channel obtained with \ci{[B,A]=butter(5,0.8)}.}]{IIR channel obtained with \ci{[B,A]=butter(5,0.8)}. The output signal was scaled to have the same power as the input.\label{fig:compensatingButterworthChannel}}
\end{figure}

Similar to the procedure for generating \figl{compensatingFIRChannel}, an IIR filter was simulated to obtain 
\figl{compensatingButterworthChannel}. In this case the group delay is less than 1 for frequencies $\dw < 0.47 \pi$ and the peak of $h[n]$ is for $n=1$. Observing the graphs of $x[n]$ and $y[n]$, a good alignment would be obtained by shifting $y[n]$ one sample to the left, i.\,e., $y[n+1]$ if manipulating algebraically or \ci{y(1)=[]} if using {\matlab}.
\eApplication


%AK-IMPROVE: An application showing gain x BW for AMPOPs
%6) Explain the discussion below:
%"For example, a unity-gain 20-kHz 5-pole, 3-dB ripple Chebyshev MFB filter with a 2nd pole-pair fn of 19.35 kHz and a Q of 8.82 needs an op amp with unity gain bandwidth of at least 17 MHz. On the other hand, a 5-pole Butterworth MFB filter, with a worst case Q of 1.62 needs only a 3.2-MHz op amp. The same 5-pole Butterworth filter implemented with a Sallen-Key topology would require a 8.5-MHz op amp in the high-Q section."
%7) Explain the Op Amp Slew Rate.

\section{Comments and Further Reading}

%From Matlab:
%[2] Kollar, I., G.F. Franklin, and R. Pintelon,
%"On the Equivalence of z-domain and s-domain Models in System Identification," Proceedings of the IEEE Instrumentation and Measurement Technology Conference, Brussels, Belgium, June, 1996, Vol. 1, pp. 14-19.S

We have explained that an impulse response $h(t)$ or $h[n]$ is capable of fully representing the behavior of a LTI system because this is the case in current engineering problems. But as discussed in \cite{Vogel12} and references therein, it is possible to define abstract examples where the LTI cannot be described by an impulse response.
%AK-IMPROVE - an experiment like the ones in \cite{Potrebic10} with impulse response

The physical dimension, unit and interpretation of the impulse response are discussed in \cite{Glover06,Potrebic10}.

The group delay can assume negative values and, more than that, the channel output signal may look like as anticipated with respect to the input signal, as if the group delay was ``negative''. This issue is discussed online, e.\,g. \akurl{http://www.dsprelated.com/showarticle/54.php}{3grp}, and also in \cite{Mitchell97b}, where an interesting example is provided.

It should be noted that the powerful Matlab function \ci{yulewalk} can be used to find an IIR filter that
approximates an arbitrary magnitude response. 


\ignore{
Efeitos especiais para guitarra estao bem explicados em:
No livro
Real-time Digital Signal Processing from MATLAB to C with the ... - Google Books Result
by Thad B. Welch, Cameron H. G. Wright, Michael G ... - 2006 - Technology \& Engineering - 363 pages
}

\ignore{
Regarding the DFT use: the third option is to have $\alpha=1$ and $\beta=1/N$. This is the option used by \matlab and allows to directly (without normalization) use the DFT (\ci{fft} command) to calculate the frequency response of a causal 	channel $H(z)$, because the FFT value \[H_k = H(e^{jw}) |_{w = \frac{2 \pi }{N} k},\]
	where $H(e^{jw}) = H(z)|_{z=e^{jw}}$.
}

Some useful links about analog filter design can be found at 
\akurl{http://focus.ti.com/docs/toolsw/folders/print/filterpro.html}{3fid}.
%\ url{http://www.circuitsage.com/filter.html} and
%\ url{http://www.linear.com/designtools/software/filtercad.jsp}.

%More information about chips with filtering functionality can be found at
%\ url{http://www.maxim-ic.com/appnotes.cfm/an_pk/1795/} and
%\ url{http://para.maxim-ic.com/en/results.mvp?fam=filt}, where one can find switched-capacitor and continuous filters up to order 8.
%A discussion about switched-capacitor circuits in active filters is provided at~\ url{http://technologyinterface.nmsu.edu/3_3/3_3g.html}


%Esse livro eh bom:
%Analog filters
% By Kendall L. Su, Kendall Ling-chiao Su

%Design and analysis of analog filters: a signal processing perspective
%By L. D. Paarmann
%\ url{http://www.google.com/books?id=l7oC-LJwyegC&dq=analog+filters+implementation&printsec=frontcover&source=in#PPA161,M1}
%frequency scaling


%Analog and digital filter design
% By Steve Winder
%Tem cap. 8 - impedance matching networks, mas nao achei muito bom o livro. O autor eh %meio confuso.

There are several topologies for analog filters. For example, at \akurl{http://www.maxim-ic.com/app-notes/index.mvp/id/1762}{3fto} one can find a discussion about a better topology than Sallen-Key.

%The surface acoustic wave (SAW) filters are important in telecommunications (\ url{http://en.wikipedia.org/wiki/Electronic_filter}). They are useful for filtering signals at the intermediate frequency (IF) around $70$ MHz, for example.

%The method minimizes the error between the desired magnitude represented by a vector D and the magnitude of the IIR filter H(ejw) in the least-squares sense.

\ignore{
impulse response, and so forth were represented with 64 bits; therefore, these
numbers have a range approximately between $10^{-308}$ and $10^{308}$ and a precision
of $\tilde 2^{-52} = 2.22 10^{-6}$. Obviously this range is so large and the precision with
which the numbers are expressed is so small that the numbers can be assumed to
have
}

